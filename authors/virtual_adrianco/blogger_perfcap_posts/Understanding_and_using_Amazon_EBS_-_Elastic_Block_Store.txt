Title: Understanding and using Amazon EBS - Elastic Block Store
URL: https://perfcap.blogspot.com/2011/03/understanding-and-using-amazon-ebs.html

There has been a lot of discussion in the last few days about EBS <a href="http://blog.reddit.com/2011/03/why-reddit-was-down-for-6-of-last-24.html">since it was implicated in a long outage at reddit.com</a>.<br /><br /><span style="font-weight:bold;">Rule of Thumb</span><br /><br />The benchmarking Netflix did when we started on AWS highlighted some inconsistent behavior in EBS. The conclusion we reached is a rule of thumb for EBS - If you sustain less than 100 iops (input+output per second) long term average it works fine. Short term bursts can be 1000 iops. By short term I mean less than a minute, long term more than 10 minutes. YMMV.<br /><br />If you are doing benchmarks like this, collect response time and throughput and plot your data over time. You need to run long enough that the performance shows steady state behavior. The problem with EBS is that it doesn't have a particularly steady state. To explain why we need to look at the underlying architecture. I don't know the details of how EBS is implemented, but there is enough information available to explain how it behaves.<br /><br /><span style="font-weight:bold;">EC2</span><br /><br />The AWS EC2 architecture is built out of commodity low cost servers, they have a single 1 Gbit network, a few CPUs, a few disks and a few GBytes of RAM. Over time the models have changed, and EC2 does have a 10Gbit network option now, but for the purposes of this discussion, we will concentrate on the 1Gbit network models. Individual servers are virtualized into the familiar EC2 models by slicing up the RAM, CPUs and disk space, and sharing the network bandwidth and disk iops. When EC2 instances break or are de-configured any data on the internal disks is lost.<br /><br /><span style="font-weight:bold;">Elastic Block Store</span> <a href="http://aws.amazon.com/ebs/">http://aws.amazon.com/ebs/</a><br /><br />The AWS EBS service provides a reliable place to store data that doesn't go away when EC2 instances are dropped, but it provides the same mounted filesystem capability as the internal disks. If you need more disk space or iops you can mount more EBS volumes on a single EC2 instance and spread out the load. The EBS volume is connected to the EC2 instance over the same 1Gbit network as everything else. In a datacenter this would normally be built using commercially available high end storage from NetApp, EMC or whoever, it would be quite expensive (cost much more than the EC2 instance itself) and be fast and reliable up to the limits of the network. To build a low cost cloud, the alternative is to use RAIN (Redundant Array of Inexpensive Nodes) which could be based on standard EC2 instances, or variants that have more disks per CPU. Software is then used to coordinate the RAIN systems and provide an EBS service that will be slower than high end storage, but still be very reliable and be limited by the 1Gbit network.<br /><br /><span style="font-weight:bold;">S3 and Availability Zones</span><br /><br />AWS also has an S3 storage service that behaves like a key/value store accessed via http requests and a REST API rather than a directly mounted filesystem. It is possible to rapidly snapshot an EBS volume to and from S3, including incremental backups and restores that fill as they go so you don't have to wait before using them. This implies to me that they share a common back-end infrastructure to some extent. The primary additional difference is that EBS volumes only exist in a single AWS Availability Zone, and S3 data is replicated across two or three Availability Zones. It takes longer to replicate the data for S3, so it is slower, but it is very robust and it is almost impossible to lose data. You can think of an Availability Zone as a complete datacenter. All the zones in a region are separate datacenters that are close enough together to support a high bandwidth and low latency network between them, but they have separate power sources and connections to the Internet.<br /><br /><span style="font-weight:bold;">Multi-Tenancy</span><br /><br />The most efficient chunk of compute and storage resource to buy and deploy when building a cloud is either too big or too small for the actual use cases of real applications. Virtualization is used to sub-divide the chunks, but then each individual machine is supporting several independent tenants. For local disks, the space is divided between the tenants, and for network, everyone is sharing the same 1Gbit interface. This works well on average, because most use cases aren't network or disk bound, but you cannot control who you are sharing with and some of the time you will be impacted by the other tenants, increasing variance within each EC2 instance. You can minimize the variance by running on the biggest instance type, e.g. m1.xlarge, or m2.4xlarge. In this case there isn't room for another big tenant, so you get as much as possible of the disk space and network bandwidth to yourself. The virtualization layer reserves some of the capacity. It's possible to tell that another tenant is keeping the CPU busy by looking at the "stolen time", but there are no metrics for stolen iops or network bandwidth.<br /><br />The EBS service is also multi-tenant. Many clients mount disk space from a common backend pool of EBS disks. You don't get to see how the disk space is allocated, or how data is replicated over more than one disk or instance for durability, but it is limited to that availability zone. A busy client can slow down other clients that share the same EBS service resources. EBS volumes are between 1GB and 1TB in size. If you allocate a 1TB volume, you reduce the amount of multi-tenant sharing that is going on for the resources you use, and you get more consistent performance. Netflix uses this technique, our high traffic EBS volumes are mostly 1TB, although we don't need that much space.<br /><br />This is actually no different in principle to large shared storage area network (SAN) backends (from companies like EMC or NetApp) that are in common datacenter use. Those also have unpredictable performance when pushed hard, and they mask this issue with lots of battery backed memory. The difference is cost. EBS is 10c per Gbyte per month. If you build a competing public cloud service using high end storage, you could get better performance but your cost base would be far higher.<br /><br /><span style="font-weight:bold;">Visualizing Multi-Tenant Disk Access</span><br /><br />I have come up with some diagrams to help show what happens. I'm basing them on a simplified view of AWS where the only instance type family is m1 and everything they have is made out of one underlying building block. This consists of a fairly old specification system, 8 cores, 16GB RAM, four 500GB disks and a single 1Gbit network. In reality, AWS is much more complex than this, but the principles are the same.<br /><br />Starting with internal disks, this is what an m1.xlarge looks like, it takes up the whole system apart from a small amount of memory, disk space and network traffic for the VM and AWS configuration/management information. You can expect to have minimal multi-tenant contention for network or disk access.<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXsbnHDP9fYeYb6KVc4ZZ2eVR6rCNVxSxtspAraH2sIPmrL76yQ9ab1YJGb9Gl1-dAU2VAPTOOWYRJeIvWcvMoUwDYw3fXpdSdh7ZS_4CqK2H1cACT95ieCmq2ABZmwKEYXwqv/s1600/Slide1.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXsbnHDP9fYeYb6KVc4ZZ2eVR6rCNVxSxtspAraH2sIPmrL76yQ9ab1YJGb9Gl1-dAU2VAPTOOWYRJeIvWcvMoUwDYw3fXpdSdh7ZS_4CqK2H1cACT95ieCmq2ABZmwKEYXwqv/s320/Slide1.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904272363288306" /></a><br /><br />The m1.large instance type halves the system, each instance has two disks rather than four, so it shares the network and some of the disk controller bandwidth, but it should have minimal iops contention with the other tenant.<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzstDMnIhK9Z62HppBiUUwr3XA6itj1Im8Ka4OYJ0S-3M5QtTbSYf-yD4iSYbyFm2MOPgatw9t1R8726_dIghMAVoDhyJGlVTNhhhT8NlLfgyWEvRTssbeJfzFErMyeYOx99aZ/s1600/Slide2.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzstDMnIhK9Z62HppBiUUwr3XA6itj1Im8Ka4OYJ0S-3M5QtTbSYf-yD4iSYbyFm2MOPgatw9t1R8726_dIghMAVoDhyJGlVTNhhhT8NlLfgyWEvRTssbeJfzFErMyeYOx99aZ/s320/Slide2.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904141661806594" /></a><br /><br />The low cost m1.small instance type has 160GB of disk per instance, so we can fit three per disk for a total of 12 instances per machine. (Note that the memory for a real m1.small is 1.7GB, so only 9 would fit in 16GB RAM, however the c1.medium instance has 1.7GB, 350GB disk, and more CPU, so six m1.small and three c1.medium fits). You can see the multi-tenancy problem here, any of the instances could generate enough traffic to fill the network and make one of the disks busy, and that is going to affect other instances in an unpredictable and random manner.<br /><br />Here's an analogy, you can rent a whole house, rent a room in a house, or rent a couch to sleep on, you get what you pay for.<br /><br />If you ever see public benchmarks of AWS that only use m1.small, they are useless, it shows that the people running the benchmark either didn't know what they were doing or are deliberately trying to make some other system look better. You cannot expect to get consistent measurements of a system that has a very high probability of multi-tenant interference.<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkjhnXyNNJ1BfQZh81gw22dJk3dR27JVN-GxSfv5Kau4CISRvnzBeQcam-YnWAeZkNKSFmCbBslhsLJND72cBogC4B6kX87y80-indOFwKGZUxDNwoftqlgQb3UWzZVa2dVFtn/s1600/Slide3.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkjhnXyNNJ1BfQZh81gw22dJk3dR27JVN-GxSfv5Kau4CISRvnzBeQcam-YnWAeZkNKSFmCbBslhsLJND72cBogC4B6kX87y80-indOFwKGZUxDNwoftqlgQb3UWzZVa2dVFtn/s320/Slide3.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904139472765474" /></a><br /><br /><span style="font-weight:bold;">EBS Multi-Tenancy</span><br /><br />The next few diagrams show the flow of traffic from an instance to the EBS service, which makes two copies of the data on disks connected to separate instances. I don't know if this is how EBS works, but if we wanted to build an EBS-like system using the same building block it could look like this. In practice it would make sense to have specialized back-end building blocks with much more disk space.<br /><br />The first diagram shows how Netflix runs EBS, we start with an instance that has the maximum network bandwidth with no other tenants, we allocate maximum size 1TB volumes (we stripe many of them together) and the service has to use most of the disk space in the back-end to support us, so we have less chance of another tenant making the EBS disks busy. The performance of EBS in this simplified case would be higher latency than local disk, but otherwise similar. I suspect that in reality the EBS volume is spread over more disks in the backend which gives higher throughput but with higher variance.<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgXtg_x53emLqE72JvWa8MEpHUw66x8Tx_lTFNo8B70MxjEa_sisc0ErzbNOIs_TashZtePyXgaDHT5IcfUx2wOaOStGqzjgl8GcC_a1a4JaqSKxLG-LGHgEtJcL2Hx40wGi3H/s1600/Slide4.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgXtg_x53emLqE72JvWa8MEpHUw66x8Tx_lTFNo8B70MxjEa_sisc0ErzbNOIs_TashZtePyXgaDHT5IcfUx2wOaOStGqzjgl8GcC_a1a4JaqSKxLG-LGHgEtJcL2Hx40wGi3H/s320/Slide4.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904134814057154" /></a><br /><br />If we drop down to a more typical m1.large configuration with 100GB of EBS each, two instances are sharing network bandwidth, the EBS service is servicing two sets of requests, and the EBS back end has many more tenants per disk, so we would expect better peak performance than the two internal disks in the m1.large but more variance.<br /> <br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfxGOEXR92LzJgrZPVSy6PT7D4Rja5KhCDkWz_OZ09lP9hsaroaMYbfU7d7VXQGXEw7JUZ6LmDMwO3SHZAGHa9oBYcAaE6hWyXetbtUUd4le67miqojWQHbA08hY9reGjcHh-R/s1600/Slide5.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfxGOEXR92LzJgrZPVSy6PT7D4Rja5KhCDkWz_OZ09lP9hsaroaMYbfU7d7VXQGXEw7JUZ6LmDMwO3SHZAGHa9oBYcAaE6hWyXetbtUUd4le67miqojWQHbA08hY9reGjcHh-R/s320/Slide5.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904137434583618" /></a><br /><br />For the case where we have many m1.small instances each accessing a 10GB EBS volume, it is clear that the peak performance is going to be far better than a share of a local disk, but the contention for network, EBS service and backend disks will be extremely variable, so performance will be very inconsistent.<br /><br /><a onblur="try {parent.deselectBloggerImageGracefully();} catch(e) {}" href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkgMGgvbUBecBiXDitmU1mca2mECRB6FH3YiAyDZfN6xEKgm8eBeK8_Vn5v0-kKbxnGbpJBxqgN14DVH36G51nspNpFE3E5aqQSoesAlEkutMSOp21nY2fu93sgJxWAQsEGvwZ/s1600/Slide6.jpg"><img style="cursor:pointer; cursor:hand;width: 480px; height: 360px;" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkgMGgvbUBecBiXDitmU1mca2mECRB6FH3YiAyDZfN6xEKgm8eBeK8_Vn5v0-kKbxnGbpJBxqgN14DVH36G51nspNpFE3E5aqQSoesAlEkutMSOp21nY2fu93sgJxWAQsEGvwZ/s320/Slide6.jpg" border="0" alt=""id="BLOGGER_PHOTO_ID_5585904130808746306" /></a><br /><br /><span style="font-weight:bold;">How To Measure Disk and Network Performance</span><br /><br />Someone should write a book on that (I already did, but for Solaris), however <a href="https://forums.aws.amazon.com/thread.jspa?messageID=124044#124044">there is a useful AWS forum post</a> that explains how to interpret Linux iostat. This blog post is too long already, so Linux iostat will have to wait for another time.<br /><br /><span style="font-weight:bold;">Best Practices for Cloud Storage with Cassandra</span><br /><br />There are two basic patterns for Cassandra, one is a persistent memory cache, where we size the data to fit in memory so that all reads are fast, and writes go to disk. The m2.4xl instance type with 68GB RAM and two 850GB disks is best. The second pattern is where there is a much larger data set than memory, and m1.xlarge with 16GB RAM and four 420GB disks will have the best iops for reads, and a much lower overall cost per GB for storage. In both cases, we get all the network bandwidth for servicing clients and the inter-node replication traffic, and minimal multi-tenant variance.