{
  "metadata": {
    "author": "virtual_adrianco",
    "version": "1.0",
    "last_updated": "2025-06-01T11:26:58.016051+00:00",
    "content_count": 611,
    "content_types": {
      "youtube": 135,
      "podcast": 52,
      "infoqvideo": 2,
      "book": 10,
      "story": 18,
      "file": 394
    },
    "processing_stats": {
      "total_items": 611,
      "processed_items": 611,
      "failed_items": 0,
      "processing_time": 0.259007
    }
  },
  "content": [
    {
      "id": "virtual_adrianco_youtube_0",
      "kind": "youtube",
      "subkind": "transcript",
      "title": "From Netflix to the Cloud: DevOps, Microservices and Sustainability",
      "source": "Platform Engineering Podcast",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=FY3asCV9qOE",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1",
      "kind": "podcast",
      "subkind": "",
      "title": "Chat about open source and platforms",
      "source": "Kubernetes for Humans",
      "published_date": "3/27/2024",
      "url": "https://komodor.com/resources/022-kubernetes-for-humans-with-adrian-cockcroft-nubank/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_2",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia's Superchips for AI: Talk about GH200 and flip.ai",
      "source": "The New Stack Analysts with Alex Williams and Sunil Mallya",
      "published_date": "3/14/2024",
      "url": "https://thenewstack.io/nvidias-superchips-for-ai-radical-but-a-work-in-progress/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_3",
      "kind": "podcast",
      "subkind": "",
      "title": "Monitoring for AI",
      "source": "The New Stack Analysts with Alex Williams",
      "published_date": "3/28/2024",
      "url": "https://thenewstack.io/the-new-monitoring-for-services-that-feed-from-llms/",
      "content": {},
      "tags": [
        "ai",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4",
      "kind": "podcast",
      "subkind": "",
      "title": "From Code to Climate",
      "source": "Unlearn Podcast with Barry O'Reilly",
      "published_date": "1/31/2024",
      "url": "https://barryoreilly.com/explore/podcast/sustainable-future-for-technology/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_5",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices and Teraservices",
      "source": "Infoq with Wesley Reisz",
      "published_date": "",
      "url": "https://www.infoq.com/podcasts/adrian-cockcroft/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_6",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_7",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft-ep-24/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "John Myer",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_9",
      "kind": "podcast",
      "subkind": "",
      "title": "Netflix on AWS",
      "source": "CloudFix",
      "published_date": "",
      "url": "https://cloudfix.com/podcast/when-netflix-bet-on-aws/",
      "content": {},
      "tags": [
        "aws",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_10",
      "kind": "podcast",
      "subkind": "",
      "title": "Serverless and continuous resilience",
      "source": "Charles Humble/Container Solutions",
      "published_date": "",
      "url": "https://blog.container-solutions.com/adrian-cockcroft-on-serverless-continuous-resilience",
      "content": {},
      "tags": [
        "resilience",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_11",
      "kind": "podcast",
      "subkind": "",
      "title": "Future Architectures",
      "source": "HPC Podcastmwith Shahin/OrionX",
      "published_date": "11/1/2023",
      "url": "https://orionx.net/2023/11/hpcpodcast-77-adrian-cockcroft-on-future-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_12",
      "kind": "podcast",
      "subkind": "",
      "title": "ESG and HPC",
      "source": "HPC Podcast with Shahin/OrionX",
      "published_date": "5/1/2023",
      "url": "https://orionx.net/2023/05/hpcpodcast-57-decarbonization-renewable-energy-esg-w-adrian-cockcroft/",
      "content": {},
      "tags": [
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_13",
      "kind": "podcast",
      "subkind": "",
      "title": "Evolution of Microservices",
      "source": "ACM Bytecast",
      "published_date": "6/17/2016",
      "url": "https://learning.acm.org/techtalks/microservices",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_14",
      "kind": "podcast",
      "subkind": "",
      "title": "Scheduling",
      "source": "Software Engineering Daily",
      "published_date": "7/6/2016",
      "url": "https://softwareengineeringdaily.com/2016/07/06/schedulers-with-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_15",
      "kind": "podcast",
      "subkind": "",
      "title": "Architecting for cloud",
      "source": "Infoq interview with Michael Floyd",
      "published_date": "1/24/2013",
      "url": "https://www.infoq.com/interviews/Adrian-Cockcroft-Netflix/",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_16",
      "kind": "podcast",
      "subkind": "",
      "title": "Interview, early life etc",
      "source": "Sam Newman's Magpie Talkshow",
      "published_date": "10/8/2016",
      "url": "https://samnewman.io/blog/2016/10/08/magpie-talkshow-episode-22-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_17",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices",
      "source": "SE Radio with Stefan Tilkov",
      "published_date": "12/10/2014",
      "url": "https://se-radio.net/2014/12/episode-216-adrian-cockcroft-on-the-modern-cloud-based-platform/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_18",
      "kind": "podcast",
      "subkind": "",
      "title": "Cloud and HPC",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/09/hpcpodcast-aws-sun-ebay-netflix-and-others-vet-adrian-cockcroft-talks-cloud-hpc-ai-and-the-amazon-sustainability-data-initiative/",
      "content": {},
      "tags": [
        "cloud",
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_19",
      "kind": "podcast",
      "subkind": "",
      "title": "SC23 summary",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/hpcpodcast-tech-analyst-adrian-cockcroft-on-trends-driving-future-hpc-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_20",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn't learn from Netflix",
      "source": "Infoq - QCon London talk/video integrated together",
      "published_date": "3/28/2023",
      "url": "https://www.infoq.com/presentations/microservices-netflix-industry/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_book_21",
      "kind": "book",
      "subkind": "",
      "title": "Q&A: Serverless Development on AWS - sustainability expert section",
      "source": "O'Reilly - Sheen Brisals and Luke Hedger",
      "published_date": "3/14/2024",
      "url": "",
      "content": {},
      "tags": [
        "aws",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_book_22",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Building Green Software",
      "source": "O'Reilly - Anne Curry",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_23",
      "kind": "book",
      "subkind": "",
      "title": "Foreword1: The Value Flywheel Effect",
      "source": "IT Revolution - David Anderson",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_24",
      "kind": "book",
      "subkind": "",
      "title": "Foreword2: Reaching Cloud Velocity",
      "source": "AWS: Jonathan Allen, Thomas Blood",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_book_25",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: In Search of Certainty 2nd Edition",
      "source": "Mark Burgess",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_26",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Irresistable APIs",
      "source": "Kirsten Hunter",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_27",
      "kind": "book",
      "subkind": "1,21-187,215-233",
      "title": "Capacity Planning for Internet Services",
      "source": "Sun Press Blueprints - adrianco, Walker",
      "published_date": "2000",
      "url": "./mcp_resources/virtual_adrianco/pdfs/caphi_extracted_1_21-187_215-233.pdf",
      "content": {
        "metadata": {
          "word_count": 48047,
          "processing_status": "success",
          "processing_errors": [],
          "character_count": 320294,
          "excerpt": "Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131CapacityPlanningfor\n\nAlmost every business, from a corner shop to a multinational corporation, is faced\n\nwith competitive pressure to \u201cgo online\u201d and provide services via an Internet site. Inaddition, a large number of new online businesses are being implemented in a maddash to capture the attention and wallets of a huge and fast-growing number ofInternet users. Success is measured by growth in the number of pages viewed,registered users, ..."
        },
        "summary": "Quick planning techniques for high growth rates. Success is measured by growth in the number of pages viewed,registered users, and in some cases, by the amount of business transacted. Growth rates for successful si. nies spend most of their investors\u2019 funds on advertising as they attempt to establish their name in the collective consciousness of consumers. Established companies are concerned about maintaining theirpreexisting brand image. Capacity planning is an optimization process. Service level requirements can be predicted and balanced against their costs. This Sun BluePrints book charts a course through the available techniques and tools. The topics covered in this book can be divided into the following sections:Principles and processes. Scenario planning techniques. The effective use of tools. This chapter presents methods for managing performance and establishing servicelevel agreements. It also examines the IT frameworks designed to providesolutions for business requirements. Additional ITframeworks from ISO FCAPS are presented, and tips are offered for implementation. performance management is the measurement, analysis, and optimization ofcomputer resources to provide an agreed-upon level of service to the end-user. Bydefining performance management and identifying the key components required tosafely and accurately implement performance management in the datacenter, youcan minimize the risks associated with high growth rates. Resourceutilization and resource utilization planning are the cornerstones of capacityplanning. Utilization is a measure of system resource impact, throughput defines thequantity of services, and latency defines the quality of the services being provided. Several layers of resources and resource consumption can be defined, tuned, and measured. The Business layer often provides the most significant opportunities for \u201ctuning\u2019 The Applicationlayer and Hardware layer can also provide a significant and obvious impact on the overall performance of the architecture. The Operating System and Infrastructure layer are often where administrators look for some magic cure. Performance management can be applied in an iterative, cyclic, four-phase approach. The output of each phase is used as input to the following phase. Each phase mustreach a steady state in which the results can be published and transferred to thefollowing phase. Once the four phases are locked in place and all results have beenpublished, those results are fed into the next generation of phases. Historical revisions should be retained for future examination and change analysis. Baselining creates a snapshot of a system as it currently exists and generates reports that describe the system performance. An accurate representation including hardware, software, and operating systemversions is critical to creating an accurate inventory. This configuration inventory isconsidered \u201clocked down\u201d for the life span of the baselining process. Load planning accounts for changes or new demands on the system. Any available data describing the changes and new demands is collected and analyzed. The old snapshot isretained as a historical record of configuration. Load Planning is a cyclical process that comprises three activities: load assessment, load data gathering, and load profiling. Each of these activities may need to berepeated multiple times before valid assumptions can be made about the datacollected. System capacity and resource consumption are modelled, calibrated, and validated against the current baseline data. The model compares the relative resourceconsumptions (measured during baselining activities) measured during the load-gathering phase. Capacity planning produces a new system and service configuration architecture. The relationship betweenresource consumption and system architecture capabilities creates the workloadcharacterization used to model and predict the effect of changes. The operating environment can be adjusted as necessary to meet the established performance requirements defined in the SLAand KPI. Resource management defines the controls, shares, quotas, and priorities necessary to manage the workload. System resource planning also provides additional input for creating the new baseline of systemperformance. Service Level Agreements (SLA) outline the level of availability of mission-critical services between the IT department and end-users. f load planning and capacity planning to improve model accuracy. The SLA is really just a list of expectations for a given service. It defines the transactional capabilities, networklatency projections, availability requirements, maintenance windows, escalationprocedures. ld take in theanalysis of the end-users\u2019 computing resource requirements, capabilities of the ITarchitecture, and formulation of the agreement. SLAs should be used as a basis of understanding between all parties involved. They should define key roles,responsibilities, and expectations that have been discussed and agreed upon by all parties. The SLA should be reviewed and modified regularly. Service Level Agreements (SLAs) can be difficult to work out. They can result in many unpleasantries, not the least of which are the perception of poor service. SLAs can improve the business process, affect the bottom line, andimprove availability. Long-term benefits of SLAs include increased end-user satisfaction, more accurate resource accounting, and more efficientprioritization. SLAs are not more widely used for a number of reasons. The following issues are the most common downfalls. A well-conveyed SLA can help develop higher serviceavailability and greater end-user satisfaction. With the SLA, anIT manager can assign personnel to requirements more effectively. preferable control over IT budgets and increased accuracy inlong-term budgeting can also be direct benefits of SLAs. Production Environment Engineering is the product of careful planning and design. This section addresses the needs of the datacenter production environment by using the ISO FCAPS model, the ITIL (information technology infrastructure library)framework, and the SunReady roadmap to production. This discussion provides an encompassing view of the infrastructure requirements and timelines necessary to successfully launch, manage, and maintain a new datacenter service or refresh an existing one. The dot-com startup Internet business has a related challenge. While their businessmodel relies on exponential growth for survival, they often lack the datacenter policies, procedures, andexperience. By learning from the cumulative experience of their more traditional predecessors, the Dot-com companies can implement best practices for the production environment. Some of the categories defined in the framework are company-wide, some are system-specific, and others are application-specific. Forexample, the security group could manage security policy and administration for theentire company, while a facilities management resource could be designated toseveral disparate datacenters. The FCAPS High-Level IT Framework. re defined five categories of.managed objects, covering the basic categories of enterprise support operations. These categories can be applied to network management systems, which is theintended purpose, or they can be expanded to include the skills, tools, and.capabilities in datacenter operations and management. An ITEF can be further divided to establish functional responsibilities and toprovide more detailed information about the services the ITEF offers. Several levels of criticality can be assigned to thecategories of IT services. FCAPS IT Extended Framework defines an ITEF based on FCAPS. This ITEF is bestrepresented in a tree structure that is viewable as a series of Web pages. The IT service coverage document should include a general statement describing the category of coverage. It should also include statements describing the individual servicesand areas of authority in the IT environment. The servi The change management system is the vehicle for recording ,authorizing ,monitoring , andreporting all changes to the open systems environment. By establishingcategories of coverage, the services provided by the IT department are well-defined, whichallows the establishment of enforcement and auditing policies. The IT service model (ITSM) introduces levels of criticality, levels of service, and \u201cbig rules\u201d to the ITEF. This multilevel approach extends the natural drill-downstructure for IT services. The ITSM can be implemented across platforms and used to create service leveldescriptions. The following is a sample of a service level description for user data backup. It should be modified to adhere to local datacenter policies. The Central Computing and Telecommunication Agency (CCTA) of the UK began publishing volumes of their established best practices for IT operations in 1978. The ITIL has evolved over the years, being studied, adopted, and adapted by many IT organizations. The ITIL establishes policies, not implementations. The ITEF can then beimplemented according to service levels, operating system, hardware, andapplication capabilities. The FCAPS framework was originally generated from the ITIL. The Sun Professional Services team has developed a project methodology for applying the FCAPSframework. This methodology creates a unified extended framework for IT operations. The implementation of supplemental service capabilities shadows the applicationdevelopment life cycle from the architecture phase to the pilot, development,integration, deployment, and sustaining phases. The IT service capabilities can thenbe tested and assessed simultaneously with the application\u2019s functional, stress,crash, and disaster recovery test cycles. Service level management is the key to maintaining the production environment. By establishing the rules of engagement, the capabilities necessary to support the service level objectives, the entire production infrastructure can be monitored and measured. The service desk controls information, brokers requests, audits request completion, andprovides service level reporting services. The service desk also brokers action items for the support organizations, bothinternal and external. With the dispatch of support activities controlled, theaffected end-user service levels can be monitored. Measurement Principles: Anything that is not measured is out of control. Measure the business as well as the computer systems and people. Measure and record trends in performance, cost, and availability. A system should be built to help the IT staff log arbitrary data into performancedatabases. These databases should have summariesand graph plotting that anyone can use. Parameterize each component separately and combine them into anavailability hierarchy. Production environment engineering and performance management may appear tobe obscure forms of art. By studying the collective experiences of the traditionaldatacenter community and applying proven best practices, the art can be converted into a well-understood science. The next chapter provides a methodology for service level management and looks at some problems of capacity estimation across multiple platforms. The next chapter looks at what actions are necessary to keep the modern open-system datacenter well-organized and well-managed as the best of the traditional mainframepredecessors. Measuring and reporting key performance criteria in an applicationdevelopment environment can help set release time expectations for performanceand capacity. These practices can also help you recognize features and identifychanges of performance implications of the application being developed. Use a strict change control process, and schedule batches of changes to manage risk. Arrange for the appropriate staff to be on site during change periods, to concentrate fire-fighting efforts. Service level management is a synthesis of traditional datacentermonitoring and the end-to-end availability of the service. The goal is to eventually be able to measure service performance from an end-user's perspective. Service Level Agreements (SLAs) are negotiated between consumers and providers of ITservices. There could be multiple SLAs between a single business unit and multipleservice providers. Some providers can also have SLAs in place with other providers. The SLA should not be mired in legalese; rather, it should be easily understood by anyone familiar with the service and the specified area of business. The SLAshould be implemented as part of the system runbook and maintained in an areaaccessible to both the business unit using the service. Service definition \u2013 Define the service being covered, location of the service (both servers and end-users), number of users, transaction volume expected,and service capacity. Determine coverage and schedules for the service. The SLA should be unique, based on the service being provided and the n. thod by which the KBIs are measured, and track compliance with the service definition. The process of defining these agreements requires tact and IT skills. An SLA is beneficial to both parties, providing resource consumption and service sizing information. Service Level Management is the process of defining a service. The goal is to include the lowest possible latency and maximum supported transaction rate. The service definition should be:Meaningful, Understandable, Cost-effective, Attainable. The SLA should be cost-effective for both the end-users and the IT department, while still meeting the demands of the business. The goals should be attainable, based on testing and historic performance. Service specifications should be based on the needs of the users and the performance data acquired from previous years (ifavailable) Business hours are defined as 0600 EST to 2000 EST, Monday through Sunday. Maintenance windows are defined from 2030 EST until 0530 EST. No transactions are permitted during these maintenance windows. In the event of a \u201cdisaster\u201d requiring the recovery of data or the restoration ofsignificant portions of the system, the help desk will notify the users of the event. Regular updates will be provided every 30 minutes until theservice is restored. The system shall maintain functional availability of 99.8% of business hours. Outages are periods during defined business hours that fall outside emergency maintenance windows. Planned or scheduled emergency maintenance is not included in outage calculations. The order processing system will sustain a transaction of 30,000 transactions per hour during business hours. 95% of all order transactions will be completed in less than 5 seconds (on average) with up to 300 users online. The system architecture, both hardware and software, corresponds directly to the impact on the service. Benchmarking of the application should be performed to identify the limits of the server\u2019s capabilities. Most IT departments choose not to include KPIs in the actual SLA. KPIs are intended for the IT department personneldirectly involved in the management of the server system. Commercial monitoring tools like SunMC, BMC Patrol, BMC Best/1, andTeamQuest can be configured to measure, monitor, and report on performance status. Most can then be integrated intoenterprise management tools like Solstice Enterprise Manager\u2122 and HP\u2019sOpenView. Table 3-2 lists example tool definitions that correspond to the sample thresholds in Table 3-1. Some metrics are vendor- or release-dependent, so multiple definitions need to be maintained. A KPI document can also include actions for immediate relief or resolution ofperformance events. These actions might include stopping, rescheduling, orprioritizing running batch jobs. The procedures should be tested and updated as necessary. Severity of problems can range anywhere from a single user having slow response to a global outage. To allocate resources appropriately, severity levels should be bedefined. A simple call sheet may suffice for the contact information. The contact information for different components of theproduction environment should be documented in the system runbook. This includes how to contact the help desk itself, owners of different categories of problems, and Vendors responsible for different parts of the environment. Problems should be reported to the IT help desk by phone to (606) 555-5300, by email to help@helpdesk, or by fax to (619) 515-5301. Delays in the resolution of a problem can also result in an escalation in severity. The escalation path and events causing theescalation should be well-documented in the system runbook. The help desk will assign a severity level based on the problem description and will contact the appropriate IT personnel. The IT manager for the service will be notified of the problem every 60minutes, until the problem is resolved. Regular reports should be made available to the end-users, business unitmanagement, and IT management. Reports should indicate how well the IT department is doing with meeting the service description. Meetings should be held monthly or weekly to determine any changes that need to be made. Identify problems that arose during the previous week and possiblerecommendations for ways to improve those problems. A sample weekly report is described in \u201cManagement Viewpoint Implementation\u2019 on page 135. It is much better to resolve questionsearly on in the SLA, rather than later through arbitration. For the end-user department to hire the talent needed, the department would need to spend approximately $480,000 a year. The datacenter environment, rather than the standalone environment, provides higher quality, sharable, standardized resources. By identifying the projects on which engineers are spending the most time, the IT manager can identify recurring problems, find ways to rectify those problems. This increases the ability of the IT staff to addressproblem resolution. It also increases the leverage that the support organizations canwield over vendors. In some IT departments, the IT department staff\u2019s bonuses are directly tied to meetingor exceeding stated service level goals. Failure to meet stated goals results in discounted or free services to the end-users. Arbitration need not be an outside organization, though it very well can be. It can be a person or a panel that can review the problems and reach a final, binding solution. The managers and representatives of the service for both the IT department and theend-users should meet regularly to review the existing SLA and to propose changesto the agreement. Either party may propose changes, but both parties must approve the changes. A complete TCO analysis is a long and involved process, but ca. n the systemrunbook. The system runbook should also contain the point of contact in one of theinvolved organizations. The computing needs for the business function must also be quantified in some way. This quantification can resolve to individual business functions such aspayroll and marketing data or can be further subdivided into more specific metrics. Baselining the performance of business services is the second step in understandingexactly what is happening in the datacenter. Performance baselines should measureeach business function in terms of transaction type, transactional volume, and wherepossible, average response times for the driving server systems. An analysis of an online sales and distribution system might state that:Shift average transactional volumes for the sample period of 10/1/99through 12/31/99 were measured as 12,000 catalog system lookups perhour. For shift hours measured at 11,500 to 12,500 transactions per hour,catalog system lookup response time averaged 3.2 seconds, with a worst-case query response time of 5.1 seconds. By creating a scalar representation of the performance characteristics of a particular business service, we accomplish two goals. We help applicationdevelopers and database administrators identify bottlenecks and inefficiencies inserver and application software. If there is an industry-standard benchmark that closely imitates your workloadcharacteristics, then use that benchmark as a guideline for platform comparisons. Very few business implementations come at all close to the popularindustry-standard benchmarks. Two examples that we can use as samples of market offerings areBMC (formerly BGS) Best/1 and TeamQuest. Best/1 Performance Console for UNIX has an internal database of hardware andoperating capabilities. Performance of a given system is quantified and can be used as input into the BEST/1 modeller for system sizing. Without strict resource management implementations, the rule ofthumb for target utilization is 70%. With resource management software and policies, target utilizations of 90% or more can be safely projected. The Sun Enterprise 10000 platform supports dynamic system domains (DSDs), allowing a single-server platform to act as several distinct servers. System resources can be migrated manually between system domains, allowing administrators to reallocate idle resources. Solaris OE processor sets allow the administrator to bind applications to a set ofdefined processors. processor sets create hard walls of CPU resource allotment foreach designated workload. Idle CPU resources within a processor set cannot be used by other workloads outside that processor set. Resources are defined as either static orrenewable , depending on their particularcharacteristics. A CPU is one example of a renewable resource because CPUresources are dynamic for any given point in time. There is no finite limit of CPUpower; as a machine runs, new processing power is always available. System platforms, operating systems, applicationsoftware, and the business functions that use them are in a continuous evolutionaryprogression. To properly consolidate server functions and satisfy business requirements, a complete understanding of the enterprise is necessary. Even the hardware and software procurement process is circular. Business groupscontract service levels with capacity planners. Capacity planners define a workloadmetric for the service levels contracted. Systems engineers specify a hardwareplatform that can satisfy the workload metric. The next chapter provides a recipe for successful scenario planning. The next chapter presents amethodology for modelling capacity and load. The last chapter suggests processes for quantifying capacity and consolidating workloads. Trending techniques use a mixture of step functions that track changes in yourconfiguration. This process extracts cyclic variations from your data so that you can see the underlying trends. With this technique, you use analytical models topredict the behavior of complex workloads. A Recipe for Successful Scenario Planning. Successful planning must be based on firm foundations. This recipe provides a step-by-step guide to the process. The model should concentrate o. he same job, show them as asimple replicated set. Be ruthless about paringdown the complexity to a minimum to start with. The bottlenecks will change over time, so they must be listed explicitly each time ascenario is modelled. It may be sufficient to look at just the CPU load on a central database server, or you may also need towatch network capacity or a wider range of systems. The primary bottlenecks appear to be the CPU and disk on the back-end database and the CPU on the searchengines. Disk utilizations were found to be very low on the other systems. Measure service levels and workload intensities. Performance modelling assumes that the workload is essentially constant. One of the main modes occurs when backups are taking place. Fast-growing environments will find that the amount of data to back up rapidly. e back-end server\u2019snetwork interface would be a bottleneck during the update. The utilization levels aredifferent in each mode, so they need to be recorded separately. The initial model should be a crude and oversimplified base for planning futurescenarios. The hardest thing to learn for successful modelling is to let go of thedetails. You can build a very useful model with incomplete and inaccurate data. As long as that mixture remains fairly constant, you can average all transactions into ageneric transaction rate. If the back-end server load level fluctuates a lot because of competing applications, you need to perform a workload breakdown. If this is the primary load on the system, then you can probably getaway with using the overall CPU utilization. You need to collect historical data that spans a time period similar to the period you are trying to predict. Lack of data is no excuse; however, you should start to plan immediately. The rest of the scenario modelling and planning process is illustrated with a simpleparameterized spreadsheet in the next section. This section explains how a spreadsheet can be used to quickly generate estimates of capacity and load. In the next section, a simple spreadsheet-based scenario planning model is explained. create a spreadsheet that estimates the \u201cgrowth factor\u201d from a baseline. If additionalinfluences have an impact on the load, they can be computed in a manner similar to the influences described below and can be combined with the other influences. The growth factor is simply thefactor that the original (baseline) load must be multiplied by to get the estimatedload supported at the time in question. If the baseline load was measured in Januaryand the estimated growth factor for June is 2.0, this means that the load can beexpected to increase two-fold from January to June. The units of measurement cancel each other out. The spreadsheet method outlined here is designed for systems with onlyone significant bottleneck. Systems with multiple bottlenecks need to use morecomplex methods.  spreadsheet model is flexible enough to allow fortailoring the models. More complex formulas can be used, or different formulas canbe used for different time spans. Modifying the spreadsheet so that it better estimates the load and capacity of the systems in question is discussed in\u201cTweaking the Model\u201d on page 74. The number of data points (spreadsheet cells) should also be the same for eachinfluence. It is important that the seasonal variations begin with thebaseline in the correct season. The important data is the prime time daily peak level. You can see that the system never sleeps. There is a predictable pattern that peaks in the evening during weekdays and falls off a little during the weekend. The chart inFIGURE 4-6 shows the peak level day by day. In this case, Monday isshown as the busiest day, with Friday a little lower, and a quieter weekend. Sundaysare often busier than Saturdays, partly because systems located in California arenear the end of the international dateline. Data is taken from 1996 Web site hit rate datacollected in 1996 on www.www .sun.com by Adrian Cockcroft. Analysis of variance, or ANOVA, is used to fit a model to the data. If you have at least a years\u2019 worth of collected data, you can use it to generate your \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0estimates for the monthly variations. Keep in mind that if you have limited historicaldata, it is likely that the perceived seasonal effects will be a combination of theactual seasonal effect and other (potentially large) influences. \u201cTweaking the Model\u201d on page 74 offers guidelines on how tomake the model fit the data. The growth of the Internet combined with a growth in awareness of your site maycause a geometric exponential growth in load levels. This can be easily expressed intwo numbers as a growth factor over a time period. For example, growth might beexpressed as 50% per quarter, or doubling in a year. A spreadsheet computes the growth factor and monthly factors for the coming months with a growth rate and duration provided by a user. ibed here is mathematically referred to as geometric exponential growth. It is usually (somewhat incorrectly) referred to asexponential growth. The load at a given time ( Lt) is computed based only on the time ( t-t0). The value t0is zero if tismeasured in relation to it. In the following table, the values used are G= .22 and L0= 1, which makes the following formula: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Lt= 1.14t. This formula is provided in a capacity planning spreadsheet. There is an underlying growth factor involved in the size of the total Internet marketplace. At one point this was doubling about every 100 days, but recentgrowth rates suggest a doubling period of a year or more. Several large Internet sites are seeing business grow at arate of about 50% every three months. Growth rate over several different time ranges (for instance, January to March,March to May, and May to July) and averaging the results may yield a much betterestimate. tune the growth rate as you obtain more data. The spreadsheet available athttp://www .sun.com/blueprints/tools uses a formula that estimates the marketing reach of a campaign. This formula is appropriate insome circumstances; however, other formulas may better represent a marketingimpact depending on your environment. Most formulas can be easily implementedin a spreadsheet to work with the rest of this procedure. A marketing campaign starts in the fourth month to counteract theexpected seasonal drop in load level. It continues for four months, then thememories fade away and a residual increased level of activity is seen. The short-termand residual gain should be something that the marketing department uses to justify the campaign. In practice, many smaller marketing boosts may be modelled rather than one large boost. modelled in the same way as itsimpact on the business. The campaign is not repeated in the second year; the chart inFIGURE 4-9 shows why it was not needed. The factors that affect capacity can be separately computed in aspreadsheet and then combined to get a combined capacity estimation. Manycapacity measurements can be modelled. The two influences on capacity shownbelow are efficiency variations and capacity increases. Capacity increases include anything that allows increased capacity because ofupgrades to the bottleneck. Large changes in the application or transaction mix need to beaccounted for in the scenario plan. Table 4-6 shows the effects of heavy application tuning on the CPU usage pertransaction in the above example. Although these results may be unrepresentative ofwhat can be achieved in many situations, it is sometimes possible to achieve a four-or five-fold performance improvement. The factors in TABLE 4-6 are plotted in FIGURE 4-11. To counteract the increased load levels, the application is tuned and the hardware is Upgrade. ompared to the untuned one. Database tuning or application software upgrades give the first two gains. Then there is a reversal as a new, more complex user interface is introduced. After a while, several more tuning improvements are made, until after two years, the computer resources used per transaction is reduced to 15% of that at the start. Appendix A, \u201dSun Constant Performance Metrics,\u201d shows some performancenumbers for various machines that may be appropriate for projecting the capacity ofnew hardware. A notional level of 70% capacity utilization was also factored in. The third column shows the CPU utilization in each month. Thisassumes a start point, which for this case is entered into the spreadsheet as 70% busyat month zero. The utilization is calculated by taking into account all the differentgrowth factors explained in the next section. The combination of efficiency and hardware capacity upgrades can be plotted on the same axis to see if the system is capable of sustaining the load. The plot in FIGURE 4-13 shows that the combination of a marketing campaign and theseasonal boost overloads is more effective. If this model is correct, something else needs to be done to sustain this futurebusiness scenario. Either a faster system could be installed earlier, or more efficiency could be squeezed out of the system. Another option is that the work itselfcould be split in some way, possibly by functional or geographical aspects. When manipulating the data, bear in mind that the analysis is based on peak loads, not average loads. If you take measurements atintervals of a few minutes, they can be averaged. As soon as you zoom out toconsidering intervals of an hour or more, you want to know the peak level duringthat hour and the peak hour during the day.  monthly data is based on the peak for the wholemonth, and it could occur at any time during the month. For example, if the weekly peak is a Monday, then the monthly peak could occur on any Monday in themonth. Set the workload growth parameters shown in TABLE 4-3 to get a first approximation of the overall long-term growth rate. Tweak the seasonality. Set up marketing factors. Spreadsheet-based models are a good starting point, but are extremely limited andcan be clumsy to use and extend. Large disk subsystems are expensive and may be the primary bottleneck of yourapplication. alues should be moved to be seasonal. This chapter examines the problems in more detail. By identifying the primary bottleneck, theexamination shows where the system can be expected to run out of headroom first. Solaris 7 OE includes the kstat command, which obtains and processes throughput counters. The priority_paging option is available in the appropriate kernel update patch. The latest SE toolkit is available at http://www .setoolkit.com. Solaris 8 OE changes the memorymanagement subsystem in several ways to make paging much simpler tounderstand. There is no need for priority paging because a moresophisticated scheme is used by default. You should see no scanning, a lot morepage reclaims, and a much higher free memory value reported byvmstat. A system can be modelled as a whole or broken down into individual services to build separate models for different functions. Separate SLAs might exist for system performance during system maintenance tasks, systemperformance during business hours. Resource management schemes, such as processor sets and SRM software, can be used to control the backup and reporting systems to reduce impact on OLTP. At this point in the capacity and performance management cycle, intimateknowledge of the business system being modelled is required to prioritize the workloads. The sample period should reflect the maximum period oftime for which degraded system performance is to be tolerated, with a critical threshold of maximum degradation to be allowed. In an online processing systemthat has direct interaction with the paying customer of the business, the sampleperiod could be as low as one minute. isolated from periods of customer interaction. H processing windows and transactional processing should be avoided where possible. Batch reports, database record purging, system backups,and other batch system maintenance should be scheduled into time windows. One immediate change to workload scheduling can be derived from this graph. There appear to be batch reports running during the system maintenance window. By moving these batch reports outside the defined maintenance window and creating a defined shift for reporting tasks, we can enforce resource management.  utilization of system resources when workloads could otherwise coexist without contention or minimized through the proper use of resource management tools. To illustrate this case, midday reports are allowed between 12:00 p.m. and 1:00 a.m., but all other report generation is assigned to the 6:00p.m.-0:00am window. Isolating the different system processing tasks greatly simplifies the implementationof monitoring tools and system resource data collection. Brian\u2019s method is commonly referred to as the \u201cM-value process,\u201d after thetraditional mainframe performance metric. By creating a series of generic terms todefine work on a computer, it is possible to quantify several statistics. These statistics describe the capacity of the machineplatform, as well as the performance characteristics of a given workload. SCPMs are a real-world estimation of Amdahl\u2019s Law applied to system architectures and operating system versions for cases of regular,predictable, and normal workloads. Occasionally, significant patches may influencethe scalability of an operating system. Amdahl\u2019s Law provides the calculations for predicting the benefit of running aworkload on a scalable multiprocessor environment. Measuring the potential parallelization of your workload provides a more accurate model of the performancecharacteristics. Application of Amdahl\u2019s Law measures CPUarchitectures and presumes that there are no bottlenecks in the disk or networksubsystems. Speed-up is calculated using the time taken to accomplish a defined set ofrepresentative transactions. The speed-up factor that is calculated with a single CPU and two CPUs can be used to calculate the scalability factor ( S) of a given workload. S is the percentage of the workload that is parallelized, and not the finite resource constrained.  scalability of a given machine for a given workload can be predicted with theScalability factor. Scalability factor may be reduced if there is resource contention. Adding CPUswill increase the contention, thereby reducing the percentage gains. Lower scalability factors can influence system architecture decisions toward wider server architectures. With deeper architectures, a direct relationship is establishedbetween the scalability of an application architecture and the potential total cost ofownership (TCO) With lower scalability factors, the progressively lower increase inperformance benefit of adding additional CPUs can outweigh thepotential savings in TCO of managing fewer systems. Larger system architecturesgenerally provide more reliability and serviceability features. The scalability of any particular software on a particular server might displaynongeometric scalability characteristics. For most normal workloads that we havemeasured, the variances from the geometric scalability curves are minimal. Factorsthat might cause a significant variance include:CPU cache/code efficiency. Limits to application software threading. The SCPM capacity planning process applies the basis of the Utilization Law to measure how much of a server\u2019s resources are being consumed during a sample period. Examples of resource contention areas include locking mechanisms forexclusive access to resources.  Processing consumes measurable resources in the serverplatform. An observer of the server during the observed time frame canidentify the amount of time that the server resource component is busy processing tasks. The SCPM process measures the amount of work being done on a machine. By measuring realworkloads and their relative disk I/O, CPU, memory, the process can determine how well a system is performing. You can predict the effective resource consumption of that workload. You can alsoproject resource consumption into other SCPM-rated platforms or modify thevolumes of work being performed within the model. The output of sar can easily be imported into a spreadsheet for further Calculations. The %wio column is not considered for this exercise, because it indicates the time that the CPUs werewaiting, and not processing. We can also project the measured system utilization into the system SCPM value to produce the quanta consumed (QC) over time (see FIGURE 5-9 ). This representationcan sometimes be easier to understand than percentage utilization graphs. If we divide the QC on a given platform by the number of users accomplishing that work, we now have the quanta consumed per user (QC \u00f7User). You can thencompute the total users that a given machine will support: SCPM\u00d7Target Utilization \u00f7(QC\u00f7User) Capacity planning with SCPM values provides a simple method for measuring,reporting, and predicting consumption of CPU resources. The same method can bemodified slightly and applied to disk subsystems. The activity being measured is only related to disk activity, so data generated by tape drive, Network File System (NFS), or CD-ROM activity mustbe \u201ccleaned\u201d before being used for capacity planning. The number of disk activities is measured and reported for total disk activity. The number of disk activities during the measured period can be referenced inrelation to the service levels provided in that sample time. By dividing the diskactivities per second by the transactional rate in transactions per second, wecan compute a metric indicating the disk activity per transaction. The relative disk I/O content for a given workload should remain almost constant. Changes to a workload (new features, new data structures) can change the R value. As the system grows, thisvalue should approach reality and become more accurate as we calibrate themeasured average I/O sizes against the data access I-O size.  err on the side of safety andoverestimate the maximum required disk throughput. The combination of these two trendswould cause the R value to reduce over time. Average CPU Utilization 75%Average Disk Reads per Second 3000Average Disk Writes per Second 1500Total Disk Avtivities per Second 4500Filesystem I/O Size 8KB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0QC (40,000 (Q))  *  (.75) 30,000 QQC / User  /  (300 (Users)  / (50 (Transactions)   600 Q*secS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W /  User (1500 (writes) / (300) 10 IOPS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0S    User  (4500 (I/Os) /   ( 300 (Use) (Use))   10 IOPSRSR   Users (3000 (reads) (3000) During our monthly load planning meeting, the marketing department presented a growth projection of 50% for the next fiscal quarter. The server running the load has an SCPM value of 40,000 Q, and the load projection shows aprocessing requirement of 45,000Q. From these values, it is obvious that the currentserver will not be able to process the projected workload. If we upgrade the serverthrough module count increases, CPU module upgrades, or a \u201cforklift upgrade\u201d System Utilization =QC\u00f7SCPM (Q) 45,000\u00f772,000 =62.5% = 62.5%. The projected disk utilization can be predicted based on the capabilities of the disk subsystem. OLTP servers generally consume disk, CPU,and memory resources at a similar rate. The SCPM process has proved very effective in modellingand predicting workloads and resource consumption for a large percentage of themachines sampled. The SCPM process is very useful in a first pass of a TCO analysis as part of a serverconsolidation effort. The details of how disk measurements are derived are covered in-depth by Adrian Cockcroft. More complex systems do not follow the same rules as simple systems whenit comes to response time, throughput, and utilization. Even the simple systems are not so simple, so we start by looking at measurements available for a single disk. Then observe combinations, and finally show you how to combine the availablemeasurements into capacity estimates. The most important measurement to take on each disk is the average transfer size. For example, a high-end server 18.2 GB FC-AL drive is the Seagate ST318203FC. A standard disk is SCSI-based and has an embedded controller. The disk drive contains a small microprocessor andabout 1 MB of RAM. It can typically handle up to 64 outstanding requests via SCSItagged-command queuing. Large systems have another level of intelligence and buffering in a RAID controller. The service time, as measured by the device driver, varies according to the load leveland queue length. Disks that spin fasterand seek faster have lower (better) service times. If there is always a request waiting, the device is 100% busy. A busy disk can operate more efficiently than a lightly loaded disk. The difference is that the service time is lower for the busy disk, but theresponse time is longer. This difference is because all the requests are present in thequeue at the start. Solaris OE instrumentation measures a two-stage queue. A reador write command is issued to the device driver and sits in the wait queue until theSCSI bus and disk are both ready. When the command is sent to the disk device, itSector 0. iostat -x was introduced in Solaris 2.6 OE, and at the same time, disk slices, tapes, and NFS mountpoints were added to the underlying kernel data source. The problem with iostat is that it tries to report the new measurements in some of the subsystems. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. Theword wait is being used to mean two different things (seeFIGURE 5-15 ). The meaning of service time ( S) is as close as you can get to the old-style disk service time. To calculate it from iostat output, divide the utilization ( U) by the total number of reads and writes. %iostat -xn \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  r/s  w/s, kr/s and kw/s wait actv wsvc_t asvc_t  %w  %b device \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021.9 63.5 1159.1 2662.9 0.7 0.0 31.8 0 93 c3t15d0UwaitWait Queue Utilization iostat %w100Bwait\u00d7 worrisomeThires. iostat reports 31.6 vs. 31.8. The difference is due to rounding errors in the reported values, 2.7 and 85.4. With fullprecision, the result is identical, since this is how iostat calculates the response time. This method uses a theoretical model of response time. It assumes that as youapproach 100% utilization with a constant service time, the response time increasesto infinity. This occurs because the disk is much more complex than the model. When a simple system reaches 100% busy, it has alsoreached its maximum throughput. When the device being monitored is an NFS server, hardwareRAID disk subsystem, or striped volume, the situation is clearly much morecomplex. All of these can process many requests in parallel. As long as a single I/O is being serviced at all times, the utilization is reported by iostat as 100%. However, there is enough capacity for additionalI/Os to be serviced in parallel. Compared to a simple device, the service time is the same, but the queue is being drained more quickly. If several requests can be serviced at once, then when a queuing model is used, the model works as if the averageservice time was reduced by the same factor. So, there is some confusion here over how to handle this case. Cached Disk Subsystem Optimizations can be used to improve performance of complex disk systems. The cache allows many optimizations that combine small accesses into fewer largeraccesses. Some of the common optimizations are: read prefetch clustering and modified cacheblocks written back asynchronously. Multiple writes to the same block often occur when file systemmetadata is updating. Multiple writes to adjacent data are coalesced into a singlelarger write. If the larger write exceeds the underlying disk block size, then thereis no need to read a large block. The disk cache can be inserted into the disk subsystem in two places: in the hostcomputer or in the disk array. The Sun Prestoserve and Sun StorEdge\u2122 Fast WriteCache products add nonvolatile memory to the host computer and use it primarily as a write cache. A very small amount of  the large block back. The capacity modelfor reads is the same as that of a simple striped disk setup. The worst-case performance for writes is limited by the write-backrate. There can be a choice ofpolicies for the cache: it might cache only synchronous data. UltraSCSI runs at close to 40 MB/s for large transfers. This interconnect is used for simple disk packs and the Sun D1000and A1000. The transfer latency for small transfers over UltraSCSI is around 0.6 ms. This section provides an overview of performance factors and capacity planningmodels for complex disk subsystems. It includes RAID5 and cached stripe examples. The capacity model for a single disk is simple and has already been described. When disks are concatenated or the request size is less than the interlace, individual requests will only go to one disk. In practice, writes take a small hitand reads a small benefit, but these are second-order effects that can be ignored for the sake of simplicity. Small requests of 2 to 8kilobytes are used and it is impractical to attempt to make a stripe interlace thissmall. The reduced size of the request on each underlying stripe reduces service time for large transfers, but you need to wait for all disks to complete. The operation of RAID5 is described in detail in Configuration and Capacity Planningfor Sun Servers by Brian Wong. In essence, a parity disk is maintained alongside astripe to provide storage that is protected against failure, without the fullduplication overhead of mirroring. Reading the RAID5 combination performs like a normal stripe. It has similar read and write performance and thethroughput of N-1 disks. The service time for the transfer is reduced as the size ofeach write is reduced. The cache provides fast service time for all writes. Interconnect transfer time is the only component. The cache optimizes RAID5 operations because it allows all writes,whether large or small, to be converted to the optimal full-stripe operations. Write-caching for stripes provides greatly reduced service time. It is worthwhile for small transfers, but large transfers should not be cached. For consistently largertransfers, better performance is obtained without a write cache. The configuration parameters for this system are: number of data disks M= 4 (ignore mirrors) RAID5 parity disks I= 64K (use whole disk size if unstriped) Max SCSI rate Brw = 40000K/s or FC-AL Br=Bw= 100000K/ s. Max disk data rate D= 24500K/S. We use the notation of Mdisks, with a workload concurrency of N. mance varies from 1 to stripe width M, depending upon configuration and workload. The performance factor Pcan vary for read and write, but it does not include cacheeffects. There is an effective increase in concurrency at the disk level because each thread ofaccess in the workload is being subdivided by the disk interlace to cause morethreads of disk accesses. This division allows more performance to be obtained froma disk subsystem, up to the limit of the number of disks in the stripe. This example is based on the same iostat data shown above, but for M= 6 and I= 16K interlace. The change in the interlace would change the data reported by \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0iostat in practice, so the results are not comparable with Example 1. The primary benefit of cache is fast response time. There is also a small overhead from copying data to and from thecache on its way to the disk. You should make the pessimistic assumption thatthroughput is unchanged by the cache. Solid-state disks have H= 1.0 for both reads and writes as data is engulfed by the cache. Other disk types have varying values for H, but characteristic values for E. The measured service time calculated from iostat data is. Smeasured =Sdisk(1 \u2013 H)+H\u00d7Sdisk\u00f7E. Main memory provides the best cache for reads, and NVRAM for writes. The only disadvantage of the NVRam cache is that in clustered,highly available disk subsystems, it cannot be used because it is internal to eachsystem. The next chapter concentrates on the importance of observability requirements for information collection and presentation to three different audiences. This chapter looks in detail at gathering, organizing, andpresenting system performance data to these three audiences. Alert-based monitoring should be combined with high-level problem diagnosis. A single-page daily summary of status and problems is more appropriate. In the following sections, techniques and examples for implementing theseviewpoints are presented. The operations viewpoint is demonstrated with the SunManagement Center (SunMC, a.k.a. Sun Enterprise SyMON\u2122 2.1). The engineeringviewpoint is implemented with SunMC and the SE toolkit to collect additionaldetailed data. The management viewpoint is implemented as a spreadsheet. The steps covered in this section illustrate how to use a system monitor to automate the process of tracking multiple systems for unusual or problematic conditions. With SunMC, basic rules are enabled but you can load additional health monitoring tools. If you have never seen a problem with a particular metric, then set its threshold alittle higher than the highest value you have ever seen. This way, you \u201csurround\u201dthe systems being monitored with a multidimensional behavior envelope. If thebehavior goes beyond that envelope in any dimension, an alarm tells you thatsomething abnormal is happening. The number of alarms that occur at each of the problematic levels (e.g.,yellow and red) should be collected daily so it can be reported to management. The alarm log should bearchived. The number of problems analyzed each day matches the number ofproblems reported, so you are keeping on top of the situation. SunMC is becoming increasingly important as a platform for the management and monitoring of Sun systems. Many third-party tools could be used in this role. They are not Sun specific and can managemany kinds of systems from several vendors. SunMC has third-party vendor support for managing Windows NT-basedsystems, and relational databases for performing generic system administrationfunctions. Contact Halcyon Inc. for more details. SunMC can be used to monitor systems using complex rule-based alerts to diagnoseproblems. SunMC\u2019s SNMP version 2 with user security implementation ismore efficient at bulk transfer of data. The SunMC health monitor is based on a set of complex rule objects. The health monitor rules are based on thoseimplemented by Adrian Cockcroft in the SE Toolkit script virtual_adrian.se. A simple rule can be placed on any system. To load the health monitor module, start SunMC with the default administrativedomain. Select the system, pop up a menu, then select the Load Module option from the menu. The Browser tab of the host Details window shows the modules that are loaded. When it is selected, a secondwindow opens that is specific to the system being monitored. Under Local Applications, you will find the Health Monitor module. Inside that you find the eight rules that are implemented to monitor several system components. Handling Alarms in SunMC can monitor hundreds of systems from a single console. SunMC gives each group a \u201ccloud\u201d icon; or, you can load a backgroundimage to form a map or building plan and then position the icons on the map. When a simple rule or one of the health monitoring rules generates an alarm, it islogged by SunMC. At the domain-level console, the worst alarm state for eachsystem being monitored is counted. This means that with only one system beingmonitored, only one alarm will be indicated. If you double-click the alarm or select the alarm and click the Details...button, the Details window for that system opens with its Alarms tab selected. The next step is to select one or all of the alarms and acknowledge them by clicking the Acknowledge button. It takes some time to perform the acknowledgment, since it involves communicating all the way back to the agent on the server being monitored. The Sun BluePrints book Resource Management shows how to separate and monitormultiple workloads with SunMC. Use this capability to track application processes and define alarms that will tell you when something goes wrong. Almost all commercial performancetools supply data in real time for display in ways that are suitable for an operationsmonitoring display. An example of a useful display is a spreadsheet that reads in data logged to a file using the SE toolkit. Main memory is managed by reclaiming itself once it reaches a low threshold, so it will tend to hover around that threshold. The page residence time is another useful measure, as explained here. System is quiet overnight, gets busy and plateaus during the day, then peaks in theearly evening. CPU and disk plots also show that there is extra system CPU timeduring the night and a higher disk data rate associated with an online backup. There are a few times when the page residence time drops to a low level during thebackup period. This value is calculated by dividing the page scan rate into a systemparameter called the handspread pages. An optional setting for Solaris 7 OE turns on priority paging. Solaris 8 OE uses a completely different algorithm, which allows the file cache to be included in the free memoryvalue. The absolute value of free memory is used directly to see how much capacity is available for use. In Solaris8 OE, scanning only occurs when thesystem is extremely short of memory. The example report shown in FIGURE 6-15 is customized for use by a large Int. atus. The report must becustomized to your own situation, so we describe the construction and reasoningbehind the report in detail. Thereport is designed to be delivered each Tuesday. It covers from Monday of theprevious week to the Monday before the report and predicts whether there isenough site capacity to survive the load expected for the following Monday. The nightly online backup took too long and spilling over into the high-loaddaytime period, where it adversely affected user response times. The schedule for hardware upgrade on 12-May is at risk due to component delivery. The entire capacity planning process is described in more detail in Chapter 4,\u201cScenario Planning.\u201d The output from the scenario planning process includes aschedule of upgrade events that increase site capacity and a schedule of expectedincreases in load. The site ran fairly smoothly the past week. A new record bandwidth of 253.8 Mbit/s was seen on Mondaynight. User-visible problems caused by nightly backup overrunning into daytime operation on 14, 17, and18th. The schedule for hardware upgrade on 12-May is at risk due to component deliveryleadtimes. The upgrade is estimated to give a 22% performance increase, which (ifimplemented the day of the report] would reduce the 71% utilizationto 58%. This section gives a view of events over the previous week or so. It includes both problem counts and a graph of activity from Monday of the previousweek through to about midday the next Tuesday. The first row gives the total number of problems that were unresolved at the end ofeach day. The second row shows the overall status of the day according to the severity ofproblems that occurred at any time during the day. The graph is aligned to the daily problem summary. how many problems were resolved during a day and how many new problems were registered. Problems can occur at a relatively constant ratethroughout the week on average, but analysis and resolution tend to take placeduring the week when all the staff are available. Management Viewpoint Implementation 141Thursday night/Friday morning. might be better to schedule downtime at a different time. Two kinds of Web pages on a site can betested: a standardized page that allows comparisons between Web sites, and a site-specific page. A full Keynote report covers the performance of many components of the Web site. Other vendors in this market are Mercury Interactive and Envive Corp. Asingle measurement is sufficient for the management report. The SunMC product provides data on some of the Sun specific capabilities. It does not include a performance database for long-term historical data storage. For a full and detailed picture of what is happening on a Solaris OE system, additional information generally needs to be collected. The standard system utility sar is already set up through cron to collect data. It just needs to be enabled. The sar data files are collected in the directory /var/adm/sa. The default collection scripts overwrite data after one month. The Sun 100 Mb Ethernethme andqfe and the Gigabit Ethernet gecollect many detailed metrics that are not reported by the netstat utility. It is important to obtain byte count metrics for capacity planning. Web server access logs provide a useful source of performance information. The overhead of collecting data from the system utilities is low as long as the collection interval is set to areasonable level, such as 30 seconds. The best organization on disk is to build a collection script that maintains a dailydirectory that contains data files for each tool for each hour. The best starting point is the orcollator.se script, which is in use at many Sun based Internet sites. The disadvantage of using the SE toolkit is that it is unsupported free code. You must devote some engineering resources to maintaining the SE scripts yourself. SAS/ITSV has very good graphical and data analysis capabilities. Some Sun/Solaris OE data can be imported fromorcollator data files to MXG for analysis. Sun\u2019s own tools are currently focused on system managem. This chapter describes several useful Sun and third-party tools for capacityplanning. It also offers several ways to balance peak load both within and across systems. Over time, more of these functions are likely to be integratedinto Sun\u2019s product line. By analyzing the resource requirements of multiple applications on distinct servers,you may be able to consolidate those services on fewer machines. eavor for IT departments to decreasethe datacenter space, simplify management, increase reliability, and decrease costs. Solaris Resource Manager (DR) in the Sun Enterprise 10000 server provides discrete \u2018hard\u2019 partitioning ofdomains. DR also enables you to move a systemboard containing memory and CPUs from one domain to another. This tool can be extremely useful when you combine mixed workload applications on a single server. CPU and virtualmemory to specific applications or users within a single image of Solaris OE. Theseresource allocations act as a form of \u201csoft\u201d partitioning. The SRM software enables the administrator to establish resource usage policies for the CPU and virtual memory resources. Interactive users are alloted an 80% share and the batchprocesses a 10% share of the resources during working hours. Solaris Bandwidth Manager (SBM) software can be used to give each site itsown equal share. Poorly written cgi-binscripts and searches from a single Web site can saturate the server and affect the other sites. The SBM software provides the framework for quality of service (QoS) guarantees and SLAsfor critical networked applications. It allows the administrator to establish controls for the amount ofbandwidth that applications, users, and departments are allowed to use. This can be useful in corporate intranets and can also be used to constrain denial-of-service attacks on the network without shutting off access completely. load sharing facility (LSF) from Platform Computing Corporation is a tool that takes advantage of idle CPU cycles on a network to assist with batch processes. LSF enables users to submit batch jobs and gives the administrator the capability to set up rule sets to prioritize the requests. There are three roles to which hosts can be assigned within a cluster, and hosts can have more than one role. SunMC can be used to manage hundreds of systems running Solaris 2.5.1, 2.6, 7, and 8 OEs from a single console. This module is free of charge to download fromhttp://www .sun.com/softwar e/sunmanagementcenter/hwds/ and is an add-on solution for SunMC. SunMC Hardware Diagnostic Suite enables you to run comprehensive, data-safe, and non-resource-intensive hardware diagnostics testing in enterprise environments. The testing does not corrupt dataon the system and uses minimal system resources. SunMC modules tie into the SunMC alarm framework so users can executecorrective action scripts or programs that eliminate faults. Scheduled routine hardware validation can replace other maintenancethat requires system downtime. CST fills an important hole in the overall strategy of proactive system management. It provides a macroscopic view of the system configurationchanges caused by failures or upgrades and service patterns over extended periods. The CST agent creates a snapshot of the system configuration and determines the changes since the previous snapshot. It then catalogs the event andchanges in a change log file. The agent also provides a facility for maintaining an electronic log of service events. BMC Best/1 is a real-time performance monitor and prediction tool. Users can view the configuration report and change log for any host on the network. Best/1 is a set of tools for UNIX, PC and Mac computers. It can be used to create, edit and analyse data. The tool is not easy to use. Visualizer is one of the few tools that can analyze trend data in terms of cyclicfluctuations. It implements MASF, which is a sophisticated and automated approach to the problems discussed in Chapter 4, \u201cScenario Planning\u201d Patrol has been criticized for imposing a larger load on the system being managed than other tools. The product has a good underlying architecture that scales well in large, distributedinstallations with multiple data storage points. Foglight Software was initially known as Resolute Software and more recently hasbecome a division of Quest Software. RAPS uses very efficient data collectors and has anadvanced distributed storage architecture and a good GUI. SAS has a wide range of statistical analysis products. MXG Toolset is a notable alternative to the SAS/CPE functions that runs on the basic SAS platform. SES Workbench runs onUNIX systems and NT. SES Strategizer runs only on WindowsNT. rvices capability to assist in capacity planning simulation development. SarCheck is a relatively inexpensive tool that can be very useful to help withdiagnosis in an operations environment. On SolarisOE it reads data from sar andps, then writes a detailed explanation of what it sees, almost like an essay. Performance data is monitoredand recorded in real time, with both reporting and alert features. The data can beviewed live, showing system and workload performance measures, while the data isbeing recorded in the TeamQuest database. The performance data for a previouslyrecorded period can also be viewed in context. Capacity Planning with TeamQuest Model 155. The system being modelled is a database server, with the database instance and twomiddleware applications running on the same system. The Stretch Factor statistic represents the time spent waiting for a resource. A higherstretch factor indicates a higher relative quantity of resource wait time. The standard reports produced include severalinteresting statistics for further analysis. The Active Resource report includes resource consumption statistics such asthroughput, service time, wait time, average queue length, and number of server instances. Stretch factors greater than 2.0 shouldbe analyzed\u2014they indicate a significant resource shortage. The calibrated model, including the new workload growth calculations, can now be exported to create graphs and tables of performance data in Microsoft Excel. The first graph that we will examine is the Stretch Factor by Workload graph. The \u201cDatabase_App2\u201d workload appears to have a response time increase, which indicates a resource contention problem. This is to be expected, with the increase inrelative queue wait time to resource service time represented by the stretch factor. We can further analyze this workload\u2019s behavior by using the Components ofResponse Time graph for the \u201cDatabase_App2\u201d workload. The graph in FIGURE 7-7illustrates the time consumed by a transaction, or unit of work within the workload,with time spent in the workload. This is broken down by active resource service timeand active resource queue delay.  CPU Queue Delay is increasing at an alarming rate and that itaccounts for more than half of the per-transaction time by Step 6. This indicates that the system is on the verge of a serious performance degradation. The CPU resource utilization is projected over 95% average utilization by the time the model reaches the sixth step. Step 1 shows that we are running at approximately 70% utilization, with the active resourceutilization for the measured workloads showing very little resource queue delaytime. At Step 2, the workloads arebeginning to show some contention and CPU queue delay times, with CPUutilization just passing 75%. This trend would indicate that we should target 70% for the KPI maximum CPU utilization threshold. System was a Sun Enterprise 10000 serverwith six 250 MHz UltraSPARC CPUs with 4 MB of e-cache. The chosen solution inthis case is to perform a one-to-one upgrade of the CPUs in the system to400 MHz UltraSpARC CPUs. With this solution, we can return to the TeamQuest Model window, create alternatives based on the current model, and modify the system resources to predict the projectedperformance of the growing workload. The Stretch Factor graph now shows stretch factors under 2.0 in all steps, for all workloads modelled. In addition, the \u201cDatabase_App2\u201d workload displays anincremental increase in delay time per unit of work being done. We can also examine the total system CPU utilization by looking at the resultingspreadsheet table data or by viewing the Active Resource Utilization graph. The CPU and disk active resources are graphed over the current, measured workload, as well as over the five steps of compound growth. The Active Resource Utilization graph shows that disks are becoming rather busy throughout the model. Wecould now return to the model and upgrade the disk subsystems to provide moreresource capabilities or move workload resour. This chapter presented a detailed survey of useful tools for capacity planning anddiscussed the benefits of using both Sun and third-party tools. In addition, this chapter offered several ways to balance peak load both within and across systems. Alternative pathing (AP) works in conjunction with dynamicreconfiguration (DR) to provide redundant disk and network controllers and their respective physical links. The main purpose of AP is to sustain continuous network and disk I/O when system boards are detached from amachine or dynamic system domain. Short jobs are said to backfill processors reserved for largejobs. Short jobs fit into the time slot during \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0which the processors are reserved. The blacklist file is read andprocessed at startup. class-based queuing (CBQ) The underlying queuing technology used in the Solaris BandwidthManager (SBM) software. Cluster A collection of computers interconnected via a high-speed interface. Multiple servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if high availability is important, multiple servers are configured as siblings. A Workload Manager (WLM) component that communicates policies, metrics, and control data between Sysplex nodes. A function, used by dynamic reconfiguration, or DR (called from dr_driver ), provides the ability to attach a particular instance of a driver withoutaffecting other instances. E is a function, used by dynamic reconfiguration, or DR (called from dr_driver ), that provides the ability to detach a board that contains the kernel cage (OBPOpenBoot PROM), kernel, and non-pageable memory. The kernel cage canonly be relocated after all of the drivers throughout the entire dynamic system, or DSD (not just on the board being detached) are quiesced. The goal of this group is to tie together usersand applications with network elements, protocols, and services throughspecific relationships. By complying to this model, different network equipment and application vendors should be able to build interoperable network elements. direct control A means of control that operates on the resource you want to control. Dynamic internal service class created by the Workload Manager. Distributed queuing system. Sun Microsystems technology allows system boards to be added (attached) orremoved (detached) from a single server or domain. A Starfire independent hardware entity formed by the logicalassociation of its system boards. Exclusive scheduling is a type of scheduling used by the load sharing facility (LSF) A job only runs exclusively if it issubmitted to an exclusive queue. An exclusive job runs by itself on a host. LSFdoes not send any other jobs to the host until the exclusive job completes. Fairshare scheduling is an alternative to the default first come, first servedscheduling. Fairshare scheduling divides the processing power of the LSFcluster among users and groups. LSF allows fairshare policies to be defined at the queue level. Gigaplane-XB provides main memory access through a point-to-point data router. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Hostview is a graphical user interface (GUI) program that runs on the system serviceprocessor (SSP) machine. Hostview enables you to monitor and control an Enterprise 10000. SunNetManager or HP OpenView products collecting and displaying the data. IBM WorkloadManager (WLM) provides an automated resourcemanagement environment driven by high-level business goals. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. Intimate shared memory is often the largest component of a database\u2019s memoryrequirements. A JavaBeans based framework for developing and deploying dynamic management based applications. Autonomous agents can be deployed in realtime to perform management tasks for devices on the network. When dynamic reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. A lightly damped system is veryresponsive to sudden changes, but will probably oscillate back and forth. A node in a special resource tree used by the Solaris ResourceManager (SRM) software. The SRM is built around lnodes, which are afundamental addition to Solaris kernel. ln nodes correspond to UNIX UIDs, and may represent individual users, groups of users, applications,and special requirements. High-Performance Computing (HPC) package includes the LSF as a vehicle forlaunching parallel applications on an HPC cluster. In addition to starting batchjobs, the L SF also provides load balancing. The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager (SBM) software. The maximum bandwidth is dependent on thepercentage of bandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. NetFlow allows for detailed network measurements that can be sent to other software pack. Network File System (NFS) An application that uses TCP/IP to provide distributed file services. Sterling Software offers a distributed version of NQS calledNQS/Exec. PC NetLink is a product from Sun Microsystems that is based on the AT&T Advanced Server for UNIX. PC NetLink adds functionality that was not previously available onSolaris servers with products such as Samba. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. A network element that does not explicitly support policy control using mechanisms defined in the applicable standard policy. The policy protocol can be any combination ofCOPS, Simple Network Management Protocol (SNMP), and Telnet/CLI. priority A relative importance level that can be given to the work done by a system. priority decay Seeprocess priority decay. Application memory is allocated at a higherpriority than file system memory. processor reservation is a method that allows job slots to be reserved for a parallel job until enough slots are available to start the job. Processor reservation helps to ensure that large parallel jobs are able to run without underutilizing resources. A \u201cprovider domain\u201d is the domain where a system board gets logically detachedfrom to then have it attached to a \u201creceptor domain.\u201d A proxy caching Web server sits between a large number of users and the Internet, funneling all activity through thecache. The SolarisBandwidth Manager (SBM) software provides the means to manage yournetwork resources to provide QoS to network users. Dynamic reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider dynamic system domain. repository access protocol used to communicate between a policy repository and the repository client. RSVP provides a way for an application to communicateits desired level of service to the network components. Scheduler is a component of the Solaris Resource Manager (SRM) software that schedules users and applications. Security policy aims at preventing access to certain resources or allowing designated users to manage subsystems. Service class is a class that defines a set of goals, together with periods, duration, and importance. A number of individual processes and CICS/IMS transactions can be assigned membership to a service class. They will then become subject tospecified goals and constraints. service provider In a network policy, the service provider controls the network infrastructure and may be responsible for the charging and accounting of services. Service time The time it takes for an I/O device to service a request. The SE Toolkit is a freely available butunsupported product for Solaris systems. It can be downloaded fromhttp://www .sun.com/sun-on-net/performance/se3. The SHRScheduler manages the scheduling of individual threads. It alsoportions CPU resources between users. Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris ManagementConsole (SMC) An application that provides a generic framework for gathering together operating system administration tools. Service levelagreements (SLAs) can be defined and translated into SBM software controls and policies. Resource management is done on a per-network basis, often by controlling the priority of data flow through intelligent routers and switches. For High-Performance Computing (HPC), Sun HPC servers use the platform computingload sharing facility (LSF) to perform load balancing. System service processor; Starfire\u2019s system administrator and systemmonitoring interface. The SSP configures the Starfire hardware through aprivate Ethernet link to create domains. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 GB ofmemory and over 20 TB of disk space. StoreX enables management of any storageresource in a heterogeneous distributed environment. SunMC is a Java-based monitor with multiple userconsoles that can monitor multiple systems using the secure extensions to Simple Network Management Protocol version 2 (SNMPv2) to communicate over the network. System level measurements show the basic activity and utilization of the memory system and CPUs. Per-process activity can be aggregated at a per-system level, then combined withnetwork measurements to measure distributed applications. The Solaris BandwidthManager (SBM) software can use this information when classifying a packet. It can also change the information, to influence how the packet is routed. Virtual memory is not directly related to physical memory usage. Virtual Web hosting is a configuration where a single server is configured to respond to hundreds or thousands of Internet addresses. g system will create 16 MB of memory within that application\u2019saddress space, but will not allocate physical memory to it until that memory isread from or written to."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_28",
      "kind": "book",
      "subkind": "1,31-65,80-107,117-148,339-356",
      "title": "Resource Management",
      "source": "Sun Press Blueprints  - McDougall, adrianco etc.",
      "published_date": "1999",
      "url": "./mcp_resources/virtual_adrianco/pdfs/rm_extracted_1_31-65_80-107_117-148_339-356.pdf",
      "content": {
        "metadata": {
          "word_count": 35852,
          "processing_status": "success",
          "processing_errors": [],
          "character_count": 234635,
          "excerpt": "Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource Management\n\nSend comments about this document to:  blueprints@sun.com\n\nThis chapter introduces the business problems that resource management addresses"
        },
        "summary": "Resource Management is published by Sun Microsystems, Inc. 901 San Antonio Road \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource ManagementPart No. 805-7268-10July 1999, Revision ASun MicroSystems, Jr. The scope of this book can be summarized as: The best way to use combinations of Sun Microsystems products to manageresources. How can we combine lots of small servers into a few big ones? Sun\u2019s range of products provide a lot of built-in flexibility that can be deployed in many ways to meet the requirements of several markets. To manage the service level you provide, you must be able to measure and control the resources consumed by it. The Solaris operating environment is one of the best instrumented UNIXimplementations. Not all commercial tools report Solaris-specific metrics. Resource management for UNIX systems is in its infancy. This chapter describes the overall methodology of service level management. It alsodescribes and compares several approaches to resource management. Service level management is the process by which information technology (IT) infrastructure is planned, designed and implemented. Service level management interactions are shown in FIGURE 2-1. Each interactionconsists of a service definition combined with a workload definition. The processes involved in Service Level Management include creating service and workload definitions and translating from one definition to another. System managers first establish a workload definition and the service level requirements. Vendors then propose a system that meets these requirements. The basis of the sizing estimate can be a published benchmarkperformance. Service Level Management 5benchmark is used as the basis of a sizing estimate. Vendors provide reliability data for system components. They can also provide availability and performance guarantees for production systems. The agreed-upon service levels could be too demanding or too lax. The actual service levels experienced by users with a real workload are subjective measures that are hard to capture. The real service levels cannot always be captured directly, but the measurementstaken are believed to be representative of the real user experience. Atransaction may take place over a wide variety of systems. An order for goods willaffect systems inside and outside the company. System managers create policies that direct the resources of the computer system. A policy workload definition is closely related to the service levelagreement workload definition. The policy can controlresources directly or indirectly. The measured workload and service levels should be analyzed to extract trends. Acapacity planning process can then be used to predict future scenarios. The accrued usage of resources by each user or workload may be accumulated into an accounting system. A complete system implements many control loops. Designing a control loop is more complex and requires explicit understanding of the situation. A brief digression into basiccontrol theory is provided to help explain the behavior of such systems. You apply a control input by turning the steering wheel to the point thatwill get you around the corner. After a delay the car responds, You measure theresponse, compare it with what you wanted, and obtain the error difference. If the difference is zero you don\u2019t need to change the control input. You may decide to over- or under-correct, and apply the correction gradually or quickly. In control terms, you are applying negative feedback to the system. The rate at which you measureand apply corrections is called the control interval. The amount of the error that you feed back changes the behavior of the control loop. When you apply these principles to computer system resource management you can see that it is important to average measurements over an appropriate time scale. l, the system islightly damped and will be very responsive to sudden changes but will probablyoscillate back and forth. If you feed back a small proportion of the error over alonger control interval, thesystem is heavily damped. Diverse Methods occurs both because of the products that are available andbecause of the expectations of the end users who are purchasing solutions. Anetwork-centric methodology can be extended for use in other areas, so can astorage-centric or server-centric viewpoint. SunMicrosystems, Inc. egratedmethodology and solve all resource management problems. The closer you get tothis ideal, the more expensive and complex the tool becomes. So it is harder to justify purchasing it. The system-centric viewpoint focuses on what can be done on a single server using a mixture of hardware and operating system features. Dynamic Reconfiguration (DR),processor sets, Solaris Resource Manager\u2122, and Sun Enterprise\u2122 10000 (also knownas Starfire\u2122) Dynamic System Domains (DSDs) are all system-focused resourcemanagement technologies. The Starfire system uses its own HostView interface running on a separate system serviceprocessor (SSP) to manage domains. The next release of this software will be extended to viewan SSP and all the domains in a Starfiresystem as a special kind of cluster. There is no way to control the usage of memory by a workload. The only way to constrain a workload is to slow down or stop its CPU usage. Sun has two kinds of clusters. The highly integrated SPARCcluster\u2122 product range is focused on improvedavailability in commercial environments. Its management tools will eventuallybecome an integrated extension to the SyMON software. It is notcurrently possible to migrate a running job from one node in a cluster to another, orto checkpoint a job to disk and restart it again later. erconnect utilization andproportion of remote data access are important additional measures. The Solstice\u2122 Enterprise Manager product is a Telco-oriented CMIP and SNMPmanagement system. In theory it could beused to manage computer systems and local area networks, but it was not developed to do this. Network controls are based on delaying and prioritizing packets. There is no user orprocess identifier in a packet, so it is hard to directly map network activity to system activity. Storage has recently moved from being a simple attached computer systemperipheral to a complex managed entity in its own right. Networked storage usingfibre channel puts an interconnection layer in between multiple servers or clusters. Project StoreX is based on a distributed pure Java technologyplatform that can run anywhere a JVM is available. Project StoreXenables management of any storage resource in a heterogeneous distributedenvironment. Project StoreX enables resource management of capacity, performance, andavailability of data storage. Measurements of capacity and performancecharacteristics can be combined with availability policies. Backup and archival policies can be used to automatemigration of data to a tape library. Solaris software can trace storage accesses on a per-process, per device basis. There may be a need for a Project StoreX Solaris Storage BandwidthManager to bridge the two management viewpoints. Large and complex applications such as SAP R/3, Baan, and Oracle Financialscontain their own resource management concepts and controls. For example, OracleFinancials implements its own batch queue system to decouple the generation oflarge reports from the interactive response time of the users. SAP R/3 measures the response time of important transactions and breaks these down into application server time and backend database server time. As many usersconnect to an application server and many database servers connect to a single database, there is no concept of which user is doing work on the backend. The application itself must have the instrumentation to keep track of what is going on. The consolidation process starts when you identify candidate systems and Applications. The next chapter introduces several example workloads. The product overview section of this book discusses their capabilities inmuch greater detail. Local password filesmay need to be merged, and any conflicting port numbers specified in the/etc/services file may need to being cleared. If you use a name service such as NISfor all your password and services information, then the systems should already beseeing the same name space and definitions. The number of system footprints may betoo high with midrange servers. It is hard to reduce the total number of serverseffectively. Consolidated upgrades benefit from systems that can perform dynamic reconfiguration. DSDs are supported on the Solaris 2.5.1, 2.6, and 7 releases. The total number of DSDs can be reduced as applications are consolidated. There is a common set of measurements to collect per workload. o choose relevantprocesses and aggregate to measure them. The remainder is overhead or unplannedactivity. If it is significant, it should be investigated. When you are accumulating measurements don\u2019t accumulate the pscommandCPU% . It\u2019s a decayed average of recent CPU usage, not an accurate measure of actual CPU usage over an interval. You need to measure the actual process CPU time used in each interval by taking the difference of two measurements. One of the simplest policies is to define limits on a measurement and associate itwith an action. Products such as the Sun Enterprise SyMON 2.0 predefine many simple limit rules. An error event is different because it is treated as a discrete on/off event. A complex rule is used to establish the state of a component or asubsystem. When rules are combined, they are ranked so that critical problems takeprecedence over warnings. A hierarchy of rules can be built for a network of systems. Solaris Resource Manager and others like it assign shares to each user according to a policy decided by an administrator. The Solaris BandwidthManager software uses this mechanism to provide a way to specify policies on a per-network packet basis. A control loop is a complex thing to manage because its stabilitycharacteristics, time constant, and damping factor must be set correctly. Policy manipulatescontrols when a measurement deviates from its desired range. Automated goal-based workload management is a feature found only on mainframes running OS/390 software. Some policies are implemented manually as part of operations management. Security and network-based policies can be stored in an LDAP based name service. The Solaris BandwidthManager configuration is updated to take into account that user\u2019s network address. A direct control operates on the resource you want to control. An indirect controlworks via dependent resources. some operate on a per-user basis (such as a file system quota) Solaris Bandwidth Manager product implements a direct control on networkpacket rates. This can be used to implement an indirect control on the CPU resourcestaken up by the NFS server code in the kernel. The terminology definitions are network oriented but apply equally well to systemlevel policies. Some of the terms defined in this standard are listed here. ultiple devicetypes. Policy elements are expected to be independent of which Qualityof Service signaling protocol is used. Examples of policy elements includeidentity of the requesting user or application, user/app credentials, and so on. The general architecture shown FIGURE 3-1 illustrates one common implementationof a policy that combines the use of a policy repository, a PDP , and a PEP. This diagram is not meant to imply that these entities must be located in physicallyseparate devices. The policy protocol can be any combination ofCOPS, SNMP , and Telnet/CLI. Given this rich diversity, a common language isneeded to represent policy rules. The rest of the standard document describes theManagement tool. The Solaris software implements a blanket login ban for non-root users. It also limits the total number of processes that can be started via the nproc kernel tunable. This feature scales with the memoryconfiguration. A further limit is placed on the total amount of processes per user. The main class of application-level controls are those provided by relationaldatabases and transaction processing monitors. The Oracle8 idatabase also implements controls on resourceconsumption and policies for relative importance. On the Starfire system, the CPUs can be partitioned into dynamicsystem domains, and a separate copy of the Solaris operating environment booted in each domain. CPU boards can then be moved from one dynamic system domain toanother. This is described in detail in Chapter 8. SRM works by biasing CPU usage on a per-user basis, using shares to determine the relativeimportance of each user. LSF software implements a distributed batch queuing system where jobs are submitted and, when resources are available, sent to be run on a system. Block input and output counters are not incremented correctly in current releases of the Solaris operating environment. The block counter problem is filed as bugid 1141605 and is fixed in the next release of Solaris software. An application consumes virtual memory when it requests memory from the operating system. Virtual memory usage is not directly related to physical memory usage. Not all virtual memory has physical memory associated with it. When you restrict or control the amount of virtual memory that an application canhave, you are not controlling the amounts of RAM it can have. Virtual memory can be limited at the system level and at the process level. At asystem level, the total amount of virtual memory available is equal to the totalamount of swap space available. Each time virtual memory is used, the amount ofswap space available drops by the same amount. Base Solaris software can do simple resource management of a process\u2019s virtualmemory usage. Limits are enforced perprocess, thus preventing any one process from using an unreasonably large amount of virtual memory. But a user can run many processes, so this does not preventdenial of service attacks. It is important to note that the amount of swap space used by a user does notrelate directly to the sum of all that user\u2019s processes. A user may have threeprocesses, where each shares a single shared memory segment between them. SRM software isconfigured to limit the total virtual memory to 1.012 Gbytes. If one of the three processes has a memory leak, the limit would be hit for that user. Theper-user limits mus mus be set to 1 Gbytes for each user. Physical memory is controlled by applying importancepolicies to different types of memory. In the future, it may be possible to apply limitor allocation style policies to physical memory, but that capability is not available in Solaris today. The Default Memory Allocation Policy is, by default, allocated on a demandbasis with equal importance to each subsystem. When a subsystem requestsmemory, it is allocated from a central pool of free memory. If sufficient memory isavailable in the free pool, then an application\u2019s request is granted. The Solaris memory allocation policy takes into account recent usage in an attempt to choose the correct application from which to steal memory. The most aggressivememory requests gets the majority of the memory assigned to it. This situation can be avoided by configuring physical memory in the system so that there is always enough memory for each application\u2019s requirements. In examples, if we configured the system with 128 Mbytes of memory, then bothnetscape andgimp could execute at the same time. The memory system will takememory from other applications that haven\u2019t used portions of their memoryrecently. For example, if you start a file-based mail tool such as dtmail , the memoryused to cache the file when dt mail reads a 23-Mbyte mail file will be taken from other portions of the system. Priority paging prevents the file system from consuming toomuch memory. Priority paging implements a memory policy with differentimportance factors for different memory types. Application memory is allocated at a higher priority than file system memory. The new memory allocation policy can be extremely important for larger systems. A large database system with a 50 Gbyte+ database on the file system willcontinuously put memory pressure on the database application. But priority paging will ensure that thefile system onlyuses free memory for file system caching. The Solaris Bandwidth Manager product provides controls on network traffic. The product can be used as part of a resource control framework. It can distribute incoming traffic over multiplesystems according to the current load on each system. This chapter describes tools that help perform process-based analysis. Analysis of per-process measurements separates the raw data into applicationworkloads. When several applications are consolidated onto a single system,resource contention can occur. In a distributed environment with discrete applications on separate systems,workloads are analyzed by monitoring the total resource usage of each wholesystem. The busiest systems can be identified and tuned or upgraded. This approach does not work when multiple workloads are combined on a single system. The Solaris software provides a great deal of per-process information. The data can be viewed and processed by a custom-written process monitor. The SE Toolkit is freely available for Solaris systems and is widelyused. The psinfo data is what the pscommand reads and summarizes. The usage data is extra information that includesthe microstate accounting timers. Sun\u2019s developer-oriented Workshop Analyzer usesthis data to help tune code during application development. Thepea.se script is an extended process monitor that acts as a test program for the system. It is based on the microstate accounting information described in chapter 5 of Workload Management 51. There are two display modes: an 80-column format(which is the default and is shown inFIGURE 4-15 ) and the wide mode, which displays much more information. The initial data display includes all processes and shows their average data since the process wascreated. Idle processes are ignored. Thepea.se script is 90 lines of code containing a few simple printf s in a loop. The real work is done in process_class.se (over 500 lines ofcode) It can be used by any other script. When the command is run in wide mode, the following data is added:Metadata input and output blocks per second.Characters transferred by read and write calls. System call per second rate over this interval. CPU time consumed between each context switch. The SE Toolkit alsoincludes a workload class, which sits on top of the process class. If you group them by user name and command, then you can formworkloads, which is a powerful way to view the system. Thepw.se Test Program for Workload Class. 53on user name, command and arguments, and processor set membership. It can work on a first-fit basis, where each process is included only in the first workload that matches. The pw.sh script sets up a workload suitable for monitoring a desktop that is also running a Netscape web server. The script runs with a one minute update rate and uses the wide mode by default. A high number of page faults fora workload indicates that it is either starting a lot of new processes, doing a lot. of I/O, or tha %more pw.sh. t it is short of memory. The script is compiled with the following command: #!/bin/csh -DWIDE pW.se 60. The command is followed by a list of command lines. Once you have collected the data, you can write a rule that examines each process orworkload and determines, using thresholds, whether that workload is CPU-bound,memory-bound or I/O bound. A prototype of this rule is implemented in theSE Toolkit, and it can produce the kind of information shown in the example. The Internet provides a challenge for managing computer systems. External synchronizing events can cause a tidal wave of users to arrive at the server at the same time. Sports-related web sites in the USA get a peak load during key games in the\u201cMarch madness\u201d college basketball season.  proxy caching web servers sit between a large number of users and the Internet. When all the users are active at once, regardless of where they areconnecting to, these proxy cache servers get very busy. Many web sites get little or no activity most of the time. A caching web server acts as an invisible intermediary between a client browser and servers that provide content. It cuts down on overall network traffic andprovides administrative control over web traffic routing. Performance of accesses to all the other virtual sites can be affected, and resource management tools are needed. Caches commonly have a hit rate of about 20 to 30 percent, with only 60 percent ofthe data being cacheable. If cached items are read more than once by different users in areasonably short time interval, the cache will work well. Each cache transaction takes some time tocomplete and adds significant latency. Direct web service allows the browser to contact web servers directly whenever possible. This contact may involve a firewall proxy thatdoes not provide caching to get out to the public Internet. This funnel effect can increase network load at a local level. The real reason to set up a proxy cache intranet infrastructure is the administrativecontrol. Security is a big problem. Setting up the firewall gateways to the Internet so that they route web traffic only to and from the proxy caches. Restricted routing also forces every end user who wants to get out to the Internet to use a proxy cache. The cache makes routing and filtering decisions and logsevery access with the URL and the client IP address. If the corporate policy is \u201cInternet access is provided forbusiness use only during business hours,\u201d then employees who clog up thenetworks with non-business traffic can be identified. Solaris Bandwidth Manager software can be used effectively in this environment to implement policies based on the URL, domain or protocol mix. Apache and Netscape\u2122 proxy caches are extensions of a conventional webserver. The Harvest Cache was the original development and is now a commercial product. TheSquid cache is a freely available spin-off and is the basis for some commercial products. Squid is used at the biggest cache installations, caching traffic at thecountry level for very large Internet service providers. ICP-based connections are much more efficient than individual HTTPtransfers. ICP connections are kept open, and transfers have lower latency. In the Solaris operatingenvironment, it is possible to configure more than one IP address. The Solaris 2.6 operating environment was tuned tospeed up this process. P addresses to an interface and specify virtual interfaces with atrailing colon and number. Use the ndd command to query or set the maximum number of addresses per interface. %ifconfig -a \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0lo0: flags=849<UP,LOOPBACK,RUNNING,MULTICAST> mtu 8232. or each connection, moreprocesses are spawned for the addresses that are most active at any point. The administrator of the web site can log inas that user to configure their own site. Cgi-bin scripts that it starts all run as that user so that accounting, performancemonitoring and resource management tools can manage each virtual web site\u2019sactivity separately. Solaris Bandwidth Manager software can also be used effectively, asthere is an explicit administrative mapping from each network address to the useridthat is running that web server. The SRM software manages resources on web servers by controlling the amount ofCPU and virtual memory. Three basic topologies are used on systems hosting webservers. Resource management is used to control the behavior of a single web server. For example, a web server may be shared among many users. An error in one CGI-BIN program could cause the entire web server to run slowly. You can run each web server as a different UNIX userid by setting a parameter in the web server configuration file. For example, the Solaris Web Server has the followingparameter, as shown inFIGURE 4-25. The memory limit can limit the amount of virtualmemory that the web server can use. Thisprevents any one web server from causinganother to fail from memory allocation. The following are the server wide aliases. Proc mem limit [ memory.plimit ] The per-process memory limit can limit theamount of virtual memory a single CGI-bin process can use. It effectively limits thenumber of concurrent cgi-bin processes. This section examines some aspects of the resource management that apply to thesetypes of problems. It focuses on workload consolidation and resource managementof databases. The success of workload consolidation is bound closely to the ability to partition resources between the applications. The unique characteristics of an NFS workload place multiple resource demands on the system. No single product can provide complete resource management of anNFS workload. The resources used by NFS and the different products or techniquesthat can be used to manage them. NFS servers are implemented in the Solaris kernel as kernel threads and run in thesystem class. You can control the amount of resource allocated to the NFS server inthree ways:Limit the number of NFS threads with nfsd. The actual number of threads required will vary according to the number of requestscoming in and the time each thread spends waiting for disk I/O to service therequest. There is no hard tie between the maximum number of. threads and thenumber of NFS threads that will be on the CPU concurrently. The best approach is to approximate the number. of threads needed (somewhere between 16 to 64 perCPU), and then find out if the NFS. server is doing its job or using too much CPUtime. Solaris 2.6 changed to instrument NFS client mount points thesame way it does disks. NFS mounts are always shown by iostat andsar. The full instrumentation includes the wait queue for commands in the client that have not yet been sent to the server. Utilization ( %busy ) indicates the server mount-point activity level. An NFS server is much more complex than adisk drive and can handle many more simultaneous requests than a single disk drive. NFS uses physical memory in two ways. In the kernel each NFS thread consumessome space for a stack and local data. Outside the kernel the data being servedby NFS consumes memory as it is cached in the file system. The amount of memory used by the file systems for NFS servers is verylarge and much harder to manage. NFS uses a very small amount of swap space, and there should be no inter-workloadswap space issues from NFS. You can manage NFS disk storage using UFS disk quotas. Prioritypaging should be mandatory for any system that has NFS as one of the consolidation applications. Solaris Bandwidth Manager provides the means to do this. You can control amount of resources consumed by NFS indirectly by curtailing theamount of network bandwidth on port 2049. The filter in the above example is for managing outgoing NFS traffic to the 129.146.121.0 network. You could decide to leave out the destination and manageNFS Traffic to all clients, from wherever they come. The key variableismax_bandwidth specifies an upper bound to the consumed bandwidth that never will be exceeded. Thepriority variable is less important. It will be a factor if other types of traffic are being managed. Higher priorities will have lower average latencies. The typical data center strategy is to consolidate more workloads onto each system. Resource management of the whole environment requires careful planning and a solid understanding of theinteraction between these packages and the database. A single database instance provides database services for two different applications that require access to the same data. No single control or policy can assign resources to ensure that adequate CPU is provided tomeet response times. The user issues an http request from the desktop, which must travel across the intranet/Internet to the web server. This component can bemanaged by controlling the bandwidth and network priorities into the web servers. Managing the bandwidth on the networkport on the machine that hosts the web server is a useful strategy to ensure thatkeyhttp requests get the required bandwidth. The Solaris BandwidthManager software can control the bandwidth allocation into the web servers. The database listener process accepts the incomingconnection from database client applications. The database listener forks a database process and connects the clientapplication to the database. The same control as in the web server component (2) is implemented here. The SRM software and resource limits can limit the amount of virtual memory used by processes, users, and workloads. This capability does not manage physicalmemory, but it effectively restricts theamount of global swap space consumed by each user. It is a high risk to ever let adatabase server reach its virtual memory limit. If the limit is reached, the databaseengine may hang or crash, and the database may have to be restarted. The objective for successful consolidation is to provide strong insulation between each database instance. This requires careful resource management of CPU, memory,I/O, and network traffic. System resources are not wasted because spare CPU cycles can be used by any otherworkload requiring them. The more workloads consolidated, the flatter the totalresource usage becomes. The shared memory area is often the largest component of the database\u2019s memory usage. It is the easiest to insulate between instances because the memory isallocated as shared memory. Resource Managements wires down the memory so that it can\u2019t be paged. One instance cannot steal memory from another. D-down memory must be unallocated before it can be removed from the system, requiring quiescing of the database. Private memory is the regular process memory used by each process. The Solaris page cache causes a huge memory demand, which places undue pressure on the database private memory. Priority paging puts a hard fence between the file systems and applications. If you plan to run a database on file systems, consider this a mandatoryrequirement. Ensure that I/O activity from one application does not affect another application inan undesirable way. The best way to insulate I/o within a single database instance is to make sure that application tables are placed on separate I-O devices. This completely insulates one database from another. If you must use a single storage controller, use capacity planning so that sufficient bandwidth is available to combine both. For OLTP applications this is rarely an issue because the bandwidth requirements are so low. Adecision support application is completely different. A single decision support workload can generate several hundred megabytes a second of I/O. When you add CPU resources to an instance ofOracle 7, the Oracle engine automatically picks up those resources. Someother databases must be restarted to use the newly available resources. Tounderstand how different databases work within a resource managementenvironment. The SE Toolkit is based on a C language interpreter that is extended to make all theSolaris measurement interfaces available in an easy form. It is used to prototype ideas that can then be promoted for incorporation inproducts, in particular for Sun Enterprise SyMON 2.0 software. The SEToolkit has been jointly developed by Richard Pettit and Adrian Cockcroft as a\u201cspare time\u201d activity since 1993. e code that takesmetrics and processes them is provided as C source code and runs on the interpreter. The business operation can be broken down into several applications such as sales and distribution, e-commerce web service, email, file, and print. Use aform that makes sense to managers and non-technical staff to represent the part ofthe business that is automated by the computer system. The Application Resource Measurement (ARM) standard aims to instrumentapplication response times. Per process activity can be aggregated at aper system level then combined with network measurements to measuredistributed applications. ARM can be used to measure user response time. All vendors support the one standard, and severalplementations exist. Application vendors have shown interest, so more measurements will become available. CCMS is used by several tools such as BMC Best/1 to providemore detailed application-level management than can be done with just system and device-level information. This information is provided by afacility called CCMS. Data is collected on a Netscape 2.5 proxy cache that is serving most of the thousandor so employees in Sun\u2019s United Kingdom facility. SE Toolkitpercollator.se script can parse Netscape and Squid proxy cache log formats andsummarize the data. The cache finish status was analyzed, and operations are divided into four categories. The NO-CHECK and UP-TO-DATE states are cache hits. The WRITTEN,REFRESHED, and CL-MISMATCH states are misses that cause cache writes. TheDO-NOT-CACHE and NON- CACHEABLE states are uncacheable, and anything elseis an error or incomplete transfer. On a quiet weekday, 280,000 accesses went via this cache, and 56 percent of theaccesses went out to the Internet. 34 percent ofthe accesses hit in the cache, 16 percent missed and caused a cache write, 49 percentof the accesses were not cacheable, and 1 percent ended in some kind of error. The plot in FIGURE 5-1 shows bands of transfer times that depend upon the user\u2019s location. Many users are locally connected, but others are operating over slowernetworks. The transfer time includes the time spent waiting for the remote server torespond. Log files are a rich source of performance information. You can apply thistechnique to many other applications, such as ftp and mail servers, as well as toany other application that can write a line to a file. The underlying data structures provided by the Solaris operating environment are described in full in the proc(4) manual page. The data that psuses is called PIOCPSINFO , and this is what you get back from ioctl . The data is slightly different if you read it from the pseudo-file.  p is a pointer to a prpsinfo structure containing at least the following fields. The structure is a typedef structure with a number of fields for each process. For a multithreaded process, you can get the data for each lightweight process separately. There\u2019s a lot more useful-looking information there, but no sign of thehigh-resolution microstate accounting that /usr/proc/bin/ptime and SEToolkit scripts display. proc(4) returns the process usage information; when applied to an   process file descriptor, PIOCUSAGE returns theprocess usage information. The file format is: proc(4), proc(5),proc(6) and proc(7) P points to a pr usage structure which is filled by the operation. The pr Usage structure contains at least the following fields. The psCommand returns usage information for the specific lWP file descriptor. Pr_dftime;   /* data page fault sleep time */    Timestruc_t   pr_slptime;  /* all other sleep time /* kernel page faults sleep time. pr_wtime; /* wait-cpu (latency) time   u_long,   Pr_majf,  /* major page faults, */ pr_nswap, /* swaps */ u_mrcv, u_msnd, u-msnd. PIOCUSAGE can be applied to a zombie process. PIOCRESET can be used to disable microstate accounting. To access process data you must have access permissions for entries in /proc. In the Solaris 2.6 release, you can access the /proc/pid entry for every process. This means that any user can use the full functionality of ps. Microstate accounting is not turned on by default. It slows the system down slightly. Microstate accounting works as follows: A high-resolution timestamp is taken on every state change, every system call, every pagefault, and every scheduler change. Thenormal measures of CPU user and system time made by sampling can be wrong by20 percent or more because the sample is biased, not random. For example, consider a performance monitor that wakes up every ten seconds,reads some data from the kernel, then prints the results and sleeps. Processes that sleep then consume less than their CPU time quanta always run at the highest timeshare priority. The error is an artifact of the dual functions of the clock interrupt. If twoindependent, unsynchronized interrupts are used (one for scheduling and one forperformance measurement), then the errors will be averaged away over time. You can increase the CPU clock rate to get more accuracy. The best solution is to use a microstate accounting-based tool, or to disablesome of the CPUs so that the measurements are made on a fairly busy system. The data provided by the SE processmonitoring class is shown in Figure 5. double elapsed; /* elapsed time for all lwps in process */double total_user;  \u00a0    /* current totals in seconds */ double total_system;doubletotal_child; /* child processes that have exited */double user_time; \u00a0 /* user time in this interval */double system_time, double trap_time and double child_time. ulong outblocks;   /* output blocks/interval - metadata - metadata only - not interesting */ulong vmem_size; \u00a0 /* size in KB */ ulong maj_faults; /* minf/intervals - always zero - bug? */ulONG total_swaps; /* swapout count */long  priority;\u00a0 /* current sched priority */ long  niceness; /* current nice value */char  sched_class[PRCLSZ]; /* name of class */ulongs messages;  /* msgin+msgout/interVAL */ulongo signals;  \ufffd    \u201c  \u201d; /* signals/Interval */ulangs; /* voluntary Most of the data in FIGURE 5-6 is self explanatory. All times are in seconds in double precision with microsecond accuracy. The minor fault counter seems to be brokenbecause it always reports zero. The inblock andoutblock counters only refer to file system metadata for the old-style buffercache. Many processes have very short life spans. You cannot see such processes with ps, but they may be so frequent that they dominate the load on your system. The overhead of collecting accounting data is always present but isinsignificant. Accounting can be started immediately by using the acctcom command. Add some crontab entries to summarize and check the accounting logs. Collecting and checkpointing the accounting data puts a negligible additional load onto the system. The commands reported are sorted by KCOREMIN, which is the product of theamount of CPU time used and the amount of RAM used while the command wasactive. A high factor means that thiscommand hogs the CPU whenever it is running. CHARS TRNSFD counts thenumber of characters read and written.  /usr/lib/acct/monacct. og30    8   10   9   12   13   14   15   16  17  18  19  20  21  22  24  25  257617.65 688.46 16456.60 0.02 88735308080 2649maker4X 10 426182.31 43.77 5004.30 9736.27 4.38 0.01 803267592 3434wabiprog 53 355574.99 44.32 972.44 8022.87 0.05 355871360 570imagetoo 21 25 Resource Management devices (basically, local disk file system reads and writes) The underlying data that is collected can be seen in the acct (4) manual page. See Chapter 7 for more information onSRM Accounting.  /* 3-bit base-8 exponent in the high */ /* order bits, and a 13-bit fraction */ /* in the low order bits. */ /* memory usage in clicks (pages) */ /* ticks */ The Solaris Bandwidth Manager software has built-in support for Cisco NetFlow\u2122software. This feature allows for detailed network measurements that can be sent to other software packages. NetFlow-enabled devices send out NetFlow datagrams, which contain records for one or more flows. Combining multiple flow records in one datagram reducesnetwork overhead caused by NetFlow. The NetFlow FlowAnalyzer application uses the output from NetFlowFlowCollector. It provides elaborate processing, graphing, and reporting options for network analysis, planning, troubleshooting and more. There are six basic disk access patterns. Read, write, and update operations caneither be sequentially or randomly distributed. This section explains the basic Solaris softwaremeasures and discusses more complex disk subsystems. You cannot automatically tell which processes are causing disk activity. You may be able to work out where the workloadcomes from by looking at how an application was installed. When a large number ofdisks are being reported, the iostat -x variant provides extended statistics. The Starfire system supports a maximum configuration of several thousanddisk drives. When more than one type of data is stored on a disk, it\u2019s  to read because each disk is summarized on a separate line. The size of eachdisk is also growing. Solaris 2.6 has a number of new features to help solve problems. It is now possible to separate root, swap, and home directory activity. Full data is saved from the first SCSI probe to a disk. iostat -M shows throughput in Mbytes/s rather than K bytes/s for high-performance systems. Dead or missing disks can still be identified because there is no need to send themanother SCSI probe. Another option ( -n) translates disk names into a more useful form. Tapes are instrumented in the same way as disks; they appear in sar andiostatautomatically. Tape read/write operations are instrumenting with all the samemeasures that are used for disks. Rewind and scan/seek are omitted from these. The output format and options of sar(1) are fixed by the generic UNIX standard. In the Solaris 2.6release, existing iostat options are unchanged. New options that extendiostat are as follows. Solaris 2.5 includes a self-describing trace output format. A set of libraries allows user-level programs to generate trace data. The trace data helps analyze and debug multithreaded applications. Device statistics: Device Not Ready: 0 No Device: 0 Recoverable: 0Illegal Request: 0 Predictive Failure Analysis: 0. 0. 0000000%iostat -E deities. The tnfxtract routin controls probe execution for both user and kernel traces. While user-level probes canwrite to trace files, the kernel probes write to a ring buffer. This buffer scheme avoids any need tolock the data structures, so there is no performance loss. The command sequence to initiate an I/O trace is quite simple. You run thecommands as root, and you need a directory to hold the output. Thetnfdump program does quite a lot of work to sort the probe events into time order. In the other window we extracted and dumped the data to take a look at it. To really understand the data presented by iostat ,sar, and other tools, you must look at the raw data being collected. A standard disk is SCSI based and has an embedded controller. The diskdrive contains a small microprocessor and about 1 Mbyte of RAM. It can typicallyhandle up to 64 outstanding requests via SCSI tagged-command queuing. In large systems,there is another level of intelligence and buffering. The same reporting mechanism is used for client side NFSmount points and complex disk volumes setup using Solstice\u2122 DiskSuite\u2122software. In the old days, if the device driver sent a request to the disk, the disk would donothing else until it completed the request. Disks that spin fasterand seek faster have lower service times. The problem with iostat is that it tries to report the new measurements in some of the original terminology. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. The SE Toolkit uses the kstat (3K)-based data structure. The underlying metrics are cumulativecounters or instantaneous values. We need to take two copies of the above data structure together with ahigh resolution timestamp for each and do some subtraction. Ong    number$;   /* linear disk number */   string   name$;\u00a0 /* name of the device */ /* number of bytes read */\u00a0 ulonglong nread; \u00a0 /* number  of bytes written */\u00a0  longlong  wtime; /* wait queue - time spent waiting */\u00a0 longlong wlentime; /* active/run queue - sum of queue length multiplied by time at that length */}; We assume that all disk commands complete fairly quickly. We obtain the utilization or the busy time as a percentage of the total time. A similar calculation gets us the data rate in kilobytes per second. The meaning of Srunis as close as you can get to the old-style disk service time. The disk can run more than one command at a time. The data structure contains an instantaneous measure of queue length, but we want the average over the time interval. We get this from that strange \u201clength time\u201dproduct by dividing it by the busy time. The Solaris 2.6 disk instrumentation is complete and accurate. Now that it has been extended to tapes, partitions, and client NFS mount points, there much more can bedone with it. As long as a single I/O is being serviced at all times, a single queue is being used. When the device being monitored is an NFSserver, hardware RAID disk subsystem, or a striped volume, it is clearly a muchmore complex situation. All of these can process many requests in parallel. In practice, some other effects come into play. The drives optimize head movement, so that as the queue gets longer, the average service time decreases. In effect, the load oneach disk is divided by the number of disks. Using SNMP counters is a good way to get an overall view of network throughput. The Solaris software provides an SNMP daemon which provides the dat. Most networking devices support SNMP , and the SyMON software can incorporateany third-party MIBs so all links from a switch can be monitored at the same time. Many other commercial and free applications and utilities manage SNMP devices. The collision rate is (Collis / Opkts) * 100% . In our case, that is less than a tenth of a percent. Collisions are absolutely normal and should cause no concernunless collision rates become very high (in the order of 10 to 20 percent or higher) Most of these statistics are related to the Ethernet MAC and the network interfacecard hardware itself. Every network interface card typically has different counters, which canchange with the operating system releases. Sun does not officially support thenetstat option. The SE Toolkit script nx.se lists TCP as if it were an interface, with input and output segment and data rates. For interfaces that provide this information (at present, only leandhme)nx. se reports kilobytes in and out. % /opt/RICHPse/bin/se nx.se.se current. sing in the TCP/IP stack and lack of buffering on input. Defr shows thenumber of defers that took place. A defer happens when an Ethernet tries to sendout a packet, but it finds the medium to be busy. Solaris Bandwidth Manager is a tool for configuring Solaris. The main purpose of Alternate Pathing is to sustain continuous network and disk I/O when system boards are detached. Short jobs are said to backfill processors reserved for large jobs. Blacklist is a file that enables you to specify components, such as system boards, that should not be configured into the system. Class Based Queuing (CBQ) is the underlying queuing technology used in Solaris BandwidthManager. The classifier analyzes the packet protocol,ToS value, URL information, source information, destination information and allocates the packet to a class queue. E servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if highavailability is important, multiple servers are configured as siblings. The CommonInformation Model provides a conceptual framework within which it ispossible to organize information about a managed environment. The Oracle8 iDatabase Resource Manager enables the administrator to limit the degree of parallelism of any operation. DDI/DKI is specified in the \u201cWriting Device Drivers\u2019 section of the DriverDeveloper Site 1.0 AnswerBook. These are function call entrypoints that device drivers should implement in order to support DR. DDI_SUSPENDsuspends the drivers to begin the quiesce period. DDI_RESUMEresumes the drivers after the quyingce period, and so on. The goal of this group is tooffer a standard information model and directory schemas. DIMM Dual In-Line memory Module. Diff-Serv addresses network management issues related to end-to-end Quality of Service (QoS) within diverse and complex networks. The set of system resources it understands is host (by name),system architecture, operating system type, amount of memory, and CPU usage. Thus, if the external orstandard service class goal is not being met, the associated DISCs can bemanaged. Dynamic SystemDomains (DSD) Starfire independent hardware entities formed by the logical association of its system boards. rvers which allows system boards to be added (attached) orremoved (detached) from a single server or domain. LSF uses the Load Information Manager (LIM) as its resourcemonitoring tool. To modify or add load indices, an Extended Load InformationManager can be written. Fairsharescheduling divides the processing power of the LSF cluster among users. The FlowCollector aggregates this data, doespreprocessing and filtering, and provides several options to save this data todisk. Other applications such as network analyzing,planning, and billing can use these files as input. Hierarchical fairshare enables resources to be allocated to users in a hierarchical manner. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Computer-oriented local and wide area networks are normally managed usingSNMP protocols. Both products provide some visibility intowhat is happening in the computer systems on the network, but they arefocused on network topology. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. The sharedmemory area is often the largest component of a database\u2019s memoryrequirements. ISP Internet Service Provider, a company that provides Point-of-Presence access to the Internet. JTAG is an alternate communicationsinterface between the SSP machine and the Enterprise 10000 server. A special data structure that controls the dynamic growth of all non-relocatable memory. When Dynamic Reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. The Load InformationManager process running on each execution host is responsible for collectingload information. The load indices that are collected include: host status,length of run queue, CPU utilization, paging activity, available swap space,available memory, and I/O activity. The Load Share Facility is a vehicle for launching parallel applications on an HPC cluster. When the LSF softwareinitializes, one of the nodes in the cluster is elected to be the master host. maximum bandwidth The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager. The maximum bandwidth is dependent upon the percentage ofbandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. mestamp is taken on every state change, every system call, every page fault, and everyscheduler change. A database topology where a single process serves many users. NQS/Exec is geared toward a supercomputer environment. Limitedload balancing is provided as there is no concept of demand queues. There is also no control over interactivebatch jobs. PC NetLink adds file and print services, andenables Solaris servers to act as Microsoft\u00ae Windows NT\u2122 Primary DomainControllers (PDC) or Backup Domain Controllers (BDC) PC NetLink 1.0 offers many new options for utilizing hardware resources and minimizing systemadministration overhead. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. LSF provides several resource controls toprioritize the order in which batch jobs are run. Batch jobs can be scheduled torun on a first come first served basis, fair sharing between all batch jobs, andpreemptive scheduling. Process Monitor is an optional module within Sun Enterprise SyMON that can be used to view all processes on a system. The Process Monitor can also be configured topattern match and accumulate all the processes that make up a workload. Project StoreX is based on adistributed pure Java framework. It can run on servers from any vendor, interface to other storage management software, and manage any kind of attachment.  Proxy caches are used in corporate intranets and at ISPs. Solaris Bandwidth Manager provides the means to manage your network resources to provide Quality of Service to network users. receptor DSD Dynamic Reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider DSD. The same system board is then attached to a receptorDSD. The Solaris Resource Manager (SRM) ensures that a particular user or application receives its fair share of resources. It requires each hopfrom end-to-end be RSVP-enabled, including the application itself. Solaris software can provide a great deal of per-process information that is notcollected and displayed by the ps command or Sun Enterprise SyMON 2.0software. The data can be viewed and processed by a custom written processmonitor. Service Level Agreement captures expectations and interactions between end users, system managers,vendors, and computer systems. In essence, this is analogous to the SRMlnode, which effectively defines a resource management policy that can be besubscribed to. The time it takes for an I/O device to service a request can be complex tomeasure. The SolarisResource Manager (SRM) is based on ShareII. SHR Scheduler is a component of the Solaris Resource Manager (SRM) Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris Resource Manager software is the key enabler for serverconsolidation and increased system resource utilization. Service LevelAgreements can be defined and translated into Solaris Bandwidth Managercontrols and policies. Computer-oriented local and wide area networks are normally managed using SNMP protocols. Resource management is done on a per-networkbasis, often by controlling the priority of data flows. The SSP configures the Starfire hardware, through a private ethernetlink, to create domains. The SSP collects hardware logs, provides bootfunctions, and produces consoles for each domain. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 Gbytes of memory and over 20 Tbytes of disk space. Sun Enterprise SyMON 2.0 is a Java-basedmonitor with multiple user consoles that can monitor  storage hardware. System level measurements show the basic activity and utilization of the memory system and CPUs. Perprocess activity can be aggregated at a per system level then combined withnetwork measurements to measure distributed applications. Trace normal form (TNF) is a format used to implement tracing. It makes it possible to trace the execution steps of user and kernel processes. ToS is a header field contained in IP packets. Virtual memory is not directly related to physical memory usage. Virtual web hosting is often used in situations where web sites receive little or no activity most of the time.  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0WLM SeeIBM Workload Manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0XCF SeeCross-System Coupling Facility. a large geographical area. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W LM SeeIBm Workload manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 XCF See cross-system coupling facility."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_29",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: Java and the Internet",
      "source": "Prentice Hall - adrianco and Rich Pettit",
      "published_date": "1998",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_book_30",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: SPARC and Solaris",
      "source": "Prentice Hall - adrianco",
      "published_date": "1995",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_story_31",
      "kind": "story",
      "subkind": "",
      "title": "Microservices and DevOps",
      "source": "Infoq Charles Humble",
      "published_date": "7/11/2014",
      "url": "https://www.infoq.com/interviews/adrian-cockcroft-microservices-devops/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_story_32",
      "kind": "story",
      "subkind": "",
      "title": "High availability",
      "source": "Richard Seroter on Infoq",
      "published_date": "",
      "url": "https://www.infoq.com/articles/cockcroft-high-availability/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_33",
      "kind": "story",
      "subkind": "",
      "title": "Failing Over Without Falling Over",
      "source": "Stackoverflow",
      "published_date": "2020",
      "url": "https://stackoverflow.blog/2020/10/23/adrian-cockcroft-aws-failover-chaos-engineering-fault-tolerance-distaster-recovery/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_68173563",
      "kind": "file",
      "subkind": "medium_post",
      "title": "A lot of new content and detail in this swardley business scenario",
      "source": "Medium Archive",
      "published_date": "2017-01-16",
      "url": "https://medium.com/@adrianco/2017-01-16_A-lot-of-new-content-and-detail-in-this--swardley-business-scenario--cb113160abb8",
      "content": {
        "text": "A lot of new content and detail in this @swardley business scenario. looks very plausible to me. answer to the question at the end isn\u2019t obvious\u2026",
        "metadata": {
          "word_count": 26,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A lot of new content and detail in this @swardley business scenario. looks very plausible to me. answer to the question at the end isn't obvious."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d25e047d",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What I learned at GlueCon 2023 Tipping Points and Generative AI",
      "source": "Medium Archive",
      "published_date": "2023-06-02",
      "url": "https://medium.com/@adrianco/2023-06-02_What-I-learned-at-GlueCon-2023---Tipping-Points-and-Generative-AI-898cd5219040",
      "content": {
        "text": "What I learned at GlueCon 2023\u200a\u2014\u200aTipping Points and Generative AI\nThe final slide of my GlueCon keynote featuring a sunset over a pool in Maui\u200a\u2014\u200apicture by\u00a0Adrian\nI\u2019ve presented at GlueCon many times over the last decade or so. It\u2019s an unusual event, held at the end of May at an isolated hotel between Denver and Boulder Colorado, the agenda is curated by Eric Norlin, and the 2-day event is run by his wife Kimberley. The theme is whatever Eric thinks is interesting that year, but many of the same attendees and speakers return so there\u2019s a kind of tribe of friends feel to it. It\u2019s big enough to be interesting but not too big (this year had fewer attendees than some in the past). It\u2019s friendly, diverse, has good food and is very much an in-person make new friends kind of event. There are no videos of the talks, so I tend to be a bit more experimental and try out new content at GlueCon.\n\nThis year the main focus was on Generative AI, ChatGPT and the implications of all the things that are going on in that space. There were also side tracks on Web Assembly and Observability. Previous years have covered APIs (which is where the GlueCon name came from) and Robotics and whatever seemed new and interesting that year. I usually learn a lot, and this year I went with some half formed ideas about Generative AI, and a bit of time playing with ChatGPT, and had a fairly deep dive into what\u2019s happening in this space, in the talks and in the chats in-between.\n\nEric asked me to give one of the keynotes at the end of the first day, and invited local Denver people to join the conference for an AI focused meetup that included the final three keynotes, the drinks and snacks reception and some additional talks in the evening. My talk was on Innovation and Tipping Points, the first half was based on some content I\u2019ve given before on how to get out of the way of innovation by speeding up time to value or idea to implementation. The second half was an exploration of some tipping points, when something that was expensive becomes cheap or something that was cheap becomes expensive. You need to be able to innovate fast enough to pivot or reinvent your business model and leverage the change. One example is that Netflix launched it\u2019s streaming service in 2007, just at the point when the cost of streaming a movie over the network (which was dropping fast) became less than the cost of shipping a DVD. Netflix streaming content was relatively cheap to license when Netflix was small, but when it got to have more subscribers than the biggest cable TV operators the licenses hit a tipping point and cost more than making the content in the first place, and Netflix pivoted to making it\u2019s own original content, and created a major new movie studio from scratch. Working from home is another tipping point, enabled by widespread deployment of laptops, home internet with capacity to run video (thanks in part to Netflix and other streaming services), and distributed productivity tools, then kicked over the line by the COVID-19 lockdown. My view on this is that execs who now want remote workers back in the office would be better off spending their time focused on improving their remote collaboration tools and culture, scheduling occasional concentrated in-person events, and cutting back on real-estate spending as fast as they can.\n\nThe maturing of various AI technologies has also hit some tipping points and is evolving extremely quickly week by week. The purpose of my talk was to get people to understand how fast they could innovate, and to use prior examples as patterns to detect and jump on emerging tipping point opportunities. Here\u2019s my initial guesses at where these may be:\n\nMost people are focusing on ChatGPT, Github Copilot and similar conversational AI tools, which have have become competitive with the average expert or developer, have very broad general knowlege, are remarkably good at responding to prompts, but which don\u2019t have any sense of purpose, agency, or their environment and context, have a high error rate, hallucinate, and are often working on old information. In my talk I brought up a different development thread of AI in the self-driving car context, and in particular Tesla FSD beta, which I have been using for the last few months. If you haven\u2019t seen it in action, you should watch the latest video by @AIDRIVR. FSD has a sense of purpose, a planning capability, has real time agency and responds to its environment via an ego model, predicting the behavior of pedestrians and other road users, and is being tuned to drive in a very human way, so that other road users interact with it as a predicable normal driver. FSD isn\u2019t finished yet, but like ChatGPT\u2019s exam results, it\u2019s better than the average human in most situations, and it\u2019s getting better quickly. There are other self-driving car developments that may be more advanced in some ways, but Tesla FSD beta has deployed to a few hundred thousand cars, so it\u2019s progress is much more publicly visible, and the release notes give some insight into how it\u2019s capabilities are structured.\n\nOne area where generative AI is already very competitive is the creative arts, generating images, artwork, essays, poems, advertising copy etc. where there is no wrong answer, and what is good or not is highly subjective. This is disrupting journalism, education, marketing, and search engine optimization, as well as causing concerns around deep fake images. There are already a few hundred AI authored books that have been published. There was a good talk on How to Build Responsible Systems While Leveraging Generative AI Capabilities by Sriram Subramanian of Microsoft, and I had several interesting conversations with him.\n\nThe current sweet spot for tools like ChatGPT is where there is a lot of documentation and consensus around a subject. As Uwe Friedrichsen discusses in his blog post ChatGPT Already Knows, the value of detailed knowledge used to be high, but is now being commoditized. This tipping point is going to impact process oriented \u201ccomplicated\u201d jobs like how to use programming languages and web services first, but is spreading to include \u201ccomplex adaptive system\u201d operations like driving cars over time. We\u2019re already seeing the beginning of this direction with conversational programming and operations tooling like DoTheThing.ai by CtrlStack who were exhibiting at GlueCon. Problems occur when the subject moves away from the sweet spot \u201cinto the weeds\u201d and ChatGPT starts to confidently hallucinate output. Some of the talks at GlueCon were about techniques for adding domain knowledge via re-training, prompting or plug-ins to maintain quality output.\n\nSome of the things that were hard but will become easy are personalized art, music and stories (generated, not just selected like Spotify or Pandora). I also think that it may become easier to adopt new languages, frameworks, APIs and other tools if we can train popular AI platforms to know when and how to use them for specific purposes, rather than having to run a developer relations and marketing team at scale. On the other hand the sheer volume of content for popular ecosystems like Python may squeeze out alternatives. There\u2019s some kind of Generative AI Optimization (GAIO) that\u2019s going to become important, like how Search Engine Optimization (SEO) changed the way web pages were constructed to optimize for web crawlers feeding search engines. We may even see road signs and intersection layouts optimized to make them work better for self driving cars, as the proportion of self driving cars increases on the roads.\n\nI went to several talks and snapped pictures of a few slides that looked interesting. Here\u2019s a list of models that helps make sense of some of the terminology that\u2019s floating around.\n\nCtrlStack are building AI driven DevOps automation tools and they showed a few examples.\n\nSriram Subramanian gave a talk in the evening meetup that provided a useful summary of the landscape.\n\nRob Hirschfeld of RackN had this perspective on the impact of AI on his domain of infrastructure automation.\n\nTristan Zajonc, CEO of Continual.ai talked about using generative AI to build products.\n\nRussell Kaplan of Scale AI talked about prompt engineering. My opinion is that prompt engineering is a short term problem, over time the systems we actually use will be pre-loaded with prompts and have conversations with us to build the goals and contexts that we\u2019re currently injecting via prompts.\n\nJoe Shockman of Grounded AI and Allen Romano of Logoi talked about how to prevent hallucinations.\n\nThey also provided a useful link to some references on the subject.\n\nChai Atreya of Alteryx did an impressive live demo of using ChapGPT as a guide to developing some analysis in a Python notebook. Starting with raw data in a file, and some idea of what outcome he wanted, ChatGPT was able to provide guidance and code to paste into the notebook that did a good job.\n\nFinal thoughts\u2026 this blog post took me a week to finish writing, and it\u2019s already out of date. Developments in generative AI are moving faster than most people who are part of it can keep up with, so beware of any claims you see about this area, and respond by asking how many weeks old that claim is. In particular, it appears that the trend to open source models is accelerating, and they are getting better more quickly than the well known ChatGPT, Bing and Bard services. Another trend is that even though the size of models is increasing, the training costs are coming down by orders of magnitude, and by starting with an open source model, good results can be obtained by anyone with a small amount of hardware. If you also think things are moving fast now, then be ready for an acceleration, Over the last few months VC firms have invested billions of dollars in this space, and the results of that infusion haven\u2019t really played out yet.\n\nI think that people saying that ChatGPT et. al. aren\u2019t really intelligent, will have to revise what they mean by intelligent, because human intelligence is continually re-defined as whatever computers haven\u2019t got good at doing yet. When the kind of capabilities that FSD has are combined with generative AI, it would have a goal, a plan to get there, an ego model for it\u2019s own safety and actions, model the egos it\u2019s interacting with, and humanize those interactions. We are going to have to get used to the idea that the entities we are interacting with, whether via email, social media, news and entertainment, or on the roads are increasingly AI operated, and we won\u2019t be able to tell. Some kind of mandatory labeling seems to be needed. There\u2019s talk of regulation, but it\u2019s clear that politicians don\u2019t understand this area well enough to address the issues successfully. Finally here\u2019s a labelling idea I came up with\u200a\u2014\u200amaybe we should make cars flash their hazard warning lights slowly when they are being self driven, so that other drivers can tell?",
        "metadata": {
          "word_count": 1868,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "GlueCon is an unusual event, held at the end of May at an isolated hotel between Denver and Boulder Colorado. The agenda is curated by Eric Norlin, and the 2-day event is run by his wife Kimberley. It\u2019s big enough to be interesting but not too big (this year had fewer attendees than some in the past) This year's GlueCon was focused on Generative AI. Previous years have covered APIs and Robotics. I usually learn a lot, and this year I went with some half formed ideas. Netflix pivoted to making it\u2019s own original content, and created a major new movie studio from scratch. Working from home is another tipping point, enabled by widespread deployment of laptops and home internet. The maturing of various AI technologies has also hit some tipping points and is evolving extremely quickly week by week. Most people are focusing on ChatGPT, Github Copilot and similar conversational AI tools, which have become competitive with the average expert or developer. Tesla FSD beta has deployed to a few hundred thousand cars. FSD has a sense of purpose, a planning capability and real time agency. It responds to its environment via an ego model, predicting the behavior of pedestrians and other road users. The current sweet spot for tools like ChatGPT is where there is a lot of documentation and consensus around a subject. The value of detailed knowledge used to be high, but is now being commoditized. There are already a few hundred AI authored books that have been published. Some of the things that were hard but will become easy are personalized art, music and stories (generated, not just selected like Spotify or Pandora) It may become easier to adopt new languages, frameworks, APIs and other tools if we can train popular AI platforms to know when and how to use them for specific purposes. Generative AI is going to become important, like how Search Engine Optimization (SEO) changed the way web pages were constructed to optimize for web crawlers feeding search engines. We may even see road signs and intersection layouts optimized to make them work better for self driving cars. Chai Atreya of Alteryx did an impressive live demo of using ChapGPT to develop some analysis in a Python notebook. Joe Shockman of Grounded AI and Allen Romano of Logoi talked about how to prevent hallucinations. The trend to open source models is accelerating, and they are getting better more quickly than the well known ChatGPT, Bing and Bard services. Even though the size of models is increasing, the training costs are coming down by orders of magnitude. We are going to have to get used to the idea that the entities we are interacting with, whether via email, social media, news and entertainment, or on the roads are increasingly AI operated. Some kind of mandatory labeling seems to be needed. Maybe we should make cars flash their hazard warning lights slowly when they are being self driven."
      },
      "tags": [
        "ai",
        "automation",
        "devops",
        "engineering",
        "infrastructure",
        "netflix",
        "observability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_b0c0764b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "How to finish writing a book by Adrian Cockcroft Originally posted 15th March 2006",
      "source": "Medium Archive",
      "published_date": "2016-06-06",
      "url": "https://medium.com/@adrianco/2016-06-06_How-to-finish-writing-a-book---by-Adrian-Cockcroft---Originally-posted-15th-March-2006-5b5fc629993b",
      "content": {
        "text": "How to finish writing a book | by Adrian Cockcroft | Originally posted 15th March\u00a02006\n\nI\u2019ve written four books, and several years ago I developed \u201cCockcroft\u2019s law of book writing\u201d. This states that a book will grow in size as you write it, and that the number of pages left to write will increase as you write. This seems counter-intuitive, but it has been confirmed many times in practice. I hope this posting provides some useful advice for writers, and helps people finish what they have started.\n\nTo make a concrete example, let\u2019s say you decide to write a book and you come up with an outline that adds up to 200 pages. You start work and write 50 pages, then, when you revisit your outline to update the page count estimates, you find that they now add up to 300 pages. You wrote more than you expected to cover each subject, and discovered more subjects that needed to be discussed. The essential problem here is that there are now 300\u201350 = 250 pages left to go. Before you started you only had 200 pages left to go.\n\nThis problem is recursive, if you write another 50 pages you will find that you have now written the first 100 pages of a 400 page book, and you now have 300 pages left to write. This explains why there are so many people who have written part of a book, but never finished it.\n\nThe aproach I took in writing my later books was to maintain a spreadsheet that tracks the pages left to write or edit, update it very regularly, and generate a plot with a trend line from the data. You can then see when (or if) you will finish the book. In order to get the trend line to target a specific delivery date, you have to force the number of pages left to go down. You do this by writing pages that you promise never to edit again, and by deleting whole sections and chapters. I deleted three entire chapters from one of my books to get it finished.\n\nAnother problem you can run into is that the content you wrote at the start of the process is less well written than later content, so you think you have finished, re-read parts of the book that were finished ages ago, and discover that it needs a complete rewrite.\n\n[Updated section] I often get asked if I will write another book, and I\u2019m not planning to. It\u2019s definitely worth having a well regarded book on your resume, but having more books doesn\u2019t add much to your career prospects unless you can write them efficiently and they sell enough to become a significant source of income.\n\nOriginally published at perfcap.blogspot.com",
        "metadata": {
          "word_count": 462,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Cockcroft\u2019s law of book writing states that a book will grow in size as you write it. This seems counter-intuitive, but it has been confirmed in practice. I hope this posting provides some useful advice for writers. If you write another 50 pages you will find that you have now written the first 100 pages of a 400 page book, and you now have 300 pages left to write. This explains why there are so many people who have written part of a book, but never finished it. I often get asked if I will write another book, and I\u2019m not planning to. It\u2019s definitely worth having a well regarded book on your resume, but having more books doesn\u2019t add much to your career prospects."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_be70d098",
      "kind": "file",
      "subkind": "medium_post",
      "title": "AWS re Invent 2020 talks related to sustainability",
      "source": "Medium Archive",
      "published_date": "2020-12-01",
      "url": "https://medium.com/@adrianco/2020-12-01_AWS-re-Invent-2020-talks-related-to-sustainability-d241fe18b6ba",
      "content": {
        "text": "AWS re:Invent 2020 talks related to sustainability\nPhoto taken by Adrian\u200a\u2014\u200aOrange mid-day sky at San Gregorio Beach\u200a\u2014\u200asmoky from wildfires\nI\u2019ve recently become involved with the new Linux Foundation Open Source Climate Finance organization (OS-Climate). I\u2019m helping manage AWS contributions to the project, as we build an open source data lake and analysis service that can be used to model climate related asset risks for investors. This includes physical asset risk from things like sea level rise and extreme weather events, as well as market changes from customers deciding to buy more sustainable products, and economic risk as the world economy is impacted by climate change. This is becoming topical as governments and public companies around the world are looking for efficient and standardized ways to report their sustainability impact, and investors and asset managers are looking for common datasets and models to base their risk analysis on.\n\nAWS is supporting customers who are working to change the way they operate and build more sustainable products. I\u2019m also interested in ways that we can optimize cloud architectures to reduce their carbon footprint. There are several talks at this year\u2019s free online re:Invent that are related to sustainability, and it\u2019s a topic I will cover in my talk on Dec 17th.\n\nHere are all the sustainability related sessions:\n\nLinks from the Talk ID go to the first session chronologically, but the main pages do list alternate times (each timeslot has a unique url). I\u2019ve sorted by date of first broadcast, and sessions will be available at the end of each week to stream on-demand.\n\nAlready Shown\n\nZWL207\u200a\u2014\u200aEmpowering entrepreneurs through eco-initiatives\n\nAIM254-G\u200a\u2014\u200aDesbloqueando dados do espa\u00e7o para resolver desafios do planeta Terra (Unlocking space data to solve challenges on planet Earth)\n\nZWL201\u200a\u2014\u200aScaling sustainable community development with tech\n\nZCW206\u200a\u2014\u200acharity: water and Twisthink keep water flowing with AWS IoT\n\nZBF206\u200a\u2014\u200aAutomating wind farm maintenance using drones and AI\n\nAUT301\u200a\u2014\u200aAlexa, charge my car! Test drive the Accenture EV roaming platform\n\nThursday Dec 3rd\n\nWPS210\u200a\u2014\u200aUsing open data for sustainable agriculture\n\nAES22\u200a\u2014\u200aClimate Pledge Arena: The future of innovation and sustainability\n\nTuesday Dec 8th\n\nWPS206\u200a\u2014\u200aDetecting extreme weather events from space\n\nThursday Dec 10th\n\nPNU203\u200a\u2014\u200aFighting wildfire with artificial intelligenceFriday Dec 11th\n\nIOT205\u200a\u2014\u200aHow Carrier is transforming the cold chain to reduce food & medicine waste\n\nWednesday Dec 16th\n\nIOT207\u200a\u2014\u200aAWS at the edge: Using AWS IoT to optimize Amazon wind farms\n\nThursday Dec 17th\n\nARC213\u200a\u2014\u200aAdrian Cockcroft\u2019s architecture trends and topics for 2021\n\nHere\u2019s the youtube video of my talk.",
        "metadata": {
          "word_count": 435,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS is building an open source data lake and analysis service that can be used to model climate related asset risks for investors. This includes physical asset risk from things like sea level rise and extreme weather events. There are several talks at this year\u2019s free online re:Invent that are related to sustainability. I\u2019m also interested in ways that we can optimize cloud architectures to reduce their carbon footprint. Here are all the sustainability related sessions. Here\u2019s the youtube video of my talk. The talk was held at the Climate Pledge Arena in London. It was on the future of innovation and sustainability."
      },
      "tags": [
        "ai",
        "architecture",
        "artificial intelligence",
        "aws",
        "cloud",
        "platform",
        "scaling",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_62492de5",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Excellent patterns for enterprises to move fast and leverage cloud are being developed by Drew",
      "source": "Medium Archive",
      "published_date": "2016-11-09",
      "url": "https://medium.com/@adrianco/2016-11-09_Excellent-patterns-for-enterprises-to-move-fast-and-leverage-cloud-are-being-developed-by-Drew--ba18867d8973",
      "content": {
        "text": "Excellent patterns for enterprises to move fast and leverage cloud are being developed by Drew Firment and team. Impressive progress.",
        "metadata": {
          "word_count": 20,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Drew Firment and team are developing patterns for enterprises to move fast and leverage cloud. Impressive progress. Excellent patterns are being developed by Drew Firment's team."
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_3ce9e841",
      "kind": "file",
      "subkind": "medium_post",
      "title": "So many bad takes What is there to learn from the Prime Video microservices to monolith story",
      "source": "Medium Archive",
      "published_date": "2023-05-06",
      "url": "https://medium.com/@adrianco/2023-05-06_So-many-bad-takes---What-is-there-to-learn-from-the-Prime-Video-microservices-to-monolith-story-4bd0970423d4",
      "content": {
        "text": "So many bad takes\u200a\u2014\u200aWhat is there to learn from the Prime Video microservices to monolith\u00a0story\nExcerpt from Serverless First deck first published in\u00a02019\nThe Prime Video team published this story: Scaling up the audio/video monitoring service and reducing costs by 90%, and the internet piled in with opinions and bad takes, mostly missing the point. What the team did follows the advice I\u2019ve been giving for years (here\u2019s a video from 2019):\n\u201cWhere needed, optimize serverless applications by also building services using containers to solve for lower startup latency, long running compute jobs, and predictable high traffic\u201d\nThe Prime Video team had followed a path I call Serverless First, where the first try at building something is put together with Step Functions and Lambda calls. They state in the blog that this was quick to build, which is the point. When you are exploring how to construct something, building a prototype in a few days or weeks is a good approach. Then they tried to scale it to cope with high traffic and discovered that some of the state transitions in their step functions were too frequent, and they had some overly chatty calls between AWS lambda functions and S3. They were able to re-use most of their working code by combining it into a single long running microservice that is horizontally scaled using ECS, and which is invoked via a lambda function. This is only one of many microservices that make up the Prime Video application. The problem is that they called this refactoring a microservice to monolith transition, when it\u2019s clearly a microservice refactoring step, and is exactly what I recommend people do in my talks about Serverless First. I don\u2019t advocate \u201cServerless Only\u201d, and I recommended that if you need sustained high traffic, low latency and higher efficiency, then you should re-implement your rapid prototype as a continuously running autoscaled container, as part of a larger serverless event driven architecture, which is what they did. If you built it as a microservice to start with, it would probably take longer (especially as you have to make lots of decisions about how to build and run it), and be less able to iterate as you figure out exactly what you are trying to build.\nExcerpt from Serverless First deck published in\u00a02019\nIn contrast to commentary along the lines that Amazon got it wrong, the team followed what I consider to be the best practice. The result isn\u2019t a monolith, but there seems to be a popular trigger meme nowadays about microservices being over-sold, and a return to monoliths. There is some truth to that, as I do think microservices were over sold as the answer to everything, and I think this may have arisen from vendors who wanted to sell Kubernetes with a simple marketing message that enterprises needed to modernize by using Kubernetes to do cloud native microservices for everything. What we are seeing is a backlash to that messaging, and a realization that the complexity of Kubernetes has a cost, which you don\u2019t need unless you are running at scale with a large team. Ironically, many enterprise workloads are intermittent and small scale and very good candidates for a serverless first approach using Step Functions and Lambda. See The Value Flywheel Effect book for more on serverless first, and read Sam Newman\u2019s Building Microservices: Desiging Fine-Grained Systems book to get the best practices on when and how to use the techniques to effectively build, manage and operate this way. His first edition in 2015 was foundational, and he updated it in 2021 with a second edition. He is also clear about when microservices aren\u2019t useful.\n\nFinally, what were they building? A real-time user experience analytics engine for live video, that looked at all users rather than a subsample. This is a very good thing to have, in fact Netflix built in monitoring for all users at the start of it\u2019s streaming launch in 2007, and it was the very first workload that moved to AWS in 2009. Now that Netflix has also added live broadcasts, I assume they\u2019ve extended their own capabilities to do something similar to what Prime Video describes. If you happen to be running a video streaming service and don\u2019t have real time user experience monitoring built in to your architecture, I suggest you take a look at Datazoom.io which provides this as a service and where the chief architect and CTO are both ex-Netflix colleagues of mine. So maybe the answer to the question of whether to build with microservices or a monolith is neither, you should be calling an existing service rather than rolling your own.",
        "metadata": {
          "word_count": 780,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Prime Video team published this story: Scaling up the audio/video monitoring service and reducing costs by 90%. The internet piled in with opinions and bad takes, mostly missing the point. What the team did follows the advice I\u2019ve been giving for years (here\u2019s a video from 2019): The Prime Video application was refactored from a microservice to a monolith. This is exactly what I recommend people do in my talks about Serverless First. I don't advocate \u201cServerless Only\u201d, and I recommended that if you need sustained high traffic, low latency and higher efficiency, then you should re-implement your rapid prototype as a continuously running autoscaled container. In contrast to commentary along the lines that Amazon got it wrong, the team followed what I consider to be the best practice. The result isn\u2019t a monolith, but there seems to be a popular trigger meme about microservices being over-sold, and a return to monoliths. Many enterprise workloads are intermittent and small scale and very good candidates for a serverless first approach using Step Functions and Lambda. Read Sam Newman\u2019s Building Microservices: Desiging Fine-Grained Systems. If you're running a video streaming service and don't have real time user experience monitoring built in to your architecture, I suggest you take a look at Datazoom.io. The chief architect and CTO are both ex-Netflix colleagues of mine."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "cloud native",
        "containers",
        "kubernetes",
        "microservices",
        "monitoring",
        "netflix",
        "scaling",
        "serverless",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_60d19aa7",
      "kind": "file",
      "subkind": "medium_post",
      "title": "COVID 19 Hazard Analysis using STPA",
      "source": "Medium Archive",
      "published_date": "2020-03-18",
      "url": "https://medium.com/@adrianco/2020-03-18_COVID-19-Hazard-Analysis-using-STPA-3a8c6d2e40a9",
      "content": {
        "text": "COVID-19 Hazard Analysis using\u00a0STPA\nMemorial at Hawaiian native burial ground, Kapalua, Maui. Picture taken by\u00a0Adrian\nMarch 17, 2020\n\nA resilient system continues to operate successfully in the presence of failures. There are many possible failure modes, and each exercises a different aspect of resilience. The system needs to maintain a safety margin that is capable of absorbing failure via defense in depth, and failure modes need to be prioritized to take care of the most likely and highest impact risks. In addition to the common financial calculation of risk as the product of probability and severity, engineering risk includes detectability. Failing silently represents a much bigger risk than when the same failure is clearly and promptly reported as an incident. Hence, one way to reduce risk is to make systems more observable. Another problem is that a design control, intended to mitigate a failure mode, may not work as intended. Infrequent failures exercise poorly tested capabilities that tend to amplify problems in unexpected ways rather than mitigate them, so it\u2019s important to carefully exercise the system to ensure that design controls are well tested and operating correctly. Staff should be familiar with recovery processes and the behavior of the system when it\u2019s working hard to mitigate failures. A learning organization, disaster recovery testing, game days, and chaos engineering tools are all important components of a resilient system.\n\nIt\u2019s also important to consider capacity overload, where more work arrives than the system can handle, and security vulnerabilities, where a system is attacked and compromised.\n\nThe opening paragraph above is the same as my previous discussion focused on hardware, software and operational failure modes, but we are now in the middle of a pandemic, so I\u2019m going to adapt the discussion of hazards and failure modes to our current situation.\n\nThere are many possible failure modes, and since they aren\u2019t all independent, there can be a combinatorial explosion of permutations, as well as large scale epidemic failures to consider. While it\u2019s not possible to build a perfect system, here are two good tools and techniques that can focus attention on the biggest risks and minimize impact on successful operations.\n\nThe first technique is the most generally useful. Concentrate on rapid detection and response. In the end, when you\u2019ve done everything you can do to manage failures you can think of, this is all you have left when that virus that no-one has ever seen before shows up. Figure out how much delay is built into your observability system. Try to measure your mean time to respond (MTTR) for incidents. If your system is mitigating a small initial problem, but it\u2019s getting worse, and your team responds and prevents a larger incident from happening, then you can record a negative MTTR, based on your estimate of how much longer it would have taken for the problem to consume all the mitigation margin. It\u2019s important to find a way to record \u201cmeltdown prevented\u201d incidents, and learn from them, otherwise you will eventually drift into failure [Book: Sydney Decker\u200a\u2014\u200aDrift into Failure]. Systems that have an identifiable capacity trend, have a \u201ctime to live\u201d (TTL) that can be calculated. Sorting by TTL identifies the systems that need attention first and can help focus work during a rapid response to a problem.\n\nThe second technique starts with the system constraints that need to be satisfied to maintain safe and successful operation and works in a top down manner using System Theoretic Process Analysis (STPA), or the more specific technique System Theoretic Accident Model and Processes (STAMP). [Book: Engineering a Safer World by Nancy G. Leveson]. STPA is based on a functional control diagram of the system, and the safety constraints and requirements for each component in the design. A common control pattern is divided into three layers, the business function itself, the control system that manages that business function, and the human operators that watch over the control system. The focus is on understanding the connections between components and how they are affected by failures. In essence in a \u201cboxes and wires\u201d diagram most people focus on specifying the boxes and their failure modes, and are less precise about the information flowing between boxes. With STPA there is more focus on the wires, what control information flows across them, and what happens if those flows are affected. There are two main steps: First identify the potential for inadequate control of the system that could lead to a hazardous state, resulting from inadequate control or enforcement of the safety constraints. These could occur if a control action required for safety is not provided or followed; an unsafe control is provided; a potentially safe control action is provided too early, too late or in the wrong sequence; or a control action required for safety is stopped too soon or applied for too long. For the second step each potentially hazardous control action is examined to see how it could occur. Evaluate controls and mitigation mechanisms, looking for conflicts and coordination problems. Consider how controls could degrade over time, including change management, performance audits and how incident reviews could surface anomalies and problems with the system design.\n\nThe criticality and potential cost of each failure mode is context dependent, and drives the available time and budget for prioritized mitigation plans. The entire resiliency plan needs to be dynamic, and to incorporate learnings from each incident, whether or not the failure has noticeable customer impact.\n\nApplying this concept to a pandemic, the system we are controlling is the spread of infection in the human population, and the capacity of the healthcare system to triage the people who get sick. The control system attempts to detect the spread, applies rules to societies to limit human interactions that communicate disease, and allocates resources to provide capacity to the healthcare system. The human operator layer is the government and politicians who allocate controls and resources.\n\nSTPA Model for\u00a0COVID-19\n\nOne of the models shown in the book is our starting point, showing the controlled process (data plane), the automated controller (control plane), and the human controller (who is looking at metrics to decide if the system is working or needs intervention).\n\nIf we change this model for the pandemic situation, the government is the human controller, their rules, laws and funding priorities are the automated controller, and the spread of the virus through the population and it\u2019s treatment are the controlled process.\n\nThe hazards in this situation are that the government could do something that makes it worse instead of better. They could do nothing, because they hope the problem will go away on its own. They could freak out at the first sign of a virus and take drastic actions before it is needed. They could take actions too late, after the virus has been spreading for a while, and is harder to control. They could do things in the wrong order, like developing a custom test for the virus rather than using the one that\u2019s already available. They could take a minimal action, not enough to stop the spread of the virus, and assuming it\u2019s fixed. They could spend too long deciding what to do. They could get into internal arguments about what to do, or multiple authorities could make different or incompatible changes at once. The run-book of what to do is likely to be out of date (see studies of Spanish Flu) and contain incorrect information about how to respond to the problem in the current environment.\n\nEach of the information flows in the control system should be examined to see what hazards could occur. In the monitoring flows, the typical hazards are a little different to the control flows. In this case, the sensor that reports infection counts could stop reporting, and get stuck on the last value seen (like the CDC report every weekend). It could report zero infections, even though people are still being infected. The data could be corrupted by political interference and report an arbitrary value. Readings could be delayed by different amounts so they are seen out of order. The update rate could be set too high so that the people can\u2019t keep up with the latest news. Updates could be delayed so that the monitoring system is showing out of date status, and the effect of control actions aren\u2019t seen soon enough. This often leads to over-correction and oscillation in the system, which is one example of a coordination problem. Sensor readings may degrade over time, especially between pandemics, when there is little attention being paid to the problem.\n\nThe STPA three level control structure provides a good framework for asking questions about the system. Is the model of the controlled process looking at the right metrics and behaving safely? What is the time constant and damping factor for the control algorithm, will it oscillate, ring or take too long to respond to inputs? How is the government expected to develop their own models of the controlled process and the automation, and understand what to expect when they make control inputs? How is the user experience designed so that the government is notified quickly and accurately with enough information to respond correctly, but without too much data to wade through or too many false alarms?\n\nWhat Happens\u00a0Next?\n\nI think this discussion provides a high level model for understanding what is happening now. The essential component of a control system is a low latency and dependable way to measure the thing we are trying to control. This corresponds to the WHO guidelines, and the relatively successful policy of mass testing shown in South Korea in particular. It also shows why the UK and USA response of limited testing means that the pandemic is literally \u201cout of control\u201d in those countries. The UK\u2019s short-lived policy of Herd Immunity was based on a bad model of the controlled process, where they hadn\u2019t taken into account the expected death rate in the short term, and the lack of capacity in the healthcare system. Until the South Korean approach of mass testing is implemented globally, we won\u2019t be able to control COVID-19.\n\nWe can expect pandemics to recur every few years, and this one is bad enough to setup some long term changes in the system that should provide resilience to new viruses and to recurrence of existing ones. One way to operate a global economy in the presence of viral pandemics is to have testing be a continuous part of everyone\u2019s life and to be a gate on movement of people, even when there isn\u2019t a pandemic. So in order to get on a scheduled airline flight or possibly event to attend a large public event you would have to take a test to show you aren\u2019t carrying any of the known viruses that are bad enough to kill people. That could include the flu, but might not include the common cold. The cost per test for testing in huge volume can be driven down to a very low level over time.\n\nThe blanket application of shelter in place rules, as are in place in Santa Clara county (where I\u2019m currently writing this) is currently affecting millions of people, with a few hundred confirmed cases. If everyone was tested regularly, then we could find the actual people who should be isolated and the rest of us would be confident that we aren\u2019t spreading the virus, and get on with our lives. The social and financial costs of the shutdown are big enough that the ongoing blanket testing alternative may end up looking like a good deal.",
        "metadata": {
          "word_count": 1934,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A resilient system continues to operate successfully in the presence of failures. Failing silently represents a much bigger risk than when the same failure is clearly and promptly reported as an incident. Infrequent failures exercise poorly tested capabilities that tend to amplify p There are many possible failure modes, and since they aren't all independent, there can be a combinatorial explosion of permutations. Staff should be familiar with recovery processes and the behavior of the system when it\u2019s working hard to mitigate failures. Focus on rapid detection and response. Figure out how much delay is built into your observability system. Try to measure your mean time to respond (MTTR) for incidents. Systems that have an identifiable capacity trend, have a \u201ctime to live\u201d (TTL) that can be calculated. Sorting by TTL identifies the systems that need attention first. In a \u201cboxes and wires\u201d diagram most people focus on specifying the boxes and their failure modes. With STPA there is more focus on the wires, what control information flows across them. understanding the connections between components and how they are affected by failures. The criticality and potential cost of each failure mode is context dependent, and drives the available time and budget for prioritized mitigation plans. The entire resiliency plan needs to be dynamic, and to incorporate learnings from each incident. The hazards in this situation are that the government could do something that makes it worse instead of better. They could freak out at the first sign of a virus and take drastic actions before it is needed. Or they could take actions too late, after the virus has been spreading for a while. Each of the information flows in the control system should be examined to see what hazards could occur. In this case, the sensor that reports infection counts could stop reporting, and get stuck on the last value seen. The data could be corrupted by political interfere. The STPA three level control structure provides a good framework for asking questions about the system. Sensor readings may degrade over time, especially between pandemics, when there is little attention being paid to the problem. The UK\u2019s short-lived policy of Herd Immunity was based on a bad model of the controlled process. The essential component of a control system is a low latency and dependable way to measure the thing we are trying to control. We can expect pandemics to recur every few years, and this one is bad enough to setup some long term changes in the system that should provide resilience to new viruses and to recurrence of existing ones. The cost per test for testing in huge volume can be driven down to a very low level over time. The social and financial costs of the shutdown are big enough that the ongoing blanket testing alternative may end up looking like a good deal. ting millions of people, with a few hundred confirmed cases. If everyone was tested regularly, then we could find the actual people who should be isolated."
      },
      "tags": [
        "automation",
        "engineering",
        "metrics",
        "monitoring",
        "observability",
        "performance",
        "resilience",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_83e1d973",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Update Experiences renting electric cars from Hertz",
      "source": "Medium Archive",
      "published_date": "2023-04-25",
      "url": "https://medium.com/@adrianco/2023-04-25_Update--Experiences-renting-electric-cars-from-Hertz-c662d28d55cc",
      "content": {
        "text": "Update: Experiences renting electric cars from\u00a0Hertz\nPolestars and a few Teslas at\u00a0Heathrow\nHertz recently started to rent electric cars, and we\u2019ve tried them out a few times. Most people are used to renting cars, but renting electric cars is a fairly new thing, so I\u2019m sharing to help encourage people to try it out and to hopefully smooth the way with some learning and feedback ideas.\n\nWe\u2019re experienced electric car owners, and early adopters. We leased a Nissan Leaf 2011\u20132014, bought a 2010 Tesla Roadster in 2013, leased a Fiat 500e 2014\u20132017, and bought a Tesla Model 3 Performance in early 2020. We\u2019ve also driven a friend\u2019s 2015 Tesla Model S on some long trips.\n\nSummer 2022 we rented a car in the UK for a month from National/Enterprise, and got upgraded to an Audi A6 plug-in hybrid. It didn\u2019t come with any charging cables, or charge activation cards, and I only managed to get it plugged in and charging once during our trip. Several times I found chargers that I couldn\u2019t use without a physical charge card that had to be sent to a UK address in advance. There\u2019s a mix of chargers with captive cables, and ones where you have to bring your own cable, in the UK.\n\nOur first electric rental from Hertz was on a short trip to Honolulu in early 2023 to see some friends. We knew we weren\u2019t going to do many miles, so had no range anxiety, but wanted the flexibility to drive around a bit. The Hertz site showed a picture of the Tesla Model 3 when I was booking, but when we turned up we got a Chevy Bolt. It was easy to figure out and drive, we used AirPlay to navigate, and the apartment complex we were staying in had a charger. Since then Chevy have said they will drop AirPlay in the future, but I think it\u2019s a bad idea for the rental car market. Plugging in a phone make it far easier to manage a car that you don\u2019t own. The Bolt seems like a good lower cost rental option, it feels more like a normal car, and reminded us of our Nissan Leaf.\n\nOur second rental was in Portugal in April 2023 for ten days. There\u2019s very good electric charging infrastructure in Portugal. We rented at Lisbon airport and drove to Sintra and Porto and back, so we wanted a good range. The booking guaranteed a Tesla Model 3, and it said it would be the long range version. I then got repetitive emails from Hertz every few days for weeks, trying to explain how to drive a Tesla. When we were picking it up at Lisbon Airport, the agent offered to switch to a Model Y, but we prefer the Model 3. When I asked, they also said it would be fully charged, we could return it empty (we wouldn\u2019t be charged to refill), and that we should be able to get our phone\u2019s Tesla apps to connect to it. When we got to the car itself we found it was a Model 3 Performance (maybe that\u2019s why they tried to get us into a Model Y?) and was about 50% charged. After dropping off our bags and things at the hotel, I found a 50KW charging station, in a fairly remote place near Sintra, and topped the car up for an hour or so using the Shell Recharge card they provided. I think they said the card came with 20 Euros, and any extra charges would be on our bill at the end. It turns out this wasn\u2019t really needed, but at the time I wanted to be sure we had a full battery for our drive to Porto.\n\nWe were given a card key, which is much less convenient than using the Tesla phone app, and there was no opportunity to add the car to our own Tesla apps on a temporary basis. This is the biggest thing that Tesla and Hertz need to fix in my opinion. (Update: Tesla fixed this for the US market in September 2023 https://electrek.co/2023/09/19/tesla-builds-in-car-software-car-rental-hertzs/\u200a\u2014\u200ait works well, there\u2019s a QR code on the car screen when you get in and that adds the car to your Tesla app for the duration of the rental with no hassles) The car itself was set in \u201cchill mode\u201d when we got it, which is probably a good idea for inexperienced drivers, but it was easy to setup the same as our own car in sport mode. The biggest annoyance is that it didn\u2019t have autopilot enabled, and it felt really dumb compared to our own autopilot/FSD enabled car. It did have the basic cruise and lane control needed to follow the car in front, but this can\u2019t handle curves well, and needed a lot more handholding than autopilot. I think Tesla should use the same autopilot/FSD code for cruise control and lane following, just disable the actual ability to change lanes and navigate unless people pay, it would be safer and would help sell the upgrade. Indicating to change lane had a delay before it disabled lane following and it had to be manually re-enabled once the maneuver was done, which was annoying.\n\nWe drove to Porto via the freeways, and the car picked a Tesla Supercharger site next to a hotel/restaurant where we had lunch and filled it up again at 150KW. There were enough Supercharger sites that we could have skipped the slow charge on the first day. When we got to Porto, the parking garage near the hotel had a row of 16KW charging stations that accepted the Shell Recharge card, and we left the car overnight before moving it to a normal parking spot.\n\nWe had a few excursions from Porto, and recharged again at the garage, then used a Supercharger on the way back to Lisbon to drop the car off. I was encouraged by the experience and would do it again.\n\nOur next stop was Gatwick, for a few days in the UK, and we rented a car to return to Heathrow. The charge to do a different drop off for an electric car was prohibitive so had I rented a small standard car. When we got to the rental counter, they wanted to give us a Toyota Hybrid, and I mentioned that we\u2019d just rented an electric at Lisbon. The agent asked if we wanted an electric instead, for the same price, and that the drop off charge would be the normal amount. So we got a Polestar 2. The initial drive-away took a bit longer as I figured out how to operate it, and it was about 75% charged. It\u2019s a hatchback that\u2019s a similar size to the Model 3, we had the base model, about half the power of the M3P but fast enough. The built in navigation and operation interface worked ok, but we used Apple Carplay to navigate most of the time. The Polestar 2 is a pretty good car, it had a bit more road noise than the Tesla Model 3, but was comfortable and drove well. It has cruise and lane following capability that worked well enough to use, but was worse than the Tesla, it responded more slowly and got confused more often. However indicating to change lane was better, it immediately stopped lane following then when the indicator was turned off it re-enabled it. I wish Tesla did it like that. We stopped for lunch and a visit to a museum, and found a BP charging station that didn\u2019t take the Shell Recharge card, but I managed to get it to take a one-time credit card payment using my phone and the BP web app. It wasn\u2019t great, but I was happy to get it to work. At our destination we found a parking lot with 16KW chargers that took Shell Recharge and filled the car up overnight. I tried to use the Shell Recharge app to find fast chargers and the UI was terrible. It always started out showing me charging stations in the USA. When I used the web app it was also terrible, and started out showing me stations in Amsterdam. It was really hard to find fast chargers along our route, and the car itself didn\u2019t seem to have this built in. This compares poorly to Tesla\u2019s Supercharger experience, or Chargepoint that I\u2019ve used in the US, and Shell really need to invest in making their app and web site more usable.\n\nWe got all the way back to Heathrow a few days later without needing another charge and dropped the car off. Then I noticed a lot more Polestars and Teslas that had been returned, some getting charged, and a complete row of them ready to go, as shown in the first picture in this post. It\u2019s clear that they have stocked up and have a lot of electric cars at Heathrow. However I\u2019d suggest that if you want one, you try booking a regular car and see if they will give you a free upgrade on the spot, as that\u2019s what they seem to have excess inventory of at the moment, in the UK at least.\n\nIn summary, I\u2019m going to be booking electric rental cars as my default going forward. Hertz seem to be investing a lot, and I expect they will get their systems and apps figured out to make the experience smoother over time. I\u2019ve also used Turo to book a BMW i3 for a trip to Denver in May, so that will be interesting\u2026\n\nUpdate May 2023, good experience with Turo.\n\nThe Turo rental in May worked well, the BMW i3 is quite fun to drive and had enough range for that trip. I\u2019ll use Turo more going forward.\n\nUpdate September 2023, bad experiences with Hertz.\n\nThe Hertz car choice and pricing is pretty good, but their booking process, online presence and app integration is un-usable, particularly for UK rentals. We had a short Tesla Model 3 rental in the USA in August, which went well, the car had basic autopilot turned on, which worked better than our experience in Portugal, but still relied on the keycard for everything. I did get surprised by the $25 recharge fee, this wasn\u2019t explained to me before I rented (update\u200a\u2014\u200ait\u2019s now more prominent in the booking info), and it would have been much cheaper to have returned it full (over 80% seems to be what they want). In my opinion it\u2019s much less work for them to plug in an electric car, which they are likely to do anyway, than to drive it to get gas, so it should be much cheaper to recharge.\n\nThen I booked a series of Polestars for various locations in a trip to Europe. The Polestar 2 is a good size, drives well enough, and is priced much less than Teslas. The Hertz pickup process at Heathrow was very slow and unhelpful compared to previous experiences with Enterprise at Heathrow or Hertz elsewhere. Then we had a bit more hassle finding a charger that worked and getting the UK based provider MER to work with a US iPhone\u200a\u2014\u200atheir app won\u2019t load, and their web app is clunky. In the end we got it going, charging at 50KW near where we are staying. I\u2019ve got used to Tesla superchargers running at 150\u2013250KW, but it was fast enough. Then we had an issue that made me want to extend the rental and my problems started\u2026\n\nThe current rental doesn\u2019t appear in the same Hertz app that I used to make the reservation. Future rentals also don\u2019t appear on the home page under that heading, unless I visit my account page, where they sometimes do appear. The history of previous rentals does eventually appear. So in the same app, in different screens, it can\u2019t decide what to show. However I can\u2019t get the current rental to show at all, which is what I needed. It said I could simply extend my rental using the app, but even if I explicitly loaded the rental number into that page (which only exists on the printed rental docs) it recognised it then immediately reverted to a previous screen. The reservation number from the emails and clicking on the text messages about the car did not work. This is stupid, they should be able to look this up from the reservation number, and give a sensible error message if something isn\u2019t right. So I tried to phone them, the US number worked but couldn\u2019t deal with my UK rental. The only number they provide for Heathrow is an 0845 area code premium cost number. They charge by the minute to provide support\u2026 However my US based cell phone refused to connect to that number. Eventually I used my Dad\u2019s land line, didn\u2019t get a human to talk to and left them a message. I didn\u2019t get a call back, so a few days later I tried again and got a human. He made the change and said that UK rentals can only be extended by phoning them, not via the app or the web site. I can\u2019t find this information anywhere in the app, or on the hertz.com or hertz.co.uk web sites (which tell me I can \u201csimply do this online\u201d).\n\nThe rental return etc. was fine in the end. The Shell keyfob for charging didn\u2019t work anywhere this time, including at a Shell station. However swiping a credit card on the charger did work for one of the random confusing charging station brands that I can\u2019t remember. The Polestar is a nice large cheap electric car to rent, as long as you don\u2019t have to get any support from Hertz.\n\nFor the next rental we needed in Edinburgh, I cancelled the Hertz Polestar and tried renting a Tesla Model Y from Sixt. Their web site and app are far better than Hertz (good job by their team and CTO Boyan Dimitrov, who I met recently), but their pickup process was pretty slow and manual compared to Enterprise. Like Hertz, their systems don\u2019t really know about electric cars yet, there\u2019s no mention in their online FAQ, and the staff don\u2019t really know the electric car options well. Sixt tried to sell me a Tesla Supercharger subscription for about \u00a330 that they said would give me unlimited recharges, but I was only renting for 3 days, and the hotel had a destination charger. The car was setup strangely, and hadn't been reset since the last driver, and it\u2019s a pain to have to use the Tesla card every time rather than the Tesla app. Tesla need to fix their app and car software so that a time limited car rental can be hooked up to the app, and charging should be at the normal Tesla rates, the car could also know my setup preferences from the app. We have had a Model 3 for several years, and driving a Model Y is similar, but it has a much worse ride, and I didn\u2019t enjoy driving it as much. No problem with the car, or dropping it off.\n\nNext up, a Hertz Polestar in Italy. Sixt don\u2019t have any electric cars for rent, so we\u2019ll try it again. We\u2019ve already seen plenty of electric cars and chargers around Rome. We\u2019re going to drive to Pompeii. (Update: this trip went well, the Polestar and charging experience in Italy was fine).\n\nWe live near Monterey airport but need to fly out of San Francisco fairly often, about a 2hr drive which is an expensive short flight, if the times work, or an expensive Uber or Taxi ride. I like to pick up an electric car from MRY the day before and do a one way drive to SFO. The cost of this varies a lot. In January 2024 we ended up with a Hertz Model Y for over $200 and a few weeks later a Model 3 for under $100. Pickup is very fast at MRY\u200a\u2014\u200acheck ID, here\u2019s your key, done.",
        "metadata": {
          "word_count": 2685,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Hertz recently started to rent electric cars, and we\u2019ve tried them out a few times. Most people are used to renting cars, but renting electric cars is a fairly new thing. I\u2019m sharing to help encourage people to try it out and to hopefully smooth the way with some learning and feedback. The Chevy Bolt is a good lower cost rental option. It feels more like a normal car, and reminded us of our Nissan Leaf. There\u2019s a mix of chargers with captive cables and ones where you have to bring your own cable, in the UK. The booking guaranteed a Tesla Model 3, and it said it would be the long range version. We rented at Lisbon airport and drove to Sintra and Porto and back, so we wanted a good range. After dropping off our bags and things at the hotel, I found a 50KW charging station. We were given a card key, which is much less convenient than using the Tesla phone app. There was no opportunity to add the car to our own Tesla apps on a temporary basis. The car itself was set in \u201cchill mode\u201d when we got it, which was probably a good idea for inexperienced drivers. We drove to Porto via the freeways, and the car picked a Tesla Supercharger site next to a hotel/restaurant where we had lunch and filled it up again at 150KW. Indicating to change lane had a delay before it disabled lane following and it had to be manually re-enabled once the maneuver was done. I think Tesla should use the same autopilot/FSD code for cruise control and lane following. We rented a Polestar 2 for a few days in the UK. It's a hatchback that\u2019s a similar size to the Model 3. We had the base model, about half the power of the M3P. The Polestar 2 is a pretty good car, it had a bit more road noise than the Tesla Model 3, but was comfortable and drove well. It has cruise and lane following capability that worked well enough to use. Carplay to navigate most of the time. Shell\u2019s app and web site are terrible. It was hard to find fast chargers along our route, and the car itself didn\u2019t seem to have this built in. Shell really need to invest in making their app more usable. The Hertz car choice and pricing is pretty good, but their booking process, online presence and app integration is un-usable, particularly for UK rentals. We had a short Tesla Model 3 rental in the USA in August, which went well, the car had basic autopilot turned on. The Polestar 2 is a good size, drives well enough, and is priced much less than Teslas. The Hertz pickup process at Heathrow was very slow and unhelpful. Future rentals don't appear on the home page under that heading, unless I visit my account page, where they sometimes do appear. The history of previous rentals does eventually appear. However I can\u2019t get the current rental to show at all, which is what I needed. The Polestar is a nice large cheap electric car to rent, as long as you don't have to get any support from Hertz. The Shell keyfob for charging didn't work anywhere this time, including at a Shell station. Swiping a credit card on the charger did work for one of the random confusing charging station brands. Sixt tried to sell me a Tesla Supercharger subscription for about \u00a330. But I was only renting for 3 days, and the hotel had a destination charger. The car was setup strangely, and hadn't been reset since the last driver. Tesla need to fix their app and car software. We live near Monterey airport but need to fly out of San Francisco fairly often, about a 2hr drive. I like to pick up an electric car from MRY the day before and do a one way drive to SFO. In January 2024 we ended up with a Hertz Model Y for over $200 and a few weeks later a Model 3 for under $100. Pickup is very fast at MRY\u2009\u2009check ID, here\u2019s your key, done."
      },
      "tags": [
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_c469a84b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thanks for this lots of good ideas put together to make a system",
      "source": "Medium Archive",
      "published_date": "2016-07-17",
      "url": "https://medium.com/@adrianco/2016-07-17_Thanks-for-this--lots-of-good-ideas-put-together-to-make-a-system--342b3c36e6a8",
      "content": {
        "text": "Thanks for this, lots of good ideas put together to make a system. I also have the problem of putting too many ideas into a talk\u2026",
        "metadata": {
          "word_count": 26,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "\"I also have the problem of putting too many ideas into a talk,\" he says. \"Thanks for this, lots of good ideas put together to make a system\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_56c90d3b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thanks for the references Simone",
      "source": "Medium Archive",
      "published_date": "2017-01-04",
      "url": "https://medium.com/@adrianco/2017-01-04_Thanks-for-the-references-Simone--d89531bb44cf",
      "content": {
        "text": "Thanks for the references Simone. However you missed Snowball Edge\u200a\u2014\u200awhich is the clusterable AWS appliance in a box you should have been talking about. Snowmobile is a sign of the huge scale of some AWS migrations but it doesn\u2019t enable remote services like Snowball Edge and Greengrass. Also, while I\u2019m working with the Blox team, my initial focus at AWS is to build an Open Source engagement team that works across several product and initiatives. best wishes for 2017!",
        "metadata": {
          "word_count": 81,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Snowmobile is a sign of the huge scale of some AWS migrations but it doesn\u2019t enable remote services like Snowball Edge and Greengrass. My initial focus at AWS is to build an Open Source engagement team that works across several product and initiatives."
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_file_d47d4fd9",
      "kind": "file",
      "subkind": "medium_post",
      "title": "I agree with this in general however to avoid premature optimization it may still be worth",
      "source": "Medium Archive",
      "published_date": "2017-02-20",
      "url": "https://medium.com/@adrianco/2017-02-20_I-agree-with-this-in-general--however-to-avoid-premature-optimization-it-may-still-be-worth--778e0693ceee",
      "content": {
        "text": "I agree with this in general, however to avoid premature optimization it may still be worth building a prototype that is pure serverless, then as you roll it out to high volume production use, some of those functions can be run more effectively (lower latency/cost) by autoscaling permanent containers. If you think about a service containing a bundle of functions, there are likely to be some functions in heavy use, and others that are infrequently invoked, so you are paying the memory overhead of having them sitting around waiting. In some cases that matters, in others it\u2019s not a big deal.",
        "metadata": {
          "word_count": 101,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "To avoid premature optimization it may still be worth building a prototype that is pure serverless. Some of those functions can be run more effectively (lower latency/cost) by autoscaling permanent containers."
      },
      "tags": [
        "containers",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_file_ad91edb7",
      "kind": "file",
      "subkind": "medium_post",
      "title": "The Epistemology of Information Technology",
      "source": "Medium Archive",
      "published_date": "2017-03-30",
      "url": "https://medium.com/@adrianco/2017-03-30_The-Epistemology-of-Information-Technology-a3f6944cf02c",
      "content": {
        "text": "The Epistemology of Information Technology\n\nCompliance teams and auditors are fighting a losing battle as they try to know the un-knowable with their never-consistent CMDB\nSometimes there are gaps in things you would like to depend on, like this road on my commute to\u00a0work\nWhich questions have answers? What would you like to know about your infrastructure and applications, what can you know and what do you need to know?\nEpistemology is the study of knowledge, and how we know what is knowable.\nTake a look at your datacenter, perhaps someone sold you a CMDB (Configuration Management Database) a few years ago, promising to help you keep track of the state of your infrastructure and applications, how did that work out? It turns out that CMDBs are never-consistent databases.\nThe only thing you can guarantee is that the state of the datacenter will always be different from what the CMDB tells\u00a0you.\nIn theory your CMDB is supposed to record exactly where every piece of infrastructure is located, its asset record, and history, but I\u2019ve never heard of a clean physical audit result in practice.\n\nHere\u2019s a more typical scenario: there are things in your datacenter that no-one has any record or recollection of, what they do or how they got there. There are things that are powered down, so your network scans don\u2019t find them. There are things that are powered up, but not connected to anything, or in a crashed/un-booted state. There are production services running on systems being used as foot-rests under office desks, that you think are in the datacenter.\n\nYour software installations don\u2019t match their actual licensed usage. There are systems that were moved between racks and reconfigured during the scramble to recover during an outage a few years ago. There is a datacenter population of rogue Wifi interfaces, service modems, USB flash drives etc.\n\nI know of one datacenter that was declared \u201cfull\u201d and unable to take more deployments. After a full physical audit, over six hundred idle, unallocated servers were uncovered, and a lot of free space created by clearing out obsolete equipment.\n\nThis is a bit of a problem, because you would really like to be able to make a few strong assertions about the state of your IT. Here\u2019s just a few examples:\n\nThe attack surface of this entire datacenter building consists only of this set of IP addresses.\n\nThis is a tamper-free log of every role that has made any production change since the last audit.\n\nI know where every cable goes, the exact network topology.\n\nHere is the configuration of the virtual machine (that no longer exists) which wrote this interesting log entry last Thursday at 8:43pm.\n\nFor many organizations a messy datacenter is just an inconvenience, and some \u201cspring cleaning\u201d now and again is enough to clean up the worst of the mess. However in regulated industries, a compliance or risk management team will be trying to maintain conformance to policies and controls so that when the external auditor visits, they can pass without too many variances to fix.\n\nThe auditor has a clip-board, a list of questions, and interviews people to make sure that they are filing tickets for changes, and processing the tickets according to policy. The compliance team and auditor are fighting a losing battle, as they are trying to know the un-knowable, but they do their best, with their never-consistent CMDB, trying to slow down the rate of change, so they can keep up.\n\nIt gets worse! The \u201cdigital transformation\u201d arrives with a collection of new buzzwords. The business needs a large scale globally distributed AWS cloud based microservices \u201csystem of engagement\u201d, that connects them directly to their customers, and wants to move from agile to continuous delivery and DevOps practices.\n\nEphemeral infrastructure and applications change continuously and the CMDB collapses under the load. In addition developers want to use API driven automation to create infrastructure rather than file tickets. Some of the auditors and compliance team start counting the years to retirement, but others dig in to the new world, and start to realize that rather than losing control, they may be some new capabilities they can leverage.\n\nI was in an executive briefing session with the IT team from a new AWS customer, a very large organization, and one of the team was in charge of security and compliance, and looked worried. I asked if they had looked at the AWS CloudTrail audit system yet, and pointed out that they could get a record of every change to every account, from empty new account to any point in time. He perked up a bit as this sank in.\n\nThat could be fed into a CMDB, but really it just needs to be queried, so you could stream it to Amazon ElasticSearch or Splunk. Also AWS Identity and Access Management roles can be used to control who can do what, and have the action, role and identity of the user for every change recorded into AWS CloudTrail. He starts to look happier, and asks what the auditors would think about those logs? Well, the logs are written into an S3 bucket, you can search them directly with Amazon Athena and you can provide a secure key that proves they haven\u2019t been tampered with.\n\nCheck aws.amazon.com/compliance to see how AWS features and services meet SOC1, HIPPA, PCI-DSS and many other compliance standards around the world. Help with best practices is available from AWS and partners.\n\nWhile many companies are already running highly regulated applications on AWS, it\u2019s still early days. There is still a large inventory of datacenter applications that AWS is helping to migrate. However, if we think about where we will end up over the next few years, as auditors get used to being able to know exactly what is going on in an AWS account, it\u2019s going to become increasingly difficult to pass an audit in a datacenter.",
        "metadata": {
          "word_count": 988,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Compliance teams and auditors are fighting a losing battle as they try to know the un-knowable with their never-consistent CMDB. The only thing you can guarantee is that the state of the datacenter will always be different from what the CMDB tells you. There are things in your datacenter that no-one has any record or recollection of, what they do or how they got there. There are systems that were moved between racks and reconfigured during the scramble to recover during an outage a few years ago. There is a Datacenter population of rogue Wifi interfaces, service modems, USB flash drives. The attack surface of this entire datacenter building consists only of this set of IP addresses. This is a tamper-free log of every role that has made any production change since the last audit. I know where every cable goes, the exact network topology. Compliance team and auditor are fighting a losing battle, as they are trying to know the un-knowable, but they do their best, with their never-consistent CMDB. The business needs a large scale globally distributed AWS cloud based microservices \u201csystem of engagement\u201d, that connects them directly to their customers. AWS CloudTrail can record every change to every account, from empty new account to any point in time. That could be fed into a CMDB, but really it just needs to be queried, so you could stream it to Amazon ElasticSearch or Splunk. Also AWS Identity and Access Management roles can be used to control who can do what. Check aws.com/compliance to see how AWS features and services meet SOC1, HIPPA, PCI-DSS and many other compliance standards around the world. Help with best practices is available from AWS and partners."
      },
      "tags": [
        "agile",
        "automation",
        "aws",
        "cloud",
        "devops",
        "infrastructure",
        "microservices",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_278608c7",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Rapid development in R with lots of help from ChatGPT",
      "source": "Medium Archive",
      "published_date": "2023-07-05",
      "url": "https://medium.com/@adrianco/2023-07-05_Rapid-development-in-R-with-lots-of-help-from-ChatGPT-322aedf559ad",
      "content": {
        "text": "Rapid development in R with lots of help from\u00a0ChatGPT\n\nLike many people, I\u2019ve been tinkering with ChatGPT to see what it can do, but I was working on developing some fairly complex analysis code using the R language that I wanted to talk about and open source as part of the talk I gave earlier this week at Monitorama, and decided to ask ChatGPT (the free version) to remind me how to code in R, as an alternative to reading man pages and looking for stack overflow fragments. I\u2019ve been coding in R for decades, but I don\u2019t do it every day, however every year or two I have a reason to use it, and spend a lot of time reading man pages. This time I got code that worked, that used constructs and packages that I hadn\u2019t used before, and got more coding done much more than ten times faster than usual. The rest of this blog post is an edited version of the conversation I had with ChatGPT, that also acts as documentation of the code I\u2019ve open sourced. I\u2019m pretty sure that anyone that wants this algorithm in a different language like Python or Go could have a similar conversation and make it happen.\n\nThis work built on my previous blog post\u200a\u2014\u200aPercentiles Don\u2019t Work\u200a\u2014\u200aAnalyzing the Distribution of Response Times. At that time I developed a function called as.peaks\u200a\u2014\u200awhich takes a histogram, and fits normally distributed peaks to it, and shared that code on github. Some minor updates to as.peaks and the new code developed with help from ChatGPT is provided there.\n\nThis is a long blog post, as I think it\u2019s interesting to see how the conversation developed, what worked well and what didn\u2019t for my first attempt at coding with ChatGPT. I\u2019m extremely impressed, and expect to follow this kind of development assistance on anything I write from now on.\n\nThe very first question in the conversation is shown below, it implied that I\u2019m coding in R, and I was trying to import some data in a format I hadn\u2019t tried before, which was a csv of a log file, where the details I wanted were coded as part of a json object.\n\nThere was three pages of detailed instructions, which all looked good, but I realized I hadn\u2019t asked for exactly the right thing, because autocorrect had changed CSV to CSS, and tried again. I\u2019ve reduced the font size here to show the entire response.\n\nThis worked fine. Now I needed to pull out some fields from the JSON.\n\nA few more similar small lookups helped me get the right data into the right format, which is a dataframe containing a log file, with each row having a timestamp, the latency of that request in milliseconds, and a tag that is the type of query being made. For this analysis I want to make a series of histograms by taking the log file and processing each minute separately, then looking at how the histograms change over time. The logfile I started with had several minutes of data in it.\n\nThis code worked well, but left out the partial minutes at the start and end.\n\nI decided that rather than clustering all the data at once, I wanted to eventually end up with a way to do a sliding window across time, so I asked for a re-write of the code.\n# Function to add a new data frame to the iterative clustering algorithmaddDataFrame <- function(data_frame, clusters) {  # Iterate over each point in the new data frame  for (i in 1:nrow(data_frame)) {    point <- data_frame[i, ]  # Get the current point        # Calculate distances to existing clusters    distances <- sapply(clusters, function(cluster) sqrt(sum((point - cluster$centroid)^2)))        # Find the closest cluster    closest_cluster_index <- which.min(distances)    closest_cluster <- clusters[[closest_cluster_index]]        # Check if the closest cluster is within the matching criterion    if (distances[closest_cluster_index] <= threshold) {      # Add the point to the closest cluster      closest_cluster$points <- rbind(closest_cluster$points, point)            # Update the cluster centroid      closest_cluster$centroid <- colMeans(closest_cluster$points)    } else {      # Create a new cluster with the point      new_cluster <- list(points = point, centroid = as.matrix(point))      clusters <- c(clusters, list(new_cluster))    }  }    return(clusters)}# Example usage# Initialize the clusters using the points from the first data framefirst_data_frame <- list_of_dataframes[[1]]initial_clusters <- lapply(1:nrow(first_data_frame), function(i) {  list(points = first_data_frame[i, ], centroid = as.matrix(first_data_frame[i, ]))})# Process subsequent data framesfor (i in 2:length(list_of_dataframes)) {  new_data_frame <- list_of_dataframes[[i]]  initial_clusters <- addDataFrame(new_data_frame, initial_clusters)}# Return the current cluster statecurrent_clusters <- initial_clusters\nI copied the above code here because it has long lines.\naddDataFrame <- function(data_frame, clusters) {  for (i in 1:nrow(data_frame)) {    point <- data_frame[i, c(\"x\", \"y\")]  # Extract the (x, y) coordinates        # Calculate distances to existing clusters    distances <- sapply(clusters, function(cluster) sqrt((point[1] - cluster$centroid[1])^2 + (point[2] - cluster$centroid[2])^2))        closest_cluster_index <- which.min(distances)    closest_cluster <- clusters[[closest_cluster_index]]        if (distances[closest_cluster_index] <= threshold) {      closest_cluster$points <- rbind(closest_cluster$points, point)      closest_cluster$centroid <- colMeans(closest_cluster$points)    } else {      new_cluster <- list(points = point, centroid = point)      clusters <- c(clusters, list(new_cluster))    }  }    return(clusters)}\nThe above code was working well, with a few tweaks, and my initial sample data, but I wanted to look at it visually, so I asked for a plot, function as shown below. This is all I said, and the resulting code worked fine. I think this is amazing.\n\nAt this point, it was all looking good. I was using 5 minutes of data, so I obtained a larger dataset, for 15 minutes. This gave an error, and between me and ChatGPT it took a while to figure out what was going on. The list of clusters contained a data frame for the centroid when it was first created, but that was overwritted by a vector when it was updated. The mixture of types was breaking the plot routine when there was a cluster with a single point. ChatGPT was good at suggesting alternatives, and interpreting some of the error messages, but in the end I had to spot that the data types were different and fix it.\n\nThis error was quite misleading, and the suggestion to provide the code and data led to another issue. The data structure was quite large, and when I pasted it into the conversation as a question, it seemed to overflow the context, and ChatGPT forgot what we were doing. I deleted that question and ChatGPT regained it\u2019s context. The problem was that when displayed as text, both the data frame and vector types looked alike. I ended up isolating a small test case that failed and was able to figure it out, but there was a lot of chat that I won\u2019t share here.\n\nI then tried to get ChatGPT to clean up and document the code\n\nI found that it documented the parameters of the functions well, but it actually removed some of my comments from the code, so I tried again.\n# addPeaks: Add peaks to the clusters# Input:#   - peaks: Data frame of peaks (x, y) coordinates#   - clusters: List of current clusters#   - normalize: Normalization factor for log latency#   - threshold: Maximum distance threshold to consider a point as part of a cluster# Output:#   - Updated list of clustersaddPeaks <- function(peaks, clusters, normalize, threshold = 0.05) {  for (i in 1:nrow(peaks)) {    point <- peaks[i, c(8, 1)]  # Extract the (x, y) coordinates (Latency and Density)    if (point[1] <= 0) next  # Skip if latency rounds down to zero (to avoid log(0))        point[1] <- log(point[1]) / normalize  # Normalize log latency to a consistent max bucket        # Calculate distances to existing clusters    distances <- sapply(clusters, function(cluster) sqrt((point[1] - cluster$centroid[1])^2 + (point[2] - cluster$centroid[2])^2))        closest_cluster_index <- which.min(distances)    closest_cluster <- clusters[[closest_cluster_index]]        if (distances[closest_cluster_index] <= threshold) {      closest_cluster$points <- rbind(closest_cluster$points, point)      closest_cluster$centroid <- colMeans(closest_cluster$points)      clusters[[closest_cluster_index]] <- closest_cluster    } else {      new_cluster <- list(points = point, centroid = colMeans(point))  # Create a new cluster      clusters <- c(clusters, list(new_cluster))    }  }    return(clusters)}# guillotine: Process log file by chopping into one-minute chunks, finding peaks, and clustering them# Input:#   - df: Data frame containing timestamp, latency, and query#   - plot: Boolean indicating whether to plot the clusters#   - epsilon: Epsilon value for peak detection#   - peakcount: Number of peaks to identify# Output:#   - List of clustersguillotine <- function(df, plot = FALSE, epsilon = 0.01, peakcount = 10) {  start_time <- round(min(df$time), \"mins\")  end_time <- round(max(df$time), \"mins\")  minute_intervals <- seq.POSIXt(start_time, end_time, by = \"min\")  msc <- cut(df$time, breaks = minute_intervals, right = FALSE, labels = FALSE)  last_interval <- max(msc, na.rm = TRUE)  msc[is.na(msc)] <- last_interval + 1  df$msc <- msc    hb <- hist(log(df$latency), breaks = 40, plot = plot)$breaks  mhb <- max(hb)  # Max histogram bucket - needed to normalize latency    results_list <- lapply(unique(df$msc), function(section) {    subset_df <- df[df$msc == section, , drop = FALSE]        peaks <- as.peaks(hist(log(subset_df$latency), breaks = hb, plot = FALSE), normalize = TRUE, epsilon = epsilon, peakcount = peakcount, plots = plot)      })    first_data_frame <- results_list[[1]]  first_points <- first_data_frame[, c(8, 1)]  # Extract the (x, y) coordinates (Latency and Density)  first_points[, 1] <- log(first_points[, 1]) / mhb  # Normalize log latency to max bucket    initial_clusters <- lapply(1:nrow(first_points), function(i) {    list(points = first_points[i, ], centroid = as.matrix(first_points[i, ]))  })    for (i in 2:length(results_list)) {    new_data_frame <- results_list[[i]]    initial_clusters <- addPeaks(new_data_frame, initial_clusters, mhb)  }    return(initial_clusters)}library(ggplot2)# plotClusters: Plot the current state of the clusters# Input:#   - clusters: List of clusters# Output:#   - Plot objectplotClusters <- function(clusters) {  points <- data.frame(x = numeric(), y = numeric(), cluster = factor())    for (i in 1:length(clusters)) {    cluster <- clusters[[i]]    cluster_points <- cluster$points    points <- rbind(points, data.frame(x = cluster_points[, 1], y = cluster_points[, 2], cluster = as.factor(i)))  }    p <- ggplot(points, aes(x = x, y = y, color = cluster)) +    geom_point() +    theme_minimal()    centroids <- lapply(clusters, function(cluster) cluster$centroid)  centroids_df <- data.frame(x = sapply(centroids, \"[\", 1), y = sapply(centroids, \"[\", 2))  p <- p + geom_point(data = centroids_df, aes(x = x, y = y), color = \"black\")    return(p)}\nI decided not to take the cleaned up version, as I wanted to keep my comments, but it did seem to do a reasonable job of tidying the code.\n\nI wanted to do some more work on the plots, but I haven\u2019t used the ggplot2 library before, so I asked the expert\u2026\nplotClusters <- function(clusters) {  points <- data.frame(x = numeric(), y = numeric(), cluster = factor())  for (i in 1:length(clusters)) {    cluster <- clusters[[i]]    cluster_points <- cluster$points    points <- rbind(points, data.frame(x = cluster_points[, 1], y = cluster_points[, 2], cluster = as.factor(i)))  }  p <- ggplot(points, aes(x = x, y = y, color = cluster)) +    geom_point() +    theme_minimal() +    labs(x = \"X-Axis Label\", y = \"Y-Axis Label\")  # Add x-axis and y-axis labels here  centroids <- lapply(clusters, function(cluster) cluster$centroid)  centroids_df <- data.frame(x = sapply(centroids, \"[\", 1), y = sapply(centroids, \"[\", 2))  p <- p + geom_point(data = centroids_df, aes(x = x, y = y), color = \"black\")  return(p)}\nHere\u2019s what that plot looks like.\n\nNext I wanted a new plot, to show how the peaks evolve over time, and it too me a few tries before I got the request right and got the plot I wanted. Each of the responses worked, but wasn\u2019t what I was looking for. This was a very fast way to iterate, compared to writing the plot code myself\u2026\nplotClusterDensity <- function(clusters) {  # Create a data frame to store the cluster density and time information  cluster_data <- data.frame(Time = numeric(), Cluster = numeric(), Density = numeric())    # Extract cluster density and time information from each cluster  for (i in 1:length(clusters)) {    cluster <- clusters[[i]]    density <- cluster$points$PeakDensity    time <- cluster$peaks$Time    cluster_data <- rbind(cluster_data, data.frame(Time = time, Cluster = i, Density = density))  }    # Plot the cluster densities over time  p <- ggplot(cluster_data, aes(x = Time, y = Density, group = Cluster, color = as.factor(Cluster))) +    geom_line() +    geom_point() +    labs(x = \"Time\", y = \"Peak Density\", color = \"Cluster\") +    theme_minimal()    return(p)}\nThis is what that plot looks like.\n\nI did some more work on yet another kind of plot, but ran out of time before I had to give my talk, and posted the code as it was, to github.\n\nI know that this is the kind of thing that Github Copilot and similar tools are designed to do, and that ChatGPT 4 is better at this, but I initially just wanted to look up some syntax reminders, and ended up with big chunks of working code written by ChatGPT. I think it\u2019s impressive as it is.",
        "metadata": {
          "word_count": 2067,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "ChatGPT reminds me how to code in R, as an alternative to reading man pages and looking for stack overflow fragments. This time I got code that worked, that used constructs and packages that I hadn\u2019t used before, and got more coding done much more than ten times faster than usual. The rest of this blog post is an edited version of the conversation I had with ChatGPT. This is a long blog post, as I think it\u2019s interesting to see how the conversation developed, what worked well and what didn\u2019t for my first attempt at coding with ChatGPT. I\u2019m extremely impressed, and expect to follow this kind of development assistance on anything I write from now on. The dataframe is a dataframe containing a log file, with each row having a timestamp, the latency of that request in milliseconds, and a tag that is the type of query being made. I want to make a series of histograms by taking the log file and processing each minute separately, then looking at how the histograms change over time. addDataFrame is a function that adds a new data frame to the clustering algorithm. The algorithm then iterates over each point to find the closest cluster to the current point. The cluster centroid is then updated to the closest point. I copied the above code here because it has long lines. The code below is an example of how to initialize a cluster using data frames. The above code was working well, with a few tweaks, and my initial sample data. I wanted to look at it visually, so I asked for a plot, function as shown below. I think this is amazing. The data structure was quite large, and when I pasted it into the conversation as a question, it seemed to overflow the context. I deleted that question and ChatGPT regained it\u2019s context. The problem was that when displayed as text, both the data frame and vector types looked alike. addPeaks: Add peaks to the clusters. log: Normalization factor for log latency. threshold: Maximum distance threshold to consider a point as part of a cluster. closest_cluster: Calculate distances to existing clusters. guillotine: Process log file by chopping into one-minute chunks, finding peaks, and clustering them. Data frame contains timestamp, latency, and query. Plot: Boolean indicating whether to plot the clusters. Peakcount: Number of peaks to identify. First, extract the (x, y) coordinates (Latency and Density) Second, normalize log latency to the max bucket. Third, add peaks to the cluster data. I decided not to take the cleaned up version, as I wanted to keep my comments. But it did seem to do a reasonable job of tidying the code. Next I wanted a new plot, to show how the peaks evolve over time. I haven\u2019t used the ggplot2 library before, so I asked the expert\u2026 This was a very fast way to iterate, compared to writing the plot code myself. Each of the responses worked, but wasn\u2019t what I was looking for. I initially just wanted to look up some syntax reminders, and ended up with big chunks of working code written by ChatGPT. I ran out of time before I had to give my talk, and posted the code as it was, to Github. I think it\u2019s impressive as it is."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_88736dbc",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Part 2 Sun Microsystems c1a512c8284",
      "source": "Medium Archive",
      "published_date": "2022-06-21",
      "url": "https://medium.com/@adrianco/2022-06-21_What-Adrian-Did-Next---Part-2---Sun-Microsystems-c1a512c8284",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aPart 2\u200a\u2014\u200aSun Microsystems\nAdrian Cockcroft, Richard McDougall, Allan Packer and Brian Wong. Los Altos CA, mid-1990\u2019s\u200a\u2014\u200aPhoto by\u00a0Adrian\nI spent six years at Cambridge Consultants, building some interesting systems, managing our Sun workstations and learning a lot, but by then Sun had opened a sales office across the street, and I wanted to find out what they were going to release next, before everyone else. So in 1988 I joined the Sun sales force and became a Systems Engineer (nowadays we would say Solutions Architect). This was a big jump to make, I had to wear a suit to work every day, and spend all my time paired up with a sales rep visiting customers. I really enjoyed the variety of working with several different customers every day, on different problems, and being part of an extremely innovative and fast growing company. I also learned a lot about how to work directly with customers, when to shut up and let the sales guy drive the conversation, and generally how technology sales works. The early days at Sun Cambridge were special, I absorbed a lot about networking and the technical side of the role from my fellow systems engineer Martin Baines, and we were driving all over the region in cool company cars (I had a Citroen BX 16V) selling a really hot product. The company and the team got bigger over time, but with the initial sales team of Paul Martin (Ford Sierra Cosworth\u200a\u2014\u200athe fastest sales guy in East Anglia), Martin Harris (Land Rover Discovery\u200a\u2014\u200arecruited an inflatable skeleton as an extra sales guy), David Morley (Vauxhall Carlton\u200a\u2014\u200aalways betting on the horses at Newmarket), and Gary Littlefair (BMW 320\u200a\u2014\u200adid over 200% of his target several years running as we took British Telecom to Sun UK\u2019s biggest account) we were \u201ckicking butt and having fun\u201d as Scott McNealy liked to say. I also got to visit Sun head office in California regularly for training sessions, and met some interesting people there, in particular Reynold Jabbour from New York, who is deeply insightful on both people and technology, ran the Systems Engineers team for the east coast, later went to work for various Wall Street banks, and is still one of my closest friends.\n\nI became the Sun UK local specialist in performance and hardware, and as Sun transitioned from a desktop workstation company to sell high end multiprocessor servers I was helping customers find and fix scalability problems. In 1991 I wrote a white paper on performance that was widely read, and in 1993 (with help from Brian Wong) that got me a job in the USA, working alongside Brian for Mike Briggs in technical product marketing. Another big jump, but now it was my job to run benchmarks in the lab, and write white papers that explained the new products to the world, as they were launched. I also learned how marketing worked, and began to build my presentation and training skills as I was sent around the world by Sun to teach workshops and speak at events. I was mostly coding in C, tuning FORTRAN, and when I needed to do a lot of data analysis of benchmark results used the S-PLUS statistics language, that is the predecessor to R. I still code in R\u2026\n\nOne role I had for many years was to lead technical training events for the global specialist systems engineers, which we called the Ambassador Program. I had attended these events when I was based in the UK, but became one of the organizers. I had to setup a week of talks by all the relevant product teams, with a hundred or so of the most experienced systems engineers from all over the world as an audience. We had specializations in hardware, operating systems, databases, graphics, etc. At one point we had two conflicting high availability products from different teams, who weren\u2019t cooperating. I deliberately scheduled them back to back at one event so they were all in the room at the same time, and the audience roasted them\u200a\u2014\u200aas a result there were product and messaging changes. We\u2019d sometimes manage to get Sun founder Andy Bechtolsheim to give us a talk, and he\u2019d tell us far more than the product managers wanted us to know, about all the cool things he was working on that haven\u2019t happened yet. He hasn\u2019t changed. So many global friendships were made that continue to today. I saw Erik Fisher last time I was in Budapest, and Constantin Gonzalez later became one of the first AWS solutions architects in Germany, too many names to mention...\n\nI remember organizing my first week long event, having it go well, and heading back to the office to tell my manager Mike Briggs. He was pleased, but also told me he was leaving that day, to be one of the first employees of NetApp, at the time I was shocked! We ended up hiring Phil Parkman to take over managing our group, which by then also included my friend Dave Fisk, who I still hang out with regularly. Later on after some re-orgs we ended up reporting to Greg Papadopoulos, who was CTO of the server organization at the time.\n\nI\u2019d been using the Internet since about 1984, for email and newsgroups, sharing files like my white papers via ftp sites, but in the early 1990\u2019s the world wide web came along, and Sun was a very early adopter. Corporate IT had no idea what was going on, but someone got a T1 line hooked up and we put a spare desktop machine on it and set up www.sun.com (link to archive from 1996). I ended up with root access helping Shane Sigler run the site around 1995\u20131996 when it was one of the very first corporate web sites. I learned about tuning systems for this new network connection intensive workload, helped Solaris engineering tune the network stack and helped customers run some of the early web properties. Mike Briggs suggested that Brian Wong and I write books, because then we\u2019d have more influence on the engineering teams, which turned out to be excellent advice. Brian wrote Configuration and Capacity Planning for Sun Servers, and I turned my collection of white papers into a book, Sun Performance and Tuning, which became widely read, and published a monthly Q&A column at SunWorld Online. This was followed by a greatly expanded second edition with some additional chapters by Rich Pettit.\n\nBrian Wong started a \u201crotation program\u201d that he and I ran for many years, with admin help and funding from Barb Hill, where we would borrow a systems engineer from somewhere in the world for a month, and have them work with us benchmarking something or writing a paper. We made great friends and found some amazing talent this way, and so many people ended up getting hired into permanent roles that it was called the job donation program by some of the SE managers, who lost good people to the central teams, but were mostly happy to see them succeed.\n\nRich Pettit is one of the best programmers I ever met. He turned up on a month long rotation one Monday in 1993, took a look at a complicated Perl script Brian had written to try to capture some tuning ideas, and decided to use lex and yacc to write a dedicated language to make it easier. He had it up and running on Wednesday. I adopted the language, called the SE Toolkit, which was a C-based interpreter that understood how to directly read all the performance data out of the kernel, and wrote a lot of scripts over the years, including one called virtual_adrian.se that a lot of people used. The SE toolkit was fast, had no memory leaks (monitoring scripts could run for years) and let me implement lots of cool performance monitoring ideas. Rich became co-author of the second edition of the Sun Performance Tuning book, to describe how it worked.\n\nPhil Harman is a friend of mine from Sun UK, we were the two most senior systems engineers in the UK at the time I left for the USA, and he came over on rotation a few times. He became a leading expert in Solaris multi-threading. Jim Mauro and Allan Packer visited on rotation from New Jersey, and Adelaide Australia, and worked out how to produce a database sizing guide for Oracle Financials, which I wrote up and published. It was the first database sizing guide Sun had ever produced. Allan moved to the USA for several years to work on database performance and wrote the book Configuring and Tuning Databases on the Solaris Platform. He also worked closely with IBM as they ported DB2 to run on Solaris. Jim also joined the database performance team.\n\nAllan also introduced me to Richard McDougall, who had worked with Allan for Sun Australia, and Richard visited for a rotation to build some monitoring tools. No-one could believe how much he got done in a month, building several tools including one that could lay out the entire memory usage of a machine in a way no one had ever seen before. A bit later our manager asked Brian and I who we should hire next, and we both said Richard. He wanted someone easier to hire than an international relocation, but in the end we made it happen. Later on Richard teamed up with Jim Mauro to write the Solaris Internals series of books, and while trying to figure out how something worked for the first book, ended up finding and fixing a major bug in the kernel code that was the biggest performance win in Solaris 8.\n\nPaul Reithmuller was yet another imported Australian engineer who did amazing work. One day we were chatting in a group meeting and he said he was working on a TPC-C benchmark that was spending too much time in the SCSI disk driver, so he wrote a 10-line awk script running against kernel memory to look at the SPARC machine code execution path and find which branches were being predicted wrong, then wrote another awk script to flip the branch prediction bits in memory and it showed a significant speedup. It was a memorable jaw drop moment for a room full of people, you did what!?! After that we had a running joke that every impossibly hard problem would be solved with a ten line awk script.\n\nI met Paul Strong when he was working on a rotation in 1998, and we became close friends, bonding over a shared interest in music by Hawkwind, Frank Zappa and King Crimson. He later moved to the US to work on Solaris, and for many years we worked and played together, in particular when I helped him form the progressive rock band Fractal. He\u2019s an excellent drummer, and I wasn\u2019t a good enough bass player, but I recorded many of their rehearsals and gigs and named some of their early instrumental songs (mostly with bad puns).\n\nAfter the success of the books that Brian and I had written, which we got a cut of royalties for, our management decided that we should write books as our day job, and we assembled a team of experts to write up best practice guides as books in the Sun Blueprints series. I made major contributions to two Sun Blueprints books on Resource Management (1999) with Richard McDougall, Evert Hoogendoorn, Enrique Vargas, and Jim Bialaski, and Capacity Planning for Internet Services (2001) with Bill Walker. Our team also produced The Guide to High Availability by storage expert Jeannie Johnstone Kobert, Sun Cluster Environment by Enrique Vargas, Solaris PC Netlink by Don Devitt (we had a great team outing near Boston on his sailboat), and several other books. In the end, the books we wrote in our own time were more opinionated, and I think more useful, and sold better, than the official books that were more \u201ccorrect\u201d and reviewed by lots of people. I also came up with Cockcroft\u2019s Law of Book Writing, which is that books get bigger as you write them, faster than you can write them, so you need to prune them to finish them.\n\nEventually, after becoming the go-to people for capacity planning and performance for Sun, Brian Wong and I became Distinguished Engineers, and I joined the central performance engineering team in 1999. I led a small group, and had lots of fun working with Elizabeth Purcell at the Menlo Park campus that now houses Facebook, along with Roch Bourbonnais in France, and Bob Sneed in Virginia. We ran what our datacenter designer Rob Snevely (who wrote a Blueprint book that contains a chapter by Elizabeth Purcell) named the Can of Worms Project, where we would install and run performance tests on large server configurations that were more customer-like and complex than the usual test or benchmarking environments that engineering teams ran. We found many things that didn\u2019t work properly together, filed lots of bugs, and headed off a bunch of customer issues. We called this \u201cmeltdown prevention\u201d and had to work hard to get people to see that near misses are worth tracking as well as actual meltdowns.\n\nIn the early 2000s, Sun was designing it\u2019s next generation high end server, and to avoid reliability issues that had caused problems in previous systems, adopted Six Sigma to create quality best practices. I was looking for a new challenge, so joined the program and ended up getting certified as a \u201cblack belt\u201d. It was like an intense and very technical MBA, re-learning a lot of the statistics I\u2019d forgotten from my degree, along with courses in change management, requirements analysis, failure modeling, and other things I still find useful today. I worked with the engineering team that was designing the backplane connector, but the project was eventually cancelled when Sun ran out of money. I also applied Six Sigma to capacity planning and presented this at a conference in 2003.\n\nOne fun side project I got involved in was that Sun provided the back end server systems for the 2002 Winter Olympics in Salt Lake City, and I joined the effort as the capacity planning consultant, to make sure nothing would go wrong when the events were live. I used some custom monitoring tools I\u2019d built over the years as part of Rich Pettit\u2019s SE Toolkit, that included a tool called virtual-adrian, which implemented all the ideas I had come up with. Nothing went wrong, I got to see some events, and got very cold at times. I also helped out in the run-up to the 2004 Athens Olympics.\n\nThe last role I had at Sun was as Chief Architect for the High Performance Technical Computing team led by Shahin Khan, we had fun and he was one of the best managers I\u2019ve had, developing me in lots of different directions. At some point our product boss left, and I took over the role, so I spent a year or two learning how to be the product manager for our high performance computing and high end graphics product lines. I was also working alongside a very experienced Business Development Manager, Susanna Kass, to setup relationships with many internal and external partners, and got a good insight into how BD works. The team partnered with Sun\u2019s director of standards Carl Cargill, as Sun and Oracle created a new standards body, the Enterprise Grid Alliance. I worked as the main representative for Sun, and on our competing relationship with the IBM dominated Global Grid Forum. I got Paul Strong involved in the EGA before I left Sun, the two competing bodies eventually merged, and Grid and Web Services evolved into Cloud Computing. We were thinking about the trends that eventually became cloud computing when AWS launched it, but Sun didn\u2019t have the right business model to go direct to developers and sell capacity on a credit card, which is how AWS bypassed the resistance from corporate CIOs that we saw at Sun. This job stretched me in many different ways and we were having a good time, but by then Sun was shrinking and one of the many layoffs included our entire team.\n\nWe had just launched our main HPC product at an event in Shanghai, racks of Intel Linux based compute nodes using a SPARC Solaris server to configure it and provide storage, with Infiniband or Ethernet connectivity, Sun Grid Engine etc. While I was there, we heard that there were layoffs coming, but that everyone thought our team was still needed. On the way back I went to a Global Grid Forum meeting in Honolulu, representing EGA, and met Vikas Deolaliker, who said it looked as if no-one could find the head count to save the HPC team. I remember sitting in a bar with Savas Parastatidis and Jim Webber (now CTO of Neo4j) and telling them what was going on, then later that night starting an email thread with Maynard Webb, COO of eBay, on the pretext that eBay should start a new line of business auctioning compute capacity on demand, but really fishing for a job\u2026\n\nSo in mid-2004 I was laid off, looking for a change and decided to get out of the business of selling computers to big companies and figure out consumer oriented technology at eBay. I\u2019d helped eBay recover from capacity related outages in 1999 and had setup their capacity planning processes. I had also turned down a job offer at that time, but they took me on this time as a Distinguished Engineer in their Operations Architecture team.\n\nVikas left and co-founded Sonoa systems, which later became Apigee which is now part of Google Cloud. Shahin Khan ran marketing for Azul systems for a while, and is still active in the HPC community. Susanna Kass is working on renewable energy for datacenters nowadays, and we reconnected again recently.\n\nThere\u2019s too many stories, and people to mention, so apologies to those I left out, and these are my memories, which may be faulty. Sun was a formative part of my career, a very cool place to work, and was a big concentration of talent for many years.",
        "metadata": {
          "word_count": 3052,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian Cockcroft joined Sun Microsystems in 1988 as a Systems Engineer. He spent six years at Cambridge Consultants building some interesting systems. He joined Sun because he wanted to find out what they were going to release next. The early days at Sun Cambridge were special, I absorbed a lot about networking and the technical side of the role from my fellow systems engineer Martin Baines. I also got to visit Sun head office in California regularly for training sessions, and met some interesting people there. I became the Sun UK local specialist in performance and hardware. As Sun transitioned from a desktop workstation company to sell high end multiprocessor servers I was helping customers find and fix scalability problems. I was mostly coding in C, tuning FORTRAN, and when I needed to do a lot of data analysis of benchmar. K results used the S-PLUS statistics language, that is the predecessor to R. I still code in R\u2026 I still use it. I remember organizing my first week long event, having it go well, and heading back to the office to tell my manager Mike Briggs. He was pleased, but also told me he was leaving that day, to be one of the first employees of NetApp. We ended up hiring Phil Parkman to take over managing our group, which by then also included my friend Dave Fisk, who I still hang out with regularly. Sun was a very early adopter of the web. I learned about tuning systems for this new network connection intensive workload. I helped Solaris engineering tune the network stack and helped customers run some of the early web properties. Brian Wong started a \u201crotation program\u2019 that he and I ran for many years, with admin help and funding from Barb Hill. We would borrow a systems engineer from somewhere in the world for a month, and have them work with us benchmarking something or writing a paper. The first version of the Sun Performance Tuning book was published in 1987. It was written by two engineers from the UK. The book is still being updated today. Allan also introduced me to Richard McDougall, who had worked with Allan for Sun Australia, and Richard visited for a rotation to build some monitoring tools. No-one could believe how much he got done in a month, building several tools including one that could lay out the entire memory usage of a machine. Later on Richard teamed up with Jim Mauro to write the Solaris Internals series of books, and ended up finding and fixing a major bug in the kernel code. I met Paul Strong when he was working on a rotation in 1998, and we became close friends. He later moved to the US to work on Solaris, and for many years we worked and played together. After that we had a running joke that every impossibly hard problem would be solved with a ten line awk script. After the success of the books that Brian and I had written, our management decided that we should write books as our day job. I made major contributions to two Sun Blueprints books on Resource Management (1999) and Capacity Planning for Internet Services (2001) I came up with Cockcroft\u2019s Law of Book Writing, which is that books get bigger as you write them, faster than you can write them. We ran what our datacenter designer Rob Snevely (who wrote a Blueprint book that contains a chapter by Elizabeth Purcell) named the Can of Worms Project. We would install and run performance tests on large server configurations. In the early 2000s, Sun was designing it\u2019s next generation high end server, and to avoid reliability issues, adopted Six Sigma. I was looking for a new challenge, so joined the program and ended up getting certified as a \u2018black belt\u2019 The last role I had at Sun was as Chief Architect for the High Performance Technical Computing team led by Shahin Khan. I spent a year or two learning how to be the product manager for our high performance computing and high end graphics product lines. I was also workin on the 2004 Athens Olympics. Sun didn't have the right business model to go direct to developers and sell capacity on a credit card. This job stretched me in many different ways and we were having a good time, but by then Sun was shrinking. We had just launched our main HPC product at an event in Shanghai. We heard that there were layoffs coming, but that everyone thought our team was still needed. I remember sitting in a bar with Savas Parastatidis and Jim Webber and telling them what was going on. Later that night I started an email thread with Maynard Webb, COO of eBay, on the pretext that eBay should start a new line of business auctioning compute capacity on demand. Sun was a formative part of my career, a very cool place to work. I worked with a number of great people, some of whom are still active in the community."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "hpc",
        "monitoring",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_c9b9720a",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Trends and Topics for 2022 93913f12e45",
      "source": "Medium Archive",
      "published_date": "2022-01-17",
      "url": "https://medium.com/@adrianco/2022-01-17_Trends-and-Topics-for-2022-93913f12e45",
      "content": {
        "text": "Trends and Topics for\u00a02022\nSunset over Laureles Grade\u200a\u2014\u200aDecember 26th, 2021\u200a\u2014\u200aphoto taken by Adrian Cockcroft\nA year ago I did a talk at re:Invent called Architecture Trends and Topics for 2021, so I thought it was worth seeing how they played out and updating them for the coming year. These are personal thoughts across a wide range of topics, I\u2019m not speaking for my current or past employers in this post.\n\nThere were five trends and topics for 2021, Serverless First, Chaos Engineering, Wardley Mapping, Huge Hardware, Sustainability. As well as the video, there\u2019s a long podcast where I discuss them with Mik Kersten.\n\nServerless first is continuing to grow and deliver exceptional results, and after spending time with Liberty Mutual\u2019s team as they built a big success story around it, I\u2019ve been following David Anderson, Mark McCann and Mike O\u2019Reilly\u2019s progress as they write up what they\u2019ve learned and move on in their careers in an upcoming book \u201cThe Serverless Edge\u201d. I did a few talks on this subject early in the year, and linked this to the sustainability advantages of serverless architectures.\n\nThe need for systems to be resilient is still increasing, and chaos engineering tools and techniques are developing as a key way to validate that resilience is working as designed. My talks on Failing Over Without Falling Over are still very relevant, but AWS did finally release a key new service that implements key parts of the architecture: AWS Route 53 Application Recovery Controller. If you are building resilient systems with failover, the control plane that manages failover is often the hardest part of the system to get right. R53 ARC is an extremely reliable distributed control plane that should be at the center of any resilient architecture. I was part of the team that worked with financial services customers to drive the architectural requirements for this service over several years, and there\u2019s nothing else like it. I\u2019d even use it to manage datacenter failover or failover for other cloud vendors, as what you really need is a highly available control plane that is totally independent of your own failure modes.\n\nWardley Mapping is still emerging but continuing to grow it\u2019s influence and practitioners, including the Serverless First community mentioned above. As an outcome of the DevOps Enterprise Forum I collaborated on a paper called Building Industrial DevOps Stickiness by adding an introduction to Wardley Mapping to the story. I explored live mapping again at Map Camp, focused on interpreting the Green Economy ideas of famous economist Carlota Perez. Simon continues to develop some new research areas and provides good insights into longer term trends. I blogged a summary of my map camp talk here:\nMap Camp 2021\u200a\u2014\u200alinks to my maps from the Green Economy session with Carlota Perez and Andra SoneaThis year at Map Camp, I was happy to have my Amazon Sustainability team sponsor the Sustainability track, sharing the\u2026adrianco.medium.com\nHardware continues to develop rapidly, and I still think that mainstream software development is targeting an out of date, small and simplistic hardware model, and missing opportunities to leverage the vast amounts of memory, compute and network bandwidth that are being created for machine learning and high performance computing workloads. For example AWS launched an instance type with 800 Gbits/s of network bandwith in 2021. The other continuing trend is to custom silicon, with specialized accelerators and the ARM based Graviton range from AWS, Apple\u2019s M1 series, Tesla\u2019s in car processor and training engines etc. I feel there\u2019s something missing, an operating system, database, architecture and language combination that assumes that terabytes of main memory, GPUs with their own memory, and 100\u2019s of Gbits of network are the baseline and makes it easier to leverage them together to build things that haven\u2019t been seen before.\n\nFinally, I\u2019ve been working on sustainability for most of the last year. After working on getting OS-Climate started and talking about sustainability in general in my Trends for 2021 talk, I changed my role and joined the central Amazon sustainability team. At reInvent AWS announced a Customer Carbon Footprint Calculator (CCFT) and the Well Architected Pillar for Sustainability (WAS). I contributed ideas to the WAS and helped shepherd it through the release process as executive sponsor for the cross Amazon and AWS team that wrote and released it. We presented the sustainability story including a deep dive into WAS at reInvent as ARC325 Architecting for Sustainability.\n\nFor 2022 my focus is on sustainability, and I\u2019m planning to present at events\u200a\u2014\u200aprimarily virtual\u200a\u2014\u200aand develop the ideas in this deck further. Coming up on Jan 26, 2022 I\u2019ll be presenting at the online OReilly Software Architecture Superstream event.\n\nI expect most technology conferences to have some topics related to sustainability in 2022, and over time, most technology products to have sustainability as an attribute, the same way we think about security, cost, performance and usability as attributes of all the systems we interact with.",
        "metadata": {
          "word_count": 831,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A year ago I did a talk at re:Invent called Architecture Trends and Topics for 2021. These are personal thoughts across a wide range of topics. There were five trends and topics for 2021, Serverless First, Chaos Engineering, Wardley Mapping, Huge Hardware, Sustainability. The need for systems to be resilient is still increasing. Chaos engineering tools and techniques are developing as a key way to validate that resilience is working as designed. AWS has released a key new service that implements key parts of the architecture. Wardley Mapping is still emerging but continuing to grow it\u2019s influence and practitioners. I explored live mapping again at Map Camp, focused on interpreting the Green Economy ideas of famous economist Carlota Perez. I blogged a summary of my map camp talk here. Pment is targeting an out of date, small and simplistic hardware model. Missing opportunities to leverage the vast amounts of memory, compute and network bandwidth that are being created for machine learning and high performance computing workloads. For 2022 my focus is on sustainability, and I\u2019m planning to present at events. I expect most technology conferences to have some topics related to sustainability in 2022. Over time, most technology products will have sustainability as an attribute. Ystems we interact with. ystems that are part of the culture of the world we live in. Ystems can be found at: http://www.ystems.org/."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "devops",
        "engineering",
        "machine learning",
        "performance",
        "podcast",
        "resilience",
        "security",
        "serverless",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_ccdb0225",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Enhanced headroom plot in R",
      "source": "Medium Archive",
      "published_date": "2020-12-08",
      "url": "https://medium.com/@adrianco/2020-12-08_Enhanced-headroom-plot-in-R-9273ffdf163a",
      "content": {
        "text": "Enhanced headroom plot in\u00a0R\n\nImported from a Blogger post I wrote in 2008. Code shared on GitHub\n\nFor some reason I seem to find time to write code in R when I\u2019m on an airplane. The last two trips I made resulted in significant enhancements and debugging of the code for my headroom plot. It started off simple but it now has a lot of bells and whistles, including color coding.\n\nMain changes: the quantile used to remove outliers now only removes outliers that exceed the 95th percentile response time by default. It keeps all the throughput values unless you use qx=True.\n\nIn each of the throughput bins used to draw the histogram, the maximum response time for that bin is now calculated and displayed as a staircase line unless you set max=False.\n\nThe set of data is now split into ranges and color coded. The times series plot is coded so you can see the split, and the scatterplot shows how those points fall. I have been plotting weekly data at one minute intervals with split=7, which looks pretty good.\n\nI read in some data that has been extracted from vxstat into a csv format and plotted it three ways.I plot the first 2880 data points, picking the read data rather than write, two days at one minute intervals.\n> names(sops)[1] \"DateTime\" \"vxstat_dg_operationsRead\" [3] \"vxstat_dg_operationsWrite\"> chp(sops[1:2880,2],stime[1:2880,2])> chp(sops[1:2880,2],stime[1:2880,2],q=1.0)> chp(sops[1:2880,2],stime[1:2880,2],q=1.0,splits=8)\nHere is the code that generates the plot.\n> chp <-function(throughput,response, q=0.95, qx=F, xl=\"Throughput\",yl=\"Response\",tl=\"Throughput Over Time\",ml=\"Headroom Plot\", fit=T, max=T, splits=0) {      # remove zero throughput and response values      nonzer <- (throughput != 0) & (response != 0)  # array of true/false      y <- response[nonzer]      x <- throughput[nonzer]      # remove outliers, keep response time points inside 95% by default      if (q != 1.0) {              quant <- (y < quantile(y,q))              # optionally trim throughput outliers as well              if (qx) quant <- quant & (x < quantile(x, q))              x <- x[quant]              y <- y[quant]      }      # make histograms and record end points for scaling      xhist <- hist(x,plot=FALSE)      yhist <- hist(y,plot=FALSE)      xbf <- xhist$breaks[1]                          # first      ybf <- yhist$breaks[1]                          # first      xbl <- xhist$breaks[length(xhist$breaks)]       # last      ybl <- yhist$breaks[length(yhist$breaks)]       # last      xcl <- length(xhist$counts)                     # count length      ycl <- length(yhist$counts)                     # count length      xrange <- c(0.0,xbl)      yrange <- c(0.0,ybl)      xlen <- length(x)      # make a multi-region layout      nf <- layout(matrix(c(1,3,4,2),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)      layout.show(nf)      # set plot margins for throughput histogram and plot it      par(mar=c(0,4,3,0))      barplot(xhist$counts, axes=FALSE,              xlim=c(xcl*0.00-xbf/((xbl-xbf)/(xcl-0.5)),xcl*1.00),              ylim=c(0, max(xhist$counts)), space=0, main=ml)      # set plot margins for response histogram and plot it sideways      par(mar=c(5,0,0,1))      barplot(yhist$counts, axes=FALSE, xlim=c(0,max(yhist$counts)),              ylim=c(ycl*0.00-ybf/((ybl-ybf)/(ycl-0.5)),ycl*1.00),              space=0, horiz=TRUE)      # set plot margins for time series plot      par(mar=c(2.5,1.7,3,1))      plot(x, main=tl, cex.axis=0.8, cex.main=0.8, type=\"S\")      if (splits > 0) {   step <- xlen/splits   for(n in 0:(splits-1)) {    lines((1+n*step):min((n+1)*step,xlen), x[(1+n*step):min((n+1)*step,xlen)], col=4+n)   }      }      # set plot margins for main plot area      par(mar=c(5,4,0,0))      plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl, pch=20)      if (max) {        # max curve        b <- xhist$breaks        i <- b[2] - b[1] # interval        maxl <- list(y[b[1] < x & x <= (b[1]+i)])        for(n in b[c(-1,-length(b))]) maxl <- c(maxl,list(y[n < x & x <= (n+i)]))        #print(maxl)        maxv <- unlist(lapply(maxl,max)) # apply max function to elements of list        #print(maxv)        #lines(xhist$mids,maxv,col=2)  # join the dots        #staircase plot showing the range for each max response        lines(rep(b,1,each=2)[2:(2*length(maxv)+1)],rep(maxv,1,each=2),col=3)              }      if (fit) {        # fit curve, weighted to predict high throughput        # create persistent chpfit object using <<-        chpfit <- glm(y ~ x, inverse.gaussian, weights=as.numeric(x))        # add fitted values to plot, sorted by throughput        lines(x[order(x)],chpfit$fitted.values[order(x)],col=2)      }      if (splits > 0) {   step <- xlen/splits   for(n in 0:(splits-1)) {    Sys.sleep(1)    points(x[(1+n*step):min((n+1)*step,xlen)],y[(1+n*step):min((n+1)*step,xlen)], xlim=xrange, ylim=yrange, col=4+n)   }      }}\nOriginally published at http://perfcap.blogspot.com.",
        "metadata": {
          "word_count": 598,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The headroom plot in R has been enhanced. The data is now split into ranges and color coded. The times series plot is coded so you can see the split. The scatterplot shows how those points fall. I plot the first 2880 data points, picking the read data rather than write, two days at one minute intervals. Here is the code that generates the plot.  x <- quantile(x, q), y <-quant (x, y), xlen, ylen, xbl, ybl, xcl, ycl, xrange, yrange, xlen. & (x < quantile('quant', q), q) xhist, yhist, xlib, ylib, xlim, ylim, barplot, nf, layout, layout.show(nf), barplot(xlib, axes), bar plot(xLib, axes) xlib: xlib (xLib), xlim (xLim), barPlot (xlim, axes, barPlot(x Lib), axes) nf: layout (c(1, # set plot margins for response histogram and plot it sideways. par(mar=c(5,4,0,0), barplot(yhist$counts, axes=FALSE), horiz=TRUE) # set plot margin for time series plot. plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl, pch=20) if (max) { maxv <- unlist(lapply(maxl,max)) # apply max function. Chpfit can be used to predict high throughput. It can also be used as a persistent object. The following plot shows the range for each max response. The plot is sorted by throughput."
      },
      "tags": [
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_dece5565",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Will AWS Have Anything New To Say About Sustainability at re Invent 2024 Nope",
      "source": "Medium Archive",
      "published_date": "2024-11-18",
      "url": "https://medium.com/@adrianco/2024-11-18_Will-AWS-Have-Anything-New-To-Say-About-Sustainability-at-re-Invent-2024---Nope---f54a8e2d5177",
      "content": {
        "text": "Will AWS Have Anything New To Say About Sustainability at re:Invent 2024?\u00a0(Nope\u2026)\nPhoto by Adrian of my father\u2019s \u201cround tuit\u201d which I\u2019m hoping will inspire AWS to do something\u2026\nThere\u2019s an old saying that any headline that ends in a question mark can be answered with a \u201cno\u201d. My hopes aren\u2019t high, after spending the last three years waiting for anything new to appear. There\u2019s been broken promises, management changes, I\u2019ve been told that there are teams working on things like Scope 3 carbon accounting, but nothing has been released since the Well Architected Guide to Sustainability and the (extremely inadequate) Customer Carbon Footprint Tool which were announced at re:Invent in 2021. The tool was released in March 2022.\n\n[Update: Nothing new on carbon metrics, but AWS did a press release about their latest datacenter efficiency technology, which looks very good, and released Power Usage Effectiveness (PUE) data for all their regions for 2022 and 2023, which is a welcome development. I spoke with the teams and they are working on updates, but nothing that was ready to announce. Most of the new information is covered by AWS Director of Sustainability Chris Walker in the SUS101 video.]\n\nAs I have done for the last few years, I\u2019m making a list of all the sustainability related talks at re:Invent. They are mostly customer stories, encouragement to move to Graviton, and minor tweaks to the AWS sustainability story that I originally wrote for the Well Architected Guide in 2020 (it was first released over a year later). Some of my slides were still in the decks I saw last year. This year there are a bunch more AI related talks as you\u2019d expect. I\u2019ve added notes to a few of the talks that I think are particularly interesting below.\n\nENU101 | Achieving dynamic power grid operations with\u00a0AWS\n\n(Video) Reducing carbon emissions requires shifting to renewable energy, increasing electrification, and operating a more dynamic power grid. In this lightning talk, learn how customers are using AWS to perform millions of calculations on real-time grid data to execute the scenario analysis, simulations, and operational planning necessary to operate a dynamic power grid.\n\nPaul Reed, Clean Energy & Sustainability, AWS Solutions, Amazon Web Services\n\nSUS101 | Advancing sustainable AWS infrastructure to power AI solutions\n\n(Video) In this session, learn how AWS is committed to innovating with data center efficiency and lowering its carbon footprint to build a more sustainable business. Hear how AWS infrastructure is efficient for your AI workloads to minimize environmental impact as you innovate with compute, storage, networking, and more. Learn from Nasdaq, whose AI-powered environmental, social, and governance (ESG) platform uses Amazon Bedrock and AWS Lambda. Discover how their solution saves customers hours of manual effort by automating the analysis of tens of thousands of documents to better manage investor events, report internally to executive teams, and find new investors to target.\n\nMichael Stiller, Head of New Initiatives, Nasdaq, Nasdaq\n\nChris Walker, Director, AWS Sustainability, Amazon Web Services\n\nNOTE: Chris took over as the leader for AWS Sustainability earlier in 2024. He gave some new insights into how AWS has been optimizing for circularity to reduce waste, and talked about the new PUE data.\n\nSUS102 | Advancing sustainable AWS infrastructure to power AI solutions\n\nIn this lightning talk, learn how AWS is innovating to improve the sustainability of its infrastructure and how that helps its customers reduce the carbon footprint of their workloads by as much as 99%. From optimizing its data center design to investing in purpose-built chips to implementing new cooling technologies, AWS is working on ways to increase the energy efficiency of its facilities to better serve our customers\u2019 sustainability needs and the scaled use of AI. AI.\n\nErin Petrey, Sustainability Program Manager, Amazon Web Services\n\nSUS103-R | How climate tech startups build on AWS to address climate change\u00a0[REPEAT]\n\nThe world is racing to get to net-zero carbon emissions by 2050 to mitigate global warming. Speed is critical; generative AI and cutting-edge advanced cloud computing are important tools to accelerate the build and deployment of climate solutions. In this lightning talk, learn how AWS helps climate technology startups quickly and affordably build technology that is solving big problems related to climate change. It features AWS resources for climate technology startups and offers examples of AWS services that are critical for climate technology startups looking to contribute to climate mitigation efforts.\n\nBenoit de Chateauvieux, Climate Tech Solutions Architect, AWS\n\nLisbeth Kaufman, Head of Climate Tech Startups BD, Amazon Web Services\n\nSUS104 (didn\u2019t take\u00a0place)\n\nSUS105 | Training and certification support in your sustainability journey\n\nSustainability is a global priority for AWS and is of tremendous importance to its customers as well. AWS offers multiple tools and resources to address sustainability, from solutions that help you understand the impact of your AWS workloads, to technologies that can support your organization\u2019s broader sustainability goals. In this lightning talk, learn how AWS training and certification can help your organization prepare for a sustainable transition and work toward closing the green skills gap.\n\nPerry Beaumont, Senior Product Manager, AWS\n\nAIM116-S | \u201cBlack swan\u201d vs. \u201cgreen swan\u201d: Powering sustainability with data & AI (sponsored by Capgemini)\n\n(Video) In today\u2019s rapidly evolving financial landscape, traditional data and risk models are struggling to keep pace with the complexities of climate change and sustainability challenges. This session dives into pioneering a journey as a frontrunner in using data and technology to navigate this uncharted territory to proactively identify and mitigate climate-related financial risks while unlocking the opportunities presented by the sustainability transition. Explore how to harness the power of data and AI to drive growth for a sustainable future, and learn how to implement a forward-thinking approach to transform risk management into a competitive advantage. This presentation is brought to you by Capgemini, an AWS Partner.\n\nTej Vakta, Head of Sustainability Solutions & Global Wealth Management Domain Leader, Capgemini\n\nSapna Sardana, Managing Director, Barclays Investment Bank\n\nAIM128-S | Extract insights from satellite imagery at scale with WherobotsAI (sponsored by Wherobots)\n\n(Video) Inferring objects and detecting change in satellite imagery was once reserved for companies with the talent, money, and time to build, manage, and run sophisticated, self-managed machine learning (ML) inference solutions against satellite data. In this lightning talk, learn how WherobotsAI Raster Inference makes it possible for data platform and science teams to utilize satellite imagery to analyze our planet with AI faster, more reliably, and with zero carbon footprint using SQL and Python. Raster Inference is a fully managed, high-performance, carbon neutral planetary-scale computer vision solution that makes AI and ML on satellite imagery accessible to most developers and data scientists. This presentation is brought to you by Wherobots, an AWS Partner.\n\nDamian Wylie, Head of Product, Wherobots\n\nSUS201 | Data-driven sustainability with\u00a0AWS\n\n(Video) Many AWS customers are working through core sustainability challenges such as reducing emissions, optimizing supply chains, and reducing waste. However, some face challenges such as data availability, manual data collection processes, and a lack of data standardization. In this session, learn about Sustainability Data Fabric (SDF), which provides best practices for streamlined enterprise data management, prioritizing data quality, security, cataloging, and data governance. Also hear from AWS customer Cargill, who shares their data journey and how they built Jarvis, which helps optimization of carbon emissions associated with ocean transportation and uses gen AI to enable faster decision-making.\n\nJessica Benoit, Senior Sustainability Specialist, Amazon Web Services\n\nRene Greiner, Data and Digital Lead\u00a0, Ocean Transportation, Cargill International SA\n\nNOTE: yes, there are challenges with data availability from AWS itself\u2026\n\nSUS202-R1 | Destination zero: Insights and tactics for sustainable architectures [REPEAT]\n\nIn this chalk talk, explore the principles of the AWS Well-Architected sustainability pillar, in practice. Discover data sources to gain insights into your resource efficiency and environmental impact, including the AWS Customer Carbon Footprint Tool and proxy metrics from the AWS Cost & Usage Reports. From AWS architectures to web applications to AI workloads, explore the impact of shifting responsibilities when moving along the spectrum of self-managed and managed. Take a close look at services and discuss trade-offs and considerations for resource efficiency and how to keep architecture flexible as requirements change.\n\nSteffen Grunwald, Principal Solutions Architect, Amazon Web Services\n\nAlexis Bateman, Head of AWS Sustainability Tech, Amazon Web Services\n\nNOTE: Steffen was my co-presenter when we launched the Well Architected Guide to Sustainability, and Alexis has taken over the team that is hopefully going to release some new tools and data one day, so keep an eye on them\u2026\n\nSUS203 | Enabling the future: Nuclear fusion on Elastic Fabric\u00a0Adapter\n\n(Video) Fusion energy is a potential source of clean energy that could contribute to the global transition to a net-zero economy. It has been a topic of research for the last 50+ years, and now cloud technologies are helping to accelerate its pace of development. In this session, hear from AWS energy experts on the role of cloud technologies in fusion. Also learn how Realta Fusion has leveraged Amazon EC2 HPC instances and Elastic Fabric Adapter to demonstrate, for the first time, that plasma physics stability simulations are possible using the highest-fidelity plasma models available.\n\nFaraz Ahmad, Net Zero Electricity Lead, AWS\n\nSam Frank, Senior Scientist, Realta Fusion\n\nSUS204 | How to achieve water goals with AWS technology\n\nWater is a precious resource, and organizations around the globe are stepping up to do their part in protecting it. In this session, learn about AWS efforts to conserve and reuse water across its onsite operations and in communities where it operates as part of its Water+ commitment. Also learn how AWS customer Generation Park, a McCord Development project, is leveraging the Garnet Framework and AWS Partners to build an IoT water monitoring solution to reduce water wastage and set a foundation for future smart city projects.\n\nAshwin Chandran, Director Technology Innovation, McCord Development Inc\n\nBeau Schilz, Senior Manager, AWS Water Team, AWS\n\nNOTE: AWS is in a leading position amongst cloud providers in water usage effectiveness (WUE) and replenishment, doing lots of good work.\n\nSUS205 | Integrating generative AI effectively into sustainability strategies\n\n(Video) Generative AI can materially support sustainability programs by simplifying the process of analyzing environmental data to simulating new designs to evaluating product lifecycles in a fraction of the time. This session covers the fundamentals of generative AI in sustainability programs, including how to ensure alignment with broader organizational objectives. Learn how Amazon Bedrock helps AWS customer Mercado Diferente reduce 35 tons of organic food waste per month.\n\nBianca Mota, Startup Solutions Architect, AWS\n\nPaulo Moncores, Co-founder, Mercado Diferente\n\nRahul Sareen, Global Lead, Sustainability and AI solutions, AWS\n\nNOTE: I worked closely with Rahul at AWS, worth checking out this talk, although I\u2019m skeptical that GenAI is really going to help.\n\nSUS206 | Destination zero: Insights and tactics for sustainable architectures\n\nIn this lightning talk, explore the principles of the AWS Well-Architected Framework sustainability pillar in practice. Learn about data sources to gain insights into the resource efficiency and environmental impact, including the AWS customer carbon footprint tool and proxy metrics from AWS Cost & Usage Reports. From AWS architectures to web applications to AI workloads, hear about the impact of shifting responsibilities when moving along the spectrum of self-managed to managed services. Take a deep dive on services and trade-offs and considerations for resource efficiency and how to keep architecture flexible as requirements change.\n\nSteffen Grunwald, Principal Solutions Architect, Amazon Web Services\n\nAlexis Bateman, Head of AWS Sustainability Tech, Amazon Web Services\n\nNOTE: I really hope Steffen and Alexis have some new tools and data to announce this year, otherwise it will be effectively the same talk that was at the last two re:Invents.\n\nSUS207 | Cutting costs and carbon: Using AWS Graviton3 in streaming\n\nAmazon Prime Video, a leading streaming provider, collaborated with AWS to upgrade its platform using energy-efficient AWS Graviton3 processors. This lightning talk explores how companies can cut costs and carbon emissions through architectural best practices and workload optimization. Hear how AWS customers can rethink how they approach their sustainability goals by using the cloud and how Prime Video applied AWS Well-Architected best practices and overcame the technical challenges of porting to Arm, achieving performance gains and seamless migration of thousands of servers around the globe\u200a\u2014\u200aall while meeting ambitious sustainability goals thanks to the 60% energy savings provided by Graviton3.\n\nJason OMalley, Sr. Partner Solutions Architect, Amazon Web Services\n\nElliott Nash, Head of Infrastructure, Amazon\n\nNOTE: This is probably just a long way of saying that the marketing number for Graviton 3 is 60% less energy. There\u2019s no way to actually measure the energy for a workload.\n\nSUS208-R | ESG reporting with generative AI and sustainability data fabric on AWS\u00a0[REPEAT]\n\nWith a growing number of data sources for tracking the environmental impact of your organization, it becomes challenging to discover, assess the validity of, and extract values from these assets across multiple teams. Join this lightning talk to learn how a sustainability data fabric, as well as generative AI\u2013based insights, enables environmental, social, and governance (ESG) data collection, management, and reporting. Discover how, with more reliable data, your organization can solve challenges such as more accurately calculating your estimated carbon emissions, assessing climate risk, or understanding your biodiversity impact.\n\nSundeep Ramachandran, WW Sustainability Partner Solutions Architect, Amazon Web Services\n\nSUS209 | Mitigating climate change with methane detection solutions on\u00a0AWS\n\nMethane is a potent greenhouse gas with high global warming potential. Effectively managing and reducing methane emissions is crucial for climate mitigation efforts. Learn how remote sensing, Internet of Things, and AI technologies on AWS can be used to detect and quantify methane sources, offering a cost-effective and efficient approach to scalable environmental monitoring. These technologies are crucial for industries and governments aiming to reduce methane emissions, providing actionable insights that contribute to global climate change mitigation efforts.\n\nPatricia Carroll, Sr. Sustainability Specialist, AWS\n\nSUS210\u00a0?\n\nSUS211-R | Physical climate risk assessment and analysis using AWS services\u00a0[REPEAT]\n\nLearn how to assess physical climate risk using AWS services. Physical climate risk assessment provides critical insights into the expected impacts of climate change to businesses, organizations, and communities. By using AWS solution and guidance, organizations can harness advanced climate models and datasets to generate actionable insights across their portfolios, operations, and supply chains with high fidelity and granularity.\n\nRaman Pujani, Solutions Architect, AWS\n\nPatricia Carroll, Sr. Sustainability Specialist, AWS\n\nBIZ219 | How BMW is transforming sustainability operations\n\n(Video) BMW, a renowned automotive manufacturer, is collaborating with AWS to lead the way in sustainable and transparent supply chain management practices within the automotive industry. This session explores the need for a comprehensive product carbon footprint (PCF) capability that encompasses supplier emissions data collection, augmentation, aggregation, validation, and audit to streamline the exchange of supplier certificates.\n\nAnna Kantur, Senior Product Manager, AWS Supply Chain Sustainability, AWS\n\nAmeet Vaswani, Director, Technolog, Amazon Web Services\n\nOliver Ganser, Vice President\u200a\u2014\u200aProcesses, Digitalization, Governance and Catena-X, BMW Group\n\nAES301 | Scepter, Inc. uses big data to reduce methane emissions\n\nTrace gases including methane and carbon dioxide contribute to climate change and impact the health of millions of people across the globe. Discover how Scepter, Inc. aggregates vast datasets, pinpoints emissions, and helps customers like ExxonMobil monitor and mitigate methane releases. Learn how the ScepterAir data fusion platform uses advanced AWS Cloud services to analyze and extract insights from ground-based, airborne, and in-orbit data sources with low latency. These atmospheric monitoring capabilities help governments, energy companies, agriculture, waste management, healthcare, retail, and transportation industries monitor methane, carbon dioxide, and airborne particulates to help them reach their sustainability goals and meet regulatory requirements.\n\nPhilip Father, Founder & CEo, Scepter\n\nEmily Reidy, Research Scientist, ExxonMobil\n\nChantz Thomas, Sr. Space Consultant, Amazon Web Services\n\nSUS301 | Using generative AI large language models for sustainability reporting\n\nOrganizations are making commitments to sustainability but face challenges in measuring and analyzing the data required to meet these goals. One key challenge organizations face is an ability to extract datasets from different sources and synthesize this data into coherent reports, while also preparing for compliance with sustainability reporting regimes. In this workshop, learn how to use generative AI large language models (LLMs) and AWS services, such as Amazon Bedrock, AWS Lambda, and Amazon S3, to create a draft sustainability report. You must bring your laptop to participate.\n\nParth Patel, Sr Solution Architect, AWS\n\nChristian Tomeldan, Sr. Solutions Architect, Amazon Web Services\n\nSUS302-R | Sustainable urban spaces\u00a0[REPEAT]\n\nAir quality and heat risks pose major threats to human and environmental health and safety. According to an Atlantic Council study, there are currently more than 8,500 deaths annually associated with daily average temperatures above 90 degrees Fahrenheit (32 degrees Celsius), and according to the World Health Organization, ambient air pollution leads to an estimated 4.2 million deaths per year. In this workshop, you use Amazon SageMaker to ingest data from the Amazon Sustainability Data Initiative (ASDI) and uncover patterns in weather, air quality, and temperature; understand risks to urban areas; and simulate solutions that reduce risk to communities. You must bring your laptop to participate.\n\nGuyu Ye, Sustainability Solutions Architect, Amazon Web Services\n\nPauline Ting, AI/ML Specialist, Amazon Web Services\n\nSUS303 | Traceable sustainability: How to approach biodiversity tracking\n\nSustainability challenges are complex and multifaceted, requiring innovative solutions in order to trace impact. In this chalk talk, we whiteboard the integration of services like Amazon S3, AWS Glue, Amazon SageMaker, AWS Lambda, Amazon Athena, and Amazon DataZone to create a robust offering for collecting, analyzing, and deriving actionable insights from diverse data sources to solve for things like tracking biodiversity. By harnessing advanced analytics, organizations can gain a comprehensive understanding of their environmental impact, identify areas for improvement, and implement targeted strategies to drive measurable and lasting change.\n\nThomas Attree, Senior Solutions Architect, Amazon Web Services\n\nDaisy Riley, Prototype Architect, Amazon Web Services\n\nSUS304 | Using AI/ML for sustained energy efficiency in industrial operations\n\nReducing energy consumption in operational and industrial sites is critical for organizations to stay on track with their sustainability goals. AI applications can make it radically easier for organizations to optimize their energy consumption. In this session, learn how machine learning (ML) can help derive new insights using historical data from equipment-based controllers with simulations and forecasting strategies for sustained energy efficiencies. Learn how Volkswagen Poznan and Amazon have implemented ML solutions to achieve energy optimizations across their operations and facilities.\n\nMaija Anderson, Senior Practice Manager, Amazon Web Services\n\nJorn Deiseroth, CIO Volkswagen Pozna\u0144, Volkswagen Pozna\u0144 Sp. z o.o.\n\nMarco Caserta, Applied Scientist, Amazon\n\nSUS305 | Assessing climate risk with spatial analysis on\u00a0AWS\n\n(Video) Organizations around the globe are reacting to a rapidly changing physical environment. To stay resilient, customers need to quickly develop scalable systems to ingest and analyze large datasets with real-time climate and location information. From assessing climate impact to managing environmental risk, with Amazon Redshift support for spatial data and native integration to ML models through Amazon SageMaker, customers can rapidly develop analysis that helps them respond to natural world changes. In this session, learn how Tokio Marine Highland uses CARTO\u2019s spatial analytics platform on AWS to manage climate risk and assess impacts of severe weather to its business.\n\nTracey Reid, Vice President, Tokio Marine Highland\n\nJavier de la Torre, Founder & CSO, CARTO\n\nMarco Masciola, Sr. Partner Solutions Architect, Amazon Web Services\n\nSUS306-R1 | Traceable sustainability: How to approach biodiversity tracking\u00a0[REPEAT]\n\nSustainability challenges are complex and multifaceted, requiring innovative solutions in order to trace impact. In this lightning talk, see whiteboarding of the integration of services like Amazon S3, AWS Glue, Amazon SageMaker, AWS Lambda, Amazon Athena, and Amazon DataZone to create a robust data ecosystem for collecting, analyzing, and deriving actionable insights from diverse data sources to solve for things like tracking biodiversity. By harnessing advanced analytics, organizations can gain a comprehensive understanding of their environmental impact, identify areas for improvement, and implement targeted strategies to drive measurable and lasting change.\n\nThomas Attree, Senior Solutions Architect, Amazon Web Services\n\nDaisy Riley, Prototype Architect, Amazon Web Services\n\nSUS307-R1 | Optimizing generative AI workloads for sustainability [REPEAT]\n\nWith the increasing complexity, scale, and adoption of AI workloads, it is important to consider their environmental impact. This lightning talk provides environmental sustainability insights that are specific to large language models. Explore practical best practices for different customization scenarios, including training models from the beginning, fine-tuning with additional data using full or parameter-efficient techniques, Retrieval Augmented Generation, and prompt engineering.\n\nRaman Pujani, Solutions Architect, AWS\n\nNOTE: This is an interesting new topic. It\u2019s possible to get energy data in real time from NVIDIA GPUs (because NVIDIA provides it) but not from AWS hardware. I wonder if this will be discussed\u2026\n\nCMP308 | Improve sustainability and reduce costs with AWS\u00a0Graviton\n\nAre you looking to improve your price performance and reduce your environmental impact? AWS Graviton-based Amazon EC2 instances are a great fit for organizations of all sizes looking to maximize price performance and improve sustainability. In this hands-on workshop, learn how to modernize your workloads and reduce your carbon footprint by taking advantage of Graviton-based instances. Deep dive into AWS Graviton managed services, common use cases, and best practices. You must bring your laptop to participate.\n\nChad Schmutzer, Pr SA Sustainability, Graviton, Amazon Web Services\n\nMarcin Bednarz, Principal, Solutions Architect, Amazon\n\nNOTE: Yet another Graviton talk, we had several of these last year as well. Yawn.\n\nMFG312 | Managing value-chain product carbon emissions data with generative AI\n\nManufacturers face increasing demand from customers and public authorities to accurately model, calculate, and report on their product-based carbon emissions. A credible assessment requires internal data management capabilities and the efficient exchange of high-trust information with suppliers. For larger manufacturers this may involve scaling to thousands of suppliers and/or products. Learn how connectivity, trust, and information exchange come together in the value chain, and about the mechanisms behind the modeling and calculation of product-based carbon emissions\u200a\u2014\u200aand how it all can be accelerated using generative AI services on AWS.\n\nJonas Buerkel, Senior Solutions Architect, Amazon Web Services EMEA SARL\n\nCMP313-R | How to design your HPC cluster in the cloud for sustainability [REPEAT]\n\nAs organizations strive to reduce their carbon footprint and promote environmental sustainability, optimizing high performance computing (HPC) workloads in the cloud has become a critical priority. In this hands-on builders\u2019 session, learn how to design and deploy HPC clusters on AWS that deliver both performance and sustainability. Work through the process of architecting an HPC cluster using the latest AWS services, with a strong focus on the AWS Well-Architected Framework sustainability pillar. Leave this session equipped to design and deploy sustainable HPC clusters on AWS that deliver both high performance and environmental responsibility. You must bring your laptop to participate.\n\nFrancesco Ruffino, HPC Specialist SA, AWS\n\nCMP344 | Sustainable compute optimization on Amazon\u00a0EC2\n\nThis chalk talk focuses on optimizing compute efficiency and sustainability of workloads running on Amazon EC2. Learn strategies to leverage Amazon EC2 capabilities and features including instance selection, Amazon EC2 Auto Scaling, and AWS Graviton-based Amazon EC2 instances to improve workload efficiency. The talk discusses and demonstrates how to measure and track resource utilization, price performance, and resource efficiency using workload metrics. This data-driven approach helps you identify bottlenecks, optimize resource allocation, and make informed decisions to enhance the performance and environmental sustainability of your Amazon EC2-based applications.\n\nBrett Galloway, Senior Consultant, Q-CST, AWS\n\nZi Shen Lim, Sustainability GTM, Graviton, Amazon Web Services\n\nNOTE: This looks useful, although there are no metrics for energy use or environmental sustainability on AWS at the workload level.\n\nAfter re:Invent, I will update this post with the videos from the event, as I did last year. So far about half are available\u2026",
        "metadata": {
          "word_count": 3958,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS will be at re:Invent 2024. Will there be anything new to say about sustainability? It's been three years since the Well Architected Guide to Sustainability was released. This year there are a bunch more AI related talks as you\u2019d expect. I\u2019ve added notes to a few of the talks that I think are particularly interesting below. AWS is committed to lowering its carbon footprint to build a more sustainable business. Hear how AWS infrastructure is efficient for your AI workloads to minimize environmental impact. Learn from Nasdaq, whose AI-powered environmental, social, and governance (ESG) platform uses Amazon Bedrock and AWS Lambda. Learn how AWS is innovating to improve the sustainability of its infrastructure. From optimizing its data center design to investing in purpose-built chips to implementing new cooling technologies, AWS is working on ways to increase the energy efficiency. AWS helps climate technology startups quickly and affordably build technology that is solving big problems related to climate change. Speed is critical; generative AI and cutting-edge advanced cloud computing are important tools to accelerate build and deployment of climate solutions. Learn how AWS training and certification can help your organization prepare for a sustainable transition. Explore how to harness the power of data and AI to drive growth.  WherobotsAI Raster Inference makes it possible for data platform and science teams to utilize satellite imagery to analyze our planet with AI faster, more reliably, and with zero carbon footprint using SQL and Python. WheRobotsAI is a fully managed, high-performance, carbon neutral planetary-scale computer vision solution. Learn about Sustainability Data Fabric (SDF), which provides best practices for streamlined enterprise data management. Also hear from AWS customer Cargill, who shares their data journey and how they built Jarvis.  AWS Well-Architected sustainability pillar, in practice. Discover data sources to gain insights into your resource efficiency and environmental impact. Take a close look at services and discuss trade-offs. Fusion energy is a potential source of clean energy that could contribute to the global transition to a net-zero economy. In this session, hear from AWS energy experts on the role of cloud technologies in fusion. Learn about AWS efforts to conserve and reuse water across its onsite operations and in communities where it operates as part of its Water+ commitment. Also learn how AWS customer Generation Park is leveraging the Garnet Framework and AWS Partners to build an IoT water monitoring solution. Learn how Amazon Bedrock helps AWS customer Mercado Diferente reduce 35 tons of organic food waste per month. Learn about data sources to gain insights into the resource efficiency and environmental impact. Amazon Prime Video collaborated with AWS to upgrade its platform using energy-efficient AWS Graviton3 processors. Hear how companies can cut costs and carbon emissions through architectural best practices and workload optimization. Prime Video applied AWS Well-Architected best practices and overcame the technical challenges of porting to Arm. They achieved performance gains and seamless migration of thousands of servers around the globe\u2019\u2014\u2009all while meeting ambitious sustainability goals. Methane is a potent greenhouse gas with high global warming potential. Remote sensing, Internet of Things, and AI technologies can be used to detect and quantify methane sources. These technologies are crucial for industries and governments aiming to reduce methane emissions. Learn how to assess physical climate risk using AWS services. Physical climate risk assessment provides critical insights into the expected impacts of climate change. Organizations can harness advanced climate models and datasets to generate actionable insights across their portfolios, operations, and supply chains. Scepter, Inc. uses big data to reduce methane emissions. Learn how the ScepterAir data fusion platform uses advanced AWS Cloud services to analyze and extract insights. Learn how to use generative AI large language models (LLMs) and AWS services, such as Amazon Bedrock, AWS Lambda, and Amaz. Air quality and heat risks pose major threats to human and environmental health and safety. You use Amazon SageMaker to ingest data from the Amazon Sustainability Data Initiative (ASDI) and uncover patterns in weather, air quality, and temperature. You must bring your laptop to participate. Sustainability challenges are complex and multifaceted, requiring innovative solutions in order to trace impact. By harnessing advanced analytics, organizations can gain a comprehensive understanding of their environmental impact. Learn how machine learning can help derive new insights using historical data from equipment-based controllers. Learn how Volkswagen Poznan and Amazon have implemented ML solutions to achieve energy optimizations across their operations and facilities. Learn how Tokio Marine Highland uses CARTO\u2019s spatial analytics platform on AWS to manage climate risk and assess impacts of severe weather to its business. See whiteboarding of the integration of services like Amazon S3, AWS Glue, Amazon SageMaker, AWS Lambda, Amazon Athena, and Amazon DataZone. With the increasing complexity, scale, and adoption of AI workloads, it is important to consider their environmental impact. Explore practical best practices for different customization scenarios, including training models from the beginning. AWS Graviton-based Amazon EC2 instances are a great fit for organizations of all sizes. Learn how to modernize your workloads and reduce your carbon footprint by taking advantage of Graviton. Manufacturers face increasing demand from customers and public authorities to accurately model, calculate, and report on their product-based carbon emissions. For larger manufacturers this may involve scaling to thousands of suppliers and/or products. Learn how connectivity, trust, and information exchange come together in the value chain. Learn how to design and deploy HPC clusters on AWS that deliver both performance and sustainability. Work through the process of architecting an HPC cluster using the latest AWS services. You must bring your laptop to participate. This data-driven approach helps you identify bottlenecks, optimize resource allocation, and make informed decisions. There are no metrics for energy use or environmental sustainability on AWS at the workload level."
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "big data",
        "cloud",
        "engineering",
        "hpc",
        "infrastructure",
        "machine learning",
        "metrics",
        "ml",
        "monitoring",
        "performance",
        "platform",
        "scaling",
        "security",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_1585cbce",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Perhaps Consciousness is Just Human Observability",
      "source": "Medium Archive",
      "published_date": "2019-06-29",
      "url": "https://medium.com/@adrianco/2019-06-29_Perhaps-Consciousness-is-Just-Human-Observability--84dfda40d70f",
      "content": {
        "text": "Photo taken by Adrian at the Park Hyatt NingBo,\u00a0China\nPerhaps Consciousness is Just Human Observability?\n\nI recently read Annaka Harris\u2019 new book Conscious about the mysteries and nature of consciousness and had an epiphany. What we do to make our computer systems observable is actually closely related to consciousness. The essential quality of conciseness is a model of the system. That model is fed with information and interrogated about the overall health and behavior of the system. If the system is ourselves then we can ask \u201chow do you feel?\u201d, \u201cdo you need anything?\u201d, \u201cwhat did you learn today?\u201d, \u201cwhat do you think is going to happen next?\u201d, \u201cWhat is the solution to this problem?\u201d, \u201cwhat do you think about this new information?\u201d etc. We have a verbal query engine for our model. The model isn\u2019t really in charge of us in the sense that we feel it is, it\u2019s monitoring and making sense of what is happening around it, after the event. The illusion that we consciously decide in advance is discussed and deconstructed in Sam Harris\u2019 book Free Will. (Yes, Sam and Annaka are a couple). We can query the model to evaluate how well we think we are operating, apply feedback, and pre-calculate the effect of future actions.\n\nThe modeling capacity of the human brain is very sophisticated. All mammals are built from the same components, so it makes sense that a mouse is also conscious. Basically anything with neurons (e.g. lobsters have about 100,000 neurons) is capable of modeling its environment and reacting to danger, food etc. with an ability to update the model (learn) from experience. There\u2019s also evidence that plants and bacteria are also capable of modeling and learning about their environment without neurons. We can call this consciousness as long as we understand that their models are simpler and can only store and react to a smaller set of inputs than a human. There is however no magical cutoff point where living things stop being conscious. There\u2019s a school of thought that this continues into inanimate objects however there is a clear cutoff because there needs to be a place to store and mutate the model, however simple. So an individual atom is not conscious, and taking the law of conservation of energy and Shannon\u2019s Information Theory into account, there have to be enough particles arranged in a way that can capture information and form a model that can be queried for a thing to be able to react in a way that learns and adapts to its environment. At its most basic, something like a rock formation or a crystal may be arranged in a way that reacts to its environment and captures a crude model. We build systems from crystals that we call computers which can capture increasingly sophisticated models, so in that sense computers have their own primitive form of consciousness.\n\nWe experience a very specific kind of human consciousness, so the question arises, whether we can make computers that have recognizable consciousness that we can communicate with, or whether computers will develop their own consciousness that is eventually more sophisticated than ours. I don\u2019t see why not on both counts, and think we\u2019re moving in that direction.\n\nThis is different to the question of whether we can figure out how to create artificial intelligence, as I don\u2019t think intelligence is a prerequisite for consciousness, its an attribute of more sophisticated conscious systems that allows us to interact with and view the internal model more directly than observing the raw behavior of the system. It\u2019s possible for a human to be unconscious or asleep, without losing intelligence in the process. When we wake up, we\u2019re intelligent again. Artificial general intelligence and \u201ccommon sense\u201d require a very sophisticated model of much of the world so that we can interact with the AGI as if it was a human. I think our attempts to build AGI systems will have consciousness as part of the model of their own operations that forms their monitoring and observability capability.\n\nIf we consider a conventional computer system built to perform a common task like signing up new customers via a series of forms on a web site it isn\u2019t conscious in itself. If all it does is log what it does, then a human operator looks at the logs and applies a model of how the system is supposed to work, to decide whether its working happily or not, and what might happen if lots more customers tried to use it at once. If we add a monitoring and alerting system to watch over the web site, then that system also has to encode a model by having someone create rules and tune alert thresholds. If we build a monitor that learns what normal and abnormal behavior looks like, and we can ask it whether the system is happy or not, then we have started to add a simple form of consciousness. If we add a capacity planning model then questions about the future can be asked, and the ability to query the observability model using natural language could make it have a human-like situational awareness that would operate and interact in way that would appear conscious to a human. As it absorbed new information and evaluated it\u2019s own alerts and state and capacity model, it would be thinking to itself. I don\u2019t think that\u2019s different in kind from our own consciousness.\n\nIf we accumulate the state of consciousness over time then we end up with emotions. The (dear departed) Anki Vector robot was an interesting attempt to build an emotional model that humans could interact with. A Vector that has been idle for a long time is bored, it\u2019s capable of detecting motion and using basic face detection to tell whether someone is paying attention to it. A bored robot tries to find a human and to attract attention. If it tries a task like piling blocks up and succeeds it makes happy movements. If its battery is running low it gets hungry and goes back to it\u2019s charging station. It has a simple internal model that is expressed using cues that humans can understand, and it is conscious while it\u2019s operating, and unconscious when it\u2019s sleeping and recharging.\n\nIt makes perfect sense to me for a computer system to express its state in emotional terms. Accumulating states over time and deciding whether it is happy, sad, confused, bored, stressed, worried, hurting etc. could be a useful way to express state in a way that humans would relate to more than graphs, numbers or a mailbox full of alerts. Then we could ask the system \u201cwhat\u2019s wrong?\u201d, \u201cwhy are you worried?\u201d, \u201cwhere does it hurt?\u201d, \u201cwhat do you need?\u201d and get useful replies that reflect the consciousness of the system.\n\nBack in 1995, after a vacation in Australia, I wrote a monitoring tool for Sun\u2019s Solaris based systems called \u201cvirtual_adrian\u201d. It did what I would do if I was looking at the system myself, so I could go on holiday and still be watching over systems. It didn\u2019t plot lots of graphs and pages of data, most of the time it just reported \u201cNo worries, mate\u201d. That was all you needed to know. If the system was unhappy, it would report something like \u201cDisk sd1 is slow\u201d. It was an open source free tool, and if your machine ran out of swap space and crashed, the last message from virtual_adrian you would see on in the console log was a quote from Terry Pratchett\u2019s Discworld \u201c+++OUT OF CHEESE ERROR, PLEASE REINSTALL UNIVERSE AND REBOOT+++\u201d. It seemed appropriate. To me this looks like an early attempt to give a computer a conscious sense of it\u2019s own happiness, and a bit of personality.\n\nBottom line, my answer to the question about whether consciousness is an intrinsic capability of things all the way down to inanimate objects, is that the object has to have a mechanism for adaptively modeling its health and environment to be conscious.\n\nUpdate: 2022\u200a\u2014\u200aA very interesting talk put on by the Sante Fe Institute aligns well with my thinking on this subject. I need to dig into this in more detail, Autodiagnosis and the Dynamical Emergence Theory of Basic Consciousness https://www.youtube.com/watch?v=uJabuvcQTQA",
        "metadata": {
          "word_count": 1380,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian: What we do to make our computer systems observable is closely related to consciousness. We have a verbal query engine for our model. The model isn\u2019t really in charge of us in the sense that we feel it is, it\u2019s monitoring and making sense of what is happening around it. The modeling capacity of the human brain is very sophisticated. We can query the model to evaluate how well we think we are operating, apply feedback, and pre-calculate the effect of future actions. There is however no magical cutoff point where living things stop being conscious. An individual atom is not conscious, and taking the law of conservation of energy and Shannon\u2019s Information Theory into account, there have to be enough particles arranged in a way that can capture information. r there is a clear cutoff because there needs to be a place to store and mutate the model, however simple. This is different to the question of whether we can figure out how to create artificial intelligence. It\u2019s possible for a human to be unconscious or asleep, without losing intelligence in the process. When we wake up, we\u2019re intelligent again. I think our attempts to build AGI systems will have consciousness as part of the model of their own operations. If all it does is log what it does, then a human operator looks at the logs and applies a model of how the system is supposed to work. If we add a capacity planning model then questions about the future can be asked. The ability to query the observability model using natural language could make it have a human-like situational awareness. The Anki Vector robot was an interesting attempt to build an emotional model that humans could interact with. A Vector that has been idle for a long time is bored. A bored robot tries to find a human and to attract attention. If it tries a task like piling blocks up and succeeds it makes happy movements. If its battery is running low it gets hungry and goes back to it\u2019s charging station. In 1995, after a vacation in Australia, I wrote a monitoring tool for Sun\u2019s Solaris based systems called \u201cvirtual_adrian\u201d It did what I would do if I was looking at the system myself, so I could go on holiday and still be watching over systems. Accumulating states over time and deciding whether it is happy, sad, confused, bored, stressed, worried, hurting etc. could be a useful way to express state. Rtual_adrian is a programmer at the Sante Fe Institute in New Mexico. He is interested in the theory of consciousness and how it can be applied to computers."
      },
      "tags": [
        "artificial intelligence",
        "monitoring",
        "observability"
      ]
    },
    {
      "id": "virtual_adrianco_file_50a60f81",
      "kind": "file",
      "subkind": "medium_post",
      "title": "That s not the issue there are lots of circuits feeding the lights in and around this room 78977e22e32",
      "source": "Medium Archive",
      "published_date": "2018-02-05",
      "url": "https://medium.com/@adrianco/2018-02-05_That-s-not-the-issue--there-are-lots-of-circuits-feeding-the-lights-in-and-around-this-room--78977e22e32",
      "content": {
        "text": "That\u2019s not the issue, there are lots of circuits feeding the lights in and around this room.",
        "metadata": {
          "word_count": 17,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f4212ffc",
      "kind": "file",
      "subkind": "medium_post",
      "title": "It s hard for 100 people to cooperate together to build a monolith but if you had 10 teams of 10",
      "source": "Medium Archive",
      "published_date": "2023-05-08",
      "url": "https://medium.com/@adrianco/2023-05-08_It-s-hard-for-100--people-to-cooperate-together-to-build-a-monolith--but-if-you-had-10--teams-of-10--1e8d3d94809a",
      "content": {
        "text": "It's hard for 100+ people to cooperate together to build a monolith, but if you had 10+ teams of 10 people deploying their own microservices it's easier to decouple and to deliver at whatever pace each team needs.",
        "metadata": {
          "word_count": 38,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "It's hard for 100+ people to cooperate together to build a monolith, but if you had 10+ teams of 10 people deploying their own microservices it's easier to decouple."
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_c1a75b42",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Hi Steve thanks for the write up I spent some time with Grab last year when we featured them at",
      "source": "Medium Archive",
      "published_date": "2018-01-24",
      "url": "https://medium.com/@adrianco/2018-01-24_Hi-Steve--thanks-for-the-write-up--I-spent-some-time-with-Grab-last-year-when-we-featured-them-at--2a8ec9083807",
      "content": {
        "text": "Hi Steve, thanks for the write-up, I spent some time with Grab last year when we featured them at the AWS Summit Singapore and I agree it\u2019s a good culture with a nice technology stack and they are doing very well. We\u2019re looking forward to seeing what you get up to, and let me or anyone at AWS know how we can help. Best wishes!",
        "metadata": {
          "word_count": 65,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS is looking forward to seeing what you get up to, and let me or anyone at AWS know how we can help. Best wishes!"
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_file_a569cd9f",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Signs that it s time to leave a company",
      "source": "Medium Archive",
      "published_date": "2023-12-31",
      "url": "https://medium.com/@adrianco/2023-12-31_Signs-that-it-s-time-to-leave-a-company--5f8759ad018e",
      "content": {
        "text": "Signs that it\u2019s time to leave a\u00a0company\u2026\nSeagull taking off\u200a\u2014\u200aPicture by\u00a0Adrian\nWhen the company stops growing, when the founder moves on, when HR is calling the shots, when the executives care more about real estate than products\u2026\n\nIt\u2019s time to leave, and find a new company to work for.\n\nI joined AWS about seven years ago, when it was growing fast, there was headcount and flexibility in hiring, and Amazon was famously cheap about the office space it occupied. I retired a year and a half ago, and many of the people I worked with have also left. [Update] Justin Garrison just quit, but before he left he just wrote a critical blog post \u201cAmazon\u2019s Silent Sacking\u201d about the way they are pushing people out. What changed?\n\nGrowth slowed. When companies transition from high growth to slow growth or begin shrinking it isn\u2019t just a small change, it needs a fundamentally different approach to management and the culture of the company. I saw this play out while I was at Sun Microsystems as the dot com bubble burst in 2001\u20132004. Adding headcount and budget every year covers up all kind of problems, it brings a flexibility to operations that becomes part of the culture. When growth is flat to shrinking companies freeze like deer in the headlights, and to make it worse managers start to hoard headcount and play politics to preserve their products. Innovation goes away, micromanagement appears and everyone gets less useful work done. I haven\u2019t seen a good way to gradually manage this situation. The most successful approaches I\u2019ve seen (which Sun didn\u2019t do) are to cut deeply and early by shutting down entire product lines and removing layers of management, so that the company can grow back by opening headcount again. It\u2019s like pruning a sickly tree. If you cut off the dead leaves one by one, you end up with too many branches and shriveled fruit. If you cut back hard, and remove whole branches, the roots provide enough energy for the tree to grow back. This does mean that you have to shut down or sell off entire product lines, and shrink the business. It\u2019s very hard to contemplate and navigate this, which is why most companies don\u2019t do it agressively enough. As an employee, it\u2019s usually best to leave in the first wave of cuts.\n\n[Update] The other approach that seems to work is to keep people but cut pay, and provide new stock options to compensate. I think this is a good approach for several reasons. If the main problem is a lack of revenue growth, due to a general market slowdown, then everyone is trying to cut at once. The effective market value of employees has dropped, you can hire people for less, so reducing cash flow by cutting salaries makes more sense than layoffs. Using the savings to grow by hiring lower cost remote employees reduces demand on office space, which should be the first thing you cut back on anyway. If the stock price has already dropped then issuing new options at the lower price is helpful, it defers the cost and ties it to future success. Keeping most of the people you have in place maintains product investment levels, which maximizes the chance of keeping revenue and out executing competition. In a fediverse discussion about this topic one comment was that to get through COVID individual contributors took a 10% pay cut, and the CEO took a 50% pay cut, with a sliding scale on the way up the org chart.\n\nIn the unlikely event that a deep pruning and grow back strategy is happening at your company, and you are still there, the product teams that are left after pruning should try to keep the most experienced employees, lay off the junior ones, and return managers to individual contributor positions where possible. Experienced employees have been through this before, make better judgement calls under stress, and communicate better. There is a tendency to micromanage and add process overhead that needs to be resisted. Simplify processes to reduce bureaucracy and management overhead, speed up time-to-value, and take advantage of the gaps in the market that appear as competitors fail.\n\nFounder led companies often have problems maintaining their innovation culture when the founder moves on. I think this is part of the problem at Amazon, and I was happy to be leaving as Andy Jassy took over from Jeff Bezos and Adam Selipsky took over AWS. Jeff Bezos was always focused on keeping the \u201cDay 1\u201d culture at Amazon, and everyone I talk to there is clear that it\u2019s now \u201cDay 2\u201d. Politics and micromanagement have taken over, and HR processes take up far too much of everyone\u2019s time.\n\nThere\u2019s another red flag for me when large real estate construction projects take up too much management attention. The plans for Amazon HQ2, and the building of ever larger and fancier office tower blocks in Seattle by Amazon collided with Covid, lockdown and an accelerated work from home movement. The right thing to do coming out of lockdown would have been to write down the real estate investment in one go, re-negotiate the tax incentives that cities provide, embrace remote working, and continue the policy Amazon had at the time, where each director level leader could decide what was best for their teams. Instead, we now have the situation that Amazon management care more about real estate than product. Where is the customer obsession in that? Customers don\u2019t care what the companies buildings are like. As Justin says, they are using Return To Office (RTO) to drive people out of the company without needing to fire them. By analogy, pruning by cutting off the best remaining leaves and fruit buds one at a time, leaving the dead wood behind.\n\n[Update] HR teams tend to copy policies across companies, and we are seeing more places bring in clearly obnoxious RTO policies designed to save money by avoiding redundancy payouts. WebMD is one, and Broadcom has told the newly aquired VMware employees to RTO as well.\n\n[Update] There are a few studies that seem to indicate that while there are some high profile companies driving RTO, the actual majority trend is to accept the change and make hybrid work, and employees prefer it. Work from home was 3% before lockdown, went to almost 50% during lockdown, and is now stable at 22% or so.\n\n[Update] Since posting this I\u2019ve had private messages from people at Amazon thanking me for saying what they aren\u2019t allowed to say internally. \u201cDisagree and commit\u201d has been weaponized to force the RTO policy on everyone. In a group making a decision, it\u2019s usually a majority that agree with the decision, and the people that don\u2019t agree see that they are outnumbered and commit to it. However with RTO I think it is a minority imposing an unpopular policy on everyone else, without supporting data or discussion. Disagreeing is career-limiting, so what we are seeing is \u201cDisagree and quit\u201d. People are interviewing while waiting for their next RSU grant. Someone also said that it\u2019s not \u201cDay 2\u201d it\u2019s \u201cDay 3\u201d, and did I know anywhere that is hiring?.\n\nIn summary, I don\u2019t think the situation for Amazon is as bad as it was for Sun in 2002, and in the short term they are going to continue to grow the business slowly. However I do think there\u2019s lessons to be learned, and that the delusion that they can roll back work from home and enforce RTO without killing off innovation is a big problem that will increasingly hurt them over time. I personally hired a bunch of people into AWS, in my own team and by encouraging people to join elsewhere. Nowadays I\u2019d say a hard no to anyone thinking of working there, they have shown that you can\u2019t trust whatever they say during the interview process. Local management gets overruled to relocate people hired to be remote, and to move to locations that they can change arbitrarily. Try and get a job at somewhere growing rapidly with a sensible work location policy like NVIDIA instead.",
        "metadata": {
          "word_count": 1362,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "When the company stops growing, when the founder moves on, when HR is calling the shots, when executives care more about real estate than products, it\u2019s time to leave. I saw this play out while I was at Sun Microsystems as the dot com bubble burst in 2001. When companies transition from high growth to slow growth or begin shrinking, it needs a fundamentally different approach to management and the culture of the company. When growth is flat to shrinking companies freeze like deer in the headlights, and to make it worse managers start to hoard headcount and play politics to preserve their products. I haven't seen a good way to gradually manage this situation. The most successful approaches I\u2019ve seen are to cut deeply and early by shutting down entire product lines. As an employee, it\u2019s usually best to leave in the first wave of cuts. The other approach that seems to work is to keep people but cut pay, and provide new stock options. In the unlikely event that a deep pruning and grow back strategy is happening at your company, and you are still there, the product teams that are left after pruning should try to keep the most experienced employees. Experienced employees have been through this before, make better judgement calls under stress, and communicate better. Simplify processes to reduce bureaucracy and management overhead, speed up time-to-value. Amazon has had problems maintaining their innovation culture when the founder moves on. Jeff Bezos was always focused on keeping the \u2018Day 1\u2019 culture at Amazon, and everyone I talk to there is clear that it\u2019s now \u201cDay 2\u2019 Amazon is using Return To Office (RTO) to drive people out of the company without needing to fire them. WebMD is one, and Broadcom has told the newly aquired VMware employees to RTO as well. \u201cDisagree and commit\u201d has been weaponized to force the RTO policy on everyone. In a group making a decision, it\u2019s usually a majority that agree with the decision, and the people that don\u2019t agree see that they are outnumbered and commit to it. However with RTO I think it is a minority imposing an unpopular policy onEveryone else, without supporting data or discussion. Disagreeing is career-limiting, so what we are seeing is \u201cDis disagree and quit\u201d AWS has shown that you can't trust whatever they say during the interview process. Local management gets overruled to relocate people hired to be remote. Try and get a job at somewhere growing rapidly with a sensible work location policy."
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_file_97f3ce3b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Map Camp 2021 links to my maps from the Green Economy session with Carlota Perez and Andra Sonea",
      "source": "Medium Archive",
      "published_date": "2021-11-16",
      "url": "https://medium.com/@adrianco/2021-11-16_Map-Camp-2021---links-to-my-maps-from-the-Green-Economy-session-with-Carlota-Perez-and-Andra-Sonea-2fa62401fdc8",
      "content": {
        "text": "Map Camp 2021\u200a\u2014\u200alinks to my maps from the Green Economy session with Carlota Perez and Andra\u00a0Sonea\nRed sky sunset\u200a\u2014\u200aphoto taken by Adrian in Monterey County, California\nThis year at Map Camp, I was happy to have my Amazon Sustainability team sponsor the Sustainability track, sharing the support with Liam Maxwell of AWS UK Public Sector, and to share the online stage with two awesome speakers, the famous economist Carlota Perez\u200a\u2014\u200aauthor of the book Technological Revolutions and Financial Capital, and finance researcher Andra Sonea.\nMapcamp 2021 The Green Economy on YouTube\u200a\u2014\u200athree talks in 95min\nCarlota went first, explaining the way revolutions follow each other, and that we are in the middle of the Information Technology revolution, at one of the most disruptive phases, and her prediction that there could be a more stable golden age coming next. Andra talked about the role of financial frameworks and institutions have in biasing the banking system towards investing in fossil fuels and how that needs to be changed. I went last, and produced a series of maps to try and explain some of Carlota and Andra\u2019s points. My goal was to understand their positions better, and also to show people some mapping techniques, and how I approach building maps using the onlinewardleymaps tool. I spent some time before the session, reading Carlota\u2019s book, watching a video of her speak earlier this year, and producing some maps that I hoped would represent her ideas. During both talks, I live-mapped some points that I hadn\u2019t covered in my pre-work. All the maps are available as clone links below, and these links create your own private copy of the live map that you can play around with.\n\nThe maps are made through the lens of the things I found interesting and wanted to talk about. They leave out a lot, and that\u2019s one of the things about the map maker role, you have to be mindful about what isn\u2019t being discussed, and be open to making more maps. In this case Carlota and Andra didn\u2019t have maps in their presentations, so I probably over-editorialized my view of the discussion in what follows.\n\nMy first map explored Carlota\u2019s description of the start of the Age of Automobiles, Oil, Plastics and Mass Production from a technology evolution perspective, using a consumer anchor for the value chain. This dates back to the launch of the Model T Ford, in 1908 or so. Marketing of products to consumers and financing purchase of those products were new ideas at the time. I revealed the map gradually, by uncommenting each section.\n\n[Note, apologies but I misspelt Carlota as Carlotta in the image titles at the time, but fixed it for the maps at the clone links]\n\nhttps://onlinewardleymaps.com/#clone:WHaMUc5iAqXY86zv2D\nThe start of the age of Automobiles, Oil, Plastics and Mass Production\nWe can represent the evolution of some of these components through this age, which ended in 1971, although the map starts to get hard to read.\nEvolution of the age of Automobiles, Oil, Plastics and Mass Production\nConsumer finance by this time is a utility, via credit cards. Mass production is also a utility, outsourced to places like China, not generally owned by the companies selling the products. Oil has also become a globally traded commodity with managed prices and production levels.\n\nSince this is hard to read, I created a map that only shows the end state, including moving Plastics to be a product.\n\nhttps://onlinewardleymaps.com/#clone:vzPZ45l0BwgOzDjeKy\nEnd of the age of Automobiles, Oil, Plastics and Mass Production\nNext, we can use this as the basis for the start of the IT age, by removing some components that aren\u2019t relevant, and pushing others down to make space for the innovations that depend on them, to show some technologies that were in place in the 1980\u2019s, following invention of the microprocessor in 1971.\n\nhttps://onlinewardleymaps.com/#clone:PPIlhwYoRYvf6B5YgP\nStart of The Age of Information Technology and Telecommunications\nNext I tried to update this to the technologies that we see around us now, picking a few key ones, and leaving out many more.\n\nhttps://onlinewardleymaps.com/#clone:12qJQo7buLVYqUfMkg\nNow in the Age of Information Technology and Telecommunications\nI decided to highlight a few things that I think are significant, focus on the smartphone and ignore laptops and other devices. Marketing has reversed from selling products to people, to selling people to products, as \u201cfree\u201d services like Facebook and Youtube extract value from the way people use their services. Secondly, there is an interesting trend away from commodity microprocessors, in that Apple, AWS and NVIDIA are all building highly differentiated CPUs to their own design, to optimize for very different workloads. Renewable energy is not yet a commodity, as large consumers are placing power purchase agreements and funding wind, solar and battery farms directly. However telecommunications has largely become a \u201cdumb pipe\u201d connecting smartphones to the Internet with ever higher bandwidth, to support the emergence of video based applications. The automobile is gradually being replaced by smartphone based rideshare services.\n\nThinking about where this may go next, I made the map below to highlight a few trends, and then got into some discussion with Carlota and Andra about whether this captured the essence of their ideas or not.\n\nhttps://onlinewardleymaps.com/#clone:D3EidZsAjITsKDOTeY\nA possible future for the Age of Information Technology and Telecommunications\nOne observation is that social apps are not a static product, it appears that every few years a new app appears, like Tiktok getting to billions of users in a few years, so there is a pipeline of innovations that rapidly monetize as a marketing platform. Facebook saw this and bought Instagram and WhatsApp as they took off, but Snap, Telegram, Tiktok and others provide alternatives.\n\nVideo is moving from download focused (Netflix, Youtube, etc.) to bidirectional (Zoom, Facetime etc.) which is going to enable new innovations as the deployment of 5G and broadband at home spreads around the world, and brings the developing nations into the mix.\n\nRideshare is going electric, and incrementally to autonomous rides, driven by the cost savings for \u201ceasy\u201d routes that can be navigated without having to pay for a driver.\n\nRenewable energy will become the default for grids globally, and together with the end of the gasoline powered automobile market, and the trends away from gas for heating, and from high carbon footprint plastics, there\u2019s going to be a rough ride for fossil fuel companies. I\u2019m expecting wild price swings and the eventual collapse of the sector.\n\nDuring her talk, Carlotta spoke about the role of Government, and Andra pointed out that finance needed to be included, so I live mapped what I was hearing, using an evolution axis based on the acceptance of ideas in society, rather than technology maturity.\n\nhttps://onlinewardleymaps.com/#clone:6fnXJnSUwe8DHQxO3S\nThe Role of Government in the Age of\u00a0IT\nThere are three areas of focus. Post pandemic reconstruction, the climate threat, and social sustainability. In particular, education moves from institutionalized to personalized, and supports creativity and meaning rather than rote learning of facts in a classroom. Carlota is clear that the opportunity for a golden age to develop depends upon policies that need to become socially accepted, voted for and adopted by governments around the world.\n\nI also re-worked the evolution of society map that I produced for Map Camp 2020, to align with the beginning and current points in the age of IT.\n\nhttps://onlinewardleymaps.com/#clone:uCuwB3bYGgxLJwHNnU\n\nThere are several points I made but here I will start with the minimum wage, which was introduced in 1938, but was extended to more categories of workers in the 1960\u2019s. It hasn\u2019t kept pace with inflation and is a subject that executives of large corporations have shared their opinions on to try and influence the debate.\nEvolution of Society on the concept of Minimum\u00a0Wage\nUiversal basic income is emerging as a more controversial topic. It\u2019s still building consensus and is regarded as experimental, but could be part of the golden age societal norms in the coming years. The point of this kind of map is to explore emerging concepts and figure out \u201cwhat if\u201d scenarios for executives and corporations to decide what and when they should be advocating for as ideas mature.\nEvolution of Society on the concept of Universal Basic\u00a0Income\nWe ended the discussion with a map of Andra\u2019s talk. She discussed how banks collateralize their lending and that it currently is stacked to favor fossil fuel companies. This was a good idea back when the most valuable companies in the world were fossil fuel companies, but is now causing problems as they are becoming risky bets, however the rules and policies need to change, and the more people that understand the issue, and can advocate for the change, the better. There\u2019s a more recent discussion of this issue in this Canary Media story.\n\nhttps://onlinewardleymaps.com/#clone:RiIfjg2OC86375lbKa\n\nI waited for the video of the session to be on YouTube before publishing, so this took place a few weeks ago. I hope people find it interesting and useful, and that it helps build awareness of mapping and sustainability.",
        "metadata": {
          "word_count": 1505,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This year at Map Camp, I was happy to share the online stage with two awesome speakers. The famous economist Carlota Perez and finance researcher Andra Sonea. Andra talked about the role of financial institutions in biasing the banking system towards investing in fossil fuels. The maps are made through the lens of the things I found interesting and wanted to talk about. All the maps are available as clone links below, and these links create your own private copy of the live map. My first map explored the start of the Age of Automobiles, Oil, Plastics and Mass Production. This dates back to the launch of the Model T Ford, in 1908 or so. I revealed the map gradually, by uncommenting each section. Next, we can use this as the basis for the start of the IT age, by removing some components that aren't relevant, and pushing others down. I tried to update this to the technologies that we see around us now, picking a few key ones. Marketing has reversed from selling products to people, to selling people to products. \u201cfree\u201d services like Facebook and Youtube extract value from the way people use their services. Renewable energy is not yet a commodity, as large consumers are placing power purchase agreements and funding wind, solar and battery farms directly. telecommunications has largely become a \u2018dumb pipe\u2019 connecting smartphones to the Internet with ever higher bandwidth. Social apps are not a static product, it appears that every few years a new app appears, like Tiktok getting to billions of users in a few years. Video is moving from download focused to bidirectional (Zoom, Facetime etc.) Rideshare is going electric, and incrementally to autonomous rides, driven by the cost savings for \u201ceasy\u201d routes that can be navigated without having to pay for a driver.Renewable energy will become the default for grids globally, and together with the end of the gasoline powered automobile market. Carlota is clear that the opportunity for a golden age to develop depends upon policies that need to become socially accepted, voted for and adopted by governments around the world. titutionalized to personalized, and supports creativity and meaning rather than rote learning of facts in a classroom. Evolution of Society on the concept of Universal Basic\u00a0Income. t of the golden age societal norms in the coming years. We ended the discussion with a map of Andra\u2019s talk. She discussed how banks collateralize their lending and that it currently is stacked to favor fossil fuel companies. \"I hope people find it interesting and useful, and that it helps build awareness of mapping and sustainability,\" says the creator of the map. \"I hope it helps people find out more about mapping,\" he adds."
      },
      "tags": [
        "aws",
        "netflix",
        "platform",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_609caf14",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Hi Joe thanks for the comment",
      "source": "Medium Archive",
      "published_date": "2016-06-05",
      "url": "https://medium.com/@adrianco/2016-06-05_Hi-Joe--thanks-for-the-comment--6f9d0373fab1",
      "content": {
        "text": "Hi Joe, thanks for the comment. I agree there is a lot of inertia. My point was really that you can see where disruption is going to happen eventually, but it\u2019s not usually possible to predict when it\u2019s going to happen. Trends are sometimes slow and linear but often have tipping points where a hard to predict individual event or person can accelerate change. For example if the Icelandic Pirate Party won an election and implemented direct democracy there would be a real test of the concept, if they don\u2019t win then it remains theoretical and has much less influence. https://en.wikipedia.org/wiki/Pirate_Party_(Iceland)",
        "metadata": {
          "word_count": 101,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I agree there is a lot of inertia. Trends are sometimes slow and linear but often have tipping points where a hard to predict individual event or person can accelerate change. For example if the Icelandic Pirate Party won an election and implemented direct democracy there would be a real test of the concept."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b1d2fca8",
      "kind": "file",
      "subkind": "medium_post",
      "title": "You don t add innovation to a culture you get out of its way",
      "source": "Medium Archive",
      "published_date": "2017-03-15",
      "url": "https://medium.com/@adrianco/2017-03-15_You-don-t-add-innovation-to-a-culture--you-get-out-of-its-way--2e6148349aae",
      "content": {
        "text": "You don\u2019t add innovation to a culture, you get out of its way. I\u2019ve often told the anecdote that at a CIO summit I got the comment \u201cwe don\u2019t have these Netflix superstar engineers to do the things you\u2019re talking about\u201d, and when I looked around the room at the company names my response was \u201cwe hired them from you and got out of their way\u201d. An unenlightened high overhead culture will drag down all engineers to a low level, maybe producing a third of what they would do, working on their own. A high performance culture as I\u2019ve seen at Netflix and AWS builds platforms and encourages practices that magnify the output of the engineers. The same person could produce ten times as much. This isn\u2019t for everyone though. Here\u2019s an analogy: just about everyone knows how to drive on the street, but if you take your team to a racetrack, sit them in a supercar and tell them to go as fast as they like, you\u2019ll get three outcomes. Some people will be petrified, drive slowly, white knuckles on the steering wheel, and want to get back to driving on the street. Those are the developers that should stay in a high process, low risk culture. Some people will take off wildly at high speed and crash on the first corner. They also need process and structure to operate safely. The people that thrive in a high performance culture will take it easy for the first few laps to learn the track, gradually speed up, put a wheel off the track now and again as they push the limits, and enjoy the experience.\n\nThanks for the great post Stephen!",
        "metadata": {
          "word_count": 281,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "An unenlightened high overhead culture will drag down all engineers to a low level, maybe producing a third of what they would do, working on their own. A high performance culture builds platforms and encourages practices that magnify the output of the engineers. Some people will take off wildly at high speed and crash on the first corner. They also need process and structure to operate safely. Those are the developers that should stay in a high process, low risk culture."
      },
      "tags": [
        "aws",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_4d3859d4",
      "kind": "file",
      "subkind": "medium_post",
      "title": "HPC clusters on demand Another useful step in the trend to make difficult things instant",
      "source": "Medium Archive",
      "published_date": "2016-11-18",
      "url": "https://medium.com/@adrianco/2016-11-18_HPC-clusters-on-demand--Another-useful-step-in-the-trend-to-make-difficult-things-instant--a21f4e163b7e",
      "content": {
        "text": "HPC clusters on demand. Another useful step in the trend to make difficult things instant.",
        "metadata": {
          "word_count": 15,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_file_bebdf5a8",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next 2022 Conference Appearances",
      "source": "Medium Archive",
      "published_date": "2022-08-02",
      "url": "https://medium.com/@adrianco/2022-08-02_What-Adrian-Did-Next--2022-Conference-Appearances-672710583c80",
      "content": {
        "text": "What Adrian Did Next: 2022 Conference Appearances\nWeymouth and Chesil Beach viewed from Portland Heights\u200a\u2014\u200aphoto by\u00a0Adrian\nI gave a talk at Monitorama in Portland Oregon in June, which set out the idea that carbon is just another metric to monitor, and that in a few years most of the monitoring and performance tuning tools are going to be reporting and optimizing for carbon alongside latency, throughput, availability and cost. However today carbon metrics are complicated and confusing, and the situation is evolving quickly.\n\nI\u2019m doing occasional presentations and will be attending conferences, in-between road trips and vacations, (including to visit family in the UK near the photo shown above), and I look forward to keeping in contact with old friends and colleagues.\n\nTheCube is running a virtual Supercloud event on August 9th, and I\u2019m taking part in a panel moderated by John Furrier, along with two old \u201cclouderati\u201d friends Lori MacVittie of F5, who write a blog on Supercloud a few months ago, and Chris Hoff of Lastpass, a security expert who helped lead BofA\u2019s technology platform for several years.\n\nI\u2019m presenting in person for the AWS Community day on September 9th at the Computer History Museum in Mountain View California. My talk is going to feature \u201cAdrian\u2019s Greatest Hits, B-Sides and Re-issues\u201d and I\u2019m going to rerun and comment on talks I\u2019ve given in the past, including some of the original Netflix Architecture content from 2010 and my GigaOM Cloud Trends talk from 2015.\n\nMap Camp is online on October 13th, and I\u2019m very happy that I was able to help get AWS to sponsor the event. I\u2019m speaking again, as part of a triad of three speakers on The Role of Poverty in Sustainability, moderated by Gen Ashley. Carol Freitas works on this topic in Brazil, Caitlin McDonald is a UK based researcher, and I will be helping to map the discussion, something like we did last year with Carlota Perez and Andra Sonea.\n\nI\u2019m the Green Technology track lead for the QCon San Francisco conference on October 25th, I\u2019m bringing sustainability concerns to both development and operations, which I\u2019m calling DevSusOps. We also have a talk from Thoughtworks on the Cloud Carbon Footprint tool, and two more talks that are still being confirmed.\n\nI\u2019m making a virtual appearance for the DevOps Summit Canada on Nov 7th, talking about DevSusOps again.\n\nI plan to attend the Supercomputing SC22 event in Dallas in November, and AWS reInvent in Las Vegas in December, primarily in an analyst role, I won\u2019t be presenting, but I might turn up as a guest on The Cube to comment on what\u2019s been announced\u2026\n\nI\u2019ll wrap up the year in Australia presenting in-person on the YOW! tour that goes from Brisbane to Melbourne to Sydney from the 5\u201313th of December.",
        "metadata": {
          "word_count": 468,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "TheCube is running a virtual Supercloud event on August 9th, and I\u2019m taking part in a panel moderated by John Furrier, along with two old \u201cclouderati\u201d friends Lori MacVittie of F5, and Chris Hoff. I look forward to keeping in contact with old friends and colleagues. I\u2019m presenting in person for the AWS Community day on September 9th at the Computer History Museum in Mountain View California. My talk is going to feature \u201cAdrian\u2019s Greatest Hits, B-Sides and Re-issues\u201d and I\u2019ll rerun some of the original Netflix Architecture content from 2010 and my GigaOM Cloud Trends talk from 2015. I\u2019m making a virtual appearance for the DevOps Summit Canada on Nov 7th. I\u2019ll wrap up the year in Australia presenting in-person on the YOW! tour that goes from Brisbane to Melbourne to Sydney from the 5\u201313th of December."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "devops",
        "metrics",
        "monitoring",
        "netflix",
        "performance",
        "platform",
        "security",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_file_ca86391b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Analyst content at me dm",
      "source": "Medium Archive",
      "published_date": "2023-04-13",
      "url": "https://medium.com/@adrianco/2023-04-13_Analyst-content-at-me-dm-2bcaf50e759b",
      "content": {
        "text": "Analyst content at\u00a0me.dm\n\nMedium launched me.dm a while ago as a gateway to the fediverse, but I haven\u2019t seen much activity there. Posting here, I don\u2019t see a Toot option, alongside the Tweet/Facebook options. Anyway since I have a separate account from my main account at @adrianco@mastodon.social that lives @adrianco@me.dm I decided to try and use it differently and just posted an analyst oriented tweet thread about Tesla, Rivian etc. https://me.dm/@adrianco/110190420885431817",
        "metadata": {
          "word_count": 72,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Medium launched me.dm a while ago as a gateway to the fediverse. Posting here, I don\u2019t see a Toot option, alongside the Tweet/Facebook options. I decided to try and use it differently and just posted an analyst oriented tweet thread about Tesla, Rivian etc."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_76cc7ca5",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Many thanks for building Guesstimate",
      "source": "Medium Archive",
      "published_date": "2015-12-31",
      "url": "https://medium.com/@adrianco/2015-12-31_Many-thanks-for-building-Guesstimate--f219b32d59ec",
      "content": {
        "text": "Many thanks for building Guesstimate. I\u2019ve been looking recently for something like this. I used a commercial Excel plugin to do this kind of analysis many years ago.",
        "metadata": {
          "word_count": 28,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Many thanks for building Guesstimate. I\u2019ve been looking recently for something like this. I used a commercial Excel plugin to do this kind of analysis many years ago."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_781dd4fb",
      "kind": "file",
      "subkind": "medium_post",
      "title": "I agree with your point on outcomes but in general whoever is funding developers to create",
      "source": "Medium Archive",
      "published_date": "2017-02-20",
      "url": "https://medium.com/@adrianco/2017-02-20_I-agree-with-your-point-on-outcomes--but-in-general-whoever-is-funding-developers-to-create--98d2d66efde2",
      "content": {
        "text": "I agree with your point on outcomes, but in general whoever is funding developers to create business logic should in general have a reason related to an outcome they are optimizing for. The general principle I was looking at is: given that someone wants to create business logic, what\u2019s the most effective way to do that.",
        "metadata": {
          "word_count": 56,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I agree with your point on outcomes, but in general whoever is funding developers to create business logic should in general have a reason related to an outcome they are optimizing for. The general principle I was looking at is: given that someone wants to createbusiness logic, what\u2019s the most effective way to do that?"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_315c0bc9",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Deep dive into NVIDIA Blackwell Benchmarks where does the 4x training and 30x inference",
      "source": "Medium Archive",
      "published_date": "2024-09-24",
      "url": "https://medium.com/@adrianco/2024-09-24_Deep-dive-into-NVIDIA-Blackwell-Benchmarks---where-does-the-4x-training-and-30x-inference--0209f1971e71",
      "content": {
        "text": "Deep dive into NVIDIA Blackwell Benchmarks\u200a\u2014\u200awhere does the 4x training and 30x inference performance gain, and 25x reduction in energy usage come\u00a0from?\nThe prototype NVL-72 water cooled rack on the show floor at GTC24\u200a\u2014\u200apicture by\u00a0Adrian\nI\u2019ve been trying to write this blog post since the announcements at GTC, and was planning to publish it as a story for The New Stack, but I keep getting stuck on things that don\u2019t make sense to me, so I\u2019m going to put it out there as it is on Medium, and if I get any feedback pointing out where I\u2019m wrong or missing something, I can update it easily.\n\nMost publications have simply reported the benchmark improvement claims, but if you stop to think about them, the numbers don\u2019t make sense based on a simplistic view of the technology changes. The benchmarks are documented in the Blackwell Architecture Technical Brief and some screenshots of the GTC keynote, and I\u2019ll break those out and try to explain what\u2019s really going on from a \u201cbenchmarketing\u201d approach. The TL;DR is that the benchmarks are comparing large configurations separated by two generations of technology development, some of the non-obvious speedup comes from a move to a memory centric architecture, and from my viewpoint the energy savings are good but not as big as claimed.\n\nThere are three generations of GPUs that are relevant to this comparison. The Hopper H100 was announced in 2022 and is the current volume product that people are using, so that is used as the baseline for comparison. The Grace Hopper GH200 was announced in 2023 and at the time of GTC was available in limited quantities, it\u2019s not referenced in this benchmark comparison. The Grace Blackwell GB200 is the newly announced product that is compared to H100 and isn\u2019t likely to ship in volume until 2025. So first thing to understand is that the benchmark skips a generation and compares product that differs over about a two year interval. They could have chosen to compare GB200 against GH200, but the numbers wouldn\u2019t have been as impressive, and users aren\u2019t familiar with GH200 yet, so it\u2019s a reasonable approach.\n\nThe architecture differences between the generations are that the HGX H100 is a system that is configured with eight H100s and two Intel CPUs as an air-cooled rack mounted server connected using Infiniband. The NVIDIA branded version of this product is called DGX H100, other vendors incorporate the HGX H100 system in their own packaging, DGX and HGX should have the same performance, but the benchmarks refer to the more generic HGX. The package is 14\" high, 8 rack units, so with sufficient airflow up to four of them could be configured in a standard rack. The NVIDIA benchmark document refers to each HGX H100 unit as a \u201crack\u201d which gives the impression that it takes up more space than it does.\n\nThe GH200 pairs an ARM architecture Grace CPU with a slightly upgraded H200 GPU, that has the same compute capacity but has more and faster memory. Various benchmarks show improvements of 1.4x to 1.8x on a per GPU basis depending on how memory intensive the workload is. Up to 256 GH200 modules can be connected using a shared memory architecture rather than Infiniband. For a benchmark configuration of eight networked 8xH100+2xCPU vs a shared memory 64xGH200 cluster there is some speedup due to the lower overhead and higher speed of the memory interconnect, and the extra capacity of 64 Grace CPUs vs. 16 Intel CPUs.\n\nThe specifications for Blackwell are shown in the table below, this is the performance for a single water cooled GB200 module, which is two GPUs, and this confuses the issue somewhat. It\u2019s important to be clear whether the reference is to a GB200 module or a single GPU from it, or the air cooled B100/B200. The GB200 module pairs one Grace CPU with two Blackwell GPUs rather than the GH200 that has one Hopper GPU, so on a per module basis there is a 2x factor to take into account. The Blackwell GPU is made up of two chiplets in one package that are similar in size and process to the H200, which was a small incremental improvement over H100.\n\nJust from looking at the silicon, we would expect a single Blackwell GPU to be about twice the performance of a single H200 and a bit more than twice an H100.\n\nThe HGX H100 8-GPU system is the baseline for comparison, and its datasheet performance is shown below.\n\nOn a per GPU FP8 sparse basis H100 is 32/8 = 4 Petaflops. Blackwell is 20/2 = 10 Petaflops, so the compute speedup is 2.5x, as expected. Memory bandwidth for H100 is 27/8 = 3.375TB/s, and Blackwell is 16/2 = 8TB/s, so the memory speedup is 2.37x. This is almost entirely what we would expect from the process and scale improvements between H100 and H200, then putting the equivalent of two H200 chiplets into a single Blackwell package. The extra memory bandwidth is needed to feed data to the increased compute performance.\n\nThe first benchmark claim is 4x for training performance vs. H100, rather than the 2.5x we would expect. The configuration is documented in the following figure.\n\nThis benchmark is specified for the kind of configuration that OpenAI or Meta would use for training their biggest models. It consists of 32768 H100 GPUs configured as a network of 4096 HGX H100 machines (usually configured four per rack, this would be 1024 air-cooled racks), vs. 456 NVL72 water cooled racks containing 32832 Blackwell GPUs. The speedup we would expect is 2.5x, so there must be an additional 1.6x gain from the interconnect. The H100 interconnect is NVlink at 900GB/s of coherent shared memory bandwidth between 8 GPUs, then 4096 nodes connected by Infiniband at 400Gbit/s (less than 40GB/s because of 8bit in 10bit encoding and packet protocol overhead). The NVL72 has 72 GPUs connected via NVlink at 1800GB/s, then 456 nodes connected by Infiniband at 800Gbits/s (less than 80GB/s). The probability that a GPU to GPU transfer occurs over NVlink is nine times higher and transfers occur at twice the speed for NVL72. This is a reasonable explanation for the 1.6x extra speedup. This applies regardless of the precision used, as Blackwell is 2.5x faster than H100 for FP8, FP16 etc.\n\nThe over-all 4x training speedup claim seems plausible for large configurations. However comparing small configurations of between 1 and 8 GPUs the training speedup will be more like 2.5x.\n\nThe configuration that is referenced for the 30x inference benchmark is NVL-36, which takes up half the rack shown in the photo and contains 18 GB200 modules on nine boards, plus an unspecified number of NVlink memory switch boards.\nPerformance Benchmark result taken from Blackwell Architecture Technical Brief\nThere is also a footnote at the bottom of page 14 of the technical brief:\n\n\u201cToken-to-token latency (TTL) = 50ms real time, first token latency(FTL) = 5s, input sequence length = 32,768, output sequence length = 1,028, 64 H100 GPUs air-cooled vs. 18 GB200 Superchips with NVL36 liquid-cooled, per GPU performance comparison. TCO, energy savings for 100 racks eight-way HGX H100 air-cooled vs. 1 rack GB200 NVL72 liquid-cooled with equivalent performance.\u201d\n\nFor inference there is a new FP4 format in Blackwell that doubles the floating point operation capacity over the FP8 based H100, so we would expect the per GPU benchmark claim for GB200 to be five times H100 based on the raw compute difference (4 Petaflops of FP8 for H100 and 20 Petaflops of FP4 for Blackwell per GPU), however the claim is 30x for performance and 25x for power efficiency. The big question is: How do they get to the higher comparison numbers? There\u2019s a missing factor of 6x for performance and 5x for power efficiency that needs to be accounted for.\n\nThe benchmark was performed on a 1.8Trillion parameter GPT-MoE-1.8T model although it\u2019s not clear how the model was changed to use FP8 to run on H100 and FP4 to run on GB200. There needs to be at least 1.8Tbytes of memory as FP8 just to hold the model weights, and 900Gbytes of memory as FP4. The NVL-36 system has over 6TB of high bandwidth GPU memory and the eight H100 based systems have a total of about 5TB of GPU memory. Inference configurations are usually sized based on the memory needed to hold the model first, then configurations are replicated to provide the request capacity needed.\n\nThere are two differences that improve the results. The first effect is that the FP4 model weights are 4bits rather than 8bits, so they are half the size, and that means they load twice as fast, with better cache hit rates for the same number of model parameters. The other effect is that the H100 cluster is eight systems networked with Infiniband, while the GB200 cluster is a single shared memory system.\n\nWe previously saw that increased memory bandwith and capacity gave a 1.4x to 1.8x speedup between a single H100 and H200 GPU that have the same amount of raw compute.\n\nThe diagram below is taken from the GTC keynote, and appears to be the source of the 30x speedup claim.\n\nHowever the choice of where to pick the difference between the two curves seems to be quite arbitrary to me, at the point where the H100 system performance has dropped off a lot. I added some lines to the plot to make it easier to estimate the values and came up with new differences.\n\nThe comparisons I\u2019ve labeled are 5.3x and 8.3x speedup from H100 to B200. These line up much better with what I\u2019d expect from the hardware capabilities. The 30x figure appears to be comparing an H100 that is running quite inefficiently.\n\nThe 30x inference improvement claim was for the kind of inference models that OpenAI are running to operate ChatGPT. However I think people should expect more like 8\u201310x for inference on these large systems.\n\nFor small inference models that currently fit inside a single 8-GPU HGX100 and can use FP4, a speedup of around 5x would be expected on a GB200 based water cooled NVL-72 package. However there is also a more comparable air-cooled reduced clock rate version of the Blackwell GPU that comes in two 8-way versions, called the HGX B100\u200a\u2014\u200adesigned to fit in the same power envelope as the HGX H100, and the HGX B200 that draws more power and runs at 90% of the per-GPU speed.\n\nAir cooled 8-GPU package inference speedup over the older systems for HGX B100 is likely to be 4x, and HGX B200 4.5x, driven by the doubling of silicon area and move from FP8 to FP4.\n\nThe sustainability claim is that Total Cost of Operation (TCO) and energy use is 25x better. This is where the comparisons stop making sense. First, why is the TCO the same ratio as the Energy? If the cost of energy dominates TCO then there\u2019s no point reporting the same number twice, but it doesn\u2019t seem likely that the TCO of a large number of air cooled systems vs. a much smaller number of water cooled systems would just be proportional to the energy used.\nTCO and Energy Use Benchmark result taken from Blackwell Architecture Technical Brief\nThe comparison is 9x eight way = 72 H100 GPUs, vs. NVL72 which has 72 GB200 based GPUs, the NVIDIA claim is that this results in a 30x speedup and 1.2x higher total energy usage. The NVL72 is rated at 120 kW, so the nine HGX H100 would be 100 kW, so 11 kW each. Their spec rates them at 10.2 kW, so this part of the comparison makes sense.\n\nThe following details don\u2019t make sense at all. 100 x HGX H100 = 800 GPUs and 1100 kW vs 8 x HGX B200 = 64 GPUs \u201cwith equivalent performance\u201d.\n\nI checked with NVIDIA and was told this is a typo and was given an updated comparison chart.\n\nThe comparison made in the updated note is that 65 x HGX H100\u2019s = 520 GPUs and 700 kW, are equivalent performance to one NVL72 with 72 GPUs and 120 kW, which also makes no sense. The TCO calculations also make no sense to me.\n\nBottom line, the energy comparison benchmark only makes sense if we assume the basic comparison of 72 GPUs of each type, and ignore the additional notes. However if we take a more likely speedup of around 10x rather than 30x, then the energy comparison would come out at about 8x. This is still a very good improvement in inference efficiency.\n\nThere\u2019s a more recent update to the NVIDIA roadmap that is summarized well at The Next Platform, the 25x improvement in energy claim persists there as a reduction from 10 joules/inference to 0.4 joules/inference. It\u2019s not clear how they measure this, or if they just used the previous 25x claim to come up with a different metric. https://www.nextplatform.com/2024/06/02/nvidia-unfolds-gpu-interconnect-roadmaps-out-to-2027/\n\nI\u2019ve spent too long staring at the specs and trying to make sense of of this already. I suspect that there\u2019s some additional confusion over the single GPU B200 module and the dual GPU GB200 module going on, as I\u2019ve also seen NVIDIA claim that the B200 GPU is 15x the performance of the H100, which actually makes more sense, but doesn\u2019t align with the GB200 per-GPU claims. If anyone has better ideas, please let me know!\n\n[Update 9/28/2024] David Kanter pointed out some MLPerf inference results. H200 is about 1.4x H100 as expected, but a single Blackwell GPU is only 4x the H100 (as predicted above), not the 30x headline speed up. https://developer.nvidia.com/blog/nvidia-blackwell-platform-sets-new-llm-inference-records-in-mlperf-inference-v4-1/",
        "metadata": {
          "word_count": 2277,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I\u2019ve been trying to write this blog post since the announcements at GTC, and was planning to publish it as a story for The New Stack. The benchmarks are documented in the Blackwell Architecture Technical Brief and some screenshots of the GTC keynote. I\u2019ll try to explain what\u2019s really going on from a \u201cbenchmarketing\u201d approach. The TL;DR is that the numbers don\u2019t make sense based on a simplistic view of the technology. There are three generations of GPUs that are relevant to this comparison. benchmarks are comparing large configurations separated by two generations of technology development. Some of the non-obvious speedup comes from a move to a memory centric architecture. The energy savings are good but not as big as claimed. The HGX H100 is an air-cooled rack mounted server connected using Infiniband. The GH200 pairs an ARM architecture Grace CPU with a slightly upgraded H200 GPU. Various benchmarks show improvements of 1.4x to 1.8x on a per GPU basis depending on how memory intensive the workloa Up to 256 GH200 modules can be connected using a shared memory architecture rather than Infiniband. For a benchmark configuration of eight networked 8xH100+2xCPU vs a sharedMemory 64xGH200 cluster there is some speedup. The HGX H100 8-GPU system is the baseline for comparison. Blackwell is 20/2 = 10 Petaflops, so the compute speedup is 2.5x, as expected. Memory bandwidth for H100 is 27/8 = 3.375TB/s, and Blackwell is 16/1 = 8 TB/s. The extra memory bandwidth is needed to feed data. The H100 interconnect is NVlink at 900GB/s of coherent shared memory bandwidth between 8 GPUs. 456 NVL72 water cooled racks containing 32832 Blackwell GPUs. The speedup we would expect is 2.5x, so there must be an additional 1.6x gain from the interconnect. The configuration that is referenced for the 30x inference benchmark is NVL-36, which takes up half the rack shown in the photo and contains 18 GB200 modules on nine boards. The 4x training speedup claim seems plausible for large configurations. However comparing small configurations of between 1 and 8 GPUs the training speed up will be more like 2.5x. GB200 has a 1.8Trillion parameter GPT-MoE-1.8T model. The NVL-36 system has over 6TB of high bandwidth GPU memory. The eight H100 based systems have a total of about 5TB of CPU memory. There\u2019s a missing factor of 6x for performance and 5x for power efficiency. There are two differences that improve the results. The FP4 model weights are half the size, and that means they load twice as fast. The other effect is that the H100 cluster is networked with Infiniband. For small inference models that currently fit inside a single 8-GPU HGX100 and can use FP4, a speedup of around 5x would be expected on a GB200 based water cooled NVL-72 package. However there is also a more comparable air-cooled reduced clock rate version of the Blackwell GPU that comes in two 8-way versions.  NVIDIA claim that the HGX B200 is 25x better than the NVL72. The TCO of a large number of air cooled systems vs. a smaller number of water cooled systems would just be proportional to the energy used. The energy comparison only makes sense if we assume the basic comparison of 72 GPUs of each type, and ignore the additional notes. If we take a more likely speedup of around 10x rather than 30x then the energy comparison would come out at about 8x. This is still a very good improvement in inference efficiency. I\u2019ve spent too long staring at the specs and trying to make sense of of this already. I suspect that there\u2019s some additional confusion over the single GPU B200 module and the dual GPU GB200 module. If anyone has better ideas, please let me know!"
      },
      "tags": [
        "architecture",
        "performance",
        "platform",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_file_aa644749",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thanks Eugene I agree trimmed mean is better than mean or median but it still throws away too",
      "source": "Medium Archive",
      "published_date": "2023-01-31",
      "url": "https://medium.com/@adrianco/2023-01-31_Thanks-Eugene--I-agree-trimmed-mean-is-better-than-mean-or-median--but-it-still-throws-away-too--30729ba3aef7",
      "content": {
        "text": "Thanks Eugene, I agree trimmed mean is better than mean or median, but it still throws away too much interesting information from the distribution.",
        "metadata": {
          "word_count": 24,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I agree trimmed mean is better than mean or median, but it still throws away too much interesting information from the distribution. Thanks Eugene."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_29c7e8b8",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Don t follow the sun Scheduling compute workloads to chase green energy can be counter productive",
      "source": "Medium Archive",
      "published_date": "2023-04-07",
      "url": "https://medium.com/@adrianco/2023-04-07_Don-t-follow-the-sun--Scheduling-compute-workloads-to-chase-green-energy-can-be-counter-productive-b0cde6681763",
      "content": {
        "text": "Don\u2019t follow the sun: Scheduling compute workloads to chase green energy can be counter-productive\nSunset in Morocco\u200a\u2014\u200aphoto taken by\u00a0Adrian\nWe want to reduce carbon emissions of our compute and storage workloads, and one way of doing this is to choose a time and place where the \u201cgrid mix\u201d of energy consumed is less carbon intensive. In particular, there is usually an excess of solar energy during the day that makes the grid mix better than at night. This leads to people suggesting that workloads should move around the world to follow the sun and run on green energy. This seems reasonable at first, but there are several reasons why it\u2019s not really going to help, and may actually increase carbon emissions.\n\nThe computers you stopped using aren\u2019t following the sun. They stay in the same datacenter and are still turned on and using power, whether you use them or not. If you are running on a cloud provider then someone else is likely to be using them, perhaps getting a bigger spot market discount incentive than before. Just because the carbon emissions aren\u2019t charged to your account, it doesn\u2019t make them go away. Meanwhile your workload is generating demand in a different cloud region, and all regions do demand based capacity planning, so the cloud provider buys more computers, which increases carbon emissions both for manufacturing and shipping (scope 3) and the energy they use (scope 2). In addition, the data your workload operates on either needs to be accessed over the network, which increases capacity demand for switches and undersea fibers, or an additional local copy of the data would be maintained, which all adds to the carbon footprint.\n\nCloud providers track how often customers ask for capacity of a specific instance type in a region and don\u2019t get it. AWS calls this an Insufficient Capacity Exception (ICE) and works very hard to avoid it. There were problems reported on Azure during the COVID lockdown where lots of people were suddenly working from home and the global change in workload mix meant that capacity ran out in some regions. At that time the AWS Spot market shrank to the point where some customers had to move workloads to regular instances. You can think of the spot market as the buffer capacity for a cloud provider. If it gets too small, they provision more racks of systems. When someone asks for a regular instance, if there are no idle ones available, it gets taken from a spot market user.\n\nIf you can choose where to put a workload, then it\u2019s worth picking a region that has a good grid mix, most of the EU and US based regions from the major cloud providers are already running on low carbon energy, but Asian regions are currently high carbon, reducing over the next few years. If you have global backup/archive workloads where you store copies in different regions, it would be best to avoid storing them in Asia.\n\nIf we consider how best to run a daily batch job, the default is often to start it at midnight or on an hourly boundary. This is particularly bad for the cloud providers, as many customers running cron jobs at the same time causes load spikes in network and storage services, that makes them need to deploy extra capacity, which increases the over-all carbon footprint, even if it doesn\u2019t show up on your bill. So it\u2019s good practice to cron-jitter start times. Would it be better to run it after sunrise when the grid mix is better? Again, not if that would increase capacity demand for the cloud provider. The best option would be to run the batch job when the spot market has the lowest price. Spot market prices could even be manipulated to take carbon emissions into account, and use that mechanism to smooth out peaks and incentivize workloads to move to lower carbon regions and times. If the batch job is scalable (like a Hadoop or Spark cluster doing ETL processing), and the deadline is flexible then it would be better over-all to run a smaller cluster for longer, as that smooths out the total capacity demand at the cloud provider level.\n\nAnother way of looking at grid mix and where to run a workload is to consider the mix for incremental additional usage. If you have an idle computer it uses less power (about half) than a busy one, so when you make it busy the extra energy demand comes from whatever power source is currently operating as a peaker-plant. Even if most of the total energy is renewable, the peaker-plant is most likely to be a fossil gas power plant, although in the coming years we\u2019ll see more large scale batteries being used. This is a complicated issue and there\u2019s been some discussion about it at the Green Software Foundation recently.\n\nI suggest that the best policy is to optimize your workloads so that they can run on fewer more highly utilized instances, minimize your total footprint in Asia where possible, and to use the spot market price as a guide for when to run workloads.\n\nSome people seem to have the attitude that they should just optimize for their own workloads, and it\u2019s the cloud provider\u2019s problem to solve for capacity and carbon footprint issues, but this is shortsighted. If the way we all architect and deploy workloads makes it easier for cloud providers to maximize their utilization and deploy less total capacity, then we are helping to reduce the carbon footprint of the computing industry as a whole.\n\nThe problem here is a \u201ctragedy of the commons\u201d kind of issue. If a lot of people optimize their workloads to reduce their own carbon footprint by moving workloads around, the combined effect is to increase the capacity and reduce the utilization of the cloud providers and to increase the carbon footprint over-all for the computing industry.",
        "metadata": {
          "word_count": 991,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Scheduling compute workloads to chase green energy can be counter-productive. This leads to people suggesting that workloads should move around the world to follow the sun and run on green energy. This seems reasonable at first, but there are several reasons why it\u2019s not really going to help. Cloud providers track how often customers ask for capacity of a specific instance type in a region and don't get it. AWS calls this an Insufficient Capacity Exception (ICE) and works very hard to avoid it. There were problems reported on Azure during the COVID lockdown.  AWS Spot market shrank to the point where some customers had to move workloads to regular instances. Most of the EU and US based regions from the major cloud providers are already running on low carbon energy. Asian regions are currently high carbon, reducing over the next few years. Cron jobs at the same time causes load spikes in network and storage services. Spot market prices could even be manipulated to take carbon emissions into account. Some people seem to have the idea that they should just optimize for their own workloads. This is shortsighted. If you have an idle computer it uses less power than a busy one. Even if most of the total energy is renewable, the peaker plant is most likely to be fossil gas. The problem here is a \u201ctragedy of the commons\u201d kind of issue. If a lot of people optimize their workloads to reduce their own carbon footprint by moving workloads around, the combined effect is to increase the capacity."
      },
      "tags": [
        "aws",
        "azure",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_97374d40",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Why I support the IATSE strike",
      "source": "Medium Archive",
      "published_date": "2021-10-02",
      "url": "https://medium.com/@adrianco/2021-10-02_Why-I-support-the-IATSE-strike-e0ac8ef71823",
      "content": {
        "text": "Why I support the IATSE\u00a0strike\n\nDiscussing university with Rae went something like: \u201cI\u2019d like to go to film school\u201d, \u201cThat\u2019s a tough business to get into, but if you ever get invited to an award ceremony, we get to come, OK?\u201d. Many years later, as you can see in the photo, we got to be proud parents and visit the red carpet at the Creative Arts Emmy awards, as Rae was nominated as Set Decorator for the Comedy Central TV show Drunk History. Saturday Night Live won the category that year, and Rae has since been nominated and worked on other award winning productions like the film Promising Young Woman, but this was a significant career milestone, and Laurel and I were very proud to be there.\n\nThe way this industry works, everyone on a production is on a contract for the duration of that show, working extremely long and antisocial hours, sometimes on location away from home. Then they take a break to recover and call people they\u2019ve worked with before, looking for the next show. There\u2019s no steady income, and in general larger productions are unionized and smaller ones use non-unionized staffing. It\u2019s hard to get into the union, and it took about ten years for Rae to get to that point, but Rae\u2019s earnings moved from near minimum wage (we provided ongoing support, bought a pickup truck etc.) to a sustainable level, with safer working practices, insurance and other benefits provided by the union. It\u2019s important to understand that the union is taking on a significant part of the role that those of use who work for a corporation take for granted.\n\nDifferent kinds of productions have different standard contract rates that are negotiated with the unions representing the different trades that make up a production. Back in 2008, in a world dominated by traditional TV and film production, the emerging online streaming industry didn\u2019t fit the standard model, and was mostly doing very low budget productions, so it got a special deal that had reduced residuals, pay and benefits for the creative workers. Residuals support union healthcare and pension funds. Fast forward to today, and online streaming has become the dominant model for productions, but still has the reduced pay and benefits model, which is significantly impacting the ability of creatives to make a living, even while they are working 14hr days, six or seven days a week.\n\nNegotiations haven\u2019t resolved the issues, and that\u2019s why the creative industry union IATSE members are currently voting on strike action. I worked for Netflix for seven years, and have worked for Amazon for five years (although not on the production side of the businesses), and I\u2019d like to see both companies take a lead in supporting sustainable working practices that are safe, prevent burnout, and have pay and residual rates comparable to traditional TV and film production.\n\nUpdate Monday Oct 4, 2021\u2014 the results were that 98.68% voted yes, and voter turnout among eligible members was nearly 90%. In response, the Hollywood producers have indicated that they will re-enter negotiations with IATSE.",
        "metadata": {
          "word_count": 515,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The way this industry works, everyone on a production is on a contract for the duration of that show, working extremely long and antisocial hours. There\u2019s no steady income, and in g It took about ten years for Rae to get to that point, but Rae\u2019s earnings moved from near minimum wage to a sustainable level, with safer working practices, insurance and other benefits provided by the union. eneral larger productions are unionized and smaller ones use non-unionized staffing. Residuals support union hea. IATSE members are currently voting on strike action. Hollywood producers have indicated that they will re-enter negotiations with IATSE. lthcare and pension funds."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_74a93dd6",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Amazon 2022 Sustainability Report 18 AWS Regions are 100 renewable",
      "source": "Medium Archive",
      "published_date": "2023-07-18",
      "url": "https://medium.com/@adrianco/2023-07-18_Amazon-2022-Sustainability-Report---18-AWS-Regions-are-100--renewable-6748efa82b6b",
      "content": {
        "text": "Amazon 2022 Sustainability Report\u200a\u2014\u200a19 AWS Regions are 100% renewable\n\nAmazon released their 2022 sustainability report which includes an updated list of renewable AWS regions. For 2021 there were 13 AWS regions that were \u201cover 95% renewable\u201d. For 2022, there are 19 regions that are \u201c100% renewable\u201d, notably adding four regions in China and India, and their new regions in Spain and Zurich. This is mostly due to a large increase in dedicated power purchase agreements to over 20GW, and some use of biofuels for backup generators.\n\nAmazon always releases its annual sustainability report in the middle of the following year. They don\u2019t break out AWS separately from the total, but the energy use for Amazon went from 85% renewable to 90% renewable, and the carbon footprint of their electricity use went down 29% year on year. Over-all, despite growing the business by 9% the total carbon footprint of Amazon dropped slightly for the first time.\n\nThe methodology used by Amazon is the same as is used by Microsoft Azure, it\u2019s known as the Market Method, and it is based on the energy you pay for, rather than the energy you use. It includes the energy generated by dedicated power purchase agreements, and renewable energy credits bought on the open market. Google use the Location Method, which doesn\u2019t include the energy from their power purchases, and reports more carbon emissions than the Market Method. Ideally everyone would share both.\n\nAmazon does have much more generation capacity than Google and Microsoft, and has many more projects in Asia, which is the area that is taking longest to decarbonize. I think that there isn\u2019t a lot of difference between AWS, Azure and GCP in Europe and America for their renewable energy footprint, but that AWS is significantly ahead in Asia. The scope 3 (manufacturing supply chain etc) carbon footprint of cloud doesn\u2019t vary as much by location, as the machines are the same.\n\nAWS still doesn\u2019t report scope 3 to customers, although they\u2019ve said they are working on it. However they do talk in the report about using low carbon concrete and steel in many of their most recent datacenter construction projects. The Power Usage Efficiency of datacenters (how much energy is used for cooling etc.) is reported regionally by Azure, Google reports somewhat better figures by datacenter campus, and AWS doesn\u2019t report PUE, but is likely to be similar.\n\nThe move to renewable electricity does mean that the dominant carbon footprint for cloud usage is scope 3. Users of the AWS Customer Carbon Footprint tool are mostly going to see zero carbon reports until AWS starts to report scope 3.\n\nFor a more detailed comparison of the cloud providers based on 2021 numbers see my QCon London talk from last March.\n\n[note that the first posted version of this story incorrectly said 18 regions, and the URL didn\u2019t change when I fixed it].",
        "metadata": {
          "word_count": 483,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Amazon released their 2022 sustainability report which includes an updated list of renewable AWS regions. For 2021 there were 13 regions that were \u2018over 95% renewable\u2019 For 2022, there are 19 regions that are \u2018100% renewable' Amazon has much more generation capacity than Google and Microsoft, and has many more projects in Asia. The scope 3 (manufacturing supply chain etc) carbon footprint of cloud doesn\u2019t vary as much by location, as the machines are the same. The dominant carbon footprint for cloud usage is scope 3. Report about using low carbon concrete and steel in many of their most recent datacenter construction projects."
      },
      "tags": [
        "aws",
        "azure",
        "cloud",
        "gcp",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_file_546bb129",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Looking forward to returning to Multicore World New Zealand February 2025",
      "source": "Medium Archive",
      "published_date": "2024-11-16",
      "url": "https://medium.com/@adrianco/2024-11-16_Looking-forward-to-returning-to-Multicore-World----New-Zealand-February-2025--fa9cac2db45d",
      "content": {
        "text": "Looking forward to returning to Multicore World\u200a\u2014\u200a(New Zealand February\u00a02025)\nA curious scissor sergeant fish enjoying Bora Bora with me\u200a\u2014\u200aPicture by\u00a0Adrian\nThere are a few conferences I\u2019ve attended around the world that are more about building communities than the content (although they often have the best content). They are predominantly single track, small events, run by one or two opinionated individuals rather than a committee or a corporation. It\u2019s where I like to try out brand new material, as there\u2019s a lot of opportunity for feedback. I\u2019m thinking of Monitorama, Gluecon (rebranded SW2 con this year), the High Performance Transaction Systems workshop (hpts.ws), and in February 2024 I attended Multicore World in Christchurch, New Zealand for the first time. I\u2019m going back again in February 2025.\n\nI was introduced to Nicol\u00e1s Erd\u00f6dy at Super Computing 22, by my colleague at OrionX Shahin Khan, who encouraged me to attend his event. I couldn\u2019t schedule it that year, but got it onto my calendar for February 2024. The attendees are a mixture of local community and technologists from New Zealand, and leaders from the global high performance computing community.\n\nI decided that I wanted to explore an area that I was curious about, but not expert in, and see if I could get some interest and feedback from real experts in the audience. I like to understand the basic workload characteristics of systems, and I\u2019ve seen many workloads over the years, but I haven\u2019t worked on LLM training workloads at large scale. I notice that the network bandwidth configured on these systems is extremely high by any normal standards, so I decided to try and figure out why, and how it us used.\n\nThe biggest LLM training runs are performed by companies like OpenAI, Google and Meta and use tens of thousands of GPUs running for months. The result is shared as a service like OpenAI\u2019s ChatGPT. When organizations experiment to build an LLM based service, they often start with the shared ChatGPT service or the underlying API because it produces the best results. However it\u2019s relatively expensive to use as a service, can be slow, and may be down for maintenance at inconvenient times. The alternative is to train your own smaller LLM and try to create a more specialized expert in the subjects you need. The result may be good enough quality, far cheaper to run, and you can manage it\u2019s performance and availability directly for your own use case. A collection of smaller more specialized LLMs working together is another useful approach. To create such a model, it\u2019s common to start with a pre-trained model like Meta\u2019s Llama3, and train using perhaps a few hundred GPUs for a few days, then test and iterate until it\u2019s working well enough.\n\nTwo examples of this that I\u2019ve looked at are ClimateGPT by erasmus.ai and the Mixture of Intelligent Agents architecture used by flip.ai to build their incident analysis tool. The ClimateGPT paper is a very nicely explained walk through of how to build a climate expert by feeding it a body of scientific information, with what seems to be a state of the art approach to validation, translation and operational issues.\n\nFlip.ai are at the bleeding edge, pushing the boundaries of what can be built by composing a real time incident analysis service from what they call a system of intelligent agents, with a very experienced team. I used to work with their CTO Sunil Mallya at AWS, and learned a lot from him. We visited a robot racing meetup together and dreamed up the AI driven robot racing series that launched at reInvent 2017, then Sunil got the AWS RoboRace challenge going, using a custom built car and groundbreaking reinforcement learning techniques in 2018, and he also created and ran several AWS AI services.\n\nThere are quite a lot of people running small training workloads repeatedly, and they are not cheap to run, several thousand dollars an hour for a few days is in the region of $100K per run. Optimizing that workload so it runs more efficiently and at lower cost seems like a worthwhile area to understand. My approach was to read a lot, ask a few people for ideas, and to gather my thoughts on a Miro board as I put together the story for my presentation at Multicore World 2024. I then presented an updated version of the talk at SW2con in May 2024.\n\nTo understand what is happening I went deep into the open source technology stack that NVIDIA uses, figuring out that the networking is driven via the NCCL library that batches patterns of large transfers over whatever transport is available, it\u2019s not at all like the kind of socket based request/response mechanisms most developers use. NCCL has added features over the years and there are excellent videos explaining how it works.\n\nThe transport could be Infiniband directly, or for cloud providers there is a Libfabric interface that in turn abstracts different underlying mechanisms. AWS routes Libfabric over it\u2019s EFA transport, that uses a custom underlying protocol over many ethernet connections in parallel. This doesn\u2019t have as low minimum latency as Infiniband, but it has similar bandwidth and lower variance for large transfers as it avoids head of line blocking effects.\n\nOne aspect of computer architecture that appears to be an emerging trend is that the CPU isn\u2019t the \u201cCentral Processing Unit\u201d any more. It\u2019s no longer at the center of the architecture. The CPU is an IO Processor handling filesystem and control plane work, hanging out around the edge of the central collection of GPUs that are managing memory and networking directly. If one NVIDIA Hopper or Blackwell GPU wants to talk to another GPU, it connects directly over a coherent memory bus within a rack. If one of the Grace CPUs in this configuration wants to communicate directly with another Grace CPU, it has to go through two Hopper or Blackwell GPUs to get there. This could be written off as an obscure architecture feature if it wasn\u2019t central to the highest revenue and most successful chip vendor, NVIDIA. This is where most of the money in IT is currently being spent, so optimizing for the Grace Hopper and Blackwell based architectures is going to be where most of the benefits are going to lie over the next year or so.\nGrace Hopper H200\u00a0ModuleGrace Hopper Architecture\u200a\u2014\u200aGrace to Grace via Hopper within a\u00a0cluster\nI\u2019m going to talk more about this topic in a new talk at Multicore World 2025, and I\u2019m happy to discuss it more as I figure out what I\u2019m going to say. New Zealand is a very cool place to visit, and it\u2019s worth taking some extra time there to explore the sights and Maori culture.",
        "metadata": {
          "word_count": 1135,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There are a few conferences I\u2019ve attended around the world that are more about building communities than the content. They are predominantly single track, small events, run by one or two opinionated individuals rather than a committee or a corporation. In February 2024 I attended Multicore World in Christchurch, New Zealand for the first time. I'm going back again in February 2025. The biggest LLM training runs are performed by companies like OpenAI, Google and Meta and use tens of thousands of GPUs running for months. When organizations experiment to build an LLM based service, they often start with the shared ChatGPT service. A collection of smaller more specialized LLMs working together is another useful approach. To create such a model, it\u2019s common to start with a pre-trained model. ClimateGPT is a very nicely explained walk through of how to build a climate expert. Flip.ai are at the bleeding edge, pushing the boundaries of what can be built by composing a real time incident analysis service from what they call a system of intelligent agents, with a very experienced team. I used to work with their CTO Sunil Mallya at AWS, and learned a lot from him.  NVIDIA uses the NCCL library that batches patterns of large transfers over whatever transport is available. The transport could be Infiniband directly, or for cloud providers there is a Libfabric interface that in turn abstracts different mechanisms. The CPU isn\u2019t the \u201cCentral Processing Unit\u201d any more. It\u2019s no longer at the center of the architecture. The CPU is an IO Processor handling filesystem and control plane work. If one NVIDIA Hopper or Blackwell GPU wants to talk to another GPU, it connects directly over a coherent memory bus within a rack. I\u2019m going to talk more about this topic in a new talk at Multicore World 2025. New Zealand is a very cool place to visit, and it\u2019s worth taking some extra time there to explore the sights and Maori culture."
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "cloud",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_e6584b31",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Who monitors the monitoring systems",
      "source": "Medium Archive",
      "published_date": "2018-04-18",
      "url": "https://medium.com/@adrianco/2018-04-18_Who-monitors-the-monitoring-systems--715a333f97fc",
      "content": {
        "text": "Who monitors the monitoring systems?\n\n\u201cQuis custodiet ipsos custodes?\u201d\u200a\u2014\u200aJuvenal\nPhoto taken in Lisbon Portugal by Adrian Cockcroft\nThe documentation for most monitoring tools describes how to use that tool in isolation, often as if no other tools exist, but sometimes with ways to import or export some of the data to other tools. In reality, in any non-trivial installation, there are multiple tools collecting, storing and displaying overlapping sets of metrics from many types of systems and different levels of abstraction. For each resource there\u2019s usually an underlying or bare metal version, a virtualized resource and sometimes also a containerized resource. This is true for CPU, network, memory and storage, and also for bare metal, virtual machines, containers, processes, functions and threads.\n\nThese monitoring systems provide critical observability capabilities that are needed to successfully configure, deploy, debug and troubleshoot installations and applications. In addition, for cloud native applications, a feedback loop is used to auto-scale the application resources based on metrics that report the utilization of current capacity.\n\nMonitoring systems are a critical part of any highly available system, as they are needed to detect failures and report whether users are impacted, then report whether the problem has gone away. When auto-scaling is used monitoring is also part of a critical feedback loop.\n\nWhat if your monitoring systems fail? You will be running blind, or your auto-scalers could be fed bad data and go out of control then take down your application. What happens if you have several monitoring systems and they disagree on a critical metric like CPU load or Network throughput? This can cause confusion and delay resolution of an incident. How do you even know when a monitoring system has failed? You need to monitor it, so how should you monitor the monitoring systems\u2026\n\nThe first thing that would be useful is to have a monitoring system that has failure modes which are uncorrelated with the infrastructure it is monitoring. For efficiency it is common to co-locate a monitoring system with the infrastructure, in the same datacenter or cloud region, but that sets up common dependencies that could cause both to fail together.\n\nOne approach is to use an integrated monitoring system to efficiently gather bulk metrics and maintain the long term archive as part of the same infrastructure. Then also setup an external SaaS provider that\u2019s not hosted in the same datacenter or region, and is monitoring a subset of metrics with a shorter retention period, to keep costs and traffic overheads under control. The SaaS provider can then also act as a monitor for the in-house systems.\n\nI don\u2019t know of a specialized monitor-of-monitors product, which is one reason I wrote this blog post. I would want it to have plug-ins for monitoring different monitoring systems for availability and capacity because they can get overloaded easily by lots of things and lots of metrics per thing, and high rate of change of things. There are some services like latency.at or Catchpoint that will tell you whether your service is working and reachable.\n\nI also think it would be good to compare the common metrics across different monitoring systems to analyze how much variance there is. This could be done by looking for simple differences, or using a statistical technique called gauge repeatability and reproducibility. There are lots of sources of differences between tools that are trying to report the same thing. There could be bugs, virtualization faked metrics, differences in sampling and averaging algorithms, rounding errors, and timestamp offsets between tools. Summing up totals across processes or containers may not add up to the underlying host system or may add up to more than it should.\n\nI\u2019ve seen problematic examples in the past. One is tools that accumulate CPU busy as everything except idle time, so that CPU wait time ends up being counted as busy, which is a bug. Another source of differences is that some metrics are maintained as time decayed averages, such as load average and process CPU consumption, while others are measured between two time points, like system level CPU usage. The time points at which samples are taken or the duration between samples won\u2019t line up between two tools, so they will report different values. A third problem comes when metrics are summarized, for example averaging latency percentiles over time is mathematically meaningless, so the resulting metric will be misleading. Finally, CPU clock rates vary as the CPU gets busy or overheats, and virtualization supplies partial CPU capacity or oversubscribed CPUs such as the AWS T2 instance type, so that causes variance in CPU load and response time that isn\u2019t related to changes in the workload.\n\nAs highly available cloud native infrastructure and application workloads become more prevalent, more care needs to be taken to get the monitoring systems right, and to be sure that you are using dependable metrics to dynamically manage your environments.\n\nThanks to Cindy Sridharan for review feedback on this post.",
        "metadata": {
          "word_count": 827,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The documentation for most monitoring tools describes how to use that tool in isolation, often as if no other tools exist. In reality, in any non-trivial installation, there are multiple tools collecting, storing and displaying overlapping sets of metrics. This is true for CPU, network, memory and storage, and also for bare metal, virtual machines, containers, processes and functions. Monitoring systems are a critical part of any highly available system. They are needed to detect failures and report whether users are impacted, then report whether the problem has gone away. When auto-scaling is used monitoring is also part of a critical feedback loop. For efficiency it is common to co-locate a monitoring system with the infrastructure, in the same datacenter or cloud region. But that sets up common dependencies that could cause both to fail together. One approach is to use an integrated monitoring system to efficiently gather bulk metrics. I think it would be good to compare the common metrics across different monitoring systems. This could be done by looking for simple differences, or using a statistical technique called gauge repeatability and reproducibility. There are lots of sources of differences between tools that are trying to report the same thing. Some metrics, such as load average and process CPU consumption, can be misleading. The time points at which samples are taken or the duration between samples won\u2019t line up between two tools, so they will report different values. Average latency percentiles over time is mathematically meaningless. . dharan for review feedback on this post. Please share your feedback with us on Twitter @dailymail.co.uk."
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native",
        "containers",
        "infrastructure",
        "metrics",
        "monitoring",
        "observability"
      ]
    },
    {
      "id": "virtual_adrianco_file_ac83f99d",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Cloud Native Cost Optimization",
      "source": "Medium Archive",
      "published_date": "2020-03-24",
      "url": "https://medium.com/@adrianco/2020-03-24_Cloud-Native-Cost-Optimization-f379c2f623e9",
      "content": {
        "text": "Cloud Native Cost Optimization\nA cost-optimizer at work. Photo of Corey Quinn taken at Kubecon by\u00a0Adrian\nOne of the impacts of COVID-19 has been a large rapid unplanned change in business activity for most of the global economy. There are several outcomes, some markets like work from home tools and services, healthcare and grocery retail are extra busy, social media applications are extra busy but their revenue from advertising may be impacted, and many businesses in manufacturing, travel and entertainment are have largely been shut down because their workforce and customers are in lockdown. While datacenter based IT costs are largely fixed, in a three year depreciation schedule, cloud costs are inherently variable, and can be sized to fit the need. When times get tough, a focus on cost optimization can also greatly reduce cloud costs, and the results take effect in next month\u2019s bill.\n\nI\u2019ve talked a lot about this in the past, and have found some more recent content that should be helpful to people facing these challenges.\n\nMy 2014 re:Invent talk provides a structured basis for covering all the aspects of cost optimization, although some of the examples are now out of date. In particular, reservations have been split into Savings Plans and On Demand Capacity Reservations to decouple cost optimization from capacity availability with a lot more flexibility and ease of management.\n\nOne of the customers I\u2019ve worked with more recently is Expedia, and here\u2019s a more practical and up to date set of examples from 2017.\n\nAbiade moved from Expedia to work at AWS and she presented again in 2019 at re:Invent with examples from Lyft showing how they cut the AWS spend component of their cost per ride by 40% by making cost visible and providing tools to optimize.\n\nMy old friend Constantin Gonzales (we worked together over 20yrs ago at Sun) has a regular presentation at re:Invent on running Lean Architectures, with a customer appearance by HERE.\n\nFor most people, their compute load dominates their bill, and EC2 General Manager Jeanine Banks gives an in-depth talk on cost optimization, including the new AWS Compute Optimizer tool and a summary of Savings Plans.\n\nFor a deep dive on savings plans, this video is excellent. If you haven\u2019t already got this setup, it\u2019s a good time to focus on getting it done.\n\nI hope you find these resources useful. AWS account management teams are engaging with customers globally to help them manage their costs and financial agility in these difficult times. Best wishes, and stay safe.",
        "metadata": {
          "word_count": 422,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "One of the impacts of COVID-19 has been a large rapid unplanned change in business activity for most of the global economy. While datacenter based IT costs are largely fixed, in a three year depreciation schedule, cloud costs are inherently variable. A focus on cost optimization can also greatly reduce cloud costs, and the results take effect in next month's bill. A look back at some of the most recent examples of cost optimization at re:Invent. Expedia, Lyft and Amazon Web Services are just a few of the customers who have used Lean Architectures. For a deep dive on savings plans, this video is excellent. anine Banks gives an in-depth talk on cost optimization. If you haven't already got this setup, it\u2019s a good time to focus on getting it done."
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native",
        "lean",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_2e6a0c59",
      "kind": "file",
      "subkind": "medium_post",
      "title": "To get things started",
      "source": "Medium Archive",
      "published_date": "2016-06-05",
      "url": "https://medium.com/@adrianco/2016-06-05_To-get-things-started--9a2e0205ed67",
      "content": {
        "text": "To get things started:\n\nGeoff Arnold is now CTO at@SensitySystems",
        "metadata": {
          "word_count": 10,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_43c358b8",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Sustainability Talks from AWS re Invent 2023 Post Event Update with Videos",
      "source": "Medium Archive",
      "published_date": "2023-11-26",
      "url": "https://medium.com/@adrianco/2023-11-26_Sustainability-Talks-from-AWS-re-Invent-2023---Post-Event-Update-with-Videos-969100c46a6a",
      "content": {
        "text": "Sustainability Talks from AWS re:Invent 2023\u200a\u2014\u200aPost Event Update with Videos and\u00a0Reviews\nThe Pantheon in Rome\u200a\u2014\u200aExtremely sustainable architecture\u200a\u2014\u200aphoto by\u00a0Adrian\nI wrote a medium post after AWS re:Invent 2022 summarizing the (lack of) news and all the talks related to Sustainability. The lack of progress on Scope 3 carbon reporting was picked up by Computer Weekly and the result was a statement from AWS that they are \u201cworking on it\u201d. Google Cloud and Microsoft Azure released Scope 3 data in 2021. In their annual sustainability report in July 2023 AWS did update their list of sustainable regions to highlight 19 regions that are 100% sustainable\u200a\u2014\u200ausing an annual local market methodology for scope 1 and 2.\n\nThere hasn\u2019t been any other visible progress, and most of the people I used to work with on sustainability have now left Amazon. For re:Invent 2021 my team (but mostly Elise Greve) persuaded the re:Invent organizers to include Sustainability as a track code, and that was repeated for 2022 and now for 2023. I reported on the keynotes and announcements in real time using the #reinvent tag via my @adrianco@mastodon.social account, and as I get a chance to watch the track sessions, I\u2019ll update this post with my thoughts. I\u2019ve added all the videos I could find, but they are still being posted so some are missing and I\u2019ll check again later.\n\nEnergy Project\u00a0Updates\n\nAWS are targeting 100% in-region renewable energy by 2025 and were at 90% globally in 2022. This is an annual net energy market methodology, and I\u2019ve talked elsewhere about exactly what that means and how that compares with other cloud vendors. Their policy is a good one and they are investing at a very high level, as the largest commercial purchaser of renewable energy globally:\n\nWe aim to have 100% of the electricity that we use to be attributable to renewable energy sources. We contract renewable power from utility-scale wind and solar projects that add clean energy to the grid. Many of these projects are enabled by long-term contracts such as power purchase agreements for large projects in the same energy grids as our electricity use.\n\nThe AWS list of energy projects was quietly updated just before re:Invent, to add a bunch more, but it wasn\u2019t mentioned in the conference keynotes. The AWS Energy team was represented by Abhishek Sharma\u2019s SUS101 talk (video embedded in the list of talks below) that provided the basic AWS messaging, largely unchanged from when my team created it in 2021. He updated the Amazon public committed renewable energy generation project total from last year\u2019s \u201cover 20GW\u201d to \u201cover 26GW\u201d, and reported good progress on the water usage commitment they made last year. The projects on the website aren\u2019t a complete list, as I\u2019ve heard that there are some that never get announced, and they are a mixture of rooftop solar and utility scale solar, wind and battery, distributed over Amazon retail, Amazon devices, and AWS. I\u2019ve extracted the underlying data from the Amazon map display that includes rooftop solar etc, and the public AWS utility scale energy capacity list adds up to over 18GW, with excellent global coverage. The difference between 18GW and 26GW is capacity dedicated to non-AWS parts of Amazon, which is growing rapidly as the delivery fleet and fulfillment centers decarbonize. Amazon has more capacity over-all and much better global PPA coverage, including Japan, Korea, Indonesia, India and China, than Google Cloud and Microsoft Azure, who have very few PPAs in Asia. The projects are tagged by date, and some are coming online in future years. AWS policy states that they may buy unbundled renewable energy credits on the open market, and in particular in locations where they have a committed project that isn\u2019t online yet. I think this is a good policy for limited use of RECs. I plotted the AWS capacity over time, which shows the rapid ramp up in delivered capacity and future commitments over the years:\n\nKeynotes\n\nPeter deSantis did a Monday evening technology keynote that usually has most of the sustainability related content in it. I expected an update on their water sustainability program that was announced last year, and a new number for the amount of private purchase agreement (PPA) power that Amazon has under contract. Peter had sustainability as an issue on one of his slides but didn\u2019t mention or announce anything.\n\nThe main keynote by Adam Selipsky spent a lot of time on generative AI, but also didn\u2019t have any mention of sustainability topics.\n\nWerner\u2019s keynote talked about sustainability, but only from the viewpoint of using cost as a proxy for optimization. This is pretty weak, as a dollar of CPU and a dollar of storage have very different carbon profiles, and customers need better ways to model and estimate actual carbon data in real time. There was a lot more more useful carbon optimization information in Steffen Grunwald\u2019s SUS302 talk.\n\nSome of my favorite people at AWS, VPs Bill Vass, Francessca Vasquez and David Brown gave Innovation keynote talks that are worth watching.\n\nSustainability Track Sessions and Workshops\n\nWhat follows are the catalog descriptions of the talks, after I\u2019ve watched each of the videos I\u2019ll update this post with my own commentary.\n\nACT133/ACT134/ACT135/ACT304: AWS Sustainability Showcase\u200a\u2014\u200aAll day event at the Caesars Forum\n\nInnovation is key to achieving sustainability goals\u200a\u2014\u200achallenges including water conservation and decarbonization of operations and energy are addressed through technologies that drive sustainable transformation. From the AWS family of custom chips such as AWS Inferentia and AWS Trainium to examples of customers using artificial intelligence (AI), machine learning (ML), and Internet of Things (IoT), visit the Sustainability Showcase to learn more about the tools available for organizations of all sizes and across all sectors to build and implement solutions that meet their sustainability goals.\n\nI visited the showcase and they had some good demos, including a public framework originally develped by Professional Services that can be used as the basis to build a carbon data collection and analysis service for a customer. There was also a demo of useful tool for figuring out all the terminology and concepts that are part of sustainability standards.\n\nADM302: Running sustainable real-time advertising workloads in the cloud\n\nIn this session for technology and engineering leaders, learn how companies are using the cloud to reduce costs and optimize performance for scaled real-time workloads while lowering their carbon footprint. Explore how these companies are bringing sustainability to the forefront of architectural designs. Speakers: John Briscoe (Annalect), Micah Walter, Rakshit Uppal (Amazon Ads), Stephanie Layser\n\nARC204: Sustainability: Measure, optimize, and improve costs (Workshop)\n\nBuilding a sustainable business for your organization and the world we share is top of mind for nearly every organization. Join this workshop to delve into practical strategies for constructing and measuring sustainability key performance indicators (KPIs) while implementing sustainable serverless architectures on the AWS Cloud. Discover how to make sustainable decisions using powerful tools like AWS Graviton, Amazon CodeGuru Profiler, AWS Lambda Power Tuning, Amazon API Gateway, and AWS X-Ray, and uncover the symbiotic relationship between sustainability and overall efficiency. You must bring your laptop to participate. Speakers: Jang Whan Han, Miranda Li\n\nBIZ107: Digitizing energy management for a sustainable future with Iberdrola (sponsored by Deloitte)\n\nIberdrola launched an Advanced Smart Assistant in May 2023, a connected energy solution that allows customers to take an active role in the energy transition. This session explores the solution, which runs on AWS with Deloitte as service provider, and how it helps reduce smart device energy consumption by 10% to 30%. Using AI/ML services, the application offers simple experiences to understand usage, consumption patterns, and areas of savings. For example, the solution makes it possible to reduce air conditioning consumption by considering the thermal inertia of the building. Learn how this development is just one of the growing strategic business lines of Iberdrola. This presentation is brought to you by Deloitte, an AWS Partner. Speakers: Guillermo Men\u00e9ndez (Corral), Juan Ignacio Codoner (Deloitte Consulting SLU), Carlos Pascual Lopez (Iberdrola)\n\nBLD219: EcoEfficient: Harnessing AI for sustainable energy optimization\n\nEcoEfficient combines advanced AI tech with sustainability for efficient energy management. It demonstrates how buildings can optimize energy use and promotes cost-effective and eco-friendly strategies. EcoEfficient uses AI algorithms to optimize HVAC systems, lighting, and other energy devices by analyzing historical energy usage data, weather patterns, and occupancy levels. See the demo to understand how to create real-time energy savings and the costs and environmental benefits that arise. Speakers: Ayush Agrawal, Fraser Sequeira, Supreeth S Angadi\n\nCMP212: Sustainable compute: Reducing costs and carbon emissions with AWS\n\nUncover the dual benefits of cost savings and sustainability with AWS compute. This session spotlights AWS compute services including AWS Graviton processors, Amazon EC2 Spot Instances, Amazon EC2 Auto Scaling, AWS Compute Optimizer, and Amazon EC2 Flex Instances, all geared toward optimizing resource use. Learn more about building sustainable and cost-efficient cloud operations that align with your business goals. Speakers: Robert McCone, Jackson Wagstaff, Sridhar Ayala (Adobe)\n\nCMP406: Reduce costs and improve sustainability with AWS Graviton (Workshop)\n\nAWS Graviton based instances can provide up to 40% better price performance over comparable current-generation instances for a wide variety of workloads, including application servers, microservices, high-performance computing, electronic design automation, gaming, open-source databases, and in-memory caches. Graviton based instances also use up to 60% less energy for the same performance than comparable Amazon EC2 instances would. In this workshop, learn how you can use Graviton based instances with AWS managed services and EC2 infrastructure including Amazon EKS, Amazon ECS, multi-architecture CI/CD pipelines,\u00a0.NET Core, Amazon RDS, Amazon EMR, and Amazon OpenSearch Service. You must bring your laptop to participate. Speakers: Chad Schmutzer, Marcin Bednarz\n\nCOP217: FinOps and GreenOps successes on AWS: A customer story\n\nIn this lightning talk, learn about the latest chapter of Wehkamp\u2019s AWS journey and how, after years of AWS Cloud experience, FinOps and GreenOps are driving cultural change and success in their Tech Hub. Learn about Wehkamp\u2019s optimized use of AWS services and how the latest releases can have a positive impact on cloud costs and carbon efficiency. Join this talk to learn more about this journey toward responsible and sustainable cloud use, based on practical learnings from Wehkamp. This presentation is brought to you by Xebia, an AWS Partner.\n\nENT226: Optimize SAP-based supply chains and improve sustainability with AI/ML\n\nOrganizations in manufacturing and other industrial verticals need to position themselves to thrive despite supply chain volatility, while reducing the cost and carbon footprint of their operations. For many of these companies, SAP systems are their business process backbones. In this session, a leading telecommunications provider demonstrates how they use AWS AI/ML services including Amazon SageMaker Studio and Amazon SageMaker Data Wrangler in concert with SAP to reduce costs and energy consumption while improving demand planning accuracy. Speakers: Johan Sporre Lennberg, (Ericsson), Joe Currid, Rickard Wieselfors (Ericsson)\n\nENT311: Build cost-optimized, sustainable, and resilient SAP systems (Workshop)\n\nIn this workshop, review the AWS Well-Architected Framework to understand architectural best practices for SAP, with an emphasis on cost-optimization, resilience, and sustainability. Then, learn how to prove the resilience of mission-critical SAP workloads running on AWS by simulating common failure scenarios that can impact SAP workloads to demonstrate resilience, showing that system availability remains consistent. Finally, go through a cost and sustainability optimization exercise without impacting system availability. You must bring your laptop to participate. Speakers: khaliluddin siddiqui, Rozal Singh\n\nINO106: Driving efficiency and sustainability with AWS innovation mechanisms\n\nGlobally, more priority is being given to sustainability efforts at the C-suite level. Successful sustainability programs do not just reduce an organization\u2019s carbon footprint but also affect bottom-line costs to the business. In this session, hear how an organization utilized AWS mechanisms and services like the PRFAQ and prototyping center to drive operational efficiency and support sustainability efforts. Learn how they used AWS IoT TwinMaker to optimize manufacturing processes and provide employees with real-time visibility into utilization, traffic, and equipment status. Speakers: Irina Radu, Jason Mortensen, Juha-Matti Kuusinen\n\nMAE203: How ELF reduced their carbon footprint by broadcasting on AWS\n\nIn this session, the European League of Football (ELF) shares their journey as a new sports league to adopt a built-in-the-cloud broadcasting production pipeline on AWS. By using AWS cloud-based remote production tools and infrastructure, the 2023 season saw positive impacts including lower video latency, better video quality, increased reliability, lower operational costs, and a reduction in the ELF\u2019s carbon footprint. Learn methods for architecting media workloads efficiently in alignment with the AWS Well-Architected Framework sustainability pillar and how AWS can help with your sustainability journey. Speakers: Felix Wegner, Jason O\u2019Malley, Zeljko Karajica (European League of Football)\n\nPRO203 | From factory to cloud: Enhancing operations & sustainability with IoT\n\nIn this session, dive into Siemens Energy\u2019s journey to using digitalization to shape its next-generation smart factories. In collaboration with AWS ProServe, Siemens Energy integrated IoT edge devices across more than 80 factories into the cloud. Explore the tangible benefits of smart factories, from improved problem-solving to increased machine uptime, which can lead to cost advantages. Learn about concrete examples demonstrating the impact of data connectivity on factory operations and Siemens Energy\u2019s dedication to sustainability. Speakers: Guillaume Allain, Jeff Bramlett, Mario Pilz (Siemens Energy)\n\nSUP301: Continuous cost and sustainability optimization (Workshop)\n\nIn this workshop, learn best practices for cost and sustainability optimization. Shift costs and sustainability responsibilities from the Cloud Center of Excellence (CCoE) to end users and application owners aided by automation and monitoring at scale. Explore how to use AWS services designed to optimize your infrastructure and your workloads with the aid of AI/ML algorithms to detect anomalies. Building upon real-world use cases, learn about cost efficiency implementing mechanisms that empower application owners to have clear, actionable tasks for cost and sustainability optimization. You must bring your laptop to participate. Speakers: Enrico Bonaccorsi, Francesc Sala\n\nSUS101: Sustainability innovation in AWS Global Infrastructure\n\nAWS is determined to make the cloud the cleanest and most energy-efficient way to run customers\u2019 infrastructure and business. This includes providing the efficient, resilient services AWS customers expect, while minimizing their environmental footprint. In this session, learn how AWS focuses on efficiency across all aspects of its infrastructure and is innovating to find new ways to lower its carbon footprint, as it aims to reach net-zero carbon by 2040. Find out about Amazon\u2019s progress toward powering its operations with 100 percent renewable energy, AWS\u2019s reverse logistics program, and AWS\u2019s commitment to being water positive by 2030. AWS speaker: Abhishek Sharma\n\nA good introduction to AWS Sustainability, if you haven\u2019t already seen this pitch many times over the last three years (my team created it and I came up with some of the talking points and slide designs myself). There is some new data here on renewable project totals as I mentioned earlier in this post, and a progress update on water use metrics.\n\nSUS201: Using Amazon SageMaker, AutoGluon, and ASDI to predict air quality (Workshop)\n\nAccording to the World Health Organization (WHO), ambient air pollution leads to an estimated 4.2 million deaths per year and around 91 percent of the world\u2019s population lives where air quality levels exceed WHO limits. In this workshop, investigate the relationship between air quality and weather using Amazon SageMaker Studio. Access open data from the Amazon Sustainability Data Initiative (ASDI) with Amazon S3 and an API to explore air quality by geography. Deploy a machine learning model that uses AutoGluon binary classification models to predict how weather features may result in unhealthy air quality. You must bring your laptop to participate. AWS speakers: Charles Roberts, Pauline Ting\n\nSUS202: Accelerate data-driven circular economy initiatives with AWS\n\nThe use of digital technology to accelerate circular economy represents an opportunity to transform business operations. Yet many companies find it challenging to integrate digital technologies meaningfully into their operations, and they still rely on inefficient processes that generate unnecessary waste. In this session, SK ecoplant shares how they were able to unlock the power of digital technologies to improve natural resource management and reduce waste. Learn how SK ecoplant and AWS are collaborating to use IoT sensors to collect field data and AI to deliver water quality predictions, enhancing decision-making and delivering waste management insights. AWS speaker: Jenna Leiner, Customer speaker: Kyu Sam Om\n\nSUS203: Accelerating end-to-end supply chain transparency with AI/ML\n\nOrganizations and consumers are demanding greater transparency and traceability within supply chains, while regulators are requesting that companies disclose more value chain data. In this session, learn how to build automated, serverless AWS architectures to upload, extract, process, verify, and validate supply chain documents, accelerating end-to-end supply chain transparency. Explore an implementation of this architecture with PVH, the parent company of Tommy Hilfiger and Calvin Klein. By ingesting data from suppliers and using Amazon Textract, Amazon Comprehend, and Amazon SageMaker, PVH has gained end-to-end visibility into their clothing supply chain and plans to provide visibility to their customers. AWS speaker: Laurens ten Cate, Customer speaker: Chris Verduyn\n\nSUS204: Using AI for ESG reporting and data-driven decision-making\n\nOrganizations are making commitments to sustainability but face challenges in measuring and analyzing the data required to meet these goals. One of the key challenges organizations face is an ability to extract datasets from different sources. In this session, learn how AWS generative AI services power the FlexZero platform and offer a way to ingest and process carbon data using pre-existing industry standard carbon emission factors and calculations. Rehrig Pacific, a leader in supply chain solutions, shares insights on how they are accurately measuring and reporting on their carbon footprint, in collaboration with AWS and FlexZero. AWS speaker: Aditi Suresh, Customer speaker: Brian Rowe\n\nSUS205: Slowing down deforestation by using AI, ML, and open source data\n\nOrganizations worldwide are developing solutions to evaluate, monitor, or predict land use and cover change in order to identify potential carbon credit projects, deforestation trends, and future wildfire risk areas. Open source geospatial AI/ML analysis, along with IoT-connected sensors, can provide near real-time data platforms built in the cloud and assist decision-making. In this session, learn how AWS has been supporting customers like the Territorial Intelligence Centre (CIT) in designing cutting-edge tools to address traceability in supply chains with deforestation risks by integrating governmental databases, innovative map services, and land-use data from high spatial resolution satellite imagery. AWS speaker: T. Luke Young, Customer speaker: Felipe Nunes\n\nSUS206: Measuring sustainability optimization of cloud workloads\n\nOptimizing cloud workloads for sustainability starts with defining metrics or KPIs that can help organizations adopt best practices recommended by the AWS Well-Architected Framework sustainability pillar. In this lightning talk, learn how to use AWS services and tools like the customer carbon footprint tool, AWS Trusted Advisor, and AWS Cost and Usage Reports to define proxy metrics and combine with business metrics to establish a show-back mechanism to evaluate and quantify the effect of the optimizations. Speakers: Deepthi Madamanchi, Raman Pujani\n\nSUS207: N/A\n\nSUS208: Building next-generation sustainability workloads with open data\n\nFuture sustainability digital technologies will be driven by data, but limited access to and significant costs of acquiring sustainability datasets present a barrier to aspiring builders. Programs like Open Data on AWS provide free, publicly available datasets, accelerating sustainability innovation. In this lightning talk, find out how organizations are using open data and the broad range of AWS compute, AI/ML, and database services to build sustainability workloads in the cloud. Learn about the Natural History Museum Planetary Knowledge Base, a mission-driven research tool for understanding and addressing global biodiversity change, that was built with open data using AWS Glue, Amazon Neptune, and Amazon Neptune ML. Speakers: Sam Bydlon, Vincent Smith (Natural History Museum)\n\nSUS209: Optimizing generative AI workloads for sustainability\n\nWith the increasing complexity, scale, and adoption of AI workloads, it is important to consider their environmental impact. This lightning talk provides environmental sustainability insights that are specific to large language models and how machine learning operations (MLOps) is critical to sustainable optimization. Explore practical best practices for different customization scenarios including training models from scratch, fine-tuning with additional data using full or parameter-efficient techniques, Retrieval Augmented Generation, and prompt engineering. Speakers: Isha Dua, Wafae Bakkali\n\nSUS210: Creating sustainable practices through operational excellence\n\nCreating a cloud operating model in the context of sustainability can be hard. In this lightning talk, explore the benefits to sustainable outcomes of driving a culture of operational excellence, bringing together two of the foundational pillars of the AWS Well-Architected Framework. Learn from two principal TAMs about how they help their customers build and implement sustainable IT programs. Speakers: Paul Moran, Thomas Coombs\n\nSUS211: How AWS is used to fight natural disasters and mitigate wildfire risk\n\nTechnology stands as our beacon of hope, illuminating a path where disasters like wildfires are fought with innovation. By using technologies like data analytics and machine learning, AWS is empowering firefighters and organizations, like San Diego Gas & Electric, Exci, and BlackBerry AtHoc, to detect, monitor, and respond to wildfires more effectively. Organizations use drones, AI, and predictive modeling to inspect power lines and detect brush fires using cameras and satellite imagery. Through the power of cloud computing, AWS is enabling valuable insights to improve fire management strategies, ultimately aiding in the protection of lives, property, and the environment. Speakers: Rahul Sareen\n\nSUS212: How climate tech startups use AWS to drive sustainable innovation\n\nJoin this lightning talk to explore how startups use the cloud to address the challenges of climate change. Hear real-world stories of how climate tech unicorns have built solutions on AWS to further their missions. Did you know that 87% of global climate and AI leaders believe AI is a helpful tool in the fight against climate change? Leave this talk knowing more about AI for climate use cases and relevant AWS programs to accelerate your journey. Be inspired by the potential of generative AI, IoT, analytics, and more to drive a sustainable future. Speaker: Ceren Tahtasiz\n\nSUS301: Renewable energy assets in the smart home: Managing demand flexibility (Workshop)\n\nIn this workshop, learn how to manage demand flexibility for residential properties equipped with interconnected renewable energy assets using a digital twin, which includes an electric vehicle, solar panels, a heat pump, and battery storage. Develop strategies for efficiently managing a surge in energy production by utilizing renewable energy resources to their fullest extent while ensuring grid stability and reducing the need for grid-supplied energy. Learn how to integrate AWS services with a digital twin, explore strategies for optimizing renewable energy utilization, and understand the demand-side response in the context of variable renewable energy generation. You must bring your laptop to participate. AWS Speakers: Thomas Attree, Tamara Herbert\n\nSUS302: Sustainable architecture: Past, present, and future\n\nSince the launch of the sustainability pillar of the AWS Well-Architected Framework in 2021, the attention to environmental sustainability has grown to a key nonfunctional requirement of cloud workloads. This session revisits the pillar and its best practices. Dive deep into mechanisms and tools customers use to establish visibility of the resource and energy efficiency of their architectures and how they identify and prioritize optimizations. Through customer examples and a demo, learn how you can build a sustainability showback mechanism for your application teams and drive change. AWS speakers: Steffen Grunwald, Katja Philipp\n\nThis is an excellent talk providing practical advice and demonstrating open source tooling that can be used to produce carbon accounting reports and to scan cloud formation templates to provide advice on how to be more sustainable. I co-presented the launch of the AWS Well Architected Guide for Sustainability with Steffen in 2001, and he\u2019s a good source to reach out to for questions @steffeng@hachyderm.io.\n\nSUS303: Measuring sustainability optimization of cloud workloads\n\nOptimizing cloud workloads for sustainability starts with defining metrics or KPIs that can help organizations adopt best practices recommended by the AWS Well-Architected sustainability pillar. In this chalk talk, learn how to use AWS services and tools like the AWS Customer Carbon Footprint Tool, AWS Trusted Advisor, and AWS Cost and Usage Reports to define proxy metrics and combine with business metrics to establish a show-back mechanism to evaluate and quantify the effect of the optimizations. AWS speakers: Raman Pujani, Deepthi Madamanchi\n\nSUS304: Building next-generation sustainability workloads with open data\n\nFuture sustainability digital technologies will be driven by data, but limited access to and significant costs of acquiring sustainability datasets present a barrier to aspiring builders. Programs like Open Data on AWS provide free, publicly available datasets, accelerating sustainability innovation. In this session, find out how AWS customers are using open data and AWS\u2019s broad range of compute, AI/ML, and database services to build sustainability workloads in the cloud. Learn about the Natural History Museum Planetary Knowledge Base, a mission-driven research tool for understanding and addressing global biodiversity change, that was built with open data using AWS Glue, Amazon Neptune, and Amazon Neptune ML. AWS speaker: Sam Bydlon, Customer speaker: Vincent Smith\n\nSUS305: Optimizing generative AI workloads for sustainability\n\nWith the increasing complexity, scale, and adoption of AI workloads, it is important to consider their environmental impact. This chalk talk provides environmental sustainability insights that are specific to large language models and how machine learning operations (MLOps) is critical to sustainable optimization. Explore practical best practices for different customization scenarios including training models from scratch, fine-tuning with additional data using full or parameter-efficient techniques, retrieval augmented generation, and prompt engineering. AWS speakers: Wafae Bakkali, Isha Dua\n\nWPS103: Lifesaving early warning systems and mitigation for climate disasters\n\nEvery day, people around the world are impacted by the unexpected\u200a\u2014\u200afrom pandemics, to climate and human-wrought disasters, to economic crises. AWS is at the forefront of supporting public sector organizations as they adapt to natural disasters and seek to mitigate carbon emissions. Join this session for a discussion on topics such as developing early warning systems, monitoring carbon credit projects, identifying deforestation trends, and developing future wildfire risk. From floods in Central America to land cover change in Brazil, panelists share how the use the cloud to better prepare and respond to the impact of climate change.",
        "metadata": {
          "word_count": 4335,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS re:Invent 2023 was held in Rome, Italy. Sustainability was a topic of discussion. There was no progress on Scope 3 carbon reporting. AWS are targeting 100% in-region renewable energy by 2025 and were at 90% globally in 2022. Their policy is a good one and they are investing at a very high level. The AWS list of energy projects was quietly updated just before re:Invent, to add a bunch more. It wasn\u2019t mentioned in the conference keynotes. The AWS Energy team was represented by Abhishek Sharma\u2019s SUS101 talk. Amazon's cale energy capacity list adds up to over 18GW, with excellent global coverage. The difference between 18GW and 26GW is capacity dedicated to non-AWS parts of Amazon. Amazon has more capacity over-all and much better global PPA coverage. The main keynote by Adam Selipsky spent a lot of time on generative AI. Peter had sustainability as an issue on one of his slides but didn't mention or announce anything. There was a lot more more useful carbon optimization information in Steffen Grunwald\u2019s talk. AWS Sustainability Showcase is an all day event at the Caesars Forum in Las Vegas. The aim is to show how companies can use technology to achieve sustainability goals. I visited the showcase and had some good demos. Learn how companies are using the cloud to reduce costs and optimize performance for scaled real-time workloads while lowering their carbon footprint. There was also a demo of useful tool for figuring out all the terminology and concepts that are part of sustainability standards. Iberdrola launched an Advanced Smart Assistant in May 2023, a connected energy solution that allows customers to take an active role in the energy transition. This session explores the solution, which runs on AWS with Deloitte as service provider. EcoEfficient combines advanced AI tech with sustainability for efficient energy management. It demonstrates how buildings can optimize energy use and promote cost-effective and eco-friendly strategies. EcoEfficient uses AI algorithms to optimize HVAC systems, lighting, and other energy devices by analyzing historical energy usage data. AWS Graviton instances can provide up to 40% better price performance over comparable current-generation instances. Graviton based instances also use up to 60% less energy for the same performance than comparable Amazon EC2 instances would. In this workshop, learn how you can use Graviton. Learn about Wehkamp\u2019s optimized use of AWS services and how the latest releases can have a positive impact on cloud costs and carbon efficiency. This presentation is brought to you by Xebia, an AWS Partner. Leading telecommunications provider demonstrates how they use AWS AI/ML services to reduce costs and energy consumption. Review the AWS Well-Architected Framework to understand architectural best practices for SAP. More priority is being given to sustainability efforts at the C-suite level. Hear how an organization utilized AWS mechanisms and services to drive operational efficiency and support sustainability efforts. Go through a cost and sustainability optimization exercise without impacting system availability. You must bring your laptop to participate. European League of Football shares their journey as a new sports league to adopt a built-in-the-cloud broadcasting production pipeline on AWS. Learn methods for architecting media workloads efficiently in alignment with the AWS Well-Architected Framework sustainability pillar. Workshop: Continuous cost and sustainability optimization. Explore the tangible benefits of smart factories, from improved problem-solving to increased machine uptime. Shift costs and sustainability responsibilities from the Cloud Center of Excellence (CCoE) to end users. AWS is determined to make the cloud the cleanest and most energy-efficient way to run customers\u2019 infrastructure and business. Learn how AWS focuses on efficiency across all aspects of its infrastructure. Find out about Amazon\u2019s progress toward powering its operations with 100 percent renewable energy. There is some new data here on renewable project totals as I mentioned earlier in this post. There is also a progress update on water use metrics. And there is a workshop on using Amazon SageMaker to predict air quality.  SK ecoplant shares how they were able to unlock the power of digital technologies to improve natural resource management and reduce waste. Learn how they are collaborating to use IoT sensors to collect field data and AI to deliver water quality predictions. SUS204: Using AI for ESG reporting and data-driven decision-making. Learn how AWS generative AI services power the FlexZero platform and offer a way to ingest and process carbon data usin. SUS205: Slowing down deforestation by using AI, ML, and open source data. Rehrig Pacific, a leader in supply chain solutions, shares insights on how they are accurately measuring and reporting on their carbon footprint, in collaboration with AWS and FlexZero. Learn how to use AWS services and tools like the customer carbon footprint tool, AWS Trusted Advisor, and AWS Cost and Usage Reports to define proxy metrics and combine with business metrics. Open Data on AWS provides free, publicly available datasets, accelerating sustainability innovation. Find out how organizations are using open data and the broad range of AWS compute, AI/ML, and database services to build sustainability workloads in the cloud. SUS210: Creating sustainable practices through operational excellence. SUS211: How AWS is used to fight natural disasters and mitigate wildfire risk. AWS is empowering firefighters and organizations to detect, monitor, and respond to wildfires more effectively. Organizations use drones, AI, and predictive modeling to inspect power lines and detect brush fires using cameras and satellite imagery. 87% of global climate and AI leaders believe AI is a helpful tool in the fight against climate change. Learn how to manage demand flexibility for residential properties equipped with interconnected renewable energy assets using a digital twin. Be inspired by the potential of generative AI, IoT, analytics, and more to drive a sustainable future. Sustainability is a key nonfunctional requirement of cloud workloads. Dive deep into mechanisms and tools customers use to establish visibility of the resource and energy efficiency of their architectures.  AWS speakers: Raman Pujani, Deepthi Madamanchi. Learn how to use AWS services and tools to define proxy metrics and combine with business metrics.  AWS customers are using open data and AWS\u2019s broad range of compute, AI/ML, and database services to build sustainability workloads in the cloud. Learn about the Natural History Museum Planetary Knowledge Base, a mission-driven research tool. AWS is at the forefront of supporting public sector organizations as they adapt to natural disasters and seek to mitigate carbon emissions. Panelists share how the use the cloud to better prepare and respond to the impact of climate change."
      },
      "tags": [
        "ai",
        "architecture",
        "artificial intelligence",
        "automation",
        "aws",
        "azure",
        "ci/cd",
        "cloud",
        "engineering",
        "infrastructure",
        "machine learning",
        "metrics",
        "microservices",
        "ml",
        "monitoring",
        "performance",
        "platform",
        "resilience",
        "scaling",
        "serverless",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_35f04568",
      "kind": "file",
      "subkind": "medium_post",
      "title": "AWS Lambda is Everywhere",
      "source": "Medium Archive",
      "published_date": "2016-12-08",
      "url": "https://medium.com/@adrianco/2016-12-08_AWS-Lambda-is-Everywhere-f28c855e44b9",
      "content": {
        "text": "AWS Lambda is Everywhere\n\nTwo years ago AWS Lambda was launched, and the core idea of event driven functions now has wide support as part of a larger \u201cserverless\u201d ecosystem.\n\nThe latest announcements let you take Lambda events and functions and run them everywhere. Let\u2019s take this step by step, starting with the pre-announcement functionality:\n\nLambda functions started out in the data storage tier as methods triggered by actions on the object store S3, database triggers in DynamoDB and by consuming Kinesis streams.\n\nFor processing and generating user notification events, integrations include the Simple Notification Service, Simple Email Service, Scheduled Events and Cognito mobile event service.\n\nTo track management and deployment events in an AWS account, AWS Config, CodeCommit, CloudFormation, and CloudWatch have all been integrated with Lambda.\n\nTo build user facing applications, Amazon API Gateway handles the endpoint, and Lambda provides the handlers for each type of request.\n\nGiven Lambda support from the endpoint all the way down to storage, Lambda-based applications are growing in both size and scope, with many functions and non-trivial workflow. To help build, visualize and debug these applications there are open source projects from AWS providing a serverless application model, the Express framework, and a Python microframework called Chalice.\n\nFrom the community there are more options including the serverless framework, WSGI support from Zappa, and Go language support via Apex.run. Two new AWS Lambda related services were announced at re:Invent. Workflow oriented applications can be designed and operated using AWS Step Functions, and at runtime, end to end tracing of requests through the system can be collected and visualized using AWS X-Ray.\n\nLambda functions already run behind end-points in 10 AWS regions worldwide, with additional regions coming, but there are points of presence for AWS at many more CloudFront CDN locations, with low latency connectivity to customers. While it\u2019s not possible to obtain an instance at a CloudFront location, Lambda@Edge provides the capability to specify a Lambda function that runs globally at those locations. This is truly \u201cserverless\u201d, as you can\u2019t get a server, you have to build your application using Lambda to reach to the edge. Naturally there are more constraints on Lambda functions that can be run at the edge, but the programming model is identical.\n\nMoving from Lambda at a regional endpoint, and at the edge of the network, the next logical step is to provide Lambda embedded in devices, with direct local connectivity between functions on the device, and support for device shadows, to cache and synchronize state during intermittent network connections. This new capability is called AWS Greengrass.\n\nFinally, the new rugged Snowball Edge device provides compute, storage and AWS services in a package that can be installed almost anywhere. AWS Greengrass is included, so the Snowball Edge can be remotely configured with AWS Lambda functions that process and manage data and services everywhere.\n\nWith AWS Lambda everywhere, including locations where there are no provisionable servers, serverless application architecture patterns provide access to new capabilities that will be used to build new kinds of applications. I was a judge at the AWS re:Invent Hackathon, and was very impressed by the highly functional entries, and how fast they were built, most of them using Lambda. You can even run map-reduce directly from Lambda, I\u2019m looking forward to seeing what everyone else comes up with!",
        "metadata": {
          "word_count": 554,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS Lambda was launched two years ago, and the core idea of event driven functions now has wide support as part of a larger \u201cserverless\u201d ecosystem. The latest announcements let you take Lambda events and functions and run them everywhere. Lambda functions already run behind end-points in 10 AWS regions worldwide. Workflow oriented applications can be designed and operated using AWS Step Functions. End to end tracing of requests through the system can be collected and visualized using AWS X-Ray. Lambda@Edge provides the capability to specify a Lambda function that runs globally at those locations. This is truly \u2018serverless\u2019, as you can\u2019t get a server, you have to build your application using Lambda to reach to the edge. AWS Lambda can be used to build new kinds of applications. I was a judge at the AWS re:Invent Hackathon and was very impressed by the highly functional entries. You can even run map-reduce directly from Lambda."
      },
      "tags": [
        "architecture",
        "aws",
        "serverless",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_file_2909ef9e",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Hi Joe Emison thanks for the comment",
      "source": "Medium Archive",
      "published_date": "2016-06-05",
      "url": "https://medium.com/@adrianco/2016-06-05_Hi-Joe-Emison--thanks-for-the-comment--262d19578f35",
      "content": {
        "text": "Hi Joe Emison, thanks for the comment. I agree there is a lot of inertia. My point was really that you can see where disruption is going to happen eventually, but it\u2019s not usually possible to predict when it\u2019s going to happen. Trends are sometimes slow and linear but often have tipping points where a hard to predict individual event or person can accelerate change. For example if the Icelandic Pirate Party won an election and implemented direct democracy there would be a real test of the concept, if they don\u2019t win then it remains theoretical and has much less influence.https://en.wikipedia.org/wiki/Pirate_Party_(Iceland)",
        "metadata": {
          "word_count": 101,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I agree there is a lot of inertia. Trends are sometimes slow and linear but often have tipping points where a hard to predict individual event or person can accelerate change. For example if the Icelandic Pirate Party won an election and implemented direct democracy there would be a real test of the concept."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_12d11b23",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Sustainability at AWS re Invent 2022 All the talks and videos I could find",
      "source": "Medium Archive",
      "published_date": "2023-02-14",
      "url": "https://medium.com/@adrianco/2023-02-14_Sustainability-at-AWS-re-Invent-2022-All-the-talks-and-videos-I-could-find--d0a19a0a1b86",
      "content": {
        "text": "Sustainability at AWS re:Invent 2022 -All the talks and videos I could\u00a0find\u2026\nLas Vegas MSG Sphere under construction next door to the Venetian Sands Expo Center\u200a\u2014\u200aPhoto by\u00a0Adrian\nThis blog post is long overdue\u200a\u2014\u200aI spent too long trying to find time to watch all the videos, and finally gave up and listed a few below that I haven\u2019t seen.\n\nThe keynotes didn\u2019t feature anything new on carbon, just re-iterated the existing path to 100% green energy by 2025. AWS did have some new goals around sustainable water use that are quite agressive and welcome.\n\nThere was some new sustainability information that was quietly added to Amazon\u2019s Sustainability in the Cloud page in October 2022 that is significant. But they didn\u2019t go out of their way to promote it. There was also no news or updates to the AWS Customer Carbon Footprint Tool. I asked around and heard that they are still working on it, but the AWS hiring freeze means that they don\u2019t have the headcount they expected and are making slow progress on an API, more detailed metrics, and scope 3, which everyone is waiting for.\n\nThe new information from October 2022 states:\nTo achieve our goal of powering our operations with 100% renewable energy by 2025\u200a\u2014\u200afive years ahead of our original 2030 target\u200a\u2014\u200aAmazon contracts for renewable power from utility scale wind and solar projects that add clean energy to the grid. These new renewable projects support hundreds of jobs while providing hundreds of millions of dollars of investment in local communities. We also may choose to support these grids through the purchase of environmental attributes, like Renewable Energy Certificates and Guarantees of Origin, in line with our Renewable Energy Methodology.As a result, in 2021, the following AWS Regions were powered by over 95% renewable energy:US East (Northern Virginia)GovCloud (US-East)US East (Ohio)US West (Oregon)GovCloud (US-West)US West (Northern California)Canada (Central)Europe (Ireland)Europe (Frankfurt)Europe (London)Europe (Milan)Europe (Paris)Europe (Stockholm)\nThis is actually a very useful statement. It basically says that near-as-dammit all the regions in the EU and US are already being supplied by renewable energy. So when you see zero or near zero carbon in your AWS Customer Carbon Footprint Tool report, that\u2019s expected. Most of the carbon emissions will be in other parts of the world, even if that\u2019s a small part of your AWS footprint, and that\u2019s what they are currently working on for 2025.\n\nIt\u2019s complex and different people have different opinions on how to account for it, but I found some details on how over 20GW of Amazon power purchase agreements (PPAs) are accounted for to zero out most of the carbon.\n\nThere were many sustainability talks at reInvent. In 2021 Elise Greve from my team at Amazon persuaded the reInvent team to create a sustainability track, and although it was small and lightly attended, it had the effect we wanted, and it was great to see so many more talks in 2022. If I missed any or have anything wrong please let me know and I\u2019ll post an update.\n\nThere is a very helpful Sustainability Talks Playlist by Ryan Burke\u200a\u2014\u200aI picked out the most relevant and watched as many as I could.\n\nCOM201 Being ethical in tech: Sustainability tips for work and life\u200a\u2014\u200ano video, but here is a video of Aerin Booth presenting an earlier version of the talk. I\u2019m working with Aerin on a Sustainability in the Cloud Maven class in the UK at the moment, and we\u2019re looking for attendees..\n\nSUS201 Detecting deforestation with geospacial images and Amazon SageMaker\u200a\u2014\u200aParth Patel AWS SA and Isha Duo AWS SA\u200a\u2014\u200ano video.\n\nSUS202 Using Amazon SageMaker, AutoGluon, and ASDI to predicst air quality\u200a\u2014\u200aCharles Roberts AWS Senior Security Consultant and Aaron Soto AWS Senior Sustainability SA\u200a\u2014\u200ano video\n\nSUS203 Carbon emissions reporting using modern data architecture on AWS\u200a\u2014\u200aAnthony Howell AWS SA and Janahan Gnanachandran AWS SA\u200a\u2014\u200ano video\n\nSUS204 Sustainability in AWS global infrastructure\u200a\u2014\u200aCornelia Robinson AWS Infrastructure Senior Manager Inclusion and Outreach and Charley Daitch AWS Director Energy and Water. Talk by the team that is actually working on reducing the carbon footprint of AWS. They optimized airflow in datacenters to reduce cooling energy use by 20%. Discussion of the water positive goal and optimizations that let AWS return water directly for irrigation rather than to the sewage system. Amazon (not just AWS) is now at 19GW of renewable capacity, 85% renewable across the world. Lots of detail on new projects including creating the renewables market in places like India, Alberta, and Singapore where the local grid is very dirty. Portfolio is currently reducing Amazons carbon footprint by 19 Million Metric Tons of CO2e. Good discussion of the management and optimization of the portfolio. Low carbon cement and steel for new buildings. Cornelia talks about how AWS does community outreach in places where the datacenter and power facilities are sited.\n\nSUS205 Architecting sustainably and reducing your AWS carbon footprint\u200a\u2014\u200aElliott Nash Amazon Senior Manager and Margaret O\u2019Toole AWS WW Tech Leader Sustainability. Margaret leads the worldwide solution architect program for sustainability, and gives an excellent talk on how customers should think about optimizing their workloads.\n\nSUS206 Sustainability and AWS silicon\u200a\u2014\u200aKamran Khan AWS Senior Product Manager Inferential/Trainium/FPGA, David Chaiken Pinterest Chief Architect, and Paul Mazurkiewicz AWS Senior Principal Engineer. Good discussion of the embodied carbon of silicon chip production. Provides comparison of inference workload on P4dn GPU instances vs. AWS Trainium saving 92% energy and 90% cost, and training workload on P4dn vs. AWS Inferentia 2.6x shorter training time, saving 54% energy and 75% cost.\n\nSUS207 Patterns for obtaining insights from sustainability data\u200a\u2014\u200aDiv Shekhar AWS SA and Sandipan Bhaumik AWS Specialist Analytics SA\u200a\u2014\u200ano video\n\nSUS208 Utilizing sustainability data at scale\u200a\u2014\u200aBishesh Adhikari AWS ML Prototyping Architect and Patti Carroll AWS Senior Sustainability Strategist\u200a\u2014\u200aDiscussion of sustainable agriculture and food security, showing architecture patterns and customer case studies.\n\nSUS209\u200a\u2014\u200athere was no talk with this code.\n\nSUS210 Modeling climate change impacts and risks at scale\u200a\u2014\u200aPierre Souchay AXA Climate CTO and Max Richter AWS Global SA.\n\nSUS211 Water stewardship and renewable energy at scale\u200a\u2014\u200aPaul Reed AWS Industry Products GM Sustainability and Clean Energy and Will Hewes AWS Global Lead Water Sustainability. AWS made a new water positive commitment at re:Invent 2022\u200a\u2014\u200aby 2030 AWS will return more water to communities than they use. They use recycled and rain water where possible. They now publish a water intensity of 0.25 liters/KWh which is 4\u20138 times better than typical unoptimized businesses, so they use less, then they return clean water to farmers for irrigation in places like Oregon. In countries like India and Indonesia, they are bringing clean water to communities along with charities like Water.org\n\nSUS212 Accelerating decarbonization and sustainability transformation\u200a\u2014\u200aSree Anandavally Magna International Senior Director Smart Factory Solutions and Jessica Benoit AWS Senior Strategist Sustainability. This is the AWS Professional Services built tooling that customers can use to track the carbon footprint of their operations and processes, along with a customer example. Good example, well presented interesting new material.\n\nSTP213 Scaling global carbon footprint management\u200a\u2014\u200aBlake Blackwell Persefoni Manager Data Engineering and Michael Floyd AWS Head of Sustainability Solutions.\n\nGEO216 (Portugese Vinicius Senger AWS Senior Developer Advocate and Ana Cunha AWS Developer Advocate), GEO030 (Italian Simone D\u2019Antone AWS Global Accounts M&E Business Leader) Architecting for sustainability\n\nSUS301 Sustainable machine learning for protecting natural resources\u200a\u2014\u200aBenoit de Chateauveiux AWS Startup SA and Nelson Gonzalez AWS Head Global Impact Computing. Engaging presentation on an important topic, with some good advice on optimizing ML workloads.\n\nSUS302 Optimizing architectures for sustainability\u200a\u2014\u200aKatja Philipp AWS SA and Szymon Kochanski AWS SA.\n\nSUS303 Delivering sustainable, high-performing architectures\u200a\u2014\u200aThomas Coombs AWS Principal TAM, Sam Mokhari AWS Sustainability Pillar Lead, Chie Hayashida AWS Japan SA\u200a\u2014\u200asummary of the Well Architected Guide (which hasn\u2019t changed since last year), a demo of the Customer Carbon Footprint tool (which hasn\u2019t changed since last year) and a worked example of reducing the impact of a data science workload that was new.\n\nSUS304 to SUS311 No talks with these codes.\n\nSUS312 How innovators are driving more sustainable manufacturing\u200a\u2014\u200aMarcus Ulmefors Northvolt Director Data and ML Platforms and Muhammad Sajid AWS SA. Building a data lake of detailed information about energy use of many physical devices. It includes a demo of AWS Twinmaker and a discussion of lithium battery production and recycling by Northvolt in Sweden, who are using serverless on AWS to build factories-as-code.\n\nSTP213 Scaling global carbon footprint management.\n\nSUP304 Continuous cost and sustainability optimization\u200a\u2014\u200aFrancesc Sala AWS Principal TAM and Enrico Bonnaccorsi AWS Principal TAM.\n\nCMP204 Build a cost-, energy-, and resource-efficient compute environment\u200a\u2014\u200aSteffen Grunwald, AWS EMEA Principal Sustainability Solutions Architect, Troy Gasaway Arm Ltd Vice President Infrastructure & Engineering, Adam Boeglin AWS Principal Specialist EC2. I gave the Well Architected talk last year with Steffen, it was good to see his updated talk for this year, along with a customer example from ARM.\n\nDOP315 Sustainability in the cloud with Rust and AWS Graviton\u200a\u2014\u200aEmil Lerch AWS Principal DevOps Specialist and Esteban Kuber AWS Principal Software Engineer.\n\nPEX201 Building a sustainable practice for tomorrow\u200a\u2014\u200aMansi Vaghela AWS SA, Deborshi Choudhury AWS Partner SA, Rolf Korski Knowit VP Technology. Partner oriented session getting everyone up to speed on what AWS sees as the customer needs, motivations, business outcomes and architectures around sustainability. Well presented, plenty of new content and viewpoints, builds on the AWS standard deck better than SUS303.\n\nIOT204 How Amazon uses AWS IoT to improve sustainability across its buildings\u200a\u2014\u200aRob Aldrich AWS Senior Sustainability Strategist, Dramel Frazier Amazon Senior TPM, Ryan Burke AWS Senior Application Consultant.\n\nIMP205 Open data for impact: Quantify and manage climate-related risks\u200a\u2014\u200aChris Stoner AWS Open Data Program, Adrienne Simonson NOAA Director Open Data Dissemination, and OS-Climate. Climate risk is the dual of carbon emissions. As well as the impact your company is having on the climate, you need to understand the impact the climate is having on your company. When I was working at Amazon, I managed the Amazon Sustainability Data Initiative which is part of the open data program. I also got Amazon to join the non-profit OS-Climate and managed the relationship and contributions to building their data products. Excellent talk on the NOAA programs to share data and build communities around it.\n\nOne more talk that I found interesting, but not related to sustainability:\n\nNFX303 The Evolution of Chaos Engineering at Netflix\u200a\u2014\u200agood explanation of the evolution of this system, including advanced traffic routing techniques that most microservice architectures don\u2019t support.",
        "metadata": {
          "word_count": 1791,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Sustainability at AWS re:Invent 2022 didn\u2019t feature anything new on carbon, just re-iterated the existing path to 100% green energy by 2025. There was some new sustainability information that was quietly added to Amazon\u2019s Sustainability in the Cloud page in October 2022. The new information from October 2022 states: To achieve our goal of powering our operations with 100% renewable energy by 2025. Amazon contracts for renewable power from utility scale wind and solar projects that add clean energy to the grid. All regions in the EU and US are already being supplied by renewable energy. Most of the carbon emissions will be in other parts of the world. Over 20GW of Amazon power purchase agreements (PPAs) are accounted for. Sustainability in the Cloud Maven class in the UK is looking for attendees. I picked out the most relevant and watched as many as I could. Here is a video of Aerin Booth presenting an earlier version of the talk. AWS is now at 19GW of renewable capacity, 85% renewable across the world. They optimized airflow in datacenters to reduce cooling energy use by 20%. Margaret leads the worldwide solution architect program for sustainability. She gives an excellent talk on how customers should think about optimizing their workloads. Good discussion of the embodied carbon of silicon chip production. AWS made a new water positive commitment at re:Invent 2022. By 2030 they will return more water to communities than they use. They use recycled and rain water where possible. AWS Professional Services built tooling that customers can use to track the carbon footprint of their operations and processes. Good example, well presented interesting new material. The conference was hosted by AWS Japan and hosted by the AWS Sustainability team. It included a discussion of the Well Architected Guide and the Customer Carbon Footprint tool. There was also a demo of AWS Twinmaker and discussion of lithium battery production and recycling. AWS EMEA Principal Sustainability Solutions Architect, Steffen Grunwald, gave the Well Architected talk. I gave the talk last year with Steffen, it was good to see his updated talk for this year. Sustainability is the dual of carbon emissions. You need to understand the impact the climate is having on your company. I managed the Amazon Sustainability Data Initiative which is part of the open data program. NFX303 The Evolution of Chaos Engineering at Netflix is a good explanation of the evolution of this system. It includes advanced traffic routing techniques that most microservice architectures don\u2019t support."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "data science",
        "devops",
        "engineering",
        "infrastructure",
        "machine learning",
        "metrics",
        "ml",
        "netflix",
        "scaling",
        "security",
        "serverless",
        "sustainability",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_e6a04437",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thanks Kent your 2011 blog post is one of the early references to Cloud Native as an architecture",
      "source": "Medium Archive",
      "published_date": "2017-08-11",
      "url": "https://medium.com/@adrianco/2017-08-11_Thanks-Kent--your-2011-blog-post-is-one-of-the-early-references-to-Cloud-Native-as-an-architecture--ff2a0b8ac4d8",
      "content": {
        "text": "Thanks Kent, your 2011 blog post is one of the early references to Cloud Native as an architecture pattern. We\u2019ve come a long way in tooling, but mostly these principles have become much more mainstream.",
        "metadata": {
          "word_count": 35,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "We\u2019ve come a long way in tooling, but mostly these principles have become much more mainstream. Thanks Kent, your 2011 blog post is one of the early references to Cloud Native as an architecture pattern."
      },
      "tags": [
        "architecture",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_file_dc125539",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Part 1 Starting Out",
      "source": "Medium Archive",
      "published_date": "2022-05-23",
      "url": "https://medium.com/@adrianco/2022-05-23_What-Adrian-Did-Next---Part-1---Starting-Out-e256e04db50d",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aPart 1\u200a\u2014\u200aStarting\u00a0Out\nAdrian playing bass with Black Tiger, 1982\u200a\u2014\u200aPicture by Peter Kostelnik\nMy bio starts with this phrase \u201cAdrian Cockcroft has had a long career working at the leading edge of technology, and is fascinated by what happens next.\u201d About every 5\u20137 years the thing I was working on that was going to happen next matured and went mainstream, and something new and interesting took its place. Sometimes it was hard to get people to take a chance on me doing something new, but they mostly worked out.\n\nMy mother Margo was an elementary school teacher, she taught me to read when I was three, before I went to school, and I grew up reading lots of books. My father Nick was a statistics lecturer at Hatfield Polytechnic (now the University of Hertfordshire) for most of his career. He\u2019d programmed in the 1960s and encouraged me to learn. Hatfield Polytechnic installed what was at the time \u201cthe largest educational computer in Europe\u201d in 1970, a DECsystem 10. My high school was affiliated with the Polytechnic and there was a 300baud line and a Teletype (paper roll and punch tape) that anyone could book time on to mess around with during lunch breaks. Starting in 1972 I taught myself BASIC, then Algol, and remember playing lunar lander. However my main academic interest was physics, rather than computing. My main non-academic distraction was playing bass guitar, which reached a peak in 1982, gigging regularly with Black Tiger, part of the \u201cNew Wave of British Heavy Metal\u201d, then I got a job, moved away, and stopped playing regularly.\n\nI graduated from The City University in London, in 1982 with a degree in applied physics and electronics, and built a maze solving robot as my final year project. It had 4Kbytes of RAM, and I designed and wired the CPU board together myself. Software was written in Motorola 6800 assembler, using a PDP-11 running Unix V6 as a development platform. That got me a job at Cambridge Consultants Ltd as a software engineer writing code (mostly in C) for embedded real time signal processing and control systems. It also got me a side job looking after the new Unix systems they started to use, and in 1984 we were one of the first customers of Sun Microsystems. I was officially a one-day-a-week sysadmin for about ten workstations and servers. The full time admin only looked after our VAX/VMS systems, and our \u201cUnix guru\u201d Dave Reynolds was a senior engineer who taught me a lot, but was too busy to do admin work. I was installing, upgrading, and doing backups, as well as writing code on projects as my primary job (An early introduction to DevOps?). There were a few formative experiences, I shared an office with Sean Martin and learned how to be a software engineer and build real time systems working on projects with him, then he was promoted to manage our group. I forget his name but there was also a brilliant engineer who we called Consultant in Most Things. He seemed to be instantly and deeply familiar with any technical subject, however novel. I was never in his league, but it was inspirational for my early career development.\n\nIn my spare time I built a home-brew computer called a Microbox II, designed by my friend Dave Rumball, soldering all the components onto the board. It had a 6809 CPU and ran an operating system called Flex9. I got hold of the code for the Dr Dobbs Small-C compiler that ran on Intel 8080, and ported the code generator to 6809 (and a few other Motorola based 8-bit CPUs). After getting deep into how a compiler worked, I was a much better C programmer! I wrote a tutorial manual for the language and sold a few copies of my compiler as Silicon Fen Software Small-C.\n\nOne day in 1985 or so Sun founder Bill Joy was in town to do a talk at Cambridge University, and dropped by to meet us. This had a big impression on me, as it was clear he was living at least five years in the future, compared to what we knew about, and that was an inspiration. The future is here, but it\u2019s unevenly distributed as William Gibson once said. Ever since then, I\u2019ve tried to make it my job to look for signs of emerging technology trends and act as an early adopter, to figure them out. One of those trends was parallel and multi-threaded processing, and I led our early use of the Inmos Transputer and Occam language on some projects, including spinning off a startup called Niche Technology so that we could do development work for them. I built a platform in Occam that ran existing FORTRAN code on a 64-node Transputer board that plugged into a deskside Sun workstation to give it about 20 times speedup. Follow-on funding didn\u2019t materialize, so it never made it to market.\n\nI spent six years at Cambridge Consultants, building some interesting systems, and learning a lot, but by then Sun had opened a sales office across the street, and I wanted to find out what they were going to release next, before everyone else. So in 1988 I joined the Sun sales force and became a Systems Engineer (nowadays we would say Solutions Architect).\n\nLink to What Adrian Did Next\u200a\u2014\u200aPart 2\u200a\u2014\u200aSun Microsystems.",
        "metadata": {
          "word_count": 910,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian Cockcroft has had a long career working at the leading edge of technology. About every 5\u20137 years the thing he was working on that was going to happen next matured and went mainstream, and something new and interesting took its place. In 1972 I taught myself BASIC, then Algol, and remember playing lunar lander. My main academic interest was physics, rather than computing. My non-academic distraction was playing bass guitar, which reached a peak in 1982, gigging regularly with Black Tiger. In 1984 we were one of the first customers of Sun Microsystems. The full time admin only looked after our VAX/VMS systems. I was installing, upgrading, and doing backups, as well as writing code on projects. In my spare time I built a home-brew computer called a Microbox II, designed by my friend Dave Rumball, soldering all the components onto the board. It had a 6809 CPU and ran an operating system called Flex9. I got hold of the code for the Dr Dobbs Small-C compiler that ran on Intel 8080, and ported the code generator to 6809. I was looking for emerging trends in computing. One of those was parallel and multi-threaded processing. I built a platform that ran existing FORTRAN code on a 64-node Transputer board. Follow-on funding didn't materialize."
      },
      "tags": [
        "devops",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_05e0025c",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Free Communication is Disrupting Hierarchies",
      "source": "Medium Archive",
      "published_date": "2013-11-24",
      "url": "https://medium.com/@adrianco/2013-11-24_Free-Communication-is-Disrupting-Hierarchies-bd182f5dcd39",
      "content": {
        "text": "Free Communication is Disrupting Hierarchies\n\nI first wrote this post in 2013 on blogger.com and an update with some minor edits felt like a good way to get started at Medium. Part of the inspiration came from this Jeff Jarvis blog quote:\nI doubt the net\u2019s creators realized how subversive it was to connect anyone to anyone, bypassing the institutions that mediated those connections: from media to government, universities to retailers. These institutions are now circling wagons to protect their prerogatives: copyright for media, secrecy for government.\nSomewhat related discussions took place at the 2013 Defrag conference and on blogs talking about corporate organizations and government surveillance.\n\nHowever this is nothing new, over centuries there has been a gradual reduction in the cost of communication between people, which has caused social changes, disrupted old industries, created new industries and led to political revolutions. The end point that we have reached in the last few years is that the cost of communication is now zero. There is no incremental cost for me to send a tweet that is delivered to thousands of people a few seconds later. If the tweet contains novel ideas or information that people want to share, then in a few seconds, millions of people could see it. With about 25,000 direct followers my own twitter mention reach is sometimes in the millions.\n\nWhen I see an industry or organization that was built on assumptions that are no longer true, I expect to see disruption follow, and opportunities to create new industries and organizations. The assumption that communication is expensive is baked into our world. The new industries and organizations that take advantage of this change are emerging now.\n\nTo see how subversive this is, we need to look at some history. Every step in the reduction of communication cost has enabled changes that have been resisted by the hierarchy, led to revolutions, and in most cases have been co-opted and controlled by the elite.\n\nThe ability to control, coordinate and influence a large group of people depends upon effective communication. Early examples include the creation of sea-going ships and roads to sustain empires, from the Romans to the British Empire as examples. The leaders of these empires could gather information via messengers and send out decrees to manage at a distance, by using a coordinated hierarchical communication network. The cost of communication for the workers was prohibitive, so all information trickled down through the local authorities, whether civil, military or religious. The only low cost communication took place within the local village.\n\nMost workers were illiterate, and taking England in the middle ages as an example, a series of disruptions were caused by the invention of the printing press. The loss of control that resulted caused major problems for authorities, and allowed ideas to spread in new ways.\n\nAs cities got bigger, local communication got faster, newsletters and newspapers started to spread ideas more quickly and the creation of postal services brought the cost of peer to peer communication down far enough that everyone could communicate over larger distances. Governments co-opted these new mechanisms, regulating the news, running the postal services. The industrial revolution created commercial entities with their own hierarchies of control and communication. The ability of ideas to spread rapidly and reach many people allowed groups to self-organize, leading to political upheaval. Revolution was enabled by the ability to spread ideas outside the incumbent ruler\u2019s hierarchy of communication and control.\n\nToday we live in societies where effectively everyone is literate, connected and mobile. However we still have the same hierarchical institutions that were developed hundreds of years ago. Governments to manage the hierarchies of society, Companies to manage the hierarchies of commerce, Religions to manage the hierarchies of belief, Universities to manage the hierarchies of education.\n\nThese hierarchies still have value, they continue to represent ownership and have responsibilities but they no longer have control. They were sustained in the past by imposing their will on people, and carefully managed what information people are exposed to. The hierarchies are now very large and deeply nested, so rather than a small ruling elite and a large peasant population, there are very many levels in the middle, which makes many of us have a vested interest in the continuation of parts of the hierarchies.\n\nWe are now all connected by a series of relatively flat peer to peer networks. Facebook for a curated network of family, friends and interest groups, twitter for real-time news and a much more dense ad-hoc interconnection of interests, email for direct communication, and many other services. The \u201cArab Spring\u201d, the \u201cOccupy Movement\u201d and other recent political revolutions were able to find and coordinate large numbers of people extremely rapidly (hours or days) without forming a hierarchy first.\n\nThe new world is a blend of hierarchies with many peer to peer links that reach across and between the old hierarchies. The problem for the elite leaders is that they are used to having control, owning the flow of information and managing deeply nested organizations. In this new world, the leaders have ownership, responsibility, influence but much less control, they control and manage a small part of the flow of information, and their organizations are flatter and less well defined. At the extreme there are organizations like Github, which are figuring out how to make an emergent self organizing structure work. To scale a flat organization there needs to be a very strong sense of culture and unity of purpose. I think Netflix has found a productive balance between a hierarchy of responsibility for aspects of the business and freedom at each level to self organize the delivery of what the business needs. Broad and open communication across the business, and many skip-level one-on-one meetings flattens the organization and minimizes \u201cpolitics\u201d.\n\nLooking beyond business, Religion only really works if you don\u2019t think too hard about any other variant of religion. Religions all claim to be true, but claim different incompatible and incoherent things, so that exposure to ideas such as the Outsider Test for Faith causes deep problems for the faithful. Religion in general is being disrupted by science and free communication, and polarization into atheism and reactionary fundamentalism is one of the outcomes in society.\n\nEducation is based on the communication of skills and information from teachers to students, and the big institutions of education are being disrupted. They no longer control the flow of education, and the costs are increasing beyond the ability of the students to repay them, so companies like Coursera and the Khan Academy are building alternatives. For software developers, the body of code you own and contributed to on github matters far more than where (or even if) you studied. The state of the art isn\u2019t being created and taught in University research departments any more, the commercial ecosystems that form around technologies are where the bleeding edge of innovation is being disseminated to new recruits.\n\nThese disruptions are on the way, but change tends to occur rapidly when a tipping point is reached rather than gradually. A point made by Nicholas Taleb in his discussion of Black Swan events, is that its easier to point out that a change could happen and plan for what to do next, than to predict when exactly it will come. So like a cartoon Wily Coyote who has run off a cliff, we ignore the change until we look down and realize that nothing is holding us up.\n\nOriginally published at perfcap.blogspot.com on November 24, 2013.",
        "metadata": {
          "word_count": 1255,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Free Communication is Disrupting Hierarchies is a series of posts on Medium.com. The series looks at the ways in which the internet is changing the way people communicate. The assumption that communication is expensive is baked into our world. The new industries and organizations that take advantage of this change are emerging now. Every step in the reduction of communication cost has enabled changes that have led to revolutions. Most workers were illiterate, and taking England in the middle ages as an example. The cost of communication for the workers was prohibitive, so all information trickled down through the local authorities. The industrial revolution created commercial entities with their own hierarchies of control and communication. The ability of ideas to spread rapidly and reach many people allowed groups to self-organize, leading to political upheaval. We are now all connected by a series of relatively flat peer to peer networks. The hierarchies are now very large and deeply nested. The \u201cArab Spring\u201d, the \u201cOccupy Movement\u2019 and other recent political revolutions were able to find and coordinate large numbers of people extremely rapidly. The problem for elite leaders is that they are used to having control, owning the flow of information and managing deeply nested organizations. In this new world, the leaders have ownership, responsibility, influence but much less control. Broad and open communication across the business, and many skip-level one-on-one meetings flattens the organization. Education is based on the communication of skills and information from teachers to students. Coursera and the Khan Academy are building alternatives. For software developers, the body of code you own and contributed to on GitHub matters far more than where (or even if) you studied. These disruptions are on the way, but change tends to occur rapidly when a tipping point is reached rather than gradually. A point made by Nicholas Taleb in his discussion of Black Swan events, is that its easier to point out that a change could happen and plan for what to do next, than to predict when exactly it will come."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_13686cec",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Cloud Trends Where have we come from and where are we headed",
      "source": "Medium Archive",
      "published_date": "2016-11-23",
      "url": "https://medium.com/@adrianco/2016-11-23_Cloud-Trends---Where-have-we-come-from-and-where-are-we-headed-3d7e5e756d16",
      "content": {
        "text": "Cloud Trends\u200a\u2014\u200aWhere have we come from and where are we\u00a0headed\n\nI gave several talks over the last few years on this subject, mostly at the Structure conference. Now that I\u2019ve joined @AWScloud my perspective shifts somewhat, but the same trends are playing out.\n\nThe cloud ecosystem continues to mature, and customers are looking for staying power, as small vendors and small clouds from some big vendors fade away and close down. The ability to support and migrate enterprise workloads is critical for winning the biggest new cloud deals. Extremely scalable cloud capacity is critical to provide room for the largest web scale customers to keep growing. Adding new regions around the world is critical to provide local jurisdiction support and low network latency.\n\nIn 2014 we saw many enterprises sign up for AWS, start proof of concept tests and launch green-field applications. In 2015 larger scale migrations started, and plans were made for entire datacenters to be replaced by public cloud accounts. In 2016 these changes moved from early adopter markets such as media and retail, and started to take root in finance, as banks, insurance companies and their regulators figured out how to run and audit public cloud applications. Next up: early adopters in the energy, transport, government, manufacturing and healthcare markets are leading the way to cloud.\n\nWhat do modern applications look like? We are seeing the combination of rapid cloud based provisioning, a DevOps culture transformation, and the journey from waterfall through agile to continuous delivery product development processes give rise to a new application architecture pattern called microservices. This shares the same principles as the service oriented architecture movement from 10\u201315 years ago, but in those days, machines and networks were far slower, and XML/SOAP messaging standards were inefficient. The high latency and low messaging rates meant that applications ended up composed of relatively few large complex services. With much faster hardware and more efficient messaging formats, we have low latency and high messaging rates. This makes it practical to compose applications of many simple single function microservices, independently developed and continuously deployed by cloud native automation.\n\nWhat does modern hardware look like? The capacity of individual physical systems continues to increase, and is far larger than most applications require. As I write this the largest single instance type on AWS is the x1.32xlarge with almost two Terabytes of RAM, four Terabytes of solid state disk, 128 vCPUs, and a 20GBit network interface. The most powerful p2.16xlarge instance type has raw performance of 70 Teraflops from about 40,000 GPU cores. Systems are sliced up using virtual machines to make appropriately sized instances that can be provisioned and boot an operating system in a few minutes.\n\nA few minutes to get an instance used to feel amazingly fast, but now we use containers to get applications running in a few seconds, and to pack more small applications efficiently into large instances. The early cloud native architectures such as @NetflixOSS used an instance to host each microservice, but many people are now moving to use more lightweight Docker containers to host each microservice.\n\nWe\u2019ve shrunk the size of microservices so that they each perform a single function, one of many that make up an application. There are plenty of situations where individual microservices sit idle most of the time, but need to be ready to respond quickly when something happens, and potentially scale up to handle a lot of requests in a burst. It\u2019s inefficient to have lots of idle containers, or to provision them when a request arrives. To meet this need AWS Lambda was launched two years ago, and has helped to create a serverless or function as a service (FaaS) programming model that is emerging as a new pattern for cloud application development. By optimizing for rapid launch of a single shot invocation, there is no need to charge when the function isn\u2019t running. By starting exactly as many functions as are needed to process incoming events there is no need to provision extra headroom or do capacity planning. For appropriate workloads AWS Lambda is simpler to operate and a small fraction of the cost of a set of permanently running microservices.\n\nAs we look forward into 2017, there is growing interest in serverless architectures and an ecosystem is developing around tooling to build, monitor and operate serverless applications. However AWS Lambda is more than just an application architecture, it has it\u2019s roots in S3, where it provides methods that are triggered by actions on the object store. Lambda functions can be attached to an increasing number of AWS services, and there are exciting possibilities for event driven automation of cloud infrastructure and services.\n\nAs the rate of change in technology increases there are the usual worries about skills shortages. Part of the answer is the democratization of access to technology. Some of the most advanced technology available in areas like big data, machine learning and cloud exists now as a combination of web services and open source software packages. Just a few years ago, to have access to leading edge technology you would have had to be at a top university or industry research lab and have a large budget and skilled staff to operate the systems. Today, services and infrastructure are available by the hour, software can be downloaded for free, and there is a huge amount of online information to help you learn and keep up to date. One challenge I\u2019m particularly interested in is how to find undeveloped talent, and connect it with the right opportunities to learn. A project manager, left behind by the journey from waterfall based projects to continuous delivery of products, can discover and develop latent talents as a data scientist. Schoolgirls can build mobile applications and could even leverage machine learning with a serverless cloud back-end. Unemployed workers can retrain on the latest technologies, and build up a reputation by entering coding contests and contributing to open source projects, as a pathway to new opportunities.\n\nI will be attending AWS re:Invent, and I\u2019m presenting at 3:30pm on Thursday in the architecture track - ARC213 Open Source at AWS\u200a\u2014\u200aContributions, Support and Engagement. More on that topic in future posts.\n\nParaphrasing William Gibson \u201cThe future is already here\u200a\u2014\u200aand now it\u2019s globally distributed.\u201d",
        "metadata": {
          "word_count": 1051,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In 2014 we saw many enterprises sign up for AWS, start proof of concept tests and launch green-field applications. In 2015 larger scale migrations started, and plans were made for entire datacenters to be replaced by public cloud accounts. Extremely scalable cloud capacity is critical to provide room for the largest web scale customers to keep growing. Microservices is a new application architecture pattern called microservices. It shares the same principles as the service oriented architecture movement from 10\u201315 years ago. The high latency and low messaging rates meant that applications ended up composed of relatively few large complex services. A few minutes to get an instance used to feel amazingly fast, but now we use containers to get applications running in a few seconds. The capacity of individual physical systems continues to increase, and is far larger than most applications require.  AWS Lambda was launched two years ago, and has helped to create a serverless or function as a service (FaaS) programming model. By optimizing for rapid launch of a single shot invocation, there is no need to charge when the function isn't running. For appropriate workloads AWS Lambda is simpler to operate and a small fraction of the cost of a set of permanently running microservices. As many functions as are needed to process incoming events there is no need to provision extra headroom or do capacity planning. Just a few years ago, to have access to leading edge technology you would have had to be at a top university or industry research lab. Today, services and infrastructure are available by the hour, software can be downloaded for free, and there is a huge amount of online information. I will be attending AWS re:Invent, and I\u2019m presenting at 3:30pm on Thursday in the architecture track. More on that topic in future posts. g coding contests and contributing to open source projects as a pathway to new opportunities."
      },
      "tags": [
        "agile",
        "architecture",
        "automation",
        "aws",
        "big data",
        "cloud",
        "cloud native",
        "containers",
        "devops",
        "docker",
        "infrastructure",
        "machine learning",
        "microservices",
        "performance",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_file_698f2ebd",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thoughts from 10 Years Ago Disruptive Innovation in the Evolution of Moving Pictures",
      "source": "Medium Archive",
      "published_date": "2016-06-06",
      "url": "https://medium.com/@adrianco/2016-06-06_Thoughts-from-10-Years-Ago--Disruptive-Innovation-in-the-Evolution-of-Moving-Pictures-2f2d91c94b68",
      "content": {
        "text": "Thoughts from 10 Years Ago: Disruptive Innovation in the Evolution of Moving\u00a0Pictures\n\n[This was written a few years before I joined Netflix, and before they launched streaming]\n\nLets look at the history of movies. The initial technology to capture and replay moving pictures was developed around 100 years ago, and the initial competition between inventors went through its first transition when movie theaters became established and began to settle on a standard form of projector. The inventors who had alternative camera/recording/projector technology died out. Consumers wanted to go see movies and the movie industry formed to provide content for that market.\n\nThe next innovation was to be able to watch movies at home on film, then there were movies on TV. The movie theaters had far bigger screens, better sound and color but the technology at home gradually caught up in features and reduced in cost, and a market transition to home viewing occurred. The total market size for equipment bought to watch movies at home is huge. Its important to note that the primary vendors in each phase of the market are different. The movie theater business is very different to the home video equipment supplier business. The early battles in the home were over the standard formats, famously Betamax failed to win over VHS for video tape, and there are continuing battles over DVD formats, but Sony is a dominant brand name in a crowded market for home video equipment.\n\nThe next innovation was video rental, and Blockbuster ended up as a major player in this market, with presence on every high street. However, that presence became unnecessary as Netflix shipped DVD\u2019s directly to consumers and took over a large share of the market.\n\nFinally, video is available directly over the internet\u00a0, its being viewed on PC\u2019s rather than TV sets, anyone can create and upload it, and YouTube is this year\u2019s hot market leading name in this space for all kinds of short videos. Its also trivially easy to take a full length movie or TV program and share it using one of the many BitTorrent services, and a growing proportion of movies are being watched for free, to the consternation of the movie industry.\n\nThe PirateBay site in Sweden was shut down and charged with copyright violation, but it appears that a significant proportion of the population of Sweden were users and they got upset as they had got used to exchanging content for free. After three days the site came back up, hosted in Holland, and with even more users due to the publicity.\n\nUnlike YouTube, BitTorrent sites such as PirateBay don\u2019t host the actual content, they just connect individual users who exchange content, they don\u2019t need to provide storage or bandwidth, just a searchable database of small index files that configure the BitTorrent transfer between a large number of seeders that have some or all of the file already, and leechers who want to get the file, and who can in turn become seeders.\n\nThe publicity gained as a side effect of trying to shut down the PirateBay site may even have the opposite effect of cementing the PirateBay brand as a market leader and accelerating growth in this space.\n\nEvery step in this history involves a disruptive innovation. There is a fundamental reduction in cost, offset by a large increase in unit volume, which has often increased the overall revenue using a new way to monetize the market for moving pictures. Each time the previous market leader is left behind (often kicking and screaming) as the new larger market emerges. Each time a new brand captures the attention span and trust of the consumer, and dominates the market.\n\n[The cost of bandwidth eventually dropped far enough that Netflix was able to emerge as the leader in subscription streaming services.]\n\nHere I\u2019ll take a more abstract view of a maturing market as each phase evolves, and refer to the development of in-home movie watching as an example.\n\nAn emerging market is characterized by competition on the basis of technology. Early adopters like to play with new technology and are able to cope with its issues. Many different products are competing for market share on the basis of \u201cmy features are better\u201d. Think of the early days of the VCR, with VHS vs. Betamax. In a mature market, few people worry about features, most VCR or DVD players have the same feature set and very good picture quality at a very low price. If you want to be sure you get a good one, you are most likely to buy using brand name (e.g. Sony) rather than poring over detailed specifications. Margins are low, but volume is high and margins can be better if you won the brand battle.\n\nThe next phase in the market is characterized by competition on the basis of service. Think of the video rental store as a service. You visit the store and pay rental according to how much you use the service. As an emerging service, anyone could setup to rent videos and DVDs. As the market matured, larger stores with a bigger selection and more centralized buying power provided a better service, and video rental chains such as Blockbuster took over the market. Again, the power of a dominant brand became the primary differentiator as the service market matured.\n\nThe third phase in the market is the evolution of a service into a utility. A utility provides a more centralized set of resources, and a regular subscription or monthly bill. It can provide similar services, but in a more automated manner. Netflix is my example of a utility based DVD provider service. You pay a monthly fee which encourages steady consumption, and Netflix have automated the recommendation system, which replaces asking the counter clerk in a video rental store for advice. The recommendations are the result of many peoples opinions, so are likely to be less biased and better informed, but the most important difference in the utility approach is that it doesn\u2019t need people to provide the service directly to the customer. This makes it fundamentally cheaper. Many traditional services were transformed into utilities by the arrival of the Internet, which allows consumers to access information based utilities in a generic and efficient manner. The network effect benefit of having a large user base also causes dominant brand names to emerge. Netflix leads mindshare in this space, despite attempts by Blockbuster to copy their business model, Netflix can grow faster with fewer people as a pure utility.\n\nThe final phase in the evolution of a market occurs as the cost of replication and distribution of the product approaches zero. For digital content the end customer already has a computer and an Internet connection. There is no additional cost to use it to download a movie. A central utility such as YouTube can use a mixture of advertising and premium services (for a minority of power users) to offset their own costs. Peer to peer systems distribute the load so that there is no central site and no incremental cost in the system. The only service that is needed is some kind of search, so that peers can find each other\u2019s content to exchange it. PirateBay is primarily a search engine, and search engines become dominant when the brand gets well known, and they find what you are looking for because they have a comprehensive index.\n\nSo the evolution of a marketplace goes from competing on the basis of technology, to competing on service, to competing as a utility, to competing for free. In each step of the evolution, competitors shake out over time and a dominant brand emerges.\n\nTo use this as a maturity model, take a market and figure out whether the primary competition is on the basis of technology, service, utility or search, and consider whether a dominant brand has emerged in that phase. The model should then indicate what the next step is likely to be, so you can try to find the right disruptive innovation to get you there. Good luck!\n\nOriginally published at perfcap.blogspot.com\u00a0.",
        "metadata": {
          "word_count": 1355,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Technology to capture and replay moving pictures was developed around 100 years ago. Consumers wanted to go see movies and the movie industry formed to provide content for that market. The next innovation was to be able to watch movies at home on film, then there were movies on TV. The movie theater business is very different to the home video equipment supplier business. The early battles in the home were over the standard formats, famously Betamax failed to win over VHS for video tape. The next innovation was video rental, and Blockbuster ended up as a major player in this market. The PirateBay site in Sweden was shut down and charged with copyright violation. After three days the site came back up, hosted in Holland, and with even more users due to the publicity. Every step in this history involves a disruptive innovation. Each time the previous market leader is left behind. Early adopters like to play with new technology and are able to cope with its costs. In a mature market, few people worry about features, most VCR or DVD players have the same feature set and very good picture quality at a very low price. If you want to be sure you get a good one, you are most likely to buy using brand name. The third phase in the market is the evolution of a service into a utility. A utility provides a more centralized set of resources, and a regular subscription or monthly bill. Netflix is my example of a utility based DVD provider service. The final phase in the evolution of a market occurs as the cost of replication and distribution of the product approaches zero. For digital content the end customer already has a computer and an Internet connection. Netflix leads mindshare in this space, despite attempts by Blockbuster to copy their business model. In each step of the evolution, competitors shake out over time and a dominant brand emerges. The model should indicate what the next step is likely to be, so you can try to find the right disruptive innovation."
      },
      "tags": [
        "netflix",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_0b4797d4",
      "kind": "file",
      "subkind": "medium_post",
      "title": "I found this recent book on brain research related to dreaming to be useful insight",
      "source": "Medium Archive",
      "published_date": "2023-11-12",
      "url": "https://medium.com/@adrianco/2023-11-12_I-found-this-recent-book-on-brain-research-related-to-dreaming-to-be-useful-insight--a6c5145d66a4",
      "content": {
        "text": "I found this recent book on brain research related to dreaming to be useful insight. To understand consciousness I think we can start by seeing what goes away during un-consciousness. https://a.co/bdFYMtF",
        "metadata": {
          "word_count": 31,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "To understand consciousness we can start by seeing what goes away during un-consciousness. I found this recent book on brain research related to dreaming to be useful insight."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5828e75c",
      "kind": "file",
      "subkind": "medium_post",
      "title": "After a month or two of living with the Noon system it s getting positive reviews from the",
      "source": "Medium Archive",
      "published_date": "2018-02-05",
      "url": "https://medium.com/@adrianco/2018-02-05_After-a-month-or-two-of-living-with-the-Noon-system--it-s-getting-positive-reviews-from-the--65c2100767dc",
      "content": {
        "text": "After a month or two of living with the Noon system, it\u2019s getting positive reviews from the \u201cnormal\u201d people and visitors in the house who just want to have lights that work well. It\u2019s easy to understand what it does and use it. Next step is to add a second room, so I\u2019m planning the master bedroom suite installation.\n\nOne glitch so far, after a month or two the director lost it\u2019s network connection and all the switches reverted to just controlling their own lights. The fix was to pop the director out of its socket and toggle the switch on it\u2019s back off-on, that brought it back online to WiFi/cloud/iPhone app, and to communicating with the extensions to manage the room. A bug that they fixed in an update.\n\nUpdate: I also installed a Noon system in the master bedroom suite, another director, eight extensions and two LIFX bulbs (wifi controlled) for the bedside lamps. It all works very nicely. After nine months with the main system, and six with the bedroom, we like it a lot. Has anyone else tried it?\n\nI\u2019ve also been looking at smoke alarms, we have nine wired+battery units that are a decade old and need to be replaced. I looked at the connected units like the Nest Protect, and they are very expensive and still suffer from false alarms. There are some bad reviews for just about every model I looked at. In the end I\u2019ve decided to go for a simple wired+battery First Alert unit that costs about $15 and fit them all with Roost connected batteries. These have a 9V PP3 format with a 5+ year life replaceable battery and a wifi connection. They will alert the mobile app indicating which named alarm sees smoke or has a low battery.",
        "metadata": {
          "word_count": 299,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Noon system is getting positive reviews from the \u2018normal\u2019 people and visitors in the house. Next step is to add a second room, so I\u2019m planning the master bedroom suite installation. We have nine wired+battery units that are a decade old and need to be replaced. These have a 9V PP3 format with a 5+ year life replaceable battery and a wifi connection. They will alert the mobile app indicating which named alarm sees smoke or has a low battery."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3824b0d3",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Proposal for a Realtime Carbon Footprint Standard",
      "source": "Medium Archive",
      "published_date": "2023-04-05",
      "url": "https://medium.com/@adrianco/2023-04-05_Proposal-for-a-Realtime-Carbon-Footprint-Standard-60b71c269948",
      "content": {
        "text": "Proposal for a Realtime Carbon Footprint Standard\n\nby Adrian Cockcroft\nThree windows lined up to let the light in through a door in the Madrassa, Marrakech\u200a\u2014\u200aPicture by\u00a0Adrian\nThe existing approaches to carbon measurement provide delayed data for carbon accounting that is more akin to billing information in the way it is structured and queried. This proposal seeks to define a standard for real-time carbon and energy data as time-series data that would be accessed alongside and synchronized with the existing throughput, utilization and latency metrics that are provided for the components and applications in computing environments.\n\nThis would open up opportunities to leverage existing performance monitoring tools and dashboards, extend optimization tools like autoscalers and schedulers, and to build new kinds of carbon analysis and optimization tools and services.\n\nThe challenge is that accurate data isn\u2019t available immediately, and cloud providers currently only provide monthly carbon data, with several months lag. This isn\u2019t useful for workload optimization so proxy metrics like utilization and cost are being substituted. The open source Cloud Carbon Footprint Tool takes billing data (which is available monthly or hourly) as it\u2019s input, along with estimated carbon intensity factors, and can produce workload carbon estimates, but not in real time.\n\nCarbon measurements are by their nature imprecise and based on estimates and models that include variation. The usual technique for modeling fuzzy systems is Monte Carlo analysis, which computes with distributions rather than precise values. This standard proposes to represent an imprecise metric in a way that can be obtained from or input to a Monte Carlo model, by reporting three values: a most likely value, and a 95% confidence interval above and below it, effectively saying that there\u2019s one chance in 20 that the value is outside the confidence interval, and the limits give separate guidance for how bad could it be, and how good could it be. In cases where a lot is known, and there isn\u2019t much variation, the interval will be narrower than in cases where missing information is being estimated or power sources like wind and solar, that are highly variable in nature, are a significant part of the mix.\n\nThe quality of carbon intensity information improves over time, for energy consumption a predicted grid mix for the next 15 minutes becomes an actual grid mix for the past hour, and an audited grid mix in the energy providers billing records a month or so later. For market based scope 2 measurements the grid mix can change for up to a year as Renewable Energy Credits are traded. Cloud providers don\u2019t disclose their own REC and Private Purchase Agreements, but these are taken into account in their monthly reports, with a few months delay. Scope 3 calculations are revised as more comprehensive supply chain data and better Life Cycle Analysis models are obtained. Given this background, for a given amount of energy reported at a point in time, the carbon footprint of that energy can be re-calculated as new information arrives, narrowing the confidence interval.\n\nEnergy data is also modeled and could be reported with confidence intervals, but it\u2019s easier to measure locally in real-time, and less likely to be revised over time, so it seems plausible to report energy as a single fixed value.\n\nEnergy data can be obtained from power delivery systems in datacenters or from intelligent plugs or power strips. At the instance level, many CPU and system architectures have direct access to power consumption metrics (but this is often blocked for security reasons by cloud providers, as discussed below). In a time series reporting schema this should be measured as an energy difference for the time interval, which would be more accurate than sampling an instantaneous power level at a point in time.\n\nEnergy data at the entire machine or raw cloud instance level needs to be apportioned to virtual machines or cloud instances which run an operating system, and to pods, containers and processes running applications and background activities that consume CPU, memory and I/O resources. Additional attached devices such as backlit laptop or mobile device screens, fans and batteries should be taken into account. The power consumed by CPUs is dynamic, and increases as they work harder, and decreases if they need to recover from overheating. Clock rates also change, some ARM architectures include a mixture of high performance and low power cores in the same CPU, and Intel architecture CPUs have hyperthreading, so the performance of an execution thread will vary over time. I published a paper and presentation in 2006 that discusses the complexities of measuring CPU utilization. The apportionment model should be identified in the collected data and be a pluggable module. The model needs to apportion the energy in the current interval (say 100 Watts for one minute, 6000 joules) to the activities and memory usage of interest on the machine in that interval\u200a\u2014\u200aidle time, background processing by operating system daemons, installed agents activity for security, performance and management (including itself if it\u2019s running on the machine), and process, container or pod level activity that is running as application workloads. The information required to do a simple CPU utilization based allocation algorithm should already be available in the system monitoring instrumentation, but a dedicated agent would be more accurate by tracking process level activity and memory usage. I built a detailed process monitoring agent like this for Solaris about 25 years ago, and while it may sound complicated the overhead of this kind of analysis isn\u2019t likely to be an issue. The Kepler project has developed what looks like a good approach to this, and I\u2019ve started a discussion there to explore what would be needed https://github.com/sustainable-computing-io/kepler/discussions/600.\n\nThe Real Time Carbon Footprint calculation would update at the same interval rate as the resource utilization data, normally once per minute. The carbon intensity estimate would change less often, maybe every 15 minutes to an hour. The energy measurement data would ideally update every minute, but if it\u2019s less often it would be averaged across the resource metric intervals. The output would be a time series in OpenTSDB format that consists of a timestamp, a value and a set of key/value pairs that categorize the data. This format could be coded as a Prometheus Exporter.\n\nThe above discussion focuses on Scope 2, energy consumption. However carbon emissions from fuel used by backup generators, Scope 1, also needs to be apportioned. It could be reported in real time, and during a power failure event that information could be used by a scheduler to send work to a different datacenter or zone, but it\u2019s more likely to be provided as a historical average that changes every month. It\u2019s also observed to be a small component of the carbon footprint, and is relatively easy to reduce to near zero by using zero carbon fuel along with larger battery backup capacity, so it\u2019s likely not worth adding much complexity for. The realtime feed should contain the historical average, and this could be updated with more accurate data, along with other carbon updates, a month or so later by reprocessing the time series.\n\nSecurity is always an issue when power is being measured. There are a number of key attacks and noisy neighbor measurements that are enabled by power signature analysis techniques. For this reason cloud vendors block access to power measurement interfaces by tenants applications. The underlying data is available to the cloud vendors themselves, so they would need to supply a one minute, per instance, energy use value as an AWS CloudWatch metric or the equivalent for other cloud providers. At the one minute granularity security concerns are minimized. In the absence of measured data, a model calibrated on datacenter systems could be used, taking into account CPU types, power usage efficiency (PUE), and over-all utilization levels. The energy model in use should be reported as a key/value in the data feed.\n\nThere is additional overhead for a cloud instance that needs to be accounted for beyond the CPU and memory footprint of some code. There\u2019s a network traffic driven allocation of the energy use of the network switches that connect computers. There are also control plane overheads, shared storage volumes and supporting services to instrument. The cloud providers themselves need this data so they can measure, optimize and report the energy and carbon usage of their higher level services.\n\nA sample OpenTSDB set of Datapoints for a single Kubernetes pod could look something like:\nMetric: carbon.footprint.energyValue: 123.4Timestamp: Tue, 21 Mar 2023 17:47:26 -0700Tags:unit=joulesmodel=measured.v1 (or sampled, or estimated, or simulated)pod=node=namespace=instance=project=account=region=us-east-1aMetric: carbon.footprint.scope2Value: 123.4Timestamp: Tue, 21 Mar 2023 17:47:26 -0700Tags:unit=gramsmodel=local (or marketplace)pod=node=namespace=instance=project=account=region=us-east-1aMetric: carbon.footprint.scope2upperValue: 234.5Timestamp: Tue, 21 Mar 2023 17:47:26 -0700Tags:unit=gramsmodel=local (or marketplace)pod=node=namespace=instance=project=account=region=us-east-1aMetric: carbon.footprint.scope2lowerValue: 45.6Timestamp: Tue, 21 Mar 2023 17:47:26 -0700Tags:unit=gramsmodel=local (or marketplace)pod=node=namespace=instance=project=account=region=us-east-1a\nI launched this proposal via a discussion at the Green Software Foundation and shared it as part of my QCon London talk on Cloud Provider Sustainability. I\u2019m on vacation in Europe at the moment, but will be devoting more time to pushing this forward from mid-April onwards.",
        "metadata": {
          "word_count": 1505,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The existing approaches to carbon measurement provide delayed data for carbon accounting that is more akin to billing information. This proposal seeks to define a standard for real-time carbon and energy data as time-series data. Carbon measurements are by their nature imprecise and based on estimates and models that include variation. This isn\u2019t useful for workload optimization so proxy metrics like utilization and cost are being substituted. The open source Cloud Carbon Footprint Tool takes billing data (which is available monthly or hourly) as it\u2019s input. For energy consumption a predicted grid mix for the next 15 minutes becomes an actual grid mix. For market based scope 2 measurements the grid mix can change for up to a year as Renewable Energy Credits are traded. Scope 3 calculations are revised as more comprehensive supply chain data and better Life Cycle Analysis models are obtained. Energy data can be obtained from power delivery systems in datacenters or from intelligent plugs or power strips. At the instance level, many CPU and system architectures have direct access to power consumption metrics. Power consumed by CPUs is dynamic, and increases as they work harder, and decreases if they need to recover from overheating. Additional attached devices such as backlit laptop or mobile device screens, fans and batteries should be taken into account. The apportionment model should be identified in the collected data and be a pluggable module. The information required to do a simple CPU utilization based allocation algorithm should already be available in the system monitoring instrumentation. A dedicated agent would be more accurate by tracking process level activity and memory usage. I built a detailed process monitoring agent for Solaris about 25 years ago. Energy measurement data would ideally update every minute, but if it\u2019s less often it would be averaged across the resource metric intervals. The output would be a time series in OpenTSDB format that consists of a timestamp, a value and a set of key/value pairs that categorize the data. This format could be coded as a Prometheus Exporter. Security is always an issue when power is being measured. There are a number of key attacks and noisy neighbor measurements that are enabled by power signature analysis techniques. The underlying data is available to the cloud vendors themselves. There is additional overhead for a cloud instance that needs to be accounted for beyond the CPU and memory footprint of some code. There are also control plane overheads, shared storage volumes and supporting services to instrument. The cloud providers themselves need this data so they can measure, optimize and report the energy and carbon usage of their higher level services. I launched this proposal via a discussion at the Green Software Foundation and shared it as part of my QCon London talk on Cloud Provider Sustainability. I\u2019m on vacation in Europe at the moment, but will be devoting more time to pushing this forward from mid-April onwards."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "containers",
        "kubernetes",
        "metrics",
        "monitoring",
        "performance",
        "security",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_file_e819a691",
      "kind": "file",
      "subkind": "medium_post",
      "title": "The AWSopen team has been working on ways to help open source communities thrive for a few years",
      "source": "Medium Archive",
      "published_date": "2019-01-16",
      "url": "https://medium.com/@adrianco/2019-01-16_The--AWSopen-team-has-been-working-on-ways-to-help-open-source-communities-thrive-for-a-few-years--508f55043b2a",
      "content": {
        "text": "There are other, more creative ways AWS can help its ecosystem thrive if what it\u2019s after is developer mindshare. First of all, it can increase its contribution to open source (and actually showcase the work it does, something it hasn\u2019t be very good at doing so far). Secondly, it can invest in open source communities, by sponsoring events, providing infrastructure, funding travel, creating a diversity funds, etc.\nThe @AWSopen team has been working on ways to help open source communities thrive for a few years now. We\u2019ve created opensource.amazon.com and have well over 100 blog posts showcasing the work we do. We sponsor most of the open source events worldwide and provide AWS credits to some key projects. We are members of several open source related foundations. We\u2019ve also been increasing the amount of code contributions from Amazon across a wide variety of projects, and hiring more developers focused on open source contributions. Amazon also has several diversity programs that aren\u2019t focused exclusively on open source, but do help developer communities. The web site has a lot more detail and will be updated as we expand our work.",
        "metadata": {
          "word_count": 188,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The @AWSopen team has been working on ways to help open source communities thrive for a few years now. We sponsor most of the open source events worldwide and provide AWS credits to some key projects. We\u2019ve also been increasing the amount of code contributions from Amazon across a wide variety of projects. The site has a lot more detail and will be updated as we expand our work. It is not open source, but do help developer communities."
      },
      "tags": [
        "aws",
        "infrastructure"
      ]
    },
    {
      "id": "virtual_adrianco_file_c3fa4aa6",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What s a Distinguished Engineer",
      "source": "Medium Archive",
      "published_date": "2012-10-26",
      "url": "https://medium.com/@adrianco/2012-10-26_What-s-a-Distinguished-Engineer--6b3e4d320ce6",
      "content": {
        "text": "What\u2019s a Distinguished Engineer?\n\nA post by John Allspaw on what it means to be a senior engineer reminded me of something I put together years ago while I was a Distinguished Engineer at Sun Microsystems Inc. The question I had then from senior engineers looking at their career path was what did it take to become a Distinguished Engineer?\n\nI\u2019ve had some more recent discussions with executives who are trying to increase talent density by attracting and retaining the best engineers in their industry, and are trying to understand the different ways to put together a distinguished engineer or fellow program.\n\nAlthough Sun is no more, across the industry, there are engineers who are \u201cdistinguished\u201d and the title is used in a few places. At Sun, there were between 50 and 100 people in the role, who were mostly director level individual contributors, although there were also Sun Fellows who were VP level, and some were also line managers.\n\nI boiled it down into a few questions.\n\nFirst I made a list of the names of all the Sun Distinguished Engineers and Fellows, and the first question was \u201chow many of these names do you recognize, and know what they did\u201d. The intent is to get a baseline level of understanding of what might be expected. The list included people who invented software languages and frameworks that lots of people use, microprocessor architects, and fundamental researchers in security and networking. There were also CTOs of companies that Sun had acquired, and a few like me who mostly got in through writing books that everyone else had read.\n\nThe next question is \u201chow many of these people know who you are?\u201d. If you think you did do something special, we would expect that the existing Distinguished Engineers would have heard of it. Since at Sun the way to become a DE involved having the existing DE and Fellows vote for you, this was critical.\n\nThe final question was \u201chow many DE and Fellows are hanging around your cube on a regular basis waiting to talk to you?\u201d. This shows that you are the go-to person for something that matters.\n\nTranslating this into a broader context, more current questions for being distinguished might be \u201cDo the top conferences invite you to speak?\u201d, \u201cHow many of the other invited speakers and conference organizers do you know?\u201d and \u201chow many know you?\u201d. The other dimension of what you did to deserve it is nowadays a mixture of open source projects that lots of people use, or key ideas shared through books or blogs. It\u2019s important to have been a key contributor to delivering a product, but there is extra credit if that product became well known as leading an industry trend.\n\nHere\u2019s my original slide of Sun Microsystems Distinguished Engineers from 2002, how many of these names do you know, what did they do then and where are they now?\n\nI\u2019m also interested to hear about similar programs at other companies.\n\nOriginally published at perfcap.blogspot.com on October 26, 2012.",
        "metadata": {
          "word_count": 506,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The term \"distinguished engineer\" is used to refer to someone who has achieved a high level of achievement in their field. The term was used to describe a senior engineer at Sun Microsystems, which is no longer in existence. The list included people who invented software languages and frameworks that lots of people use. There were also CTOs of companies that Sun had acquired. The intent is to get a baseline level of understanding of what might be expected. Translating this into a broader context, more current questions for being distinguished might be \u201cDo the top conferences invite you to speak?\u2019, \u201cHow many of the other invited speakers and conference organizers do you know?\u201d and \u201chow many know you?\ufffd\u201d"
      },
      "tags": [
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_0266a231",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Platform Engineering Teams Done Right",
      "source": "Medium Archive",
      "published_date": "2023-02-10",
      "url": "https://medium.com/@adrianco/2023-02-10_Platform-Engineering-Teams-Done-Right--b3b3d4a8ad23",
      "content": {
        "text": "Platform Engineering Teams Done\u00a0Right\u2026\nPicture by Adrian\u200a\u2014\u200aKa\u2019anapali sunset\u00a02023\nThere\u2019s been a lot of discussion about platforms recently, I talked about why I think it\u2019s a current hot meme on the WTF Podcast with Charles Humble recently, and Sam Newman just wrote a blog post \u201cDon\u2019t call it a platform\u201d.\n\nWhat I want to cover in this blog post is how I think platforms should be structured, and why it\u2019s always plural, there isn\u2019t one platform or one platform team. We used this model effectively at Netflix when I was their cloud architect from 2010 through 2013.\n\nThere are three current underlying reasons for the platform engineering meme today. The first is that people have built complicated platforms on top of Kubernetes and there\u2019s a market for tools to help do it better. The second is that some companies with tools to sell are marketing the term. The third is that the Team Topologies book defined how to create and manage Platform Teams so there\u2019s interest in the terminology and definition.\n\nHowever the concept is an old one, and I want to explain the principles and a model that I think works well for how to architect platforms in general.\n\nThe first principle I have is that it isn\u2019t one platform, it\u2019s layers of platforms that need different specialized knowledge so its usually many platform teams.\n\nThe second principle is that the platform layers are dynamic, evolve over time and tend to move \u201cup the stack\u201d as they add functionality, and shed capabilities that are subsumed by lower level platforms.\n\nThe third principle is that the interface to a platform should be driven by the users of the platform. A platform team should include a product manager, (or the team lead should perform that function) have a roadmap, and have mechanisms for prioritizing incoming requests\n\nThe fourth principle is that a very clear distinction should be made between building internal platforms optimized to change quickly to meet specific business needs, and building externalized platforms optimized for long term stability, where you may not know who or what depends on the platform, and can\u2019t always ask them to change with you.\n\nThe layers of platforms start at the bottom with hardware choices such as which CPU architectures and vendors you want to use. The next layer is operating system platforms, what flavor of Linux, what version of Windows etc. The next layer is defined by the languages you want to use, Java, Python, Go,\u00a0, Javascript, Rust etc, and the ecosystem of library functions you bought from a vendor or downloaded. These three layers all require very different expertise and are likely to be owned and configured by different teams.\n\nThe virtualization and networking platform could be datacenter based, with something like VMware, or cloud based using one of the cloud providers such as AWS EC2. Above that there\u2019s a deployment platform such as Kubernetes or AWS Lambda. Back in the day, Netflix used the EC2 Autoscale Group as its primary deployment platform for \u201cbaked\u201d Amazon Machine Images. The introduction of containers brought everyone a similar but lighter weight mechanism.\n\nAt this point we start to run into a problem with my fourth principle. You always want new functionality but if you depend on an outside supplier, they have to make it into a long term stable part of their platform. When we started with Netflix migration to cloud, we looked at platforms like Rightscale and decided not to use them. The main reason was that we didn\u2019t want to pay for another supplier in the middle of our stack that would have incentives to maximize it\u2019s value by making their platform fatter and more functional. We wanted AWS to grow its platform functionality, and to continually eat away at the bottom of the thin layer of rapidly evolving Netflix specific platform code that we had built. We cultivated our relationship with AWS to get them to (slowly) invest in platform features we needed, and AWS has also maintained excellent backward compatibility and stability for its interfaces.\n\nIf you are running a Kubernetes based platform you need a platform team and a bunch of tools to make it work well. However I would be thoughtful about the additional vendors and layering that vendor supplied tools bring. Every additional vendor adds management complexity, unexpected interactions and failure modes along with the functionality they are offering, and will evolve at their own slower pace, not the pace you would like. Open source tools which have an open development model that allows your team to make contributions may be a better approach. Netflix made many contributions to open source projects by it\u2019s cloud platform team.\n\nIf you are running serverless with AWS Lambda, you\u2019ve also bypassed the need for a platform team to run it, the serverless platform takes care of those concerns.\n\nAt Netflix we had an opinionated way of configuring AWS that was a company wide platform, for all AWS accounts, covering things like security and identity. However there were teams doing very different things on AWS, and they each built their own platform layer. We had the online personalization microservices platform that I\u2019ve discussed a lot, there was a separate platform for data science, another one for movie encoding, and another for corporate IT. Each of those organizations had their own platform team. Then within online, different groups also built platforms that exposed their own set of business objects in exactly the right model to rapidly construct new functionality. The personalization team needed a different platform from the content delivery team or the security team, or the presentation layer team, sometimes with interfaces for different languages (Java, Python, C++, Javascript etc.).\n\nI was thinking about drawing a layer cake diagram to show this but decided to put it on a Wardley Map instead (created using mapkeep.com).\n\nTalking through the map, we start with an end user who can choose a web app or a mobile app. The web app depends on a web platform and associated development tooling, these seem to be a lot of work to maintain and change rapidly so you likely need some Javascript experts in a Web Platform Team. Mobile app platforms are much more standardized around iOS and Android, with Apple and Google providing the platform.\n\nBoth mobile and web front ends call an Application Program Interface (API) that is managed by an API proxy. Some people build custom API proxies, but I think it makes more sense to use one of the off the shelf options, with its own vendor based platform team. That calls into your custom business logic, which is written using a language and libraries platform, makes calls into web services and databases, and is built and deployed by a Continuous Integration/Continuous Deployment (CICD) pipeline. You probably need an in-house Developer Experience Platform Team that knows the languages, supports the libraries, and manages the web service and database vendors. This is one of the key teams that people have been talking about recently. I think it\u2019s a different team with different expertise from the more infrastructure oriented Container Platform Team that provides a place for applications to run. I also advocate a Serverless First approach nowadays, where as much as possible is built using platforms like AWS Lambda\u200a\u2014\u200awhere there is no need for a platform team\u200a\u2014\u200aand containers are used for things that have custom instance type requirements, very high continuous traffic, or long running computation. Some of the Kubernetes focused Container Platform Team vendors are selling solutions in this space, which I see as a separate team with different skills to the Developer Experience Platform Team.\n\nHere\u2019s a public link to a copy of the map for people who would like to modify it. Note that mapkeep is in development and has a serverless backend that can have warmup issues, so if you see errors, wait a few seconds and try again.\n\nHopefully people find this interesting, and can use it to help come up with better justifications for their own combination of platform teams and tooling choices.",
        "metadata": {
          "word_count": 1353,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There\u2019s been a lot of discussion about platforms recently. We used this model effectively at Netflix when I was their cloud architect from 2010 through 2013. There are three underlying reasons for the platform engineering meme today. The first principle I have is that it isn\u2019t one platform, it\u2019s layers of platforms that need different specialized knowledge so its usually many platform teams. The second principle is that the platform layers are dynamic, evolve over time and tend to move \u201cup the stack\u201d as they add functionality, and shed capabilities. The layers of platforms start at the bottom with hardware choices. The next layer is operating system platforms, what flavor of Linux, what version of Windows etc. Above that there\u2019s a deployment platform such as Kubernetes or AWS Lambda. If you are running a Kubernetes based platform you need a platform. You always want new functionality but if you depend on an outside supplier, they have to make it into a long term stable part of their platform. We looked at platforms like Rightscale and decided not to use them. At Netflix we had an opinionated way of configuring AWS that was a company wide platform, for all AWS accounts. However there were teams doing very different things on AWS, and they each built their own platform layer. Open source tools which have an open development model that allows your team to make contributions may be a better approach. Each of those organizations had their own platform team. Different groups also built platforms that exposed their own set of business objects. The personalization team needed a different platform from the content delivery team. Both mobile and web front ends call an Application Program Interface (API) that is managed by an API proxy. You probably need an in-house Developer Experience Platform Team that knows the languages, supports the libraries, and manages the web service and database vendors. I advocate a Serverless First approach nowadays, where as much as possible is built using platforms like AWS Lambda. Some of the Kubernetes focused Container Platform Team vendors are selling solutions in this space, which I see as a separate team with different skills."
      },
      "tags": [
        "aws",
        "cloud",
        "containers",
        "continuous deployment",
        "continuous integration",
        "data science",
        "engineering",
        "infrastructure",
        "kubernetes",
        "microservices",
        "netflix",
        "platform",
        "podcast",
        "security",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_file_f8c186d4",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Open letter to my Sun friends at Oracle updated from 2010 post",
      "source": "Medium Archive",
      "published_date": "2017-09-01",
      "url": "https://medium.com/@adrianco/2017-09-01_Open-letter-to-my-Sun-friends-at-Oracle--updated-from-2010-post--1f8b2bcba693",
      "content": {
        "text": "Open letter to my Sun friends at Oracle (updated from 2010\u00a0post)\nThe final sunset. Photo by @adrianco\nIn the summer of 2010 I wrote the following blog post, and given the latest news about how it worked out, I thought it was worth migrating the post from blogspot to medium and adding some commentary to update it.\n\nHere\u2019s the key quote, which is still true today. Many people have transferable skills, but lack the specific experience: \u201cWhat can you do to \u201cescape\u201d? The tools are right there, you don\u2019t need to invest significant money, you just need to carve out some spare time to use them. Everything is either free open source, or available in the free tier or for a few dollars on the Amazon cloud.\u201d\n\nThe main update from 2010 is that I joined AWS last year, and we\u2019re now a big company ($16B annualized revenue run rate), growing fast, hiring lots of people, and still building products that are changing the world.\n\nI\u2019ve set up my twitter account for open DM\u2019s and will try to keep on top of my LinkedIn messages in the coming weeks. I\u2019m always happy to hear from old friends and colleagues looking for what to do next.\n\n[August 2010]\nI recently heard about Illumos via a tweet from Alec Muffett, and responded with my own tweet \u201cI predict that #illumos will be just as irrelevant as Solaris has been for the last few years. Legacy.\u201d\u200a\u2014\u200apersonally I haven\u2019t logged into a Solaris or SPARC machine for about four years now. There are none at Netflix.I have also been talking to a few friends who stayed at Sun and are now at Oracle, and there is a common thread that I decided to put out there in this blog post.This week I presented at a local Computer Measurement Group meeting, talking about how easy it is to use the Amazon cloud to run Hadoop jobs to process terabytes of data for a few bucks [slideshare]. I followed a talk on optimizing your Mainframe software licensing costs by tweaking workload manager limits. There are still a lot of people working away on IBM Mainframes, but it\u2019s not where interesting new business models go to take over the world.The way I see the Oracle/Sun merger is that Oracle wanted to compete more directly with IBM, and they will invest in the bits of Sun that help them do that. Oracle has a very strong focus on high margin sales, so they will most likely succeed in making good money with help from Solaris and SPARC to compete with AIX, z/OS and P-series, selling to late-adopter industries like Banking, Insurance etc. Just look where the Mainframes are still being used. Sun could never focus on just the profitable business on its own, because it had a long history of leading edge innovation that is disruptive and low margin. However, what was innovative once is now a legacy technology base of Solaris and SPARC, and it\u2019s not even a topic of discussion in the leading edge of disruptive innovators, who are running on x64 in the cloud on Linux and a free open source stack. There is no prospect of revenue for Oracle in this space, so they are right to ignore it.That is what I meant when I tweeted that Illumos is as irrelevant as Solaris, and it is legacy computing. I don\u2019t mean Solaris will go away, I\u2019m sure it will be the basis of a profitable business for a long time, but the interesting things are happening elsewhere, specifically in public cloud and \u201cinfrastructure as code\u201d.You might point to Joyent, who use Solaris, and now have Bryan Cantrill on board, but they are a tiny bit-player in cloud computing [2017 now part of Samsung] and Amazon are running away with the cloud market, and creating a set of de-facto standard APIs that make it hard to differentiate and compete. You might point to enterprise or private clouds, but as @scottsanchez tweeted: \u201cDefine: Private Cloud\u00a0\u2026 1/2 the features of a public cloud, for 4x the cost\u201d, that\u2019s not where the interesting things are happening.So to my Sun friends at Oracle, if you want to work for a profitable company and build up your retirement fund Oracle is an excellent place to be. However, there are a lot of people who joined Sun when it was re-defining the computer industry, changing the rules, disrupting the competition. If you want some of that you need to re-tool your skill set a bit and look for stepping stones that can take you there.When Sun shut down our HPC team in 2004 I deliberately left the Enterprise Computing market, I didn\u2019t want to work for a company that sold technology to other companies, I wanted to sell web services to end consumers, and I had contacts at eBay who took me on. In 2007 I joined Netflix, and it\u2019s the best place I\u2019ve ever worked, but I needed that time at eBay to orient myself to a consumer driven business model and re-tool my skill set, I couldn\u2019t have joined Netflix directly.There are two slideshare presentations on the Netflix web site, one is on the company culture, the other on the business model. It is expected that anyone who is looking for a job has read and inwardly digested them both (its basically an interview fail if you haven\u2019t). These aren\u2019t aspirational puff pieces written by HR, along with everyone else in Netflix management (literally, at a series of large offsites), I was part of the discussion that helped our CEO Reed Hastings write and edit them both.What can you do to \u201cescape\u201d? The tools are right there, you don\u2019t need to invest significant money, you just need to carve out some spare time to use them. Everything is either free open source, or available for a few cents or dollars on the Amazon cloud. The best two things you can have on your resume are hands on experience with the Amazon Web Services tool set, and links to open source projects that you have contributed to. There isn\u2019t much demand for C or C++ programmers, but ObjectiveC is an obvious next step, it\u2019s quite fun to code in and you can develop user interfaces for iPhone/iPad in a few lines of code, that back-end into cloud services. Java code (for app servers like Tomcat) on Android phones, Ruby-on-Rails, and Python are the core languages that are being used to build innovative new businesses nowadays. If you are into data or algorithms, then you need to figure out how to use Hadoop, which as I describe in one of my slideshare decks is trivially available from Amazon. You can even get an HPC cluster on a 10Gbit ethernet interconnect from Amazon now. There is hadoop based open source algorithm project called Mahout that is always looking for contributors.To find the jobs themselves, spend time on LinkedIn. I use it to link to anyone I think might be interesting to hire or work with. Your connections have value since it is always good to hire people that know other good people. Keep your own listing current and join groups that you find interesting, like Java Architecture or Cloud Computing, and Sun Alumni. At this point LinkedIn is the main tool used by recruiters and managers to find people.\nSome technology updates: nowadays I\u2019d take time to learn Python and Go rather than Objective C or Java, and play around with deep learning rather than Hadoop\u200a\u2014\u200athe gluon.mxnet.io \u201cstraight dope\u201d tutorial is a great place to start. My Netflix era and Battery Ventures era slide deck tutorials may also be useful context.\n\nGood luck, and keep in touch, you can find me on LinkedIn or twitter @adrianco.\n\nOriginally published at perfcap.blogspot.com\u200a\u2014\u200aupdated on September 1, 2017.",
        "metadata": {
          "word_count": 1310,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In 2010 I wrote an open letter to my Sun friends at Oracle. The key quote is still true today. Many people have transferable skills, but lack the specific experience. The tools are right there, you don't need to invest significant money. I predict that Illumos will be just as irrelevant as Solaris has been for the last few years. I haven\u2019t logged into a Solaris or SPARC machine for about four years now. There are none at Netflix. The way I see the Oracle/Sun merger is that Oracle wanted to compete more directly with IBM, and they will invest in the bits of Sun that help them do that. Sun could never focus on just the profitable business on its own. Solaris will be the basis of a profitable business for a long time. But the interesting things are happening in public cloud and \u2018infrastructure as code\u2019 When Sun shut down our HPC team in 2004 I deliberately left the Enterprise Computing market. I wanted to sell web services to end consumers, and I had contacts at eBay who took me on. In 2007 I joined Netflix, and it\u2019s the best place I\u2019ve ever worked.  ObjectiveC is an obvious next step, it\u2019s quite fun to code in and you can develop user interfaces for iPhone/iPad in a few lines of code. Java code (for app servers like Tomcat) on Android phones, Ruby-on-Rails, and Python are the core languages that are being used to build innovative new businesses. LinkedIn is the main tool used by recruiters and managers to find people. I use it to link to anyone I think might be interesting to hire or work with. Your connections have value since it is always good to hire people that know other good people. Good luck, and keep in touch, you can find me on LinkedIn or twitter @adrianco. I'm also on Facebook and Twitter."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "hpc",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_7233b7d3",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Adrian s recipe for gluten and dairy free boozy fruit cake",
      "source": "Medium Archive",
      "published_date": "2022-12-19",
      "url": "https://medium.com/@adrianco/2022-12-19_Adrian-s-recipe-for-gluten-and-dairy-free-boozy-fruit-cake-6e5c5ef5053f",
      "content": {
        "text": "Adrian\u2019s recipe for gluten and dairy free boozy fruit\u00a0cake\n\nThis is a synthesis of several recipes that I\u2019ve modified over the years. I usually make a cake for the holidays.\n\nGrowing up my mother made what she called a Dundee Cake, a round fruit cake with almonds on top. She would make it a month before, and soak brandy into it. My version avoids wheat and dairy, I use honey rather than sugar and I soak the fruit in spirits before I make it, then it\u2019s good to eat a few days later. I made one today to go with Xmas dinner.\n\nStart the day before you want to bake, with a bowl that has a cover. Fill it with a few cups of chopped dates and whatever fruit you have, I used dates, golden raisins and blueberries, and added spiced rum and brandy this year. Add some orange zest, a splash of orange juice, or some orange flavor booze as well. Shake it up a bit and leave in the fridge overnight. I eyeball the amounts, but it\u2019s something like: 1 cup raisins, 1 cup dates, 1 cup other fruit, 1 cup some mix of brandy, rum or sherry. The idea is that the raisins soak up the booze. If it seems too wet after a day, add some more fruit or leave it another day to soak it up. More fruit and booze doesn\u2019t hurt.\n\nThe next day, mix in a cup or so of walnuts or pecans or macadamia nuts or whatever you have lying around. It should look like this, the liquid has soaked in:\nBowl of mixed fruit and\u00a0nuts\nNext put a sieve on top of a large bowl, and add all the dry ingredients: 1 1/4 cups of almond flour, 2 tablespoons of coconut flour, 1/2 teaspoon of baking soda, 1/8 teaspoon salt, 1 1/2 teaspoons of cinnamon, 1/2 teaspoon of ground ginger, 1/4 teaspoon nutmeg. Then sieve it all into the bowl together. The almond flour will need to be helped through with the back of a spoon. Here\u2019s what that looks like:\nBowl of sieved dry ingredients\nThe liquid ingredients can be combined in a measuring jug, starting with coconut oil. It\u2019s waxy at room temp, so put some in, microwave the jug for 30sec to get it to melt down, then you can see if you have too little or two much. Get it to around 1/3 cup level. Then add 1/3 cup of honey to get to the 2/3 cup level, 2 teaspoons vanilla extract, and stir it all together.\nMeasuring jug with melted coconut\u00a0oil\nCrack two eggs into the flour, mix them in a bit then add the oil and honey from the jug and blend it together. I used an electric beater to mix it, but it\u2019s easy to do by hand. Once it\u2019s all combined, add the boozy fruit and nut and mix in with a spoon.\n\nSet the oven for 300F, line a round or rectangular baking tin with parchment paper, and spoon the mixture in. Smooth it out and add almonds to the top in whatever pattern you like. I did it in rows to make it easier to slice:\n\nCover the tin with foil and put in the oven for 1hr 15min. Then take off the foil and give it another 5 min to brown the top. Check it\u2019s cooked with a skewer, which shouldn\u2019t be goopy when you pull it out. If you made it extra wet it may take a bit longer.\n\nGave it an extra 5 minutes after taking off the foil. Let it cool for a while.\n\nI turned it out onto a platter, and covered with foil to leave it for a few days to mature. The moisture comes out of the fruit and makes it taste better.\n\nI mostly put this here to make it easier to remember the recipe for next time, as I have to look up a couple of places that I combine to get the spices and cooking time and temp right.",
        "metadata": {
          "word_count": 681,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This is a synthesis of several recipes that I\u2019ve modified over the years. I usually make a cake for the holidays. My version avoids wheat and dairy, I use honey rather than sugar and I soak the fruit in spirits before I make it. The idea is that the raisins soak up the booze. If it seems too wet after a day, add some more fruit or leave it another day to soak it up. More fruit and booze doesn't hurt. Crack two eggs into the flour, mix them in a bit then add the oil and honey from the jug and blend it together. Once it\u2019s all combined, add the boozy fruit and nut and mix in with a spoon. I mostly put this here to make it easier to remember the recipe for next time. I have to look up a couple of places that I combine to get the spices and cooking time and temp right."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_efd67f21",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Open Source at AWS re Invent",
      "source": "Medium Archive",
      "published_date": "2019-11-18",
      "url": "https://medium.com/@adrianco/2019-11-18_Open-Source-at-AWS-re-Invent-bab0c7e3fa2a",
      "content": {
        "text": "Open Source at AWS re:Invent\n\nWe\u2019re excited to let you know that we have an Open Source track at re:Invent this year! Learn about what\u2019s in it:\u00a0\u00a0https://aws.amazon.com/blogs/opensource/open-source-at-aws-reinvent-2019/\u00a0\u00a0Some selected sessions:\u00a0\u00a0OPN201 Change is coming to robotics development: The shift to ROS 2\u00a0\ud83e\udd16 Alejandra Olvera-Novack\u200a\u2014\u200aAWS Developer Relations on how the shift from Robot Operating System (ROS) 1 to ROS 2 will change the landscape for all robot lovers.\u00a0\u00a0OPN220 Build robotic cloud simulations with ROS and AWS RoboMaker\u00a0Join Camilo Buscaron, AWS Principal Open Source Technologist, and Katherine Scott, Developer Advocate, Open Robotics in this workshop to use Gazebo, a 3D simulator, and Robot Operating System (ROS) on AWS RoboMaker and learn how to spin up robotic simulations.\u00a0\u00a0OPN204 Secure your Open Distro for Elasticsearch cluster\u00a0Dive deep on Open Distro Security: secure your cluster with your own SSL certs, set up fine-grained access control for your documents and fields, assign users to roles, create multi-tenant Kibana visualizations and dashboards, and integrate with federated identity providers like LDAP, Active Directory, and Okta.\u00a0\u00a0\u00a0OPN303 BPF performance analysis (repeats)\u00a0Learn how Netflix uses eBPF to create powerful performance analysis tools. Brendan Gregg tours BPF tracing, with open source tools & examples for EC2 instance analysis.\u00a0\u00a0\u00a0OPN205-R Contributing to the AWS Construct Library (repeats)\u00a0Using and loving the AWS Cloud Development Kit and want to help make it better? Join Lee Packham, AWS Solutions Architect and Enrico Huijbers, AWS Software Development Engineer to find out how easy it is.\u00a0\u00a0OPN304 Learnings from migrating a service from JDK 8 to JDK 11\u00a0AWS Lambda improved latency by migrating to JDK 11 with Amazon Corretto. Learn about how Lambda works behind the scenes, and how you can follow these steps to migrate your application to Corretto with Niall Connaughton\u200a\u2014\u200aSoftware Engineer, AWS Serverless Applications, and Yishai Galatzer, Senior Manager Software Development.\u00a0\u00a0OPN207 PartiQL: One query language for all of your data\u00a0PartiQL is a SQL-compatible query language that makes it easy and efficient to process both structured (relational) data, as well as semi-structured, schemaless, and nested data, as typically found in open data formats in the Amazon S3 data lake and document-oriented databases. PartiQL is a unifying query language, enabling single-query-language access to multiple AWS services. Almann Goo, Principal Engineer, and Yannis Papakonstantinou, Senior Principal Scientist discuss PartiQL\u2019s core tenets and how they served AWS services and Amazon projects, and how PartiQL can benefit your applications and tools in need of unifying query access to the data lake.\u00a0\u00a0OPN308-R1 PartiQL: Solution integration and joining the community (repeats)\u00a0In this chalk talk, Almann Goo, Principal Engineer, and Yannis Papakonstantinou, Senior Principal Scientist, address how you can use the PartiQL open source in the development of your application, BI tool, and/or database service, and how to join the PartiQL community and contribute to the next steps in its evolution.\u00a0\u00a0OPN402 Firecracker open-source innovation\u00a0Since Firecracker\u2019s release at re:Invent 2018, several open-source teams have built on it, while AWS has continued investing in Firecracker\u2019s speed. Radu Weiss, Software Development Manager AWS, and Sebastian Bernheim,Customer Success Engineer Weaveworks, discuss Weave Ignite from Weaveworks, which unifies containers and VMs using Firecracker microVMs with OCI images, containerd, and CNI. Leigh Capili (DevAdv) demos how to launch apps from Git using Kubernetes orchestration, Ignite virtualization, and GitOps management. Alexandra Iordache (Firecracker maintainer) showcases Firecracker\u2019s new snapshotting capability.\u00a0\u00a0OPN211 How Zalando runs Kubernetes clusters at scale on AWS\u00a0Henning Jacobs, Senior Principal Zalando SE, shows Zalando\u2019s approach for running 140+ clusters on AWS, how it does continuous delivery for its cluster infrastructure, and how it created open-source tooling to manage cost efficiency and improve developer experience, openly sharing failures and learnings from three years of Kubernetes in production.\u00a0\u00a0OPN305 How Optum manages transient developer accounts at scale\u00a0Optum uses Disposable Cloud Environment (an open-source account management product to facilitate public cloud experimentation in isolated sandbox environments) to empower its tens of thousands of engineers to explore public cloud services and resources while mitigating business risk. Learn more from Kevin DeJong, AWS Senior DevOps Cloud Architect, Matt Meyers, Lead Cloud Engineer Optum, and Marissa Crosby,Product Manager Optum.\n\nI\u2019ll be there all week, presenting my own talks on Monday:\n\nARC203 Innovation at Speed\u200a\u2014\u200abased on my short eBook, Cloud for CEOs\n\nARC335 Failure is Not an Option\u200a\u2014\u200aa joint presentation including customer experiences, multi-region networking technology, and a section by me based on the previous blog post on failure modes.",
        "metadata": {
          "word_count": 741,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AWS has an Open Source track at re:Invent this year. Learn about what\u2019s in it:\u00a0https://aws.amazon.com/blogs/opensource/open-source-at-aws-reinvent-2019. Learn how Netflix uses eBPF to create powerful performance analysis tools. Learn about how Lambda works behind the scenes, and how you can follow these steps to migrate your application to Corretto. PartiQL is a unifying query language, enabling single-query-language access to multiple AWS services. Almann Goo, Principal Engineer, and Yannis Papakonstantinou, Senior Principal Scientist discuss PartiQL\u2019s core tenets and how they served AWS services and Amazon projects. Since Firecracker\u2019s release at re:Invent 2018, several open-source teams have built on it. Radu Weiss, Software Development Manager AWS, and Sebastian Bernheim,Customer Success Engineer Weaveworks, discuss Weave Ignite. Leigh Capili (DevAdv) demos how to launch apps from Git using Kubernetes orchestration. Optum uses Disposable Cloud Environment (an open-source account management product to facilitate public cloud experimentation in isolated sandbox environments) to empower its tens of thousands of engineers to explore public cloud services and resources. Learn more from Kevin DeJong, AWS Senior DevOps Cloud Architect."
      },
      "tags": [
        "aws",
        "cloud",
        "containers",
        "devops",
        "infrastructure",
        "kubernetes",
        "netflix",
        "performance",
        "security",
        "serverless",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_file_cf59e465",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Looking forward to hearing what differences you see in diversity of applicants and the medium long",
      "source": "Medium Archive",
      "published_date": "2016-03-29",
      "url": "https://medium.com/@adrianco/2016-03-29_Looking-forward-to-hearing-what-differences-you-see-in-diversity-of-applicants-and-the-medium-long--f2eeabb07a0c",
      "content": {
        "text": "Looking forward to hearing what differences you see in diversity of applicants and the medium/long term impact to DO. Thanks for sharing.",
        "metadata": {
          "word_count": 22,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Do you have a personal essay to share with the world? Submit at the bottom of this article. Share your thoughts on diversity of applicants and the medium/long term impact."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_01304cbd",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Evolution of business logic from monoliths through microservices to functions",
      "source": "Medium Archive",
      "published_date": "2017-02-16",
      "url": "https://medium.com/@adrianco/2017-02-16_Evolution-of-business-logic-from-monoliths-through-microservices--to-functions-ff464b95a44d",
      "content": {
        "text": "Evolution of business logic from monoliths through microservices, to functions\n\nUnderlying technology advancements are creating a shift to event driven functions and radical improvements in time to\u00a0value\n\nThe whole point of running application software is to deliver business value of some sort. That business value is delivered by creating business logic and operating it so it can provide a service to some users.\n\nThe time between creating business logic and providing service to users with that logic is the time to value. The cost of providing that value is the cost of creation plus the cost of delivery.\n\nIn the past, costs were high and efficiency concerns dominated, with high time to value regarded as the normal state of affairs. Today, when organizations measure and optimize their activities, time to value is becoming a dominant metric, driven by competitive pressures, enabled by advances in technology, and by reductions in cost.\n\nPut another way, to increase return on investment you need to find ways to increase the return, start returning value earlier, or reduce the investment. When costs dominate, that\u2019s where the focus is, but as costs reduce and software impact increases, the focus flips towards getting the return earlier.\n\nAs technology has progressed over the last decade, we\u2019ve seen an evolution from monolithic applications to microservices and are now seeing the rise of serverless event driven functions, led by AWS Lambda. What factors have driven this evolution? Low latency messaging enabled the move from monoliths to microservices, low latency provisioning enabled the move to Lambda.\n\nTo start with, ten years ago, a monolithic application was the best way to deliver business logic, for the constraints of the time. Those constraints changed, and about five years ago the best option shifted to microservices. New applications began to be built on a microservices architecture, and over the last few years, tooling and development practices changed to support microservices.\n\nToday, another shift is taking place to event driven functions\u200a\u2014\u200abecause the underlying constraints have changed, costs have reduced, and radical improvements in time to value are possible.\n\nIn what follows, we\u2019ll look at different dimensions of change in detail: delivery technology, hardware capabilities, and organizational practices, and see how they have combined to drive this evolution.\n\nThe early days of process optimization\n\nAt the start of this journey, the cost of delivery dominated. It took a long time to procure, configure and deploy hardware, and software installations were hand crafted projects in their own right.\n\nTo optimize delivery the best practice was to amortize this high cost over a large amount of business logic in each release, and to release relatively infrequently, with a time to value measured in months for many organizations. Given long lead times for infrastructure changes, it was necessary to pre-provision extra capacity in advance and this lead to very low average utilization.\n\nThe first steps to reduce cost of delivery focused on process automation. Many organizations developed custom scripts to deploy new hardware, and to install and update applications.\n\nThe DevOps\u00a0movement\n\nEventually common frameworks like Puppet and Chef became popular, and \u201cinfrastructure as code\u201d sped up delivery of updates. The DevOps movement began when operations teams adopted agile software development practices and worked closely with developers to reduce time to value from months to days.\n\nScripts can change what\u2019s already there, but fast growing businesses or those with unpredictable workloads struggled to provision new capacity quickly. The introduction of self service API calls to automatically provision cloud capacity using Amazon EC2 solved this problem.\n\nWhen developers got the ability to directly automate many operations tasks using web services, a second wave of DevOps occurred. Operations teams built and ran highly automated API driven platforms on top of cloud services, providing self service deployments and autoscaled capacity to development teams.\n\nThe ability to deploy capacity just-in-time, and pay by the hour for what was actually needed, allowed far higher average utilization, and automatically handled unexpected spikes in workloads.\n\nThe age of containers\n\nAnother wave of optimization arrived when docker made containers easy enough for everyone to use. Docker containers provide a convenient bundled package format that includes a fixed set of dependencies, a runtime that gives more isolation than processes, but less than a virtual machine instance, startup times measured in seconds, and a substantial saving in memory footprint.\n\nBy packing many containers onto an instance, and rounding off run times to minutes or seconds instead of hours, even higher utilization is possible. Container based continuous delivery tooling also sped up the work of developers and reduced time to value.\n\nWhen there\u2019s a reasonably predictable amount of work coming in, containers can be run at high utilization levels, however many workloads are spiky or drop to zero for extended periods. For example applications used in the workplace may only be active for 40 of the 168 hours in a week.\n\nTo maintain high availability, it\u2019s usual to spread application instances over three availability zones, and even to require more than one instance per zone. The minimum footprint for a service is thus six instances. If we want to scale down to zero, we need a way to fire up part of an application when an event happens, and shut it down when it\u2019s done.\n\nThis is a key part of the AWS Lambda functionality, and it transforms spiky and low usage workloads to effectively 100% utilization by only charging for the capacity that is being used, in 0.1 second increments, and scales from zero to very high capacity as needed. There\u2019s no need to think about or provision servers, and that\u2019s why this is often called the serverless pattern.\n\nAdvances in delivery technology provide stepping stones for improvements in time to value, but there are other underlying changes that have caused a series of transitions in best practices over the last decade.\n\nAdvances in CPU and network technology\n\nThe optimal size for a bundle of business logic depends upon the relative costs in both dollars and access time of CPU, network, memory and disk resources, combined with the latency goal for the service.\n\nFor the common case of human end users waiting for some business logic to provide a service, the total service time requirement hasn\u2019t changed much. Perception and expectations haven\u2019t changed as much as the underlying technology has over the last decade or so.\n\nCPU speed has increased fairly slowly over the last decade, as the clock rate hit a wall at a few GHz, however on chip caches are much larger, and the number of cores increased instead. Memory speed and size have also made relatively slow progress.\n\nNetworks are now radically faster, common deployments have moved from 1GBit to 10GBit and now 25GBit, and software protocols are far more efficient. When common practice was sending XML payloads over 1GBit networks, the communication overhead constrained business logic to be co-located in large monolithic services, directly connected to databases.\n\nA decade later, encodings that are at least an order of magnitude more efficient over 25Gbit networks\u200a\u2014\u200ameaning that the cost of communication is reduced by more than two orders of magnitude.\n\nIn other words, it\u2019s possible to send 100 to 1000 messages between services in the same amount of time as communicating and processing one message would take a decade ago. This is a key enabler for the move away from monolithic applications.\n\nAdvances in storage & database technology\n\nStorage and databases have also gone through a revolution over the last decade. Monolithic applications map their business logic to transactions against complex relational database (RDBMS) schemas, that link together all the tables, and allow coordinated atomic updates.\n\nA decade ago best practice was to implement a small number of large centralized relational databases connected via storage area networks to expensive disk arrays using magnetic disk, fronted by large caches.\n\nToday cached magnetic disks have been replaced by solid state disks. The difference is that reads move from slow, expensive and unpredictable\u200a\u2014\u200aas cache hit rate varies, to consistently fast and almost unlimited. Writes and updates move from being fast for cached disks to unpredictable for solid state disks, due to wear leveling algorithms and other effects.\n\nNew \u201cNoSQL\u201d database architectures have become popular for several reasons\u200a\u2014\u200abut the differences that concern us here are that they have simple schema models and take advantage of the characteristics of solid state storage. Simple schemas force separation of the tables of data that would be linked together in the same relational database, into multiple independent NoSQL databases, driving decentralization of the business logic.\n\nThe Amazon DynamoDB datastore service was designed from the beginning to run only on solid state disk, providing extremely consistent low latency for requests. Apache Cassandra\u2019s storage model generates a large number of random reads, and does infrequent large writes with no updates, which is ideally suited to solid state disks.\n\nCompared to relational databases, NoSQL databases provide simple but extremely cost effective, highly available and scalable databases with very low latency. The growth in popularity of NoSQL databases is another key enabler for the move away from monolithic schemas and monolithic applications. The remaining relational core schemas are cleaned up, easier to scale and are being migrated to services such as Amazon's RDS and Aurora.\n\nMoving from project to\u00a0product\n\nIt\u2019s common to talk about \u201cpeople, process, and technology\u201d when we look at changes in IT. We\u2019ve just seen how technology has taken utilization and speed of deployment to the limit with AWS Lambda, effectively 100% utilization for deployments in a fraction of a second.\n\nIt\u2019s also made it efficient to break the monolithic code base into hundreds of microservices and functions, and denormalized the monolithic RDBMS into many simple scalable and highly available NoSQL and relational data stores.\n\nThere have also been huge changes in \u201cpeople and process\u201d over the last decade. Let\u2019s consider a hypothetical monolith built by 100 developers working together. To coordinate, manage test and deliver updates to this monolith every few months it\u2019s common to have more people running the process than writing the code.\n\nTwice as many project managers, testers, DBA\u2019s, operators etc. organized in silos, driven by tickets, and a management hierarchy demanding that everyone write weekly reports and attend lots of status meetings as well as find time to code the actual business logic!\n\nThe combination of DevOps practices, microservices architectures, and cloud deployments went hand in hand with continuous delivery processes, cellular based \u201ctwo pizza team\u201d organizations, and a big reduction in tickets, meetings and management overhead. Small groups of developers and product managers independently code, test and deploy their own microservices whenever they need to.\n\nThe ratio of developers to overhead reverses\u200a\u2014\u200awith 100 developers to 50 managers. Each developer is spending less time in meetings and waiting for tickets, getting twice as much done with a hundred times better time to value.\n\nA common shorthand for this change is a move from project to product. A large number of project managers are replaced with far fewer product managers. In my somewhat contrived example, 150 people are producing twice the output that 300 people used to. Double the return a hundred times sooner, on half the investment. Many organizations have been making this kind of transition and there are real examples of similar improvements.\n\nThe early days of functions\n\nLambda based applications are constructed from individual event driven functions that are almost entirely business logic\u200a\u2014\u200aand there\u2019s much less boilerplate and platform code to manage. It\u2019s early days, but this appears to be driving another radical change.\n\nSmall teams of developers are building production ready applications from scratch in just a few days. They are using short simple functions and events to glue together robust API driven data stores and services. The finished applications are already highly available and scalable, high utilization, low cost and fast to deploy.\n\nAs an analogy, think how long it would take to make a model house starting with a ball of clay, compared to a pile of Lego bricks. Given enough time you could make almost anything from the clay, it\u2019s expressive, creative, and there\u2019s even an anti-pattern for monolithic applications called the \u201cbig ball of mud\u201d.\n\nThe Lego bricks fit together to make a constrained, blocky model house, that is also very easy to extend and modify, in a tiny fraction of the time. In addition, there are other bricks somewhat like Lego bricks, but they aren\u2019t popular enough to matter, and any kind of standard brick based system will be much faster than custom formed clay.\n\nIf an order of magnitude increase in developer productivity is possible, then my example 100 developer monolith could be rewritten from scratch and replaced by a team of ten developers in a few weeks. Even if you doubt that this would work, its a cheap experiment to try it out. The invocation latency for event driven functions is one of the key limitations that constrains complex applications, but over time those latencies are reducing.\n\nThe real point I\u2019m making is that the ROI threshold for whether existing monolithic applications should be moved unchanged into the cloud or rewritten depends a lot on how much work it is to rewrite them. A typical datacenter to cloud migration would pick out the highly scaled and high rate of change applications to re-write from monoliths to microservices, and forklift the small or frozen applications intact.\n\nI think that AWS Lambda changes the equation, is likely to be the default way new and experimental applications are built, and also makes it worth looking at doing a lot more re-writes.\n\nI\u2019m very interested in your experiences, so please let me know how you see time to value evolving in your environments.",
        "metadata": {
          "word_count": 2292,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The time between creating business logic and providing service to users is the time to value. The cost of providing that value is the cost of creation and delivery. In the past, costs were high and efficiency concerns dominated, with high time tovalue regarded as the normal state of affairs. As costs reduce and software impact increases, the focus flips towards getting the return earlier. We are now seeing the rise of serverless event driven functions, led by AWS Lambda. Low latency messaging enabled the move from monoliths to microservices. At the start of this journey, the cost of delivery dominated. It took a long time to procure, configure and deploy hardware, and software installations were hand crafted projects in their own right. Given long lead times for infrastructure changes, it was necessary to pre-provision extra capacity in advance. The DevOps movement began when operations teams adopted agile software development practices and worked closely with developers. Many organizations developed custom scripts to deploy new hardware, and to install and update applications. Docker containers provide a convenient bundled package format that includes a fixed set of dependencies. Docker containers have a runtime that gives more isolation than processes, but less than a virtual machine instance. A key part of the AWS Lambda functionality, and it transforms spiky and low usage workloads to effectively 100% utilization. Lambda only charges for the capacity that is being used, in 0.1 second increments, and scales from zero to very high capacity as needed. The optimal size for a bundle of business logic depends upon the relative costs in both dollars and access time of CPU, network, memory and disk resources, combined with the latency goal for the service. For the common case of human end users waiting for some business logic to provide a service, the total service time requirement hasn't changed much. A decade ago best practice was to implement a small number of large centralized relational databases connected via storag. A decade later, encodings that are at least an order of magnitude more efficient over 25Gbit networks. This is a key enabler for the move away from monolithic applications. Cached disks have been replaced by solid state disks. The difference is that reads move from being fast and unpredictable to consistently fast and predictable. This is due to wear and tear. The growth in popularity of NoSQL databases is another key enabler for the move away from monolithic schemas and monolithic applications. Apache Cassandra\u2019s storage model generates a large number of random reads, and does infrequent large writes with no updates. There have been huge changes in \u201cpeople and process\u201d over the last decade. To coordinate, manage test and deliver updates to this monolith every few months it\u2019s common to have more people running the process than writing the code. Small groups of developers and product managers inde Lambda based applications are constructed from individual event driven functions that are almost entirely business logic. pendently code, test and deploy their own microservices whenever they need to. Small teams of developers are building production ready applications from scratch. They are using short simple functions and events to glue together robust API driven data stores and services. The finished applications are already highly available and scalable. The ROI threshold for whether existing monolithic applications should be moved unchanged into the cloud or rewritten depends a lot on how much work it is to rewrite them. A typical datacenter to cloud migration would pick out the highly scaled and high rate of change applications to re-write. . I'm interested in your experiences. Let me know how you see time to value evolving in your environments. Please share your thoughts in the comments below."
      },
      "tags": [
        "agile",
        "architecture",
        "automation",
        "aws",
        "cloud",
        "containers",
        "devops",
        "docker",
        "infrastructure",
        "microservices",
        "platform",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_file_ec5b4666",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Supercomputing Predictions Custom CPUs CXL3 0 and Petalith Architectures",
      "source": "Medium Archive",
      "published_date": "2023-01-20",
      "url": "https://medium.com/@adrianco/2023-01-20_Supercomputing-Predictions--Custom-CPUs--CXL3-0--and-Petalith-Architectures-b67cc324588f",
      "content": {
        "text": "Supercomputing Predictions: Custom CPUs, CXL3.0, and Petalith Architectures\n\nThis post started out as a reaction to attending SC22 after about 20 years away from the HPC mainstream. I had a version of this published as a story at InsideHPC on December 1st. In early January a related paper was published by Satoshi Matsuoka et. al. on Myths and Legends of High Performance Computing\u200a\u2014\u200ait\u2019s a somewhat light-hearted look at some of the same issues by the leader of the team that built the Fugaku system I mention below. Luckily I think we are in general agreement on the main points.\n\nHere\u2019s some predictions I\u2019m making:\nJack Dongarra\u2019s efforts to highlight the low efficiency of the HPCG benchmark as an issue will influence the next generation of supercomputer architectures to optimize for sparse matrix computations.Custom HPC optimized CPUs and GPUs will become the mainstream in the next few years, with chiplets based on the ARM and RISC-V pattern libraries and tooling, with competition between cloud providers, startups and the large systems vendors.Next generation architectures will use CXL3.0 switches to connect processing nodes, pooled memory and I/O resources into very large coherent fabrics within a rack, and use Ethernet between racks. I call this a Petalith architecture, and it will replace Infiniband and other specialized interconnects inside the rack.Clouds using Ethernet that are multipath optimized using libfabric and features like EFA on AWS are going to be increasingly competitive, and Ethernet will replace other interconnects between racks.\nThis story starts over twenty years ago, when I was a Distinguished Engineer at Sun Microsystems and Shahin Khan asked me to be the Chief Architect for the High Performance Technical Computing team he was running. When I retired from my Amazon VP position earlier this year and was looking for a way to support a consulting, analyst and advisory role, I reached out to Shahin again, and joined him at OrionX.net. Part of the reason is that I\u2019ve always been interested in HPC, but haven\u2019t had the opportunity to focus on it in the intervening years.\n\nSo this November Shahin and I went to SC22 in Dallas TX together, as analysts, and started out in the media briefing event where the latest Top500 Report was revealed and discussed. Jack Dongarra talked about the scores, and pointed out the low efficiency on some important workloads. In the discussion, there was quite a lot of analysis provided, but no mention of interconnect trends, so I asked whether the fix for efficiency needed better interconnects, and got the answer yes, and that they had some slides on interconnect trends but hadn\u2019t included them in the deck. The next day Jack presented his Turing Award Lecture as the keynote for the event (HPCwire has a good summary of the whole talk), and later in the the week he was on a panel discussion on \u201cReinventing HPC\u201d where he repeated the point. Here are two of his slides:\nHPCG Top 10\u200a\u2014\u200ascreenshot from YouTube of Jack Dongarra\u2019s Turing Award\u00a0lecture\nThe Top500 Linpack results are now led by the Oak Ridge National Labs HP/Cray Frontier system at over an exaflop for 64bit floating point dense matrix factorization, but there are many important workloads represented by the HPCG conjugate gradient benchmark, where Frontier only gets 14.1 petaflops, which is 0.8% of peak capacity. HPCG is led by Japan\u2019s RIKEN Fugaku system at 16 petaflops, which is 3% of it\u2019s peak capacity. This is almost four times better on a relative basis, but even so, it\u2019s clear that most of the machine\u2019s compute resources are idle. In comparison, for Linpack Frontier operates at 68% of peak capacity. Most of the top supercomputers are similar to Frontier, they use AMD or Intel CPUs, with GPU accelerators, and Cray Slingshot or Infiniband networks in a Dragonfly+ configuration. Fugaku is very different, it uses an ARM CPU with a vector processor, connected via a 6D torus. The vector processor avoids some memory bottlenecks, and the interconnect helps as well. Over time we are seeing more applications with more complex physics and simulation characteristics so HPCG is becoming more relevant as a benchmark.\nSummary of types of applications for supercomputers\nWe spent most of the conference walking around the expo and talking to people, rather than attending technical sessions, and asked several people why other vendors weren\u2019t copying Fugaku, and what improvements in architecture were on the horizon. Some people said that they had tried pitching that kind of architecture, but customers wanted the Frontier style systems. Some people see the choice as between specialized and more off the shelf technology, and that leveraging the greater investment in off the shelf components was the way to go in the long term. In recent years GPU development that was originally funded by the gaming market (and ended up being used as a supercomputer accelerator) has been dominated by use in the AI/ML accelerator market. However the latest AI/ML optimized GPUs are reducing support for 64bit floating point in favor of 8, 16, and 32bit options that provide better performance for training and inference workloads. This is a worrying trend for the HPC supercomputer market, and while a new HPL-MxP mixed-precision benchmark shows that it\u2019s possible to leverage lower precision floating point, there\u2019s a lot of existing code that needs 64bit.\n\nDuring the Reinventing HPC panel session, there was some discussion of the cost of designing custom processors for HPC, and a statement that it was too high, in the hundreds of millions of dollars. However Shahin and I think this cost is more likely to be in the $20\u201340M range. Startups like SiPearl and Esperanto are bringing custom chips to market without raising hundreds of millions of dollars. There is a trend in the industry where Apple, Amazon, Google and others are now designing their own CPUs and GPUs, and NVIDIA has added the ARM based Grace CPU to its Hopper GPU in its latest generation. The emergence of chiplet technology also allows higher performance and integration without having to design every chip from scratch. If we extrapolate this trend, it seems likely that the cost of creating a custom HPC oriented CPU/GPU chiplet design\u200a\u2014\u200awith extra focus on 64bit floating point and memory bandwith, or whatever is needed, at relatively low volume, is going to come into reach. To me this positions Fugaku as the first of a new mainstream, rather than a special purpose system.\n\nI have always been particularly interested in the interconnects and protocols used to create clusters, and the latency and bandwidth of the various offerings that are available. I presented a keynote for Sun at Supercomputing 2003 in Phoenix Arizona and included the slide shown below.\nInterconnects slide from Adrian\u2019s Supercomputing 2003 Sun\u00a0Keynote\nI will come back to this diagram and update it to today later in this story, but the biggest difference is going to be bandwidth increases, as latency hasn't improved much over the years. The four categories still make sense: kernel managed network sockets, user mode message passing libraries, coherent memory interfaces, and on-chip communication.\n\nIf we look at Ethernet to start with, in the last 20 years we\u2019ve moved from the era of 1Gbit being common and 10Gbit being the best available to 100Gbit being common, with many options at 200Gbit, 400Gbit and and a few at 800GBit. The latency of Ethernet has been optimized\u200a\u2014\u200aThe HP/Cray Slingshot used in Frontier is a heavily customized 200Gbit Ethernet based interconnect, with both single digit microseconds minimum latency, and reduced long tail latency on congested fabrics. AWS has invested in optimizing Ethernet for HPC via the Elastic Fabric Adaptor (EFA) option, and the libfabric library. While the minimum latency is around 15\u201320us (which is far better than standard Ethernet), they deliver packets as a swarm across multiple paths out of order using their Scalable Reliable Datagram (SRD) protocol and maintain a low tail latency for real world workloads. AWS over-provisions its network to handle a wide variety of multi-tenant workloads, so there tend to be more routes between nodes than in a typical enterprise datacenter, and AWS supports HPC cluster oriented topologies. Many HPC workloads synchronize work on a barrier, and work much better if there\u2019s a consistently narrow latency distribution without a long tail. Most other interconnects operate in-order on a single path and if anything delays or loses one packet, the whole message is delayed. So while EFA has higher minimum latency for small transfers, it often has lower and more consistent maximum latency for larger transfers than other approaches.\n\nTwenty years ago there was a variety of commercial interconnects like Myrinet and several Infiniband vendors and chipsets, over the years they consolidated down to Mellanox which is now owned by NVIDIA, and OmniPath which was sold to Cornelis by Intel. Part of the attraction of Infiniband is that it\u2019s accessed from a user mode library like MPI, rather than a kernel call. The libfabric interface brings this to Ethernet, but there is still a latency advantage to Infiniband, which is around 1 microsecond minimum latency end to end on current systems. The minimum latency hasn\u2019t changed much over 20 years, but Infiniband runs at 400Gbits/s nowadays and the extra bandwidth means that latency for large amounts of data has greatly reduced. However the physical layer is the same nowadays for Infiniband and Ethernet, so the difference is in the protocol and the way switches are implemented.\n\nThe most interesting new development this year is that the industry has consolidated several different next generation interconnect standards around Compute Express Link\u200a\u2014\u200aCXL, and the CXL3.0 spec was released in mid-2022.\n\nCXL3.0 doubles the speed and adds a lot of features to the existing CXL2.0 spec, which is starting to turn up as working silicon like this 16 port CXL2.0 switch from XConn shown in the Expo. Two of each of the small plugs makes up a connection, and the two large edge connectors at the top are to take memory or other CXL cards.\n\nCXL is a memory protocol, as shown in the third block in my diagram from 2003, and provides cache coherent latency around 200ns, and up to 2 meters maximum distance. This is just about enough to cable together systems within a rack into a single CXL3.0 fabric. It has wide industry support, CXL3.0 is going to be built into future CPU designs from ARM, and earlier versions of CXL are integrated into upcoming CPUs from Intel and others. The physical layer and connector spec for CXL3.0 is the same as PCIe 6.0, and in many cases the same silicon will talk both protocols, so that the same interface can be used to connect conventional PCIe I/O devices, or can use CXL to talk to I/O devices and pooled or shared memory banks.\n\nThe capacity of each CXL3.0 port is x16 bits wide at 64GTs which works out at 128GBytes/s in each direction, total 256GBytes/s. CPUs can be expected to have two or four ports. Each transfer is a 256byte flow control unit (FLIT) that contains some error check and header information, and over 200 bytes of data.\n\nThe way CXL2.0 can be used is to have pools of shared memory and I/O devices behind a switch, then to allocate capacity as needed from the pool. With CXL3.0 the memory can also be setup as coherent shared memory and used to communicate between CPUs. It also allows switches to be cascaded.\n\nI updated my latency vs. bandwith diagram and included NVIDIA\u2019s NVLINK interconnect that they use to connect GPUs together, it\u2019s 300\u2013900GBytes/s but they don\u2019t specify minimum latency, so I assume it\u2019s similar to Infiniband. It\u2019s not a cache coherent memory protocol like CXL, and it\u2019s optimized to run over a shorter distance, mostly within a single chassis.\n\nComparing CXL3.0 to Ethernet, Slingshot and Infiniband, it\u2019s lower latency, and higher bandwidth, but limited to connections inside a rack. The way the current Slingshot networks are setup in Dragonfly+ configurations is that a group of CPU/GPU nodes in a rack are fully connected via switch groups, and there are relatively fewer connections between between switch groups. A possible CXL3.0 architecture could use it to replace the copper wired local connections, and use Ethernet for the longer distance fiber optic connections between racks. Unlike Dragonfly, where the local clusters have a fixed configuration of memory, CPU, GPU and connections, the flexibility of a CXL3.0 fabric allows the cluster to be configured with the right balance of each for a specific workload. I\u2019ve been calling this architecture Petalith. Each rack-scale fabric is a much larger \u201cpetal\u201d that contains hundreds of CPUs and GPUs, tens to hundreds of terabytes of memory, and a cluster of racks connected by many 800Gbit Ethernet links could contain a petabyte of RAM.\n\nWhen I asked some CXL experts whether they saw CXL3.0 replacing local interconnects within a rack, they agreed that this was part of the plan. However when I asked them what the programming model would be, is it message passing with MPI, or shared memory with OpenMP, they didn\u2019t have an answer and it appears that topic hadn\u2019t been discussed. There are several different programming model approaches I can think of, and I think it\u2019s also worth looking at the Twizzler memory oriented operating system work taking place at UC Santa Cruz. This topic needs further research and additional blog posts!\n\nThis should all play out over the next 2\u20133 years, I expect roadmap level architectural concepts along these lines at SC23, first prototypes with CXL3.0 silicon at SC24, and hopefully some interesting HPCG results at SC25. I\u2019ll be there to see what happens, and to see if Petalith catches on\u2026",
        "metadata": {
          "word_count": 2287,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This post started out as a reaction to attending SC22 after about 20 years away from the HPC mainstream. In early January a related paper was published by Satoshi Matsuoka et. al. on Myths and Legends of High Performance Computing. Luckily I think we are in general agreement on the main points. Next generation architectures will use CXL3.0 switches to connect processing nodes, pooled memory and I/O resources into very large coherent fabrics within a rack. I call this a Petalith architecture, and it will replace Infiniband and other specialized interconnects inside the rack. The Top500 Linpack results are now  but haven\u2019t had the opportunity to focus on it in the intervening years. Shahin and I went to SC22 in Dallas TX together, as analysts, and started out in the media briefing event where the latest Top500 Report was revealed and discussed. Jack Dongarra talked about the scores, and pointed out the low efficiency on some important workloads. HPCG is led by Japan\u2019s RIKEN Fugaku system at 16 petaflops. Most of the top supercomputers are similar to Frontier. Fugaku uses an ARM CPU with a vector processor, connected via a 6D torus. HPCG is becoming more relevant as a benchmark. Some people see the choice as between specialized and more off the shelf technology. In recent years GPU development that was originally funded by the gaming market has been dominated by use in the AI/ML market. A new HPL-MxP mixed-precision benchmark shows that it\u2019s possible to leverage lower precision floating point. There is a trend in the industry where Apple, Amazon, Google and others are now designing their own CPUs and GPUs. If we extrapolate this trend, it seems likely that the cost of creating a custom HPC oriented CPU/GPU chiplet design is going to come into reach. Fugaku is the first of a new mainstream, rather than a special purpose system. The HP/Cray Slingshot is a heavily customized 200Gbit Ethernet based interconnect, with both single digit microseconds minimum latency, and reduced long tail latency on congested fabrics. While the minimum latency is around 15\u201320us (which is far better than standard Ethernet), they deliver packets as a result of the Scalable Reliable Datagram protocol. Many HPC workloads synchronize work on a barrier, and work much better if there\u2019s a consistently narrow latency distribution without a long tail. Most other interconnects operate in-order on a single path and if anything delays or loses one packet, the whole message is delayed. The industry has consolidated several different next generation interconnect standards around Compute Express Link (CXL) The CXL3.0 spec was released in mid-2022 and doubles the speed and adds a lot of features to the existing CXL2.0. The capacity of each CXL3.0 port is x16 bits wide at 64GTs. This works out at 128GBytes/s in each direction, total 256Gbytes/s. Each transfer is a 256byte flow control unit (FLIT) With CXL3.0 the memory can also be setup as coherent shared memory. It also allows switches to be cascaded. ave pools of shared memory and I/O devices behind a switch, then to allocate capacity as needed from the pool. A CXL3.0 fabric allows the cluster to be configured with the right balance of each for a specific workload. Unlike Dragonfly, where the local clusters have a fixed configuration of memory, CPU, GPU and connections, the flexibility of a CXL 3.0 Fabric allows it to be more flexible. This should all play out over the next 2\u20133 years, I expect roadmap level architectural concepts along these lines at SC23, first prototypes with CXL3.0 silicon at SC24, and hopefully some interesting HPCG results at SC25. I\u2019ll be there to see what happens, and to see if Petalith catches on\u2026"
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_03ec274d",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thoughts from 20 years ago Help I ve lost my memory Sunworld Online",
      "source": "Medium Archive",
      "published_date": "2016-06-06",
      "url": "https://medium.com/@adrianco/2016-06-06_Thoughts-from-20-years-ago---Help--I-ve-lost-my-memory--Sunworld-Online-7add5f7cfb66",
      "content": {
        "text": "Thoughts from 20 years ago\u200a\u2014\u200aHelp! I\u2019ve lost my memory! Sunworld\u00a0Online\n\n[Just for fun, posting this for the third time]\n\nOriginally published in Unix Insider 10/1/95\u00a0Stripped of adverts, url references fixed and comments added to bring it up to date ten years later in 2006.\n\nDear Adrian, After a reboot I saw that most of my computer\u2019s memory wasfree, but when I launched my application it used up almost all thememory. When I stopped the application the memory didn\u2019t come back!Take a look at my vmstat output:\n% vmstat 5procs memory page disk faults cpur b w swap free re mf pi po fr de sr s0 s1 s2 s3 in sy cs us sy id\nThis is before the program starts:\n0 0 0 330252 80708 0 2 0 0 0 0 0 0 0 0 1 18 107 113 0 1 990 0 0 330252 80708 0 0 0 0 0 0 0 0 0 0 0 14 87 78 0 0 99\nI start the program and it runs like this for a while:\n0 0 0 314204 8824 0 0 0 0 0 0 0 0 0 0 0 414 132 79 24 1 740 0 0 314204 8824 0 0 0 0 0 0 0 0 0 0 0 411 99 66 25 1 74\nI stop it, then almost all the swap space comes back, but the free memory does not:\n0 0 0 326776 21260 0 3 0 0 0 0 0 0 1 0 0 420 116 82 4 2 950 0 0 329924 24396 0 0 0 0 0 0 0 0 0 0 0 414 82 77 0 0 1000 0 0 329924 24396 0 0 0 0 0 0 0 0 2 0 1 430 90 84 0 1 99\nI checked that there were no application processes running. It looks like a huge memory leak in the operating system. How can I get my memory back?\u200a\u2014\u200aRAMless in Ripon\n\nUpdate\n\nThis remains one of the most frequently asked questions of all time. The original answer is still true for many Unix variants. However whilewriting his book on Solaris Internals, Richard McDougall worked outhow to fix Solaris to make it work better, and to make this apparentproblem go away. The result was one of the most significantperformance improvements in Solaris 8, but the first edition of hisbook was written before Solaris 8 came out, so doesn\u2019t describe thefix!\n\nThe short answer\n\nLaunch your application again. Notice that it starts up more quickly than it did the first time, and with less disk activity. The application code and its data files are still in memory, even though they are not active. The memory they occupy is not \u201cfree.\u201d If you restart the same application it findsthe pages that are already in memory. The pages are attached to the inode cache entries for the files. If you start a different application, and there is insufficient free memory, the kernel will scan for pages that have not been touched for a long time, and \u201cfree\u201d them. Once you quit the first application, the memory it occupies is not being touched, so it will be freed quickly for use by other applications.\n\nIn 1988, Sun introduced this feature in SunOS 4.0. It still applies to all versions of Solaris 1 and 2. The kernel is trying to avoid disk reads by caching as many files as possible in memory. Attaching to a page in memory is around 1,000 times faster than reading it in from disk. The kernel figures that you paid good money for all of that RAM, so it will try to make good use of it by retaining files you might need.\n\nSince Solaris 8, the memory in the file cache is actually also on the free list, so you do see vmstat free memory reduce when you quit a program. You also should expect large amounts of file I/O to cause high scan rates on older Solaris releases, and for there to be no scanning at all on Solaris 8 systems. If Solaris 8 scans at all, then it has truly run out of memory and is overloaded.\n\nBy contrast, Memory leaks appear as a shortage of swap space after the misbehaving program runs for a while. You will probably find a process that has a larger than expected size. You should restart the program to free up the swap space, and check it with a debugger that offers a leak-finding feature (run it with the libumem version of the malloc library that instruments memory leaks).\n\nThe long (and technical) answer\n\nTo understand how Sun\u2019s operating systems handle memory, I will explain how the inode cache works, how the buffer cache fits into the picture, and how the life cycle of a typical page evolves as the system uses it for several different purposes.\n\nThe inode cache and file data caching\n\nWhenever you access a file, the kernel needs to know the size, the access permissions, the date stamps and the locations of the data blocks on disk. Traditionally, this information is known as the inode for the file. There are many filesystem types. For simplicity I will assume we are only interested in the Unix filesystem (UFS) on a local disk. Each filesystem type has its own inode cache.\n\nThe filesystem stores inodes on the disk; the inode must be read into memory whenever an operation is performed on an entity in the filesystem. The number of inodes read per second is reported as iget/s by the sar-a command. The inode read from disk is cached in case it is needed again, and the number of inodes that the system will cache is influenced by a kernel parameter called ufs_ninode. The kernel keeps inodes on a linked list, rather than in a fixed-size table.\n\nAs I mention each command I will show you what the output looks like. In my case I\u2019m collecting sar data automatically using cron.sar, which defaults to reading the stored data for today. If you have no stored data, specify a time interval and sar will show you current activity.\n% sar -aSunOS hostname 5.4 Generic_101945-32 sun4c 09/18/9500:00:01 iget/s namei/s dirbk/s01:00:01 4 6 0\nAll reads or writes to UFS files occur by paging from the filesystem. All pages that are part of the file and are in memory will be attached to the inode cache entry for that file. When a file is not in use, its data is cached in memory, using an inactive inode cache entry. When the kernel reuses an inactive inode cache entry that has pages attached, it puts the pages on the free list; this case is shown by sar -g as %ufs_ipf. This number is the percentage of UFS inodes that were overwritten in the inode cache by iget and that had reusable pages associated with them. The kernel flushes the pages, and updates on disk any modified pages. Thus, this %ufs_ipf number is the percentage of igets with page flushes. Any non-zero values of %ufs_ipf reported by sar -g indicate that the inode cache is too small for the current workload.\n% sar -gSunOS hostname 5.4 Generic_101945-32 sun4c 09/18/9500:00:01 pgout/s ppgout/s pgfree/s pgscan/s %ufs_ipf01:00:01 0.02 0.02 0.08 0.12 0.00\nFor SunOS 4 and releases up to Solaris 2.3, the number of inodes that the kernel will keep in the inode cache is set by the kernel variable ufs_ninode. To simplify: When a file is opened, an inactive inode will be reused from the cache if the cache is full; when an inode becomes inactive, it is discarded if the cache is over-full. If the cache limit has not been reached then an inactive inode is placed at the back of the reuse list and invalid inodes (inodes for files that longer exist) are placed at the front for immediate reuse. It is entirely possible for the number of open files in the system to cause the number of active inodes to exceed ufs_ninode; raising ufs_ninode allows more inactive inodes to be cached in case they are needed again.\n\nSolaris 2.4 uses a more clever inode cache algorithm. The kernel maintains areuse list of blank inodes for instant use. The number of active inodes is no longer constrained, and the number of idle inodes (inactive but cached in case they are needed again) is kept between ufs_ninode and 75 percent of ufs_ninode by a new kernel thread that scavenges the inodes to free them and maintains entries on the reuse list. If you use sar -v to look at the inode cache, you may see a larger number of existing inodes than the reported \u201csize.\u201d\n% sar -vSunOS hostname 5.4 Generic_101945-32 sun4c 09/18/9500:00:01 proc-sz ov inod-sz ov file-sz ov lock-sz01:00:01 66/506 0 2108/2108 0 353/353 0 0/0\nBuffer cache\n\nThe buffer cache is used to cache filesystem data in SVR3 and BSD Unix. In SunOS 4, generic SVR4, and Solaris 2, it is used to cache inode, indirect block, and cylinder group blocks only. Although this change was introduced in 1988, many people still incorrectly think the buffer cache is used to hold file data. Inodes are read from disk to the buffer cache in 8-kilobyte blocks, then the individual inodes are read from the buffer cache into the inode cache.\n\nLife cycle of a typical physical memory page\n\nThis section provides additional insight into the way memory is used. The sequence described is an example of some common uses of pages; many other possibilities exist.\n\n1. Initialization\u200a\u2014\u200aA page is born\u00a0 When the system boots, it forms all free memory into pages, and allocates a kernel data structure to hold the state of every page in the system.\u00a0 2. Free\u200a\u2014\u200aAn untouched virgin page\u00a0 All the memory is put onto the free list to start with. At this stage the content of the page is undefined.\u00a0 3. ZFOD\u200a\u2014\u200aJoining an uninitialized data segment\u00a0 When a program accesses data that is preset to zero for the very first time, a minor page fault occurs and a Zero Fill On Demand (ZFOD) operation takes place. The page is taken from the free list, block-cleared to contain all zeroes, and added to the list of anonymous pages for the uninitialized data segment. The program then reads and writes data to the page.\u00a0 4. Scanned\u200a\u2014\u200aThe pagedaemon awakes\u00a0 When the free list gets below a certain size, the pagedaemon starts to look for memory pages to steal from processes. It looks at all pages in physical memory order; when it gets to the page, the page is synchronized with the memory management unit (MMU) and a reference bit is cleared.\u00a0 5. Waiting\u200a\u2014\u200aIs the program really using this page right now?\u00a0 There is a delay that varies depending upon how quickly the pagedaemon scans through memory. If the program references the page during this period, the MMU reference bit is set.\u00a0 6. Pageout Time\u200a\u2014\u200aSaving the contents\u00a0 The pageout daemon returns and checks the MMU reference bit to find that the program has not used the page so it can be stolen for reuse. The pagedaemon checks to see if anything had been written to the page; if it contains no data, a page-out occurs. The page is moved to the pageout queue and marked as I/O pending. The swapfs code clusters the page together with other pages on the queue and writes the cluster to the swap space. The page is then free and is put on the free list again. It remembers that it still contains the program data.\u00a0 7. Reclaim\u200a\u2014\u200aGive me back my page!\u00a0 Belatedly, the program tries to read the page and takes a page fault. If the page had been reused by someone else in the meantime, a major fault would occur and the data would be read from the swap space into a new page taken from the free list. In this case, the page is still waiting to be reused, so a minor fault occurs, and the page is moved back from the free list to the program\u2019s data segment.\u00a0 8. Program Exit\u200a\u2014\u200aFree again\u00a0 The program finishes running and exits. The data segments are private to that particular instance of the program (unlike the shared-code segments), so all the pages in the data segment are marked as undefined and put onto the free list. This is the same state as Step 2.\u00a0 9. Page-in\u200a\u2014\u200aA shared code segment\u00a0 A page fault occurs in the code segment of a window system shared library. The page is taken off the free list, and a read from the filesystem is scheduled to get the code. The process that caused the page fault sleeps until the data arrives. The page is attached to the inode of the file, and the segments reference the inode.\u00a0 10. Attach\u200a\u2014\u200aA popular page\u00a0 Another process using the same shared-library page faults in the same place. It discovers that the page is already in memory and attaches to the page, increasing its inode reference count by one.\u00a011. COW\u200a\u2014\u200aMaking a private copy\u00a0 If one of the processes sharing the page tries to write to it, a copy-on-write (COW) page fault occurs. Another page is grabbed from the free list, and a copy of the original is made. This new page becomes part of a privately mapped segment backed by anonymous storage (swap space) so it can be changed, but the original page is unchanged and can still be shared. Shared libraries contain jump tables in the code that are patched, using COW as part of the dynamic linking process.\u00a0 12. File Cache\u200a\u2014\u200aNot free\u00a0 The entire window system exits, and both processes go away. This time the page stays in use, attached to the inode of the shared library file. The inode is now inactive but will stay in the inode cache until it is reused, and the pages act as a file cache in case the user is about to restart the window system again. The change made in Solaris 8 was that the file cache is the tail of the free list, and any file cache page can be reused immediately for something else without needing to be scanned first. 13. fsflush\u200a\u2014\u200aFlushed by the sync\u00a0 Every 30 seconds all the pages in the system are examined in physical page order to see which ones contain modified data and are attached to a vnode. The details differ between SunOS 4 and Solaris 2, but essentially any modified pages will be written back to the filesystem, and the pages will be marked as clean. This example sequence can continue from Step 4 or Step 9 with minor variations. The fsflush process occurs every 30 seconds by default for all pages, and whenever the free list size drops below a certain value, thepagedaemon scanner wakes up and reclaims some pages. A recent change in Solaris 10, backported to Solaris 8 and 9 patches, makes fsflush run much more efficiently on machines with very large amounts of memory. However, if you see fsflush using an excessive amount of CPU time you should increase \u201cautoup\u201d in /etc/system from its default of 30s, and you will see fsflush usage reduce proportionately.\n\nNow you know\n\nI have seen this missing-memory question asked about once a month since 1988! Perhaps the manual page for vmstat should include a better explanation of what the values are measuring. This answer is based on some passages from my book Sun Performance and Tuning. The book explains in detail how the memory algorithms work and how to tune them. However the book doesn\u2019t cover the changes made in Solaris 8.\n\nOriginally published at perfcap.blogspot.com.",
        "metadata": {
          "word_count": 2633,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Originally published in Unix Insider 10/1/95\u00a0Stripped of adverts, url references fixed and comments added to bring it up to date ten years later in 2006. The original answer is still true for many Unix variants. Richard McDougall worked out how to fix Solaris to make it work better. Solaris kernel tries to avoid disk reads by caching as many files as possible in memory. If there is insufficient free memory, the kernel will scan for pages that have not been touched for a long time, and \u201cfree\u201d them. Large amounts of file I/O cause high scan rates on older Solaris releases. If Solaris 8 scans at all, then it has truly run out of memory and is overloaded. Memory leaks appear as a shortage of swap space after the misbehaving program runs for a while. The number of inodes read per second is reported as iget/s by the sar-a command. The inode read from disk is cached in case it is needed again. Inodes are kept on a linked list, rather than in a fixed-size table. All reads or writes to UFS files occur by paging from the filesystem. All pages that are part of the file and are in memory will be attached to the inode cache entry for that file. When a file is not in use, its data is cached in memory. When the kernel reuses an inactive inode Cache entry that has pages attached, it puts the pages on the free list. For SunOS 4 and releases up to Solaris 2.3, the number of inodes that the kernel will keep in the inode cache is set by the kernel variable ufs_ninode. When a file is opened, an inactive inode will be reused from the cache. When an inode becomes inactive, it is discarded if the cache is over-full. The number of active inodes is no longer constrained. A new kernel thread scavenges the inodes to free them and maintains entries on the reuse list. Inodes are read from disk to the buffer cache in 8-kilobyte blocks. This section provides additional insight into the way memory is used. vidual inodes are read from the buffer cache into the inode cache. The sequence described is an example of some common uses of pages. When the free list gets below a certain size, the pagedaemon starts to look for memory pages to steal from processes. It looks at all pages in physical memory order; when it gets to the page, the page is synchronized with the memory management unit (MMU) and a reference bit is cleared. The program tries to read the page and takes a page fault. If the page had been reused by someone else in the meantime, a major fault would occur. The page is then free and is put on the free list again. It remembers that it still contains the program data. Shared libraries contain jump tables in the code that are patched, using COW as part of the dynamic linking process. If one of the processes sharing the page tries to write to it, a copy-on-write (COW) page fault occurs. This new page becomes part of a privately mapped segment backed by anonymous storage. Every 30 seconds all the pages in the system are examined in physical page order to see which ones contain modified data and are attached to a vnode. The details differ between SunOS 4 and Solaris 2, but essentially any modified pages will be written back to the filesystem. This example sequence can continue from Step 4 or Step 9 with minor variations. A recent change in Solaris 10, backported to Solaris 8 and 9 patches, makes fsflush run much more efficiently on machines with very large amounts of memory. If you see fsflush using an excessive amount of CPU time you should increase \u201cautoup\u201d in /etc/system from its default of 30s."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_18eba906",
      "kind": "file",
      "subkind": "medium_post",
      "title": "AWS at OSCON 2017 Austin TX",
      "source": "Medium Archive",
      "published_date": "2017-05-10",
      "url": "https://medium.com/@adrianco/2017-05-10_AWS-at-OSCON-2017--Austin-TX-fa4717654b91",
      "content": {
        "text": "AWS at OSCON 2017, Austin\u00a0TX\nAustin locals @erictessmerband playing on the Sunday evening before\u00a0OSCON\n#OSCON2017 @AWSopen awsopen@amazon.com\u200a\u2014\u200asee you there!\n\nThere are about 20 attendees from AWS and Amazon at OSCON this year, including the open source community engagement team, speakers, open source project committers and contributors from the engineering teams, and even some Austin area solutions architects.\n\nAfter several years of attending OSCON as individuals, we are very happy to have AWS be a Diamond Sponsor this year, and to have the AWS logo on the OSCON conference T-shirt.\n\nIn line with the #1 Amazon leadership principle of \u201cCustomer Obsession\u201d we are starting with some \u201cCommunity Obsession\u201d and want to know how we can best help open source communities and open source projects. You can come and chat with us in person in the exhibit hall, where instead of a booth, we\u2019ve setup a lounge, so you can also rest your feet and recharge your devices.\nWednesday 10th-Thursday 11th\u200a\u2014\u200aAWS Lounge\u200a\u2014\u200aEXHIBIT HALL #601\nOn Wednesday evening we\u2019ll be attending a BoF session where we can have a bigger group discussion.\nWEDNESDAY May 10\u20137:00 PM | Room 8BoF: Meet the team at AWS Open Source\u200a\u2014\u200ahow can we work together?\nWe\u2019re also sponsoring Open Container Day on Tuesday and have Arun Gupta and Abby Fuller speaking there.\nTUESDAY May 9\u200a\u2014\u200a11:30 AM | Ballroom GArun Gupta: Service discovery in container orchestrationTUESDAY May 9\u200a\u2014\u200a3:50 PM | Ballroom GAbby Fuller: Flexible container orchestration with Amazon ECS\nOn Thursday, Zaheda is giving a keynote on the people that make open source happen, and I\u2019m presenting a session on the topic of how AWS supports many open source artificial intelligence frameworks, why we\u2019ve decided to put extra effort into Apache MXNet, and an overview of the technology itself. In the afternoon Greg Bulmash from Amazon will show how to use browser based open source tooling to hook up some of the higher level AWS artificial intelligence services into a personal assistant.\nTHURSDAY May 11\u200a\u2014\u200a9:50 AM | | Ballroom DKeynote: Zaheda Bhorat: We the people\u2026are open sourceTHURSDAY May 11\u200a\u2014\u200a11:00 AM | Room 14Adrian Cockcroft: Open Source AI at AWS and Apache MXNetTHURSDAY May 11\u200a\u2014\u200a2:35 PM | Room18 A/BGreg Bulmash: Can you roll your own virtual assistant?\nWe\u2019re very happy to be able to immerse ourselves in some \u201ccommunity obsession\u201d at OSCON and gather input on where we should focus our efforts.",
        "metadata": {
          "word_count": 412,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There are about 20 attendees from AWS and Amazon at OSCON this year. You can come and chat with us in person in the exhibit hall. Instead of a booth, we've setup a lounge, so you can also rest your feet. We\u2019re also sponsoring Open Container Day on Tuesday and have Arun Gupta and Abby Fuller speaking there. On Thursday, Zaheda is giving a keynote on the people that make open source happen, and I\u2019m presenting a session on the topic of how AWS supports many open source artificial intelligence frameworks. We\u2019re very happy to be able to immerse ourselves in some \u201ccommunity obsession\u201d at OSCON and gather input on where we should focus our efforts."
      },
      "tags": [
        "ai",
        "artificial intelligence",
        "aws",
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_file_9c7ed82c",
      "kind": "file",
      "subkind": "medium_post",
      "title": "RISC V the CPU you didn t know you already have",
      "source": "Medium Archive",
      "published_date": "2024-10-23",
      "url": "https://medium.com/@adrianco/2024-10-23_RISC-V---the-CPU-you-didn-t-know-you-already-have--ff2f385f7ec6",
      "content": {
        "text": "RISC-V\u200a\u2014\u200athe CPU you didn\u2019t know you already\u00a0have\u2026\nPhoto taken by Adrian at the RISC-V Summit\u00a0Keynote\nI just attended the RISC-V Summit (Oct 22nd, 2024 at the Santa Clara Convention Center) to get up to date on the latest developments. I\u2019ve been following processor architectures for most of my career, from when I was writing embedded code in the 1980s, through my time as a SPARC specialist at Sun Microsystems, and I gave a keynote at Usenix in 2008 where I talked about how ARM architecture (at that time in embedded and mobile phones only) would eventually end up running enterprise workloads in the datacenter, which it now does.\n\nMost people are aware of the Intel x86 based architecture that has been dominant in desktop and server systems for decades, which has now been joined by the ARM architecture in our mobile devices, in Apple\u2019s current entire product range, cloud instance types like Graviton from AWS, supercomputers like Fugaku, and now the Grace CPU paired with GPUs in modules such as the GH200 and GB200 from NVIDIA. However most people don\u2019t know that NVIDIA is also going to ship a billion or so RISC-V CPU cores in 2024. They are embedded in almost all the chips they make, as local control processors. They are also to be found in many of the computerized \u201cthings\u201d you interact with in your daily life from light bulbs to cars. Why are so many embedded processors based on RISC-V? Surely that\u2019s where ARM came from, so why isn\u2019t everything using ARM?\n\nARM is a technology licensing company that develops generations of CPU architectures aimed at everything from very low power embedded processors through the most powerful enterprise class CPUs. However they own the architecture, so if you want to do something fundamentally different, extend the basic designs for a specific use case, don\u2019t want to pay licensing fees to ARM, or get into legal battles with them, people are looking for alternatives. That\u2019s where RISC-V comes in. It\u2019s an open design developed starting in 2010 by David Patterson\u2019s team at Berkeley University, freely usable, kind of like Apache licensed open source software. You can fork the design and add whatever you want without paying anyone, or you can buy a supported version from a growing range of vendors such as SiFive and Andes. RISC-V ownership was transferred to a foundation in 2015, and it\u2019s growth rates and industry support have picked up in recent years. Mainline Linux support was added in 2022, and the developer tools ecosystem is now mature enough that it\u2019s become just another option alongside ARM. There are plenty of development boards and some RISC-V based laptops. There\u2019s a growing library of standardized extensions to the architecture, and compliance testing to ensure software portability.\n\nOne question that I had is why this didn\u2019t happen with Sun\u2019s SPARC, which was used as the basis of an IEEE standard, and had several commercial and research based implementations. Fujitsu used SPARC for everything from servers to embedded systems like cameras for a long time. One reason is that SPARC was also developed by David Patterson at Berkeley in the 1980s, and RISC-V was that team\u2019s fifth try at a RISC design, moving away from some SPARC unique design decisions like register windows that turned out to be hard to optimize as transistor counts increased and CPU pipelines got more complex. Another reason is that most of the SPARC ecosystem was built around Solaris, rather than Linux, and the failure of Sun to survive and the replacement of Solaris by Linux based systems vendors took SPARC down with it. Success breeds enemies, and a dominant architecture from one vendor will cause the rest of the industry to come together around an alternative. This recurs over the years and RISC-V is driven in part by a similar reaction to the success of ARM, which was in part a reaction to the success of Intel.\n\nAt the show there were many announcements (listed at the end of this post), mostly aimed at the embedded system on a chip (SoC) market. Beyond SoC one of the vendors that I find most interesting is Semidynamics, who have a flexible design for an AI and HPC oriented combined CPU/GPU/NPU (NPU is the AI specific neuron processor) that can be customized to put the silicon budget wherever you want. So for an HPC supercomputer, it could have lots of 64bit floating point vector capability, or for AI optimize for the right balance of FP8 and NPU for training or inference. There are also enterprise server oriented designs from several vendors including Alibaba\u2019s XuanTie division, with performance comparable to leading x86 and ARM CPUs.\n\nThe most interesting architecture extension I saw was CHERI, which provides fine grained memory protection and can be used to implement secure enlaves as part of a larger system.\nPhoto by Adrian of posters about Codasip and Cheri at the RISC-V\u00a0Expo\nRight now RISC-V is a secret ingredient in almost everyone\u2019s daily lives. When will it become a headline ingredient that people notice? I think the key challenge is that RISC-V needs to be so much better or more cost effective than ARM that it ends up as a mainstream processor with it\u2019s own family of cloud instances, laptops etc. It could either be a general purpose processor at a lower cost than ARM or have some novel AI capabilities built into the architecture. This is going to happen first in China, Brazil and Europe, where they don\u2019t want to be dependent on foreign owned technology, and there is significant government investment in RISC-V.\n\nAnnouncements at the RISC-V Summit North America: RISC-V International announced the ratification of the RVA23 Profile Standard. The newly ratified RVA23 Profile is a major release for the RISC-V software ecosystem and will help accelerate widespread implementation among toolchains and operating systems.\n\nRISC-V Ecosystem News\n\nAndes Technology: Andes Announces the AndesCore\u2122 46-Series Family and the 3rd Generation Vector Processor AX46MPV with Matrix Extension\n\nArteris: Arteris and SiFive Deliver Pre-verified Solution for the Datacenter Market\n\nCodasip: Codasip Enables Secure Linux by Donating CHERI RISC-V SDK to the CHERI Alliance\n\nLauterbach GmbH: Lauterbach and Kernkonzept Enable Development of Virtualized RISC-V Systems\n\nMicrochip Technologies: Microchip Expands 64-bit Portfolio with High-Performance, Post-Quantum Security-Enabled PIC64HX Microprocessors\n\nQamcom: Olof Kindgren Published in Nature With Pragmatic and Harvard\n\nSEGGER Microcontroller: SEGGER\u2019s Ozone Offers Enhanced Debugging With RISC-V Semihosting\n\nSiFive: SiFive HiFive Premier P550 Development Boards Now Shipping\n\nTera Pines: Triton Kernel Performance on RISC-V CPU",
        "metadata": {
          "word_count": 1088,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I just attended the RISC-V Summit (Oct 22nd, 2024 at the Santa Clara Convention Center) to get up to date on the latest developments. I\u2019ve been following processor architectures for most of my career, from when I was writing embedded code in the 1980s. Most people are aware of the Intel x86 based architecture that has been dominant in desktop and server systems for decades. ARM is a technology licensing company that develops generations of CPU architectures. However they own the architecture, so if you want to do something fundamentally different, extend the basic designs for a specific use case, people are looking for alternatives. That\u2019s where RISC-V comes in. RISC-V ownership was transferred to a foundation in 2015, and it\u2019s growth rates and industry support have picked up in recent years. Mainline Linux support was added in 2022, and the developer tools ecosystem is now mature enough. RISC-V was developed by David Patterson at Berkeley in the 1980s. Most of the SPARC ecosystem was built around Solaris, rather than Linux. Success breeds enemies, and a dominant architecture from one vendor will cause the rest of the industry to come together around an alternative. The most interesting architecture extension I saw was CHERI, which provides fine grained memory protection and can be used to implement secure enlaves as part of a larger system. There are also enterprise server oriented designs from several vendors including Alibaba\u2019s XuanTie division. RISC-V could be a mainstream processor with it\u2019s own family of cloud instances, laptops etc. This is going to happen first in China, Brazil and Europe, where they don\u2019t want to be dependent on foreign owned technology. Microchip Technologies: Microchip Expands 64-bit Portfolio with High-Performance, Post-Quantum Security-Enabled PIC64HX Microprocessors. SiFive: SiFive HiFive Premier P550 Development Boards Now Shipping."
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "cloud",
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_3cbe0410",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Retire",
      "source": "Medium Archive",
      "published_date": "2022-05-23",
      "url": "https://medium.com/@adrianco/2022-05-23_What-Adrian-Did-Next---Retire-3cb529229646",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aRetire\nMoonrise\u200a\u2014\u200aphoto taken by Adrian Cockcroft\nAfter a long career working at the leading edge of technology, I retire from full time work on June 3rd 2022. I joined Amazon in October 2016, as a VP in AWS Marketing focused on building relationships with customers. I keynoted 20 AWS Summits around the world, presented on technical and management topics at many events, and hired the open source community engagement team. Moving to Amazon Worldwide Sustainability in March 2021, I led sustainability marketing for AWS, invested in the Amazon Sustainability Data Initiative, helped coordinate the rapid growth in sustainability related headcount across AWS, and helped author, launch and promote the Well Architected Pillar for Sustainability.\n\nI learned to code fifty years ago in high school, graduated in Applied Physics and Electronics from The City University, London forty years ago, and worked for Cambridge Consultants Ltd, Sun Microsystems, eBay, Netflix and Battery Ventures before joining Amazon.\n\nI\u2019m going to take a break, re-design the IoT systems for my house near Monterey California, play guitar more, spend time with my parents in the UK, travel to some new places, and go on some long driving tours with my wife Laurel. Please keep in touch via twitter @adrianco and connect via Linked in.\n\nAfter taking a break, I\u2019ll think about some part time advisory, board level, speaking and investing roles in the enterprise sustainability space that I\u2019ve been focused on for the last few years.\n\nI\u2019ll be covering the stories of my time at Cambridge Consultants Ltd, Sun Microsystems, eBay, Netflix, Battery Ventures and Amazon in future blog posts in this series on What Adrian Did Next\u2026",
        "metadata": {
          "word_count": 280,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian Cockcroft will retire from full time work on June 3rd 2022. I joined Amazon in October 2016, as a VP in AWS Marketing. I keynoted 20 AWS Summits around the world, presented on technical and management topics at many events. I\u2019m going to take a break, re-design the IoT systems for my house near Monterey California, play guitar more, spend time with my parents in the UK, travel to some new places, and go on some long driving tours. I\u2019ll be covering the stories of my time at Cambridge Consultants Ltd, Sun Microsystems, eBay, Netflix, Battery Ventures and Amazon."
      },
      "tags": [
        "aws",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_file_5499272a",
      "kind": "file",
      "subkind": "medium_post",
      "title": "I also have a Status Quo bias whatever you want",
      "source": "Medium Archive",
      "published_date": "2018-03-10",
      "url": "https://medium.com/@adrianco/2018-03-10_I-also-have-a-Status-Quo-bias---whatever-you-want------540e6b7a6194",
      "content": {
        "text": "I also have a Status Quo bias\u200a\u2014\u200awhatever you want\u2026\u00a0:-)\n\nhttps://youtu.be/krK7Q49o6uA",
        "metadata": {
          "word_count": 13,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f2dfdbb1",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Excellent work thanks for this",
      "source": "Medium Archive",
      "published_date": "2020-10-04",
      "url": "https://medium.com/@adrianco/2020-10-04_Excellent-work--thanks-for-this--442e228015c8",
      "content": {
        "text": "Excellent work, thanks for this. The 10Gbit network seems to be a bottleneck, what happens if you run pod to pod on the same node, or on a 100Gbit network?",
        "metadata": {
          "word_count": 30,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The 10Gbit network seems to be a bottleneck. What happens if you run pod to pod on the same node, or on a 100Gbitnetwork?"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_80fbd79d",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Measuring Carbon is Not Enough Unintended Consequences",
      "source": "Medium Archive",
      "published_date": "2023-11-13",
      "url": "https://medium.com/@adrianco/2023-11-13_Measuring-Carbon-is-Not-Enough---Unintended-Consequences-80dccef0d8c5",
      "content": {
        "text": "Measuring Carbon is Not Enough\u200a\u2014\u200aUnintended Consequences\nPhoto taken by Adrian in Pompeii\u200a\u2014\u200aRoman cherubs working on measuring their carbon footprint\nThere are plenty of problems in the way of measuring the carbon footprint of an IT workload, and I\u2019ve been leading a Green Software Foundation project to make it easier to get better measurements for cloud provider based workloads. Along the way I\u2019m learning a lot about the subtleties of measurement and reduction of carbon, and there\u2019s some complex and counter-intuitive nuances that I\u2019m going to try to explore in this post. I\u2019ve linked to several papers and blog posts along the way.\n\nLet\u2019s start by assuming that you have figured out a way to estimate the energy consumption and carbon footprint for a workload that is sensitive enough to register changes in the way you run the workload. That\u2019s quite a big assumption, but I want to focus on what happens next. You want to reduce the carbon footprint of the workload, and need to decide what to do.\n\nThe simplest thing to do is nothing. The electricity you use is gradually being cleaned up as solar and wind replaces coal and gas, and the embodied carbon used to make and deliver the machines you use is also getting better over time, as carbon optimizations take effect in supply chains. You trust your suppliers to solve the problem for you, but that could take a long time. This is basically the position of the major cloud providers, they will all take care of it for you by 2030 or so.\n\nBut you want to *do something* to help sooner. The first thing to understand is that doing things consumes time and energy and has a carbon footprint of its own. For a sense of proportion, a single economy flight between the USA and Europe is order-of-magnitude a ton of carbon. The carbon footprint of a large rack mounted server is order-of-magnitude a ton of carbon a year. Buildings, food and transport have a much bigger carbon footprint than IT globally.\n\nYou could try to estimate the average annual carbon footprint of employees at your company to come up with a carbon per hour per employee number, then total up the time spent discussing what to do via meetings and emails, and the time spent working a change. Like any optimization project, you need to figure out the return on investment (ROI) for different project options and pick the one that looks like it will have a worthwhile carbon reduction ROI. For many companies, that high ROI project may have nothing to do with IT, but for now lets keep focused on sustainable development and operations practices, which tend to dominate for \u201cdigital\u201d product companies.\n\nCarbon Optimization\n\nThe simplest carbon optimization projects focus on doing less work and using less of the resources that contribute to the carbon footprint. These also usually save money. However, while the measured carbon attributed to the workload goes down, the resources you stopped using are still there.\n\nIn the simplest case, you have a growing workload, and you optimize it to run more efficiently so that you don\u2019t need to buy or rent additional hardware, so your carbon footprint stays the same, but the carbon per transaction or operation is going down.\n\nHowever if you have a containerized workload that you are measuring with Kepler, or a cloud instance based workload that is running in virtual machines, you measure your carbon footprint based on how many and how big the containers and instances are, and how busy their CPUs are. CPUs consume power at idle, and due to various monitoring agents, operating daemons, application background task overhead etc. etc. they are never really idle. A rough guide if you don\u2019t have any better data is that with no traffic to a system it will be 10% utilization and use 30% of peak power, 25% utilization uses 50% of peak power, and at 50% utilization it uses 75% of peak power. So you get a small energy benefit for reducing average CPU utilization, but you get a bigger benefit for increasing utilization by downsizing the container or instance or using fewer of them, and that also reduces the share of embodied carbon attributed to the workload. There are some interesting projects based on Kepler measurements that try to optimize pod scheduling and vertical scaling to reduce power consumption.\n\nThe best way to increase utilization is to spread out or prevent load peaks, so that the system runs without problems at higher average loads. Load peaks can be caused by inefficient initialization code at startup, cron jobs, traffic spikes, or retry storms. I\u2019ve written before about how to tune out retry storms. For larger horizontally scaled workloads autoscaling by adding and removing instances from the service is the best way to increase average utilization. This can be improved by reducing start up time, avoiding the need to drain traffic from instances before they are removed, and predictive autoscaling that cuts and adds load faster than reactive autoscaling.\n\nConsequences\n\nEven though the carbon attributed to your workload has gone down, that may not be the outcome you really wanted. If you want to \u201csave the planet\u201d then the outcome you want is that the consequences of your optimization project reduce the total carbon emitted for the planet, not just for your own workload. This brings in a topic that is important and poorly understood, the difference between attributional and consequential carbon accounting. Both are well documented and standardized, but most of the discussion occurs around attributional accounting.\n\nFor an example of an unintended consequence, let\u2019s say the result of your optimization project is spare capacity at a cloud provider. Then that capacity is then offered on the spot market, which lowers the spot price, and someone uses those instances for an opportunistic CPU intensive workload like crypto or AI training, which they only run when the cost is very low. The end result is that capacity could end up using more power than when you were leaving it idle, and the net consequence is an increase in carbon emissions for that cloud region. Alternatively you could argue that eventually the cloud provider notices an increase in spare capacity and delays purchasing a capacity increase to the region, which decreases carbon emissions. It\u2019s clear that one problem with consequential models is that the choices of what to include and the timescales to take into account are somewhat arbitrary, include a lot of external information that you may not have access to, and are likely to be challenged if you publish your results. The main requirement in the Green House Gas Protocol is that the boundary of the consequential model is clearly defined and documented as part of carbon reduction project accounting.\n\nThe other area where this gets complicated is related to the sources of renewable energy, how they are attributed and what the consequences of changes could be. The normal attribution model is to use an average carbon intensity for a grid region that is calculated over an hour, a month or a year. The carbon intensity is a mixture of carbon free sources like solar, wind, battery, geothermal, hydro and nuclear, versus carbon intensive gas, oil, coal. Carbon intensity tends to reduce as solar and wind are added, but also reduces as coal and oil is replaced by gas. The lowest cost (usually solar, wind and gas) and cleanest sources are preferred, but when peak loads arrive, it\u2019s currently common to use gas to supply the peak. In some grid regions there is too much solar and wind at times, so the sources are curtailed, and some clean energy isn\u2019t generated. This explains why you might see a wind farm that isn\u2019t operating, on a windy day.\n\nAnother complication is that there are two ways to calculate carbon intensity for a grid region, the production intensity, and the consumption intensity, as described in another ElectricityMaps blog post. The production intensity is based on the energy generated within that region and is easier to figure out, the consumption intensity takes into account that significant amounts of energy can be imported and exported across regions, and it\u2019s a better measure of the carbon intensity of the energy you are using. Google\u2019s carbon data for each region is based on an accumulation of hourly location based consumption based carbon intensity measurements (sourced from ElectricityMaps) over a year. That number is different to the average carbon intensity for the year (also reported by ElectricityMaps), because if Google uses less energy when the carbon intensity is worse, or more when it is better, that improves their hourly weighted average for the year. Of course, Azure and AWS use different methodologies to calculate their carbon footprint, which are currently based on monthly averages of the energy they purchase (using the market method, rather than what they use via the location method). There is also some amount of selling of renewable energy credits across borders, in case this wasn\u2019t complicated enough.\n\nGiven the above, what is the consequence of using more or less energy? This is called the marginal emissions. In many grid regions an incremental kilowatt will always be supplied by a gas powered \u201cpeaker-plant\u201d, even if there\u2019s substantial carbon free energy in the average mix. In some grid regions batteries are beginning to be used, and if solar or wind curtailment is happening, then it\u2019s possible that the consequence is carbon free. It also depends what time of day or night you use the energy, and how close to the peak load the grid region is when you increase your load. The marginal emisions in real time can be obtained via a commercial service from Watttime, but is not freely available to a cloud end user trying to deciding what to run where, when, to minimize carbon. There is also an argument as described in this ElectricityMaps blog that it can be better to use the average carbon intensity as that drives better long term decisions. Part of the goal of the GSF Real Time Cloud project is to make more of this information available in standard ways, and to provide an interface that could eventually support better decisions on an hour by hour basis.\n\nThat\u2019s all pretty complicated, but what are you going to try and do to make a difference? Moving workloads around the world to reduce their carbon footprint sounds like a good idea on the face of it, but can backfire if you keep moving them as I described in a previous \u201cDon\u2019t follow the sun\u201d blog post. Picking a lower carbon region over a high carbon region for a workload and staying there is a better option. However the cleanest regions may cost more or be further away from your customers. In addition if we increase the energy use in a clean grid region, there\u2019s less clean energy there to export to a nearby dirty region, so the gobal consequence may be less clean than you hoped. That\u2019s a bit of an edge case though, so in general it is good to site workloads in ways that get cloud providers to grow their clean regions faster than their dirty ones.\n\nAs an example, many of us now have electric cars and solar panels. When should you charge your car? It might be cheaper at night when the load is low, and time-of-use electric costs are reduced, but there\u2019s no solar, so the marginal grid mix is probably higher carbon. I charge mine during the day, when the sun is out and my own solar panels are moving energy from an inverter just a few feet away to the car battery. This is extremely efficient, and I can be sure I\u2019m putting solar power in my car, and doing it at a time when the grid might be curtailed. A counter argument might be that the solar energy I didn\u2019t put into the grid was needed to run air conditioners during the day, so a gas peaker plant had to supply it, and the low overnight energy load could be supplied by wind farms rather than gas. I happen to live in California, close to one of the largest grid battery installations, but I have no idea what\u2019s actually happening from hour to hour.\n\nI think the good news is that over time it will get less complicated. For example if you run in the cloud provider regions near Montreal, Google reports that energy is 100% carbon free hydro powered, all the time. There is however a small residual carbon footprint related to construction and maintenance of renewable energy sources. Over the next few years grid regions around the world are de-carbonizing, along with supply chains for manufacturing, delivery and maintenance.\n\nBottom Line\n\nThe common approach is to use the previous year\u2019s annual average consumption based grid mix carbon intensity in carbon footprint calculations, as the data is freely available, and this gives you an approvimate attributional result that may be good enough to give you confidence that your optimizations are directionally helpful. To do a consequential analysis you need to use the marginal carbon intensity, which can vary a lot more in real time. The GSF SCI publishes an average marginal data set, and there are real-time data sources provided by WattTime and ElectricityMaps on a commercial basis, with free access for previous years data from ElectricityMaps, and free API access to real-time marginal data for Northern California from WattTime. The GSF Software Carbon Intensity specification has published a case study that shows how to do this for application workloads, where SCI produces a marginal carbon per transaction figure for a specific workload.\n\nI think the bottom line is that what we want is the good consequences of global carbon reduction, but in practice it may be too complicated to figure out all the externalities and data sources and it\u2019s especially complex to get everyone to agree which ones to include, and which methodology to adopt, so we will mostly end up continuing to focus on reducing our energy use, calculating and reducing our attribution model carbon footprint. However we should also think about the bigger picture, short and long term effects, and understand where there may be large unintended consequences.\n\nWhen making claims about carbon reduction projects, to be accurate, attribution model reductions should be stated along the lines of \u201cwe reduced our own carbon footprint\u201d, but if you want to claim you are actually \u201csaving the world\u201d, you should spend some time figuring out a consequential model for your project, use marginal carbon intensity data, and reading this white paper by WattTime would be a good start.",
        "metadata": {
          "word_count": 2455,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There are plenty of problems in the way of measuring the carbon footprint of an IT workload. I've been leading a Green Software Foundation project to make it easier to get better measurements for cloud provider based workloads. Along the way I\u2019m learning a lot about the subtleties of measurement and reduction of carbon. Cloud providers will all take care of it for you by 2030 or so. Buildings, food and transport have a much bigger carbon footprint than IT. A single economy flight between the USA and Europe is order-of-magnitude a ton of carbon. The simplest carbon optimization projects focus on doing less work and using less of the resources that contribute to the carbon footprint. These also usually save money. For many companies, that high ROI project may have nothing to do with IT. If you have a containerized workload that you are measuring with Kepler, or a cloud instance based workload that is running in virtual machines, you measure your carbon footprint based on how many and how big the containers and instances are. You get a small energy benefit for reducing average CPU utilization, but you get a bigger benefit for increasing utilization by downsizing the container or instance or using fewer of them. The best way to increase utilization is to spread out or prevent load peaks. Load peaks can be caused by inefficient code at startup, cron jobs, traffic spikes, or retry storms. There are some interesting projects based on Kepler measurements that try to optimize pod scheduling and vertical scaling to reduce power consumption. The difference between attributional and consequential carbon accounting. Both are well documented and standardized, but most of the discussion occurs around attributional accounting. For an example of an unintended consequence, let\u2019s say the result of your optimization project is spare capacity at a cloud provider. The main requirement in the Green House Gas Protocol is that the boundary of the consequential model is clearly defined and documented. The normal attribution model is to use an average carbon intensity for a grid region that is calculated over an hour, a month or a year. Carbon intensity tends to reduce as solar and wind are added, but also reduces as coal and oil is replaced by gas. The lowest cost (usually solar, wind and gas) and cleanest sources are preferred, but when peak loads arrive, it\u2019s currently common to use gas. Google\u2019s carbon data for each region is based on an accumulation of hourly location based consumption based carbon intensity measurements. Of course, Azure and AWS use different methodologies to calculate their carbon footprint, which are currently based on monthly averages of the energy they purchase. In some grid regions batteries are beginning to be used, and if solar or wind curtailment is happening, then it's possible that the consequence is carbon free. It also depends what time of day or night you use the energy, and how close to the peak load the grid region is when you increase your load. Picking a lower carbon region over a high carbon region for a workload is a better option. The cleanest regions may cost more or be further away from your customers. I charge mine during the day, when the sun is out and my own solar panels are moving energy from an inverter just a few feet away to the car battery. This is extremely efficient, and I can be sure I\u2019m putting solar power in my car, and doing it at a time when the grid might be curtailed. For example if you run in the cloud provider regions near Montreal, Google reports that energy is 100% carbon free hydro powered. The common approach is to use the previous year\u2019s annual average consumption based grid mix carbon intensity in carbon footprint calculations. Over the next few years grid regions around the world are de-carbonizing, along with supply chains for manufacturing, delivery and maintenance. I think the bottom line is that what we want is the good consequences of global carbon reduction. In practice it may be too complicated to figure out all the externalities and data sources. It's especially complex to get everyone to agree which ones to include, and which methodology to adopt. Use marginal carbon intensity data, and reading this white paper by WattTime would be a good start. If you need a model for your project, use marginal carbonintensity data."
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "containers",
        "monitoring",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_36634a69",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thanks for summarizing the differences but I don t see where your conclusion comes from",
      "source": "Medium Archive",
      "published_date": "2017-01-02",
      "url": "https://medium.com/@adrianco/2017-01-02_Thanks-for-summarizing-the-differences--but-I-don-t-see-where-your-conclusion-comes-from--99b90f330219",
      "content": {
        "text": "Thanks for summarizing the differences, but I don\u2019t see where your conclusion comes from. There\u2019s some aspects of ECS\u200a\u2014\u200amulti-AZ support, network management and production use at large scale\u200a\u2014\u200athat I could argue are over a year ahead of Kubernetes, and there are several features where it\u2019s behind. Blox is an open source vehicle for AWS to collaborate with its customers and add the features they want, without giving up on integration with VPC, IAM and multi-AZ support.",
        "metadata": {
          "word_count": 80,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "ECS is over a year ahead of Kubernetes, and there are several features where it\u2019s behind. Blox is an open source vehicle for AWS to collaborate with its customers."
      },
      "tags": [
        "aws",
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_file_ea6d8227",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Part 4 how I helped Netflix launch on iPad and iPhone 2007 to 2010",
      "source": "Medium Archive",
      "published_date": "2025-01-27",
      "url": "https://medium.com/@adrianco/2025-01-27_What-Adrian-Did-Next---Part-4---how-I-helped-Netflix-launch-on-iPad-and-iPhone---2007-to-2010-cbf4913aa4cc",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aPart 4\u200a\u2014\u200ahow I helped Netflix launch on iPad and iPhone\u200a\u2014\u200a2007 to\u00a02010\nScreen shots from Adrian\u2019s Instant Queue Add for Netflix iPhone App in\u00a02008\nI\u2019ve been meaning to get another installment of my career history posted for a while. The 15th anniversary of the launch of the iPad gave me a reason to pull out a chunk of that history into this post.\n\nIn part 3 I mentioned that I had developed some phone based apps while at eBay Research Labs in 2006, and I had also become involved in the Homebrew Mobile Phone Club, where a bunch of people got together in their spare time to try and build a phone that would be programmable by anyone. Before 2007 mobile phones were extremely locked down by the carriers and manufacturers and they were not openly programmable. We had some fun getting hardware figured out, and I used a 3D printer to make some cases, but the whole project was interrupted by the delivery of the iPhone by Apple in late 2007. It was clearly far better hardware than we could build, had a proper full featured operating system on it, and as soon as it shipped, people figured out how to \u201cjailbreak\u201d it and program it. I went to the launch event, got an iPhone on day 1, and when Apple finally shipped their SDK in March 2008 I was in the first wave of people who signed up as an iOS developer. My homebrew phone project was abandoned and I started to learn Objective C and Apple\u2019s mobile app development tooling. This was all a spare time project, as my day job at Netflix at that time was as a director level manager of a team working on personalization code in Java, and it wasn\u2019t my job to write the code myself.\n\nI showed the iPhone to people at Netflix, as it had excellent quality video playback, but they weren\u2019t interested. They were focused on getting Netflix onto TV sets, and thought the screen was too small, the time people would spend watching was too short, and there wasn\u2019t enough mobile network bandwidth. At that time YouTube was primarily very short low quality videos, and Netflix average viewing time was over 30 minutes of high quality video.\n\nIn September 2008 Netflix ran an internal hack day event. They were about to launch a public API and wanted internal teams to try it out before launch. I decided to build an iPhone app that connected to Netflix, and that could be used to find content to add to your Instant Watch queue. The public API didn\u2019t support streaming, so it was all about discovery and integration. One of the Java engineers on my team\u200a\u2014\u200aJian Wu\u200a\u2014\u200ajoined me to help figure out the API. The first problem we had was to get the authentication to work, there was no OAUTH code for iOS, and the MacOS code I found needed to be changed to make it work (and neither of us knew Objective C well). We spent more time learning how OAUTH worked than building our app, but we did get a list of streaming content onto the iPhone and won a prize in the Hackathon. I continued to develop the app for the next few years, and listed it in the iPhone app store. It was the top selling 99c app for a while, although it still only let you find things to watch on your TV later. Here\u2019s the trophy we won.\nPhoto by Adrian\u200a\u2014\u200a2008 Netflix Hack Day Trophy for Instant Queue Add for\u00a0Netflix\nIn January 2010, exactly 15 years ago, Apple announced the iPad. A day or so later I got a calendar appointment to have my quarterly 1on1 with Reed Hastings the next morning. He wanted to talk about the iPad and what I thought of it. I had already seen young children using iPhones, and my opinion then was that this was the ideal computer for kids and older people, who didn\u2019t want to deal with the complexities of PCs and would find the user interface intuitive. I told Reed I thought it would be successful.\n\nThe reason he was asking me about it was that Netflix had already been talking to Apple about another topic, and they had reached out to us via that contact to ask if we wanted to be part of the iPad launch. It would have to be a completely secret development program, between January and May, we\u2019d have to rearrange internal development priorities, and we couldn\u2019t tell anyone internally that didn\u2019t need to know, or hire anyone from outside. Reed wanted to know if we should do it, and whether it was possible in the time available?\n\nAn engineer on my personalization team who had left a few weeks before\u200a\u2014\u200aStephane Lunardi\u200a\u2014\u200awas ex-Next, ex-Apple, a total Objective C guru, and he left us to join Palm to work on WebOS because he wanted to work on mobile applications, and at that point there was no interest in mobile apps at Netflix. Terrible timing\u2026 but without Stephane I was the only iOS developer anywhere at Netflix, and it wasn\u2019t my job to be a developer, by then I was leading the cloud re-architecture team.\n\nWhat did happen is that we assigned Stephane Odul as Product manager (it would have been amusing to have two Stephane\u2019s working on this project), and I helped him get setup with an iOS developer account in his own name. We found an engineer from another team who knew C++ and was willing to learn Objective C and borrowed him for the project (unfortunately I\u2019ve forgotten his name). I took the core code from my own hack day Netflix on iPhone app that authenticated with the Netflix API, created an internal repo with it, and turned it over to them to develop the iPad app. (I wonder if any of my code is still present in today\u2019s Netflix apps?)\n\nAt some point a storage cupboard was taken over with a combination lock on the door, no-one was allow to ask or discuss what was going on, as Apple insisted on absolute secrecy. A prototype iPad was bolted to the desk inside, and the developer was the only person allowed in to work on it. As the iPad delivery day in May approached, I engaged again to help Stephane Odul run the app through Apple\u2019s App Store submission processes. The Netflix app was staged ready for launch a few days before the iPad app store went live, and people were rummaging around in Apple\u2019s URL spaces looking for clues about apps for the launch when someone found metadata about the Netflix app. Word got out, it became a big story, and I think we had to get Apple\u2019s permission to run the Netflix on iPad press release a day early. On the day, I was part of the first shipment and I still have my \u201cday 1\u201d iPad. The Netflix iPad app was a big hit, and has been one of the top apps for the iPad ever since.\n\nSales of my 99c Netflix app increased, although people were grumpy that it didn\u2019t stream video, but there was no public API support for streaming. The main reason for this was that Netflix didn\u2019t want to have people build apps with advertising wrappers around the Netflix content. A contractual relationship was needed to get access to the private streaming API. A year or so later, in 2011, Netflix also launched their (free) official iPhone app, and my sales plummeted! I eventually removed my app from the app store. The Netflix iPhone launch was the first platform launch that Netflix did which was entirely backed by the AWS cloud. We simply didn\u2019t have enough capacity in our datacenter to run the traffic, so it had to work. We knew that many customers already had iPhones so the traffic ramp up for the new service was extremely fast. To test the cloud based backend, we tee\u2019d a proportion of the incoming \u201chome page\u201d requests (which returned lists of about 2000 videos personalized for that user) from the datacenter to the cloud and processed them there, discarding the result. It worked really well as a stress test, and the launch had no issues.\n\nI built two more iOS apps that worked with Netflix. One was built in September 2010 at an externally run iPad DevCamp (hosted at the Paypal offices), with help from the Netflix API Evangelist Kirsten Hunter. It was called Actors for Netflix, and let you browse the Netflix catalog by pivoting through the actors and directors of videos using an ODATA API, and launched the actual Netflix iPad app to play the video. The code is still up on github. It won the best use of Web API\u2019s prize at the iPadDevCamp. I wrote the foreword for Kirsten\u2019s book Irresistable APIs, and recommend anyone designing an API should read it. The Netflix public API was badly designed in several ways and was eventually shut down, we learned a lot from the experiences.\n\nThe second iPad app I built was for another internal Netflix hack day, later in 2010. I wanted to target Netflix at children, and built a prototype of Netflix for Kids as an iPad app. This is before the current Netflix kids functionality existed, and we had just hired a product manager to take a look at it. I had to remind him I was not his development team\u2026 The app was aimed at pre-school kids, had no text in the UI, and just showed kids shows as box shots. It also had filters in settings so parents could exclude any content they deemed unsuitable or didn\u2019t want to hear for the N\u2019th time. The actual Netflix for kids functionality they launched much later was aimed at an older age bracket as a sub-profile using the main Netflix app on all platforms. I have a trophy for this Hack Day as well.\nPhoto by Adrian\u200a\u2014\u200a2010 Netflix Hack Day trophy for Netflix for Kids on\u00a0iPad\nHappy 15th birthday to the iPad! I use mine most days to watch videos. We currently have four of them around the house and I still have my day 1 iPad in storage somewhere.\n\nThere\u2019s a lot more to say about my time at Netflix, but that\u2019s a story for another day.\n\nPart 3\u200a\u2014\u200aeBay 2004\u20132007\n\nPart 2\u200a\u2014\u200aSun Microsystems\n\nPart 1\u200a\u2014\u200aStarting Out",
        "metadata": {
          "word_count": 1767,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The 15th anniversary of the launch of the iPad gave me a reason to pull out a chunk of that history into this post. In part 3 I mentioned that I had developed some phone based apps while at eBay Research Labs in 2006. I had also become involved in the Homebrew Mobile Phone Club, where a bunch of people got together in their spare time to try and build a phone that would be programmable. When Apple shipped their SDK in March 2008 I was in the first wave of people who signed up as an iOS developer. I showed the iPhone to people at Netflix, as it had excellent quality video playback, but they weren\u2019t interested. They were focused on getting Netflix onto TV sets. In September 2008 Netflix ran an internal hack day event. They were about to launch a public API and wanted internal teams to try it out. I decided to build an iPhone app that connected to Netflix, and that could be used to find content. In January 2010, exactly 15 years ago, Apple announced the iPad. Netflix had already been talking to Apple about another topic, and they had reached out to us via that contact to ask if we wanted to be part of the iPad launch. An engineer on my personalization team who had left a few weeks before\u2019s ex-Next, ex-Apple, left us to work on WebOS. At that point there was no interest in mobile apps at Netflix. We found an engineer who knew C++ and was willing to learn Objective C. The Netflix app was staged ready for launch a few days before the iPad app store went live. A prototype iPad was bolted to the desk inside, and the developer was the only person allowed in to work on it. Sales of my 99c Netflix app increased, although people were grumpy that it didn\u2019t stream video. A year or so later, in 2011, Netflix also launched their (free) official iPhone app, and my sales plummeted! I eventually removed my app from the app store. I built two iOS apps that worked with Netflix. One was called Actors for Netflix, and let you browse the Netflix catalog by pivoting through the actors and directors of videos using an ODATA API. It won the best use of Web API\u2019s prize at the iPadDevCamp. The second iPad app I built was for another internal Netflix hack day, later in 2010. I wanted to target Netflix at children, and built a prototype of Netflix for Kids as an iPad app. The actual Netflix for kids functionality they launched much later was aimed at an older age bracket as a sub-profile. There\u2019s a lot more to say about my time at Netflix, but that's a story for another day. ay to the iPad! I use mine most days to watch videos. We currently have four of them around the house."
      },
      "tags": [
        "aws",
        "cloud",
        "netflix",
        "platform",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_8b02a394",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Cloud Native Computing",
      "source": "Medium Archive",
      "published_date": "2017-08-09",
      "url": "https://medium.com/@adrianco/2017-08-09_Cloud-Native-Computing-5f0f41a982bf",
      "content": {
        "text": "Photo by @adrianco\u200a\u2014\u200aCloud native computing of refraction by a\u00a0rainbow.\nCloud Native Computing\n\nAmazon Web Services recently joined the Cloud Native Computing Foundation, and I\u2019m representing AWS as the CNCF board member, with Arun Gupta from our open source team coordinating technical engagement with projects and working groups. To explain what this is all about, I think it\u2019s useful to look at what we mean by \u201ccloud native,\u201d and where the term came from.\n\nBack in 2009, I was working at Netflix, and the engineering teams were figuring out some new application architecture patterns we would need to migrate to AWS. Some of us had learned how to automate deployments at scale from time spent working at eBay, Yahoo, and Google. We also learned new ideas from Werner Vogels and the AWS team. The result was a new set of fundamental assumptions that we baked into our architecture. In 2010, we started talking publicly about our cloud migration, and in 2012 we got the bulk of the platform released as a set of open source projects, collectively known as NetflixOSS.\n\nWhile we didn\u2019t invent most of these patterns, the fact that we gathered them together into an architecture, implemented it at scale, talked about it in public, and shared the code was influential in helping define what are often referred to as cloud native architectures.\n\nCloud native architectures take full advantage of on-demand delivery, global deployment, elasticity, and higher-level services. They enable huge improvements in developer productivity, business agility, scalability, availability, utilization, and cost savings.\n\nOn-demand delivery, taking minutes instead of weeks, is often the first reason that people move to cloud, but it doesn\u2019t just reduce the deployment time for a traditional application: it also enables a new cloud native pattern of ephemeral and immutable deployments. In the old deployment model, where it takes weeks to get a resource, you\u2019re going to hang on to it, order extra capacity in advance, and be reluctant to give it back, so you\u2019ll figure out how to update it in place. The cloud native pattern, instead, is to bake instances or build containers, deploy many identical copies just as long as they are needed, shut them down when you are done, and create new images each time the code changes. NetflixOSS pioneered these concepts by baking Amazon Machine Images (AMIs). Docker subsequently used it as a core element of the container deployment model.\n\nDeploying applications that span multiple datacenters is a relatively rare and complex-to-implement pattern, but cloud native architectures treat multi-zone and multi-region deployments as the default. To work effectively in this model, developers should have a good understanding of distributed systems concepts; a discussion of the \u201cCAP Theorem\u201d became a common interview topic at Netflix. Despite huge improvements in technology, the speed of light is a fundamental limit, so network latency, and in particular cross-regional latencies, are always going to be a constraint.\n\nCloud native architectures are scalable. When I first presented about Netflix\u2019s use of AWS in 2010, we were running front end applications on a few thousand AWS instances, supporting about 16 million customers in the USA. Nowadays, Netflix is fully migrated to AWS, has over 100 million global customers, and is running on over 100,000 instances. The implementation details have changed over the years, but the architectural patterns are the same.\n\nOver time, components of cloud native architectures move from being experimental, through competing implementations, to being well-defined external services. We\u2019ve seen this evolution with databases, data science pipelines, container schedulers, and monitoring tools. This is one place where the Cloud Native Compute Foundation acts as a filter and aggregator. The Technical Oversight Committee of the CNCF reviews projects, incubates them, and adopts projects as they move from the experimental phase to the competing implementation phase. For customers who are trying to track a fast-moving and confusing world, it\u2019s helpful to regard CNCF as a brand endorsement, for a loose collection of interesting projects. It\u2019s a loose collection, rather than a single, integrated cloud native architecture, so there\u2019s no particular endorsement of any one project over another, for members of CNCF, or for users of projects.\n\nThe CNCF currently hosts ten projects, and is incubating many more: Kubernetes for container orchestration, Prometheus for monitoring, Open Tracing for application flow monitoring, Fluentd for logging, Linkerd for service mesh, gRPC for remote procedure calls, CoreDNS for service discovery, Containerd and Rkt for container runtimes, and CNI for container native networking.\n\nFrom the AWS perspective, we are interested in several CNCF projects and working groups. AWS were founding members of the Containerd project; we are excited about participating in the Containerd community, and have lots of ideas around how we can help our customers have a better experience. Our forthcoming ECS Task Networking capabilities are written as a CNI plugin, and we expect CNI to be the basis for all container-based networking on AWS. In addition, a recent CNCF survey reports that 63 percent of respondents host Kubernetes on Amazon EC2, and Arun is blogging about his experiences with several Kubernetes on AWS installers, starting with Kops. We have plans for more Kubernetes blog posts and code contributions, and think there are opportunities to propose existing and future AWS open source projects to be incubated by CNCF.\n\nThe charter of the open source team we are continuing to build at AWS is to engage with open source projects, communities, and foundations, as well as to help guide and encourage more contributions from AWS engineering. AWS is already a member of The Linux Foundation, which hosts CNCF, and we look forward to working with old and new friends on the shared goal to create and drive the adoption of a new computing paradigm optimized for modern distributed systems.\n\nPlease follow @AWSOpen to keep up to date on open source at AWS.",
        "metadata": {
          "word_count": 972,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Amazon Web Services recently joined the Cloud Native Computing Foundation. To explain what this is all about, I think it\u2019s useful to look at what we mean by \u201ccloud native\u201d and where the term came from. Cloud native architectures take full advantage of on-demand delivery, global deployment, elasticity and higher-level services. They enable huge improvements in developer productivity, business agility, scalability, availability, utilization, and cost savings. The cloud native pattern is to bake instances or build containers, deploy many identical copies just as long as they are needed. NetflixOSS pioneered these concepts by baking Amazon Machine Images (AMIs) Docker subsequently used it as a core element of the container deployment model. When I first presented about Netflix\u2019s use of AWS in 2010, we were running front end applications on a few thousand instances. Nowadays, Netflix is fully migrated to AWS, has over 100 million global customers, and is running on over 100,000 instances. The implementation details have changed over the years, but the architectural patterns are the same. The CNCF currently hosts ten projects, and is incubating many more. It\u2019s a loose collection, rather than a single, integrated cloud native architecture. AWS were founding members of the Containerd project. The charter of the open source team we are continuing to build at AWS is to engage with open source projects, communities, and foundations. We expect CNI to be the basis for all container-based networking on AWS. 63 percent of respondents host Kubernetes on Amazon EC2, a recent CNCF survey reports. Open source at AWS. open source at Amazon Web Services. See more at: http://www.awesome.com/awesome-open-source-at-aws."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "cloud native",
        "containers",
        "data science",
        "distributed systems",
        "docker",
        "engineering",
        "kubernetes",
        "logging",
        "monitoring",
        "netflix",
        "platform",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_file_d6b7a82b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Failure Modes and Continuous Resilience",
      "source": "Medium Archive",
      "published_date": "2019-11-11",
      "url": "https://medium.com/@adrianco/2019-11-11_Failure-Modes-and-Continuous-Resilience-6553078caad5",
      "content": {
        "text": "Failure Modes and Continuous Resilience\n\nA resilient system continues to operate successfully in the presence of failures. There are many possible failure modes, and each exercises a different aspect of resilience. The system needs to maintain a safety margin that is capable of absorbing failure via defense in depth, and failure modes need to be prioritized to take care of the most likely and highest impact risks.\n\nIn addition to the common financial calculation of risk as the product of probability and severity, engineering risk includes detectability. Failing silently represents a much bigger risk than the same failure that is clearly and promptly reported as an incident. Hence, one way to reduce risk is to make systems more observable. Another problem is that a design control, intended to mitigate a failure mode, may not work as intended. Infrequent failures exercise poorly tested capabilities that tend to amplify problems in unexpected ways rather than mitigate them, so it\u2019s important to carefully exercise the system to ensure that design controls are well tested and operating correctly. In the same way that we have moved from a few big software releases a year to continuous delivery of many small changes, we need to move from annual disaster recover tests or suffering when things actually break, to continuously tested resilience. Staff should be familiar with recovery processes and the behavior of the system when it\u2019s working hard to mitigate failures. A learning organization, disaster recovery testing, game days, and chaos engineering tools are all important components of a continuously resilient system.\n\nThis discussion focuses on hardware, software and operational failure modes. It\u2019s important to consider capacity overload, where more work arrives than the system can handle, security vulnerabilities, where a system is attacked and compromised, and safety failures, but we\u2019ll defer those discussions for now.\n\nThere are many possible failure modes, and since they aren\u2019t all independent, there can be a combinatorial explosion of permutations, as well as large scale epidemic failures to consider. While it\u2019s not possible to build a perfect system, here are five good tools and techniques that can focus attention on the biggest risks and minimize impact on successful operations.\n\nThe first technique is the most generally useful. Concentrate on rapid detection and response. In the end, when you\u2019ve done everything you can do to manage failures you can think of, this is all you have left when that weird complex problem that no-one has ever seen before shows up! Figure out how much delay is built into your observability system, it may be taking samples once a minute, processing them for a minute or two, then watching for several bad samples in a row before it triggers an alert. It would take 5\u201310 minutes after the problem occurred to appear as a problem, then people have to notice and respond to emails or pager text messages, dial into a conference call, and log in to monitoring dashboards before any human response can start. Collecting some critical metrics at one second intervals, with a total observability latency of ten seconds or less matches the human attention span much better. Try to measure your mean time to respond (MTTR) for incidents. If your system is mitigating a small initial failure, but it\u2019s getting worse, and your team responds and prevents a larger customer visible incident from happening, then you can record a negative MTTR, based on your estimate of how much longer it would have taken for the problem to consume all the mitigation margin. It\u2019s important to find a way to record \u201cmeltdown prevented\u201d incidents, and learn from them, otherwise you will eventually drift into failure [Book: Sydney Decker\u200a\u2014\u200aDrift into Failure]. Systems that have an identifiable capacity trend, for example are filling up disk space at a predictable rate, have a \u201ctime to live\u201d (TTL) that can be calculated. Sorting by TTL identifies the systems that need attention first and can help focus work during a rapid response to a problem.\n\nThe second technique starts with the system constraints that need to be satisfied to maintain safe and successful operation and works in a top down manner using System Theoretic Process Analysis (STPA), or the more specific technique System Theoretic Accident Model and Processes (STAMP). [Book: Engineering a Safer World by Nancy G. Leveson]. STPA is based on a functional control diagram of the system, and the safety constraints and requirements for each component in the design. A common control pattern is divided into three layers, the business function itself, the control system that manages that business function, and the human operators that watch over the control system. The focus is on understanding the connections between components and how they are affected by failures. In essence in a \u201cboxes and wires\u201d diagram most people focus on specifying the boxes and their failure modes, and are less precise about the information flowing between boxes. With STPA there is more focus on the wires, what control information flows across them, and what happens if those flows are affected. There are two main steps: First identify the potential for inadequate control of the system that could lead to a hazardous state, resulting from inadequate control or enforcement of the safety constraints. These could occur if a control action required for safety is not provided or followed; an unsafe control is provided; a potentially safe control action is provided too early, too late or in the wrong sequence; or a control action required for safety is stopped too soon or applied for too long. For the second step each potentially hazardous control action is examined to see how it could occur. Evaluate controls and mitigation mechanisms, looking for conflicts and coordination problems. Consider how controls could degrade over time, including change management, performance audits and how incident reviews could surface anomalies and problems with the system design.\n\nThe third technique is lineage driven fault detection [Paper: Peter Alvaro]. The idea is to start with the most important business driven functions of the system and follow the dependency tree or value chain that is invoked when it is working correctly. You can then ask what happens from a business perspective when component level failures occur. Most online services have a sign-up flow that acquires new customers, and a value flow that is the main purpose of the service. For Netflix, the primary value flow is \u201cstreaming starts\u201d, where someone starts watching a show, and the global total rates for new customers and streaming starts were made into a dashboard that was the starting point for most people during an incident, to see how big the effect on the business was. At some point Netflix also figured out how to measure the total number of customers dialed into their call centers globally and add it to the same dashboard, and this became a somewhat noisy but extremely valuable and sensitive metric for understanding whether customers were unhappy. Starting with the purpose for the system, we can walk through all the steps that provide value to its users, see what might go wrong with each step, come up with an observability and mitigation strategy, and find ways to run chaos experiments to validate our design controls. This is effectively a top-down approach to failure mode analysis, and it avoids the trap of getting bogged down in all the possible things that could go wrong using a bottoms-up approach.\n\nThe fourth technique is to apply the no single point of failure (SPOF) principle. If something fails, there should be another way for the system to succeed. For high resiliency systems, it\u2019s even better to use the \u201crule of three\u201d and quorum based algorithms. This is why most AWS regions have three availability zones. When there\u2019s three ways to succeed, we still have two ways to succeed when a failure is present, and if data is corrupted, we can tell which of the three is the odd one out. When safely storing data, it\u2019s very helpful to have three locations to write to, because once a majority have succeeded, you can move on. There is no need to retry and no extra time taken when a failure is present. For systems that are latency sensitive, creating two independent ways to succeed is an important technique for greatly reducing the 99th percentile latency. Chaos tests are also an important technique to validate the hypothesis that two mechanisms are independent.\n\nThe fifth technique is risk prioritization, and there is an industry standard [ISO] engineering technique called Failure Mode and Effects Analysis which uses a set of conventions for getting an estimated risk priority number (RPN) of between 1 and 1000, by ranking probability, severity and observability on a 1\u201310 scale, where 1 is good and 10 is bad, and multiplying them. A perfectly low probability, low impact, easy to measure risk has an RPN of 1. An extremely frequent, permanently damaging impact, impossible to detect risk has an RPN of 1000. By listing and rating failure modes, it\u2019s easy to see which one to focus on. Next you record what effect you expect your mitigation strategy to have, which should drop its RPN and then let you focus on the new highest RPN, until there aren\u2019t any high values left. In practice, the easiest way to reduce RPN is to add observability, so you aren\u2019t working blind. You can then get some empirical measurements of probability, as once it\u2019s visible, you can see how often it occurs. We\u2019ll use FMEA to work through the sign-up flow example in a later section.\n\nTaking a top down approach we can divide the failure modes into four general categories. Each category is centered around the responsibilities of a different team of people, and should be discussed and developed in partnership with those teams.\n\nFollowing the value chain from the business perspective, the first team is the product managers and developers who specify and build the unique code that makes up the business logic of the system. The system itself could be a single microservice with a small team, which makes it easier to reason about, or a large monolithic application. The point is to focus on faults that are in the scope of control of the developers of the system, and that are tied directly to the business value of the application.\n\nThe second team is the software platform team, who provide standardized supported libraries, bundled open source projects, packaged commercial software, operating systems, build pipelines, language runtimes, databases, external high level services etc. that are used across multiple applications. They are indirectly supporting business value for multiple teams and use cases, have to manage components that they aren\u2019t able to modify easily, and have to deal with a complex set of versions and external supply chains.\n\nThe third team is the infrastructure platform team, who deal with datacenter and cloud based resources. They are concerned with physical locations and cloud regions, networking failures, problems with infrastructure hardware, and failures of the control planes used to provision and manage infrastructure.\n\nThe fourth team, reliability engineering, provides observability, incident management and automated remediation tools for the entire system. Failures of observability and incident management can compound a small problem into a large one, and make the difference between a short and a long time to fix problems.\n\nIn all four cases, there is a common starting point and structure to the failure modes which should be extended to take account of a particular situation. The criticality and potential cost of each failure mode is context dependent, and drives the available time and budget for prioritized mitigation plans. The entire resiliency plan needs to be dynamic, and to incorporate learnings from each incident, whether or not the failure has noticeable customer impact.\n\nFailure Modes and Effects\u00a0Analysis\n\nThe FMEA spreadsheet is used to capture and prioritize risks based on Severity, Probability and Detectability where each is rated on a 1 to 10 scale. A standard model for each follows, the exact values chosen are somewhat arbitrary, and some forms of FMEA use a 1 to 5 scale, but all we are trying to do is come up with a rough mechanism for prioritization, and in practice this is good enough for the purpose.\n\nSeverity starts with several high levels that destroy things, in other words irreversible failures like death or incapacitation of a person, destruction of machinery, flood and fire in a datacenter. The next few levels are temporary incapacitation, recoverable with degradation of performance, and finally ratings of minor or no effect.\nFMEA Severity\nFor probability, we use an exponential scale, from almost inevitable and repeated observed failures down through occasional failures to failures that haven\u2019t been seen in practice. The probabilities are guesses during the design phase, but should be measured in real life when a system is operating, and the risk updated based on what is seen in practice.\nFMEA Probability\nThe spreadsheet is organized into sections, listing failure modes for each function. There is also a recommended action, listing who is responsible and when, actions taken and the updated severity, occurrence and detectability that lead to a planned reduction in the RPN. The rows are shown split below for readability. The only formula needed is RPN=Sev*Prob*Det.\n\nApplication Layer\u00a0FMEA\n\nThis first example FMEA models the application layer assuming it is implementing a web page or network accessed API. Each step in the access protocol is modelled as a possible failure mode, starting with authentication, then the access itself. This is followed by some common code related failure modes. For a specific application team, these should be discussed, prioritized and have additional failure modes added. Judgement and discussion is needed to finish filling in all the levels and actions, but some common failure modes have been completed.\n\nhttps://github.com/adrianco/slides/blob/master/FMEA_Application.xlsx\n\nSoftware Stack\u00a0FMEA\n\nThe software stack failure modes start along the same lines, with authentication and a request response sequence analysis that needs to be repeated for each of the projects, packages and service dependencies. However the more specific failure modes relate to the control planes for services hosted in cloud regions. In general a good way to avoid customer visible issues caused by control plane failure modes is to pre-allocate identity, network, compute and storage/database structures wherever possible. The cost of failure should be weighed against the cost of mitigation.\n\nhttps://github.com/adrianco/slides/blob/master/FMEA_SoftwareStack.xlsx\n\nInfrastructure FMEA\n\nIt\u2019s not generally useful to talk about \u201cwhat to do if an AWS zone or region has an outage\u201d because it depends a lot on what kind of outage and what subset of services might be impacted. Service specific control plane outages are part of the software stack FMEA. If a datacenter building is destroyed by fire or flood, we have a very different kind of failure than a temporary power outage or cooling system failure, and that\u2019s very different to losing connectivity to a building where all the systems are still running, but isolated. In practice, we can expect individual machines to fail randomly with very low probability, groups of similar machines to fail in a correlated way due to bad batches of components and firmware bugs, and extremely rare availability zone scoped events caused by power and cooling failures, bad weather, earthquake, fire and flood.\n\nhttps://github.com/adrianco/slides/blob/master/FMEA_Infrastructure.xlsx\n\nOperations and Observability\n\nMisleading and confusing monitoring systems cause a lot of failures to be magnified rather than mitigated. While some of the failure modes can be prioritized with an FMEA, these higher level failures are better modelled using Systems Theoretic Process Analysis (STPA), which also captures the business level criticality of the application. The service interactions that make up the monitoring system can be examined starting with the same patterns used for the applications FMEA, but it\u2019s more interesting to look at the interactions with human operators and derive hazards from the information flows.\n\nhttps://github.com/adrianco/slides/blob/master/FMEA_Operations.xlsx\n\nSimplified STPA\u00a0Model\n\nThere is a lot more to SPTA but a simplified approach shows how it can be applied to the problems of finding failure modes in high availability systems. One of the models shown in the book is our starting point, showing the controlled process (data plane), the automated controller (control plane), and the human controller (who is looking at dashboards).\n\nIf we rewrite the labels to show a specific application, such as a financial services API which collects customer requests and performs actions, then the human controller monitors the throughput of the system, to make sure it\u2019s completing actions at the expected rate. The automated controller could be an autoscaler that is looking at the CPU utilization of the controlled process, scaling up and down the number of instances that are supporting the traffic to maintain CPU utilization in a fixed maximum and minimum range.\n\nIf the service CPU utilization maxes out and throughput drops sharply, the human controller is expected to notice and decide what to do about it. The controls available are to change the maximum autoscaler limit, restart the data plane or control plane systems, or to roll back to a previous version of the code.\n\nThe hazards in this situation are that the human controller could do something that makes it worse instead of better. They could do nothing, because they aren\u2019t paying attention. They could reboot all the instances at once, which would stop the service completely. They could freak out after a small drop in traffic caused by customers deciding to watch the Superbowl on TV and take an action before it is needed. They could do something too late, like notice eventually after the system has been degraded for a while and increase the autoscaler maximum limit. They could do things in the wrong order, like reboot or rollback before they increase the autoscaler. They could stop too soon, by increasing the autoscaler limit, but not far enough to get the system working again, and go away assuming it\u2019s fixed. They could spend too long rebooting the system over and over again. The incident response team could get into an argument about what to do, or multiple people could make different changes at once. The run-book is likely to be out of date and contain incorrect information about how to respond to the problem in the current system.\n\nEach of the information flows in the control system should be examined to see what hazards could occur. In the monitoring flows, the typical hazards are a little different to the control flows. In this case, the sensor that reports throughput could stop reporting, and get stuck on the last value seen. It could report zero throughput, even though the system is working correctly. The reported value could numerically overflow and report a negative or wrapped positive value. The data could be corrupted and report an arbitrary value. Readings could be delayed by different amounts so they are seen out of order. The update rate could be set too high so that the sensor or metric delivery system can\u2019t keep up. Updates could be delayed so that the monitoring system is showing out of date status, and the effect of control actions aren\u2019t seen soon enough. This often leads to over-correction and oscillation in the system, which is one example of a coordination problem. Sensor readings may degrade over time, perhaps due to memory leaks or garbage collection activity in the delivery path.\n\nThe identified STPA hazards in each control or sensor path in the system need to be prioritized in a similar manner to the FMEA failure modes. They also provide good inputs for test cases to make sure the code used to sanitize inputs from each sensor flow can mitigate each hazard.\n\nThe STPA three level control structure provides a good framework for asking questions about the system. Is the model of the controlled process looking at the right metrics and behaving safely? What is the time constant and damping factor for the control algorithm, will it oscillate, ring or take too long to respond to inputs? How is the human controller expected to develop their own models of the controlled process and the automation, and understand what to expect when they make control inputs? How is the user experience designed so that the human controller is notified quickly and accurately with enough information to respond correctly, but without too much data to wade through or too many false alarms?\n\nThe STPA model shown assumes a single instance of the controller. The next step, for a future post, is to derive a multi-region STPA model and examine the hazards during a disaster recovery primary to secondary failover event.",
        "metadata": {
          "word_count": 3423,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A resilient system continues to operate successfully in the presence of failures. Failing silently represents a much bigger risk than the same failure that is clearly and promptly reported as an incident. Infrequent failures exercise poorly tested capabilities that tend to amplify problems in unexpected ways rather than mitigate them. A learning organization, disaster recovery testing, game days, and chaos engineering tools are all important components of a continuously resilient system. Staff should be familiar with recovery processes and the behavior of the system when it's working hard to mitigate failures. Focus on rapid detection and response. Figure out how much delay is built into your observability system. It may be taking samples once a minute, processing them for a minute or two, then watching for several bad samples in a row. Systems that have an identifiable capacity trend, for example are filling up disk space at a predictable rate, have a \u201ctime to live\u201d (TTL) that can be calculated. Sorting by TTL identifies the systems that need attention first and can help focus work during a rapid response to a problem. The second technique starts with the system constraints that need to be satisfied to maintain safe and successful operation. This works in a top down manner using System Theoretic Process Analysis (STPA) The focus is on understanding the connections between components and how they are affected by failures. Identify potential for inadequate control of the system that could lead to a hazardous state. Evaluate controls and mitigation mechanisms, looking for conflicts and coordination problems. Consider how controls could degrade over time, including change management. Most online services have a sign-up flow that acquires new customers, and a value flow that is the main purpose of the service. For Netflix, the primary value flow is \u201cstreaming starts\u201d, where someone starts watching a show. The fourth technique is to apply the no single point of failure (SPOF) principle. If something fails, there should be another way for the system to succeed. For high resiliency systems, it\u2019s even better to use the \u201crule of three\u201d and quorum based algorithms. The fifth technique is risk prioritization, and there is an industry standard [ ISO] engineering technique called Failure Mode and Effects Analysis. By listing and rating failure modes, it\u2019s easy to see which one to focus on. The easiest way to reduce RPN is to add observability, so you aren\u2019t working blind. Once it\u2019s visible, you can see how often it occurs. We\u2019ll use FMEA to work through the sign-up flow example in a later section. The software platform team provide standardized supported libraries, bundled open source projects, packaged commercial software, operating systems, build pipelines, databases, external high level services etc. The infrastructure platform team deal with datacenter and cloud based resources. The point is to focus on faults that are in the scope of control of the developers of the system.  reliability engineering provides observability, incident management and automated remediation tools for the entire system. Failures of observability and incident management can compound a small problem into a large one, and make the difference between a short and a long time to fix problems. The spreadsheet is organized into sections, listing failure modes for each function. For probability, we use an exponential scale, from almost inevitable and repeated observed failures down through occasional failures to failures that haven\u2019t been seen in practice. FMEA models the application layer assuming it is implementing a web page or network accessed API. Each step in the access protocol is modelled as a possible failure mode. Judgement and discussion is needed to finish filling in all the levels and actions, but some common failure modes have been completed. Service specific control plane outages are part of the software stack FMEA. A good way to avoid customer visible issues caused by control plane failure modes is to pre-allocate identity, network, compute and storage/ database structures wherever possible. Misleading and confusing monitoring systems cause a lot of failures to be magnified rather than mitigated. While some of the failure modes can be prioritized with an FMEA, these higher level failures are better modelled using Systems Theoretic Process Analysis. There is a lot more to SPTA but a simplified approach shows how it can be applied to the problems of finding failure modes in high availability systems. One of the models shown in the book is our starting point, showing the controlled process (data plane) and the automated controller (control plane) If the service CPU utilization maxes out and throughput drops sharply, the human controller is expected to notice and decide what to do. The controls available are to change the maximum autoscaler limit, restart the data plane or control plane systems, or to roll back to a previous version of the code. Each of the information flows in the control system should be examined to see what hazards could occur. In this case, the sensor that reports throughput could stop reporting, and get stuck on the last value seen. It could report zero throughput, even though the system is working correctly. The identified STPA hazards in each control or sensor path in the system need to be prioritized in a similar manner to the FMEA failure modes. Sensor readings may degrade over time, perhaps due to memory leaks or garbage collection activity in the delivery path. The STPA model shown assumes a single instance of the controller. How is the human controller expected to develop their own models of the controlled process and the automation? What is the time constant and damping factor for the control algorithm?"
      },
      "tags": [
        "automation",
        "aws",
        "cloud",
        "engineering",
        "infrastructure",
        "metrics",
        "monitoring",
        "netflix",
        "observability",
        "performance",
        "platform",
        "resilience",
        "scaling",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_be52f46c",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Noon Lights an IoT Review",
      "source": "Medium Archive",
      "published_date": "2017-12-28",
      "url": "https://medium.com/@adrianco/2017-12-28_Noon-Lights---an-IoT-Review-cbab555d4884",
      "content": {
        "text": "Noon Lights\u200a\u2014\u200aan IoT\u00a0Review\n\nI\u2019ve been gradually installing some IoT automation at home over the last few months, and was looking for a lighting solution to fit a fairly complex situation. It\u2019s a large open plan area with several connected zones, kitchen, patio, dining area, living area, with lots of light bulbs and far too many light switches.\n\nUpgrading all the bulbs to something like Philips Hue didn\u2019t look like the right way to go, there are too many bulbs so it would be expensive, and we\u2019d need to keep the light switches on all the time. When I looked at replacing the switches themselves I found the well established Lutron and Leviton brands, along with a few others that each had some disadvantage that put me off.\n\nThe main set of lights are controlled by three switches, so I needed a multi-way controller, that excluded several otherwise promising options. I wanted a system that still works manually and during network outages, and didn\u2019t really want to have a separate hub to gateway the signal.\n\nEventually I discovered a new option called Noon (noonhome.com)\u200a\u2014\u200athey launched in October 2017, and were founded by people with experience at Nest (thermostats) and August (locks). While it\u2019s still new to the market, I thought it was worth a try, so over the last few weeks I installed it, and thought it was still novel enough to be worth writing up as a blog post.\nnoonhome.com\nTheir over-all architecture is to manage one room at a time, using one special switch as a Noon Director and up to ten Noon Extension switches. They all fit by replacing a standard light switch as long as it\u2019s the large flat rocker kind. The Noon Director has a black glass finish, and contains a small color touch screen, it acts as a hub, is connected to WiFi and lets you easily turn the entire room on and off, or switch between scenes. It\u2019s connected to the white plastic Noon Extension switches via Bluetooth Low Energy, and since they are all in the same room, the signal doesn\u2019t have to go through walls to get there. They all contain a built-in rechargeable battery, and all can act as dimmer or as on-off switches, there isn\u2019t a separate part for each as with other lighting options.\n\nThe Noon switches don\u2019t mount flush like the conventional rocker switches, and have a curved lower end. They are operated by clicking that end in, and that toggles the circuit on or off, which works directly and locally, with no network involvement. For the Director, clicking it turns all the switches on or off, which I like a lot, for my complex room setup. The Director also has a motion detector, so it lights up with the current scene icon as you walk up, and can trigger a night light option. For each Extension it\u2019s own circuit is toggled and there are also small up/down dimmer level buttons. For multi-switch groups, the network is used to coordinate the group on-off and dimming levels. This all works as you\u2019d expect, and seems intuitive to other family members and guests in the house.\n\nI setup using their iPhone/iPad app, which works well, looks good and is responsive and reliable. I shouldn\u2019t have to say that, but I\u2019ve seen so many IoT apps that look terrible, are slow and crash a lot! On each circuit, during setup Noon runs tests to see what kind of bulbs are being used by turning them on and off a few times, and lets you know if you have dimmable lights or not. I have a mixture of dimmable and non-dimmable LED bulbs installed. After setup it\u2019s easy to change the bulb type setting if you change the bulbs, but I couldn\u2019t see a way to restart the discovery process for a particular switch. I did the initial setup once I had the Director and a few Extensions installed, then easily added more Extensions as I completed the wiring work.\n\nThe systems comes with some standard scenes, and the app lets you customize with a reasonable selection of standard icons to create the scenes you want, and to change the order of them. I setup six scenes in order of ascending brightness since they are swiped through on the Director, and it made sense to go gradually from night time to full bright. When the Director is off, there\u2019s the automatic night light option, but we found that the motion detector didn\u2019t trigger reliably for the orientation/location we used, so I setup a Nightime scene that keeps a few dim lights on all the time. The next scene is called Relax, and has low lights for chilling out, then I have Outside which mostly lights the patio, Cooking which adds full light in the kitchen area, Everyday which is medium light everywhere, and Bright, which is everything on maximum. The last is mostly useful for finding light bulbs that don\u2019t match, or when cleaning. Scene setup is intuitive, can be tweaked easily via the app or by using the switches directly, and switching using the Director works pretty well. There\u2019s some helpful artistic advice built into the app and documentation to help decide which lights to use for what.\n\nHardware installation was a lot of work but not too frustrating, I did it in about two days over the holidays. About half of the time was spent up a ladder installing 30\u201340 new dimmable LED lightbulbs in key places and walking back and forth tracing which circuit breaker really connected to which switch. This has nothing to do with Noon, but if you\u2019re looking at this kind of work, you need to take it into account, and it\u2019s good pre-work even if you have a professional contractor do the wiring for you. I\u2019ve done a fair amount of house re-wiring projects over many years (I also started my career working on hardware with a degree in applied physics and electronics) so felt confident doing the full install myself.\n\nThe Noon switches only control lights. They can\u2019t be used to control ceiling fans or outlet plugs, as they have a maximum power draw limit that\u2019s too low for a plug or a fan motor. For my multi-way switch fascias that meant I was mixing some Noon switches with the existing switches. Noon provides its own fascia options that have a metal frame with a clip on cover, and since our old fascia\u2019s were fairly ugly I bought a set of replacements from Noon for most of the room so everything would look good.\n\nThe first thing I did before I ordered anything was take a photo of every switch in the room, going clockwise from a starting point, and document in Evernote what each switch did, whether it was in a multi-way group, and whether I was going to replace it with a Noon or not. There\u2019s a limit of 10 Noon Extensions per Director, and I ran into this limit, so had to decide which minor lights not to control. I think they should increase this limit, I really needed 12, if I\u2019d had more multi-way switches to manage I could have needed more. 20 would be a better limit. I positioned the Director as the switch we would pass last in the evening, going to bed and first in the morning when we get up, which was one of the three multi-way main light switches.\n\nI ordered directly from the Noon website, and it arrived in a few days, no problems, a bit over $1000 for one Director, ten Extensions and a lot of faceplates.\n\nI checked the current light bulbs to be sure I was within limits (200W total for LED per Noon in my case) and found a mixture of types (some CFL, some LED) and brands that were mostly non-dimmable. I ended up replacing 30+ bulbs with Sylvania 9W and 12W dimmable warm white LED bulbs after reading some poor reliability reviews for cheaper brands.\n\nThe circuit breakers for the house were seemingly labeled at random, with no relation to reality! I ended up removing almost all the labels and starting again until I had identified every connection that went to a switch in the room I was working on. Don\u2019t underestimate how long this will take!\n\nThe Noon website has very detailed and clear instructions on how to setup the wiring. The most complicated setup is for multi-way, and the approach Noon takes scales to allow many switches in a group, wired in parallel rather than the traditional wiring method. After I\u2019d very carefully followed these instructions I found that the lights were on all the time, and the Noon troubleshooting guide correctly suggested moving one wire, which fixed the problem. Other than that, it was repetitive work, but straightforward.\n\nThe mechanical part of the installation was hardest, I had to get the existing switches into the Noon switch mounting hardware, which is slightly deeper. I found that in some cases the existing switch screws weren\u2019t long enough to bite once the switch was re-installed. In four way switch boxes the white neutral wires form a large bundle of connections, and the supplied twist-on connector was too small to fit, when adding several additional Noon wires to the bundle. For a professional installer, they would have additional larger twist-on and longer screws at hand, but it would have been be easier if a larger twist-on and some extra long screws were provided by Noon. It was also tricky to get all the switches to align, and even so, sometimes the snap on fascia didn\u2019t want to fit until it was adjusted some more. The end result looks good though.\n\nNoon supports Alexa, and I ran the connection and discovery process which found the switches and scenes. I then ended up changing the names to simplify identifying each switch, and seem to have confused everything. I\u2019m getting around to deleting everything from Alexa and starting again. Noon doesn\u2019t support Apple Homekit, and I\u2019ve found that interface useful for some use cases, so hope they will add it eventually.\n\nBottom line, I think Noon did a good job on thinking up a new approach to automating lighting on a room by room basis, it\u2019s nicely implemented, well documented and works well in practice. I\u2019m very happy with it in a large highly traffic\u2019d room, I\u2019m not planning to go round every other room in the house adding Noon Directors for a whole house solution, although I might eventually add one or two more large rooms that have more complex lighting setups.\n\nDisclaimer: I don\u2019t have any connection to Noon, don\u2019t know anyone who works there, or know what cloud if any their back end runs on.\n\nFinal collection of suggestions for Noon\u200a\u2014\u200asupport up to 20 Extensions per Director (I\u2019ll buy two more when you do). Provide a re-discover bulb type option in the app. Supply more and longer mounting screws. Supply a larger gauge neutral wire twist-on connector. Add plug control capable and fan control Extensions (I\u2019ll buy some). Add a sensitivity adjustment for the motion detector in night light mode. Add Apple Homekit support.",
        "metadata": {
          "word_count": 1873,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Noon Lights is an IoT lighting solution for a large open plan area. The main set of lights are controlled by three switches, so I needed a multi-way controller. I wanted a system that still works manually and during network outages. N Noon is a new way to manage one room at a time. Uses one special switch as a Noon Director and up to ten Noon Extension switches. All fit by replacing a standard light switch as long as it\u2019s the large flat rocker kind. The Noon switches don\u2019t mount flush like the conventional rocker switches, and have a curved lower end. They are operated by clicking that end in, and that toggles the circuit on or off. The Director also has a motion detector, so it lights up with the current scene icon as you walk up. Noon runs tests to see what kind of bulbs are being used by turning them on and off a few times. App lets you customize with a reasonable selection of standard icons to create the scenes you want. Hardware installation was a lot of work but not too frustrating. Scene setup is intuitive, can be tweaked easily via the app or by using the switches directly. There\u2019s some helpful artistic advice built into the app and documentation. The Noon switches only control lights. They have a maximum power draw limit that\u2019s too low for a plug or a fan motor. Noon provides its own fascia options that have a metal frame with a clip on cover. There's a limit of 10 Noon Extensions per Director. I think they should increase this limit, I really needed 12. I positioned the Director as the switch we would pass last in the evening, going to bed and first in the morning. The Noon website has very detailed and clear instructions on how to setup the wiring. The most complicated setup is for multi-way, and the approach Noon takes takes takes scales to allow many switches in a group, wired in parallel. The mechanical part of the installation was hardest, I had to get the existing switches into the Noon switch mounting hardware, which is slightly deeper. I found that in some cases the existing switch screws weren't long enough to bite once the switch was re-installed. Other than that, it was repetitive work, but straightforward. N Noon is a new way to manage lighting in a home. It works with Alexa, but doesn't yet support Apple Homekit. The app is available on Google Play and is free to download. Supply more and longer mounting screws. Add Apple Homekit support. Add plug control capable and fan control Extensions (I\u2019ll buy some). Add sensitivity adjustment for the motion detector in night light mode."
      },
      "tags": [
        "architecture",
        "automation",
        "cloud",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_file_09d4624b",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Percentiles don t work Analyzing the distribution of response times for web services Updated with",
      "source": "Medium Archive",
      "published_date": "2023-01-29",
      "url": "https://medium.com/@adrianco/2023-01-29_Percentiles-don-t-work--Analyzing-the-distribution-of-response-times-for-web-services--Updated-with--ace36a6a2a19",
      "content": {
        "text": "Percentiles don\u2019t work: Analyzing the distribution of response times for web services (Updated with\u00a0code)\nPlot showing the final result of fitting multiple normal distributions to a response time\u00a0curve\nMost people have figured out that the average response time for a web service is a very poor estimate of it\u2019s behavior, as responses are usually much faster than the average, but there\u2019s a long tail of much slower responses. The common way to deal with this is to measure percentiles, and track the 90%, 99% response times for example. The difficulty with percentiles is that you can\u2019t combine them, so if you measure percentiles at one minute intervals, you can\u2019t average them together to get hourly or daily percentiles. If you do this, you end up with a number on a dashboard that may be useful as an indicator of trends, but it can\u2019t tell you what the actual percentile was for that hour or day. Another common method is to measure the proportion of responses that exceeded a service level agreement (SLA) expressed as a percentile limit\u200a\u2014\u200alike 90% of responses within one second. However it\u2019s very difficult to decide what the right SLA is, or to tell how close you are to exceeding it. For example lets say you have a 99% within 2 seconds SLA, and your current 99%ile measured over one minute is 1 second. There is no way to model how much more traffic you can send to that system before it exceeds it\u2019s SLA.\n\nThis is unfortunate, because we\u2019d really like to be able to build systems that have an SLA that we can share with the consumers of our interfaces, and be able to measure how well we are doing.\n\nThe second problem with percentiles is that response time distributions for anything other than a trivial example aren\u2019t simple shapes with one peak. The structure and behavior of your system will be visible in the responses. The mean and percentile measurements hide this structure, but the rest of this post will show how the structure can be measured and analyzed so that you can figure out a useful model of your system, understand what is driving the long tail of latencies and come up with better SLAs and measures of capacity.\n\nI\u2019ve been thinking about this for a long time. I presented this analysis of response time distributions talk in 2016\u200a\u2014\u200aat Microxchg in Berlin (video). A later version of the slides is included in my Microservices Workshop deck from later that year, slides 168\u2013200 (pdf, keynote are available in GitHub.com/adrianco/slides).\n\nBill Kaiser of NewRelic published this blog in 2017 which goes some way towards what I\u2019m talking about, but since then I have figured out a new way to interpret the data.\nWhat Is the Expected Distribution of Website Response Times?When you're trying to optimize the performance for your website, there's one question that comes up a lot: What is the\u2026newrelic.com\nIf we can obtain histograms of the response time distribution of a web site or API over a time period, I think there is interesting underlying structure that can be extracted and summarized.\n\nResponse times are a one sided distribution. There\u2019s a minimum possible response time for when everything goes right and there\u2019s no delays. Every opportunity for delay due to more work than the best case or more time waiting than the best case increases the latency and they all add up and create a long tail. This applies to other areas like software project time estimates as well, there\u2019s a tendency to focus on the best case but it always takes longer.\n\nUsing the high dynamic range hdrhistogram package to collect response time data the bins are assigned on a logarithmic basis, so components that are approximated by long tail lognormal distributions appear as symmetric normal components when the bins are plotted.\n\nI spent a long time trying to figure out how to do this analysis at all, was hopeful that it was an existing technique if I could only find an example. In the end as a side effect of a discussion on Mastodon (I\u2019m @adrianco@mastodon.social) with Donnie Berkholz (@dberkholz@hostux.social) where I was trying to explain what I was looking for, Ed Borasky (@AlgoCompSynth@ravenation.club) joined in the discussion and knew exactly what I needed. He gave me a few options, and first I looked at the R mixtools library. The documentation is written for people that understand hardcore statistics with lots of greek symbols. I have enough math and stats background that I can muddle along and figure out how to use it, so I\u2019m documenting that here without the greek. What we needed was an Expectation Maximization (EM) algorithm for obtaining a finite mixture model of the data. Each peak in the data is modeled by a normal distribution, and the EM algorithm iterates until it finds the best fit it can for some number of components. We can let the algorithm start with no guidance and take longer to find the peak, or we can specify starting guesses and the number of components we want, which will be useful when tracking changes in response distributions over time. This package assumes we have all the underlying data that was fed into the histogram, for example from a log file that records the response time of each individual request.\n\nThe code is written using the R statistics language and I downloaded a free copy of RStudio to run the examples below.\n> install.packages(\"mixtools\")> library(mixtools)> data(faithful) #sample data of old failful geyser eruption wait times> attach(faithful)> hist(waiting) # draw a histogram, which shows two peaks that look symmetricWait time between geyser eruptions as a histogram\nNext we run the model and see the results.\n> wait <- normalmixEM(waiting)number of iterations= 39 > summary(wait)summary of normalmixEM object:          comp 1    comp 2lambda  0.360887  0.639113mu     54.614896 80.091095sigma   5.871247  5.867714loglik at estimate:  -1034.002 > plot(wait, density=T)Hit <Return> to see next plot: Hit <Return> to see next plot: \nLambda is the probability density, in other words the proportional weight of each underlying component normal distribution\u200a\u2014\u200awhich is summed to 1.0. Mu is the mean of each component, the latency. Sigma is the width of each component, the variance. Loglik is the quality of the fit, the log-likelyhood of the Expectation Maximization algorithm. It can range from minus to plus infinity, and the more positive it is the better the fit. The normalmixEM function assumes that it\u2019s looking for a mix of normal distributions. There are other functions in the package for more complex situations.\n\nThe same result is obtained more quickly with fewer iterations when good initial estimates are provided. The initial loglik value is higher but trends to the same result. Without any guidance, random numbers are used to start with, and the number of iterations depends on the actual random numbers used for that run. For this demo on an old MacBook (2.7 GHz Dual-Core Intel Core i5), it takes between 2\u20136ms, around 0.2ms per iteration. By relaxing the epsilon loglik improvement requirement even fewer iterations are needed to get a good result. This seems reasonable overhead for a real time algorithm that could be applied to histogram data as part of a metric collection pipeline.\n> system.time(wait1 <- normalmixEM(waiting, mu=c(50,80), lambda=.5, sigma=6))number of iterations= 10    user  system elapsed   0.002   0.000   0.002 > summary(wait1)summary of normalmixEM object:          comp 1    comp 2lambda  0.360849  0.639151mu     54.613614 80.090295sigma   5.869094  5.869094loglik at estimate:  -1034.002 > system.time(wait2 <- normalmixEM(waiting, mu=c(50,80), lambda=.5, sigma=6, epsilon=0.1))number of iterations= 4    user  system elapsed   0.001   0.001   0.001 > summary(wait2)summary of normalmixEM object:          comp 1    comp 2lambda  0.360256  0.639744mu     54.596070 80.076551sigma   5.874538  5.874538loglik at estimate:  -1034.003 Estimate converging in 10 iterations\nThe geyser seems to have a normal distribution of inter-arrival time, but for computer systems it\u2019s going to be more skewed and we can try to superimpose several log-normal distributions to get a fit.\n\nFor high traffic systems, processing the individual response times for each request may be too much work. If instead they are fed into an hdrhistogram that is processed at regular intervals, then we need to process the histogram itself and that takes me down several rat-holes. There\u2019s an R package called mixComp that looked useful, and another one called mixdist, but they wanted an initial estimate of the number and position of the peaks to be fitted, and they gave a lot of errors even when I tried them out with good looking data. So in the end I decided to write my own peak estimation code that starts with a log-binned histogram and finds all the peaks. The starting point is a peak finding algorithm, and Ed helped me again by pointing me at Pracma\u2019s Findpeaks routine.\n\nI then used two sample histograms from the same system to develop an algorithm in R I call as.peaks. It takes a histogram, and returns a data frame containing the peaks. This took a lot of tweaking and iterating until I came up with what looks like a reliable algorithm. The basic structure is that Findpeaks is used to locate which bins are peaks, then starting with the biggest, each of those peaks has a better estimate made using a spline fit to the top 5 points, and that estimate is improved by attempting to fit a normal distribution. The normal fitted peaks are then subtracted out of the data. Once all the peaks have been removed, new peaks are revealed that were hiding between the others, and a second pass is made using Findpeaks and processed the same way. The normal fit fails sometimes, particularly on the smaller peaks, leaving the spline as a fallback.\n\nHere\u2019s the first of the histograms, and you can see that it has a lot of structure. I\u2019ve cropped the y-axis in these plots as it\u2019s the shape that\u2019s important, however this represents a very large number of data points collected over a short timeframe, something like millions of points for one minute.\n\nMy first attempt to analyze this was to manually guess where to put each normal distribution and subtract them out one at a time. The first fit and the residual after it\u2019s subtracted is shown below.\n\nThe second fit goes after the next highest peak.\n\nAfter the third peak is removed, it\u2019s clear there are at least three more interesting peaks, but after a while some peaks are artifacts of how the previous peaks were removed. That\u2019s why an algorithm is needed to take out the guesswork.\n\nI think that the structure of the system in terms of functionality distributed across a network of lambda functions, microservices, caches, databases and external services is going to determine the number and position of each peak. Changes in behavior of the system from minute to minute is going to change the height of each peak, as the workload mix and cache hit rates change. New deployments of code and infrastructure, or unusual activity like failure modes could change the characteristic positions of each peak.\n\nIf this works out then a small amount of data\u200a\u2014\u200athree numbers for each significant component in the mix\u2014 is going to act as a very compressed yet highly accurate representation of a response time distribution. It\u2019s much more useful from a modelling and analysis point of view than the mean response time or a percentile. In addition distributions can be combined over time to summarize, and mathematically correct percentiles can be derived from them. If we sort the list by latency, feed that list into the estimator for the next mix and allow for small changes in the mean latency of each component, then any sudden large changes in the structure are effectively a reset for the model, likely due to a code push, or a failure mode.\n\nThe code is about 100 lines of R plus many comments (see this github repo), and it optionally prints a sequence of plots and debug output as shown below.\n# Take a histogram h$Value h$Count and find the peaks in it, interpolating between bins# approximate finite mixed model decomposition starting with histograms of data# Intended for use on response time histograms with logarithmic bins like hdrhistogram# so that each (approximately) lognormal component is interpreted as a symmetric normal peaklibrary(pracma) # for findpeaks# h - is a data frame containing Values and Counts forming a histogram where Values for each bucket are exponential# plots - optionally shown# normalize - divides down the counts to a probability density# peakcount=0 does a quick estimate of all the peaks, otherwise it does up to the specified number of dnorm fits# epsilon=0.005 - sets a minimum interesting peak size to 0.5% of the largest peak, the ones that are visible in the plot.# printdebug turns on helpful intermediate print output to see what's going on# Returns a data frame with one row per peak, sorted biggest peak first# row names show the order the peaks were discovered in# PeakDensity is the histogram count or normalized density# PeakBucket is the center bucket for the peak# PeakMin and PeakMax are the start and end buckets for the peak# if a gaussian fit was obtained, then PeakAmplitude is non-zero, along with updated PeakMean and PeakSD# The Value of the peak is interpolated to estimate PeakLatencyas.peaks <- function(h, plots=FALSE, normalize=FALSE, epsilon=0.005, peakcount=0, printdebug=F)\nThe first plot (for the same histogram shown above) shows the actual distribution and the mean, on linear axes. This system has some responses in a fraction of a second, an average of just over a second, a few clear peaks and a long tail extending beyond 40 seconds.\n\nThe next plot is the same histogram I already showed, then it plots the result. Each peak and the distribution it\u2019s being subtracted from is in the same color. The two largest, black and red are removed first, then the left-most green one and the small one at the 18th bucket. This reveals additional peaks which are processed in the second pass, only looking for peaks that are larger than the smallest we picked the first time, to avoid focusing on noise.\n\nTrying this on my second sample histogram the response time shows some a slightly lower mean latency.\n\nThe distribution shows more structure.\n\nAnd the fitted peaks cover the sharp low latency ones in a plausible manner.\n> as.peaks(hist1, normalize=T, peakcount=10)  PeakDensity PeakBucket PeakMin PeakMax  PeakMean    PeakSD PeakAmplitude PeakLatency1 0.115124638         54      46      70 54.241346 2.1974776   0.642060169  1.571292572 0.045773931         42      29      46 42.192548 1.9606526   0.228603641  0.498339065 0.014108909         49      44      53 48.614035 2.1417907   0.078241384  0.919025476 0.011062200         37      29      40 36.732429 1.3955436   0.040165053  0.296152883 0.007798191          7       2      14  7.647565 0.9801163   0.020434690  0.018518917 0.002867469         10       8      17 10.244946 0.8583217   0.005929608  0.023720728 0.002167271         55      54      56 55.000000 0.0000000   0.000000000  1.689117144 0.001277573         18      14      21 17.528146 1.4389079   0.004426705  0.04748966\nThis shows the output for the first histogram plotted above, with the counts normalized so that the area under the histogram is 1.0.\n\nI\u2019m still working this, and I\u2019m looking for additional histograms from other systems to try it out on. So if anyone has logarithmic response time data histograms or logs of raw response time they could share, please let me know\u2026\n\nI will update this post with additional examples, the code is on GitHub, and I\u2019ve submitted a talk proposal to present this work at the Monitorama conference in June.",
        "metadata": {
          "word_count": 2553,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Most people have figured out that the average response time for a web service is a very poor estimate of it\u2019s behavior. The common way to deal with this is to measure percentiles, and track the 90%, 99% response times for example. The difficulty with percentiles is that you can\u2019t combine them. A percentile limit is like 90% of responses within one second. However it\u2019s very difficult to decide what the right SLA is. The structure and behavior of your system will be visible in the responses. The rest of this post will show how the structure can be measured and analyzed. What Is the Expected Distribution of Website Response Times? is a question that comes up a lot when you're trying to optimize the performance for your website. If we can obtain histograms of the response time distribution of a web site or API over a time period, there is interesting underlying structure that can be extracted and summarized. Using the high dynamic range hdrhistogram package to collect response time data the bins are assigned on a logarithmic basis, so components that are approximated by long tail lognormal distributions appear as symmetric normal components. R mixtools is a library that can be used to create histograms. We needed an Expectation Maximization (EM) algorithm for obtaining a finite mixture model of the data. The documentation is written for people that understand hardcore statistics with lots of greek symbols. The code is written using the R statistics language and I downloaded a free copy of RStudio to run the examples below. The data is a. g file that records the response time of each individual request. The same result is obtained more quickly with fewer iterations when good initial estimates are provided. For this demo on an old MacBook (2.7 GHz Dual-Core Intel Core i5), it takes between 2\u20136ms, around 0.2ms per iteration. This seems reasonable overhead for a real time algorithm that could be applied to histogram data. The geyser seems to have a normal distribution of inter-arrival time, but for computer systems it\u2019s going to be more skewed. We can try to superimpose several log-normal distributions to get a fit. R package mixComp looked useful, but they wanted an estimate of the number and position of the peaks to be fitted. In the end I decided to write my own peak estimation code that starts with a log-binned histogram and finds all the peaks. This took a lot of tweaking and iterating until I came up with what looks like a reliable algorithm. My first attempt to analyze this was to manually guess where to put each normal distribution and subtract them out one at a time. This represents a very large number of data points collected over a short timeframe, something like millions of points for one minute. I\u2019ve cropped the y-axis in these plots as it's the shape that\u2019s important. The structure of the system in terms of functionality is going to determine the number and position of each peak. Changes in behavior of the. system from minute to minute will change the height of each. peak, as the workload mix and cache hit rates change. New deployments of code and infrastructure, or unusual activity like failure modes could change the characteristic positions of eachpeak. The code is about 100 lines of R plus many comments (see this GitHub repo) It prints a sequence of plots and debug output as shown below. Findpeaks is a tool for finding peaks in a data frame. It can be used to plot the peaks. The data frame is a histogram of the peaks that were found. The first plot (for the same histogram shown above) shows the actual distribution and the mean, on linear axes. This system has some responses in a fraction of a second, an average of just over a second. The second plot reveals additional peaks which are processed in the second pass. PeakDensity, PeakBucket, PeakMax, PeakMean, PeakSD, PeakAmplitude, PeakLatency. atency ones in a plausible manner. The output for the first histogram plotted above is plotted above. I will update this post with additional examples, the code is on GitHub, and I\u2019ve submitted a talk proposal to present this work at the Monitorama conference in June. If anyone has logarithmic response time data histograms or logs of raw response time they could share, please let me know."
      },
      "tags": [
        "infrastructure",
        "microservices",
        "performance",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_719c6949",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Citroen SM Allez",
      "source": "Medium Archive",
      "published_date": "2018-08-23",
      "url": "https://medium.com/@adrianco/2018-08-23_Citroen-SM---Allez-a3b8b7808deb",
      "content": {
        "text": "Citroen SM\u200a\u2014\u200aAllez\nAllez pictured at the rim of Crater Lake,\u00a0Oregon\nI wanted a Citroen SM when I was a teenager in the 1970s, I bought one in 1999. This is the story of me and my car, and I will update it as I gather pictures and details of the car online, and decide on it\u2019s eventual destination.\n\nAfter 19 years with Allez, I\u2019m not driving it often enough, and feel that it\u2019s a good time for someone else to enjoy the car [Update: sold the car in January 2020]. It\u2019s in great mechanical condition, and is probably worth investing in a restoration, it could use a new paint job, have the bumpers taken off to fix a few accumulated dings, and have the interior leather replaced. I want the car to go to a good home, preferably in Northern California, so I can see it at events from time to time. I don\u2019t have an asking price, and I\u2019m not in a hurry. I\u2019m going to collect offers and pick one, if that doesn\u2019t work out I will pick another one. Allez will be with me and Laurel in the Monterey area at the end of August 2018. We will be in Carmel for the Thursday lunchtime Pebble Beach tour, Pacific Grove and Laguna Seca Raceway on Friday, on the green at Concorso Italiano on Saturday, and parked with everyone else while we attend Pebble Beach Concours on Sunday. I will have a note on the car with a link to this post, and my twitter DMs @adrianco are open for discussions and questions.\n\nI first became aware of Citroen\u2019s as a child when my father pointed out a Citroen DS and told me about it\u2019s advanced aerodynamics and technology, including the hydropneumatic suspension system. I was fascinated, and looked into the car, but felt that it was let down by a boring engine. In 1970 the Citroen SM was launched with a Maserati engine and even cooler two door body style, but they didn\u2019t make it in right-hand drive for the UK market, so they were extremely rare to see on the road as I grew up. I had one as a toy car, and even painted a picture of one for art class. At some point I came across a history of Citroen, Maserati and the SM called \u201cFrench Show with Italian Go\u201d, which was published in Playboy magazine in 1971. Much later a friend with a similar obsession about the SM gave me a copy of the magazine in return for a drive of my car. In the late 1960s Citroen bought Maserati and the Maserati Bora and Merak contain some Citroen technology. The Merak and SM share the same quad-cam V6 engine, although its tuned up more for the Merak and is installed in the back as a mid-engine rear wheel drive, while in the SM it\u2019s installed in the front, driving the front wheels. Citroen sold Maserati in the late 1970s and it ended up owned by Fiat. While I lived in the UK I owned several hydropneumatic Citroens, a 1976 Citroen GS,1981 Citroen GSA,1984 Citroen BX, 1988 Citroen BX 16V (my favorite, a very sporty car), and steel sprung 1982 Citroen Dyane, 1989 Citroen AX (as second cars), but I always wanted an SM.\n\nA total of 12,920 Citroen SMs were sold between 1970 and 1975, mostly in European markets. The USA market version had a few changes, a simpler headlight system with four round lights without a glass cover, and a rubbing strip along the side to protect it from door dings in parking lots. Earlier cars had a carburetor based engine, later ones had fuel injection, and there was a 2.7liter or 3.0liter V6 engine with automatic or 5 speed manual gearbox. My 1973 car is silver-grey with black leather interior, a US-spec body that has had glass headlight covers added, with a 3.0liter carburetor engine and 5 speed manual gearbox. The engine is rated at 180bhp and 180lbft.\n\nI think I saw my car on the streets of Palo Alto once, a year or two before I bought it. The license plate \u201cAllez\u201d which means \u201cGo!\u201d in French has been on the car since it was new, and I remember noticing that plate on one of my rare sightings of a Citroen SM in the wild.\n\nIn May 1999 I had just separated and moved to the USA on my own, I had bought a 1975 Fiat 124 Spider as my daily driver, which seemed to be working out, and I was hanging out with my car-buddy Richard McDougall at his house, when I decided to take a look at the San Jose Mercury cars for sale section. I saw an ad for \u201c1973 Maserati SM\u201d and decided that I\u2019d at least like to look at an SM close-up and test drive it, even if I didn\u2019t buy it. About a week later it was my car. My policy at the time was to have a \u201credundant array of impractical cars with issues\u201d in the hope that at least one of them was going to get me to work each day. The Citroen\u2019s kid-sized back seats, large trunk and long distance cruising capability seemed like a good compliment to the Fiat.\n\nAllez was bought in 1973 by a director of Lawrence Livermore Labs who lived in Pleasanton California. He drove it a lot. At some point in the 1980\u2019s it had some minor accident repair to the front wing and had a complete re-spray. It wasn\u2019t done all that well, and nowadays some of the clearcoat is crazed or peeling in places. It looks good enough at a car show and to enjoy driving, but isn\u2019t going to win any prizes. After about 100,000 miles the engine failed, and was completely replaced. The story is that Citroen had stored two crated brand new engines in Los Angeles at the end of the production run. The engine number on Allez is at the end of the possible range, which supports this story. I got the car with about 130,000 miles on it, and it now has 169,000 miles, so the engine has done 69,000. The original owner retired, and eventually moved to Florida and sold the car to local SF Citroen Car Club member Ralf DiTullio, who already had two SMs. Ralf kept the car for a year or so, then sold it to me, as he was out of space. The car came with a big binder of maintenance records since it was new, and along with my records of the last 19 years, the full history of the car comes with it.\n\nI have made a few changes to the car since I got it. I managed to buy a set of glass front headlamp covers on eBay and get them mounted over the round US headlamps. I upgraded the head unit and speakers in 1999 to include a radio/cassette with an external CD changer that is tucked under the passenger seat. The head unit is mounted vertically between the seats and modern units are too deep, so it has spacers to make it fit. The car was running on red hydraulic fluid when I got it, and I got it converted back to the original green Castrol LHM. Suspension and braking spheres have been swapped out and are currently in good condition. I had a starter motor relay added, and Lumenition electronic ignition was installed before I got the car. The air conditioning valves were replaced, it was re-charged and it blows cold air nicely. I bought the correct Michelin XWX tires fairly recently and they are in excellent condition. These tires are hard to find, a historic spec that is also used on the Ferrari Dino and other cars of the period with high top speeds. The leather seats are serviceable but have a few cracks and tears. At some point I bought a full set of replacement leather skins for the seats, but never got around to having them fitted, they are boxed and come with the car. I also had a crazy plan at one point to use the SM to tow another very small car to car shows, and went as far as getting the parts for a tow-bar. Not really much use, but it also comes with the car.\n\nThe SM is a quirky and unusual car to drive, the steering is very light and direct at low speeds, and gets heavier with speed. It\u2019s common to over-steer the car at first. The brakes are pressure sensitive, the pedal doesn\u2019t actually move much, it pushes back on your foot the harder you press. It\u2019s also common to over-brake at first. The engine sounds odd at idle due to it\u2019s uneven firing pattern, its smooth at speed and pulls well though. The gearing is high and there isn\u2019t an impression of speed, so it\u2019s easy to end up going faster than you think you are. I\u2019ve had a couple of speeding tickets in this car, and it\u2019s a great car for going far and fast apart from the lack of cruise control. The ride is incredibly soft and absorbs all bumps, the self leveling is needed to maintain the ride height and activates even when filling the huge 24 gallon gas tank. It gets 16\u201320mpg, and is relatively better cruising at high speed than other cars due to it\u2019s aerodynamics, but worse around town. The low fuel warning light starts flashing as fuel sloshes around when there\u2019s about 10 gallons left.\n\nAllez has been on some great road trips. The longest in 2000 along with Richard McDougall in his Lotus Esprit was to Solvang in southern California, then on to Phoenix Arizona and Red Rocks at Sedona for a wedding, and back via the Grand Canyon and Las Vegas.\n\nWe had fun at the Palo Alto Concours (also in 2000), doing a synchronized suspension tricks display alongside a second white SM.\nPalo Alto Concours\u00a02000These pictures were taken in the Sacramento delta area in 2007, after adding the headlight covers.\nThe Fiat America club ran the Fiat Mille, a 1000 mile tour of northern California a few times, and I took Allez once. Chasing Fiat X1/9\u2019s through the mountains was fun. They took the corners faster but I caught up on the straights. We also took the car to Oregon with the Fiat club, and toured Crater Lake, where I took the photo shown at the top of this story. I also have a bunch of photos taken at our home in the Los Gatos Mountains.\nAllez at\u00a0home.\nThe Best of France and Italy car show in Van Nuys California is a favorite for Citroen SMs, since they are exactly on point for the event. I\u2019ve taken Allez there several times, and once saw eight different SMs, and got to know some of the owners.\n\nIn 2009 the Maserati club created an event called La Dolce Vita, and Allez was parked right next to the land speed record turbo SM and it\u2019s trailer. The lack of crowds meant we got some great photos of the cars.\n\nLaurel and I have taken Allez to many events with the SF Citroen Car Club over the years. We had a scenic setting in August 2014 at Fort Baker, by the Golden Gate Bridge, with a lot of interesting cars.\nAllez at Pacific Grove in 2015, with Richard McDougall\u2019s Lotus Esprit in the background\nAllez has been to Concorso Italiano several times, but the only time it has won a contest was at the Concours d\u2019Lemons in 2015. I entered it in the Unmitigated Gaul class, but since no-one turned up in any Italian cars, I was voted winner of the Needlessly Complex Italian class. We bribed the judges and anyone else who walked by with shots of Limoncello, and Jay Leno stopped by to tell me that my car was much too nice to be at the Lemons show. I shook his hand but we were too surprised and didn\u2019t think to ask for a photo!\nWinning the Needlessly Complex Italian class at the 2015 Concours d\u2019Lemons is the highlight of Allez\u2019\u00a0career.\nOne of my favorite pictures was taken indoors lit by a skylight at Cowden Automotive in San Francisco, during one of the Arcane Car Club\u2019s annual meetings.\n\nSince I don\u2019t drive Allez very often nowadays, it\u2019s usually sitting for a few months between outings and I\u2019ve developed a checklist to get it going reliably.\n\nDisconnect the battery tender (keeps the battery charged up), and while the hood is up, check the water and oil levels. Rotate the spark plug wires in their socket at the distributor end, in case any corrosion has got in. This helps avoid an annoying misfire that took a while to find the cause of and comes back from time to time. The distributor is unique, as the 90 degree V6 has an un-even firing order, and there are two coils and two incoming wires feeding it.\n\nStarting the car is tricky, as the starter motor is fairly weak and the electrical system over-all is old. Turn off the interior fan and AC to make sure there\u2019s no competing current draw while starting. Turn on the ignition and listen for the fuel pump, it will run for a few seconds and stop. Turn the ignition off and on again several times, until you hear the fuel pump sound different as it finally gets fuel pressure. There\u2019s no point cranking the engine over until there\u2019s some fresh fuel in the carbs. I don\u2019t use the choke, it doesn\u2019t seem to help. Pump the throttle a few times, and turn the engine over for a few seconds. Repeat until it fires, and hold the engine at about 2000 rpm once it gets going.\n\nYou will hear the hydraulic system pump starting to pressurize the system. After 10\u201320 seconds, lightly press the brake pedal, it will make a loud noise as the fluid releases, if the car hasn\u2019t run for a long time. Eventually the car will start to raise up. Don\u2019t ever try to move or steer the car before it\u2019s risen, as you are sitting on the bump stops, the brakes aren\u2019t at full pressure, the steering is un-assisted, and the tires may catch on the front wheel arches. When the car is fully raised and at full pressure, you hear the pump cut out.\n\nCheck the the ride hight level adjustment to the left of the drivers seat. I usually run the car slightly below the normal setting, since I live in the mountains with very twisty roads and it reduces roll with the very soft suspension.\n\nI usually take the car on a short \u201cround the block\u201d drive to warm it up and see how the brakes feel. If there\u2019s any delay before the brake pedal takes effect, the front brakes need to be bled to let the air out. The air gets in if the system isn\u2019t driven often, and it sits depressurized for a long time. The two bleed nipples are just behind the radiator in the center of the engine bay, since the front brakes are in-board, mounted on each side of the gearbox, not on the wheels like most cars.\n\nWhile the suspension is up, the tire pressures can be checked, the rear wheels are partially covered, so the car needs to be moved back and forth to get at the valves.\n\nAllez was maintained by Peter Koine in San Francisco in the early years, and more recently by Lon Price in Santa Cruz. It\u2019s always been garaged. The only thing that needs fixing at present is the brake pedal return spring, for now some sponge under the carpet holds the pedal up, otherwise the brake lights stay on all the time. It\u2019s been driven a handful of times since Lon\u2019s last service about 2 years ago, where he worked on the hydraulic system and the car\u2019s heater.\n\nUpdate\u200a\u2014\u200aI sold the car in January 2020 to someone I met at the SF Citroen club Holiday Dinner. He took it to Peter Koine to get it serviced and has been putting some miles on it and enjoying the car. I\u2019m happy that it went to a good home.",
        "metadata": {
          "word_count": 2728,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "After 19 years with Allez, I\u2019m not driving it often enough, and feel that it\u2019s a good time for someone else to enjoy the car. I want the car to go to a good home, preferably in Northern California, so I can see it at events from time to time. Allez will be with me and Laurel in the Monterey area at the end of August 2018. In 1970 the Citroen SM was launched with a Maserati engine and even cooler two door body style. They didn't make it in right-hand drive for the UK market, so they were extremely rare to see on the road as I grew up. e will be in Carmel for the Thursday lunchtime Pebble Beach tour, Pacific Grove and Laguna Seca Raceway on Friday and on the green at Concorso Italiano on Saturday. A total of 12,920 Citroen SMs were sold between 1970 and 1975, mostly in European markets. The USA market version had a few changes, a simpler head. The 1973 car is silver-grey with black leather interior, a US-spec body that has had glass headlight covers added. It has a 3.0liter carburetor engine and 5 speed manual gearbox. The engine is rated at 180bhp and 180lbft. Allez was bought in 1973 by a director of Lawrence Livermore Labs who lived in Pleasanton California. After about 100,000 miles the engine failed, and was completel. At some point in the 1980\u2019s it had some minor accident repair to the front wing and had a complete re-spray. Citroen had stored two crated brand new engines in Los Angeles at the end of the production run. The original owner retired, and eventually moved to Florida and sold the car to local Citroen Car Club member Ralf DiTullio. Ralf kept the car for a year or so, then sold it to me, as he was out of space. The car was running on red hydraulic fluid when I got it. The head unit is mounted vertically between the seats and modern units are too deep, so it has spacers to make it fit. The leather seats are serviceable but have a few cracks and tears. The SM is a quirky and unusual car to drive, the steering is very light and direct at low speeds, and gets heavier with speed. The brakes are pressure sensitive, the pedal doesn't actually move much, it pushes back on your foot the harder you press. The engine sounds odd at idle due to it\u2019s uneven firing pattern, its smooth at speed and pulls well though. It gets 16\u201320mpg, and is relatively better cruising at high speed than other cars. The low fuel warning light starts flashing as fuel sloshes around when there\u2019s about 10 gallons left. The Best of France and Italy car show in Van Nuys, California is a favorite for Allez. I\u2019ve taken Allez there several times, and once saw eight different SMs. I also have a bunch of photos taken at our home in the Los Angeles area. Allez won the Needlessly Complex Italian class at the 2015 Concours d\u2019Lemons. Jay Leno stopped by to tell me that my car was much too nice to be at the Lemons show. Starting the car is tricky, as the starter motor is fairly weak and the electrical system over-all is old. Turn off the interior fan and AC to make sure there\u2019s no competing current draw while starting. Hold the engine at about 2000 rpm once it gets going. After 10\u201320 seconds, lightly press the brake pedal, it will make a loud noise as the fluid releases. Eventually the car will start to raise up. Don't ever try to move or steer the car before it\u2019s risen. Allez was maintained by Peter Koine in San Francisco in the early years, and more recently by Lon Price in Santa Cruz. The only thing that needs fixing at present is the brake pedal return spring, for now some sponge under the carpet holds the pedal up. \"It went to a good home. get it serviced and has been putting some miles on it and enjoying the car. I\u2019m happy that it went to the good home,\" he said."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_45165ea1",
      "kind": "file",
      "subkind": "medium_post",
      "title": "The day that I gave the founder of Tesla a ride in my Lotus Elise in return for a ride in his",
      "source": "Medium Archive",
      "published_date": "2021-02-12",
      "url": "https://medium.com/@adrianco/2021-02-12_The-day-that-I-gave-the-founder-of-Tesla-a-ride-in-my-Lotus-Elise-in-return-for-a-ride-in-his--4060dc6e1f0b",
      "content": {
        "text": "The day that I gave the founder of Tesla a ride in my Lotus Elise in return for a ride in his T-Zero prototype\nMy Elise at a track day\u200a\u2014\u200aLaguna\u00a0Seca\nThere\u2019s an interesting video interview on CNBC with Martin Eberhard and Marc Tarpenning, the original founders of Tesla, that talks about the early days, and how they decided to base the Tesla Roadster on the Lotus Elise. I had a very small part in helping them along that path at the time, and decided to write down the story as I remember it.\n\nI grew up in the UK, and moved to Silicon Valley in 1993 to work for Sun Microsystems. I moved back to the UK in 1996 and ended up buying one of the original Lotus Elise series 1 cars in 1998. In 1999 I moved back to Silicon Valley, and had to sell the Elise, but missed it, as there was nothing like it in the USA. In 2000 I found a company that was planning to import the Elise by fitting a different engine to replace the Rover engine that wasn\u2019t approved for the US market. In March 2002 I finally took delivery of the car, with an Acura Integra TypeR engine fitted, and registered it as a kit car. The Lotus Elise wasn\u2019t imported by Lotus until 2005, so I had one of just a handful of these cars in the USA for several years.\n\nA co-worker at Sun sent me an email, telling me that his wife was advising the founders of an electric car startup, who were still in stealth mode at the time. They were looking at doing a deal with Lotus to build an electric car based on the Elise, but they\u2019d never seen one in person. He\u2019d seen a car in Sun\u2019s parking lot and found out that it was mine. Would I be prepared to meet them and take the founding CEO Martin Eberhard for a ride? Of course I said yes, so we setup a meeting.\n\nIn the video, Martin mentions that they went to the LA Auto Show and spoke to Lotus, and got an invitation to visit them in the UK. I think this meeting was after the LA Auto Show, but before they went to visit Lotus in the UK.\n\nI met Martin one weekend at a cafe in Woodside, which is in the foothills not far from Stanford University. Martin drove his T-Zero there, and explained to me that it had an electric drivetrain, a lithium-ion battery but the body was based on a race car chassis and they needed to find a good donor vehicle as a starting point. They were in talks with Lotus, but they wanted to see the car and how it drove, to build confidence before their trip to the UK.\n\nMartin got in my Elise, and we drove some back roads then took a highway 280 onramp. As I was merging in, I saw a Lamborghini go by, so I put my foot down, caught up and followed it for a while, going \u201cquite quickly\u201d. After we passed a couple of exits, we pulled off and retraced our route a bit more sedately. My Elise, with it\u2019s engine transplant, has a lot more noise and vibration than usual, but that wasn\u2019t an issue, given their plans to put in an electric drivetrain. Martin enjoyed the ride, and I think that helped them build confidence for their plan to work with Lotus.\n\nThen it was my turn to get a ride in the T-Zero, and Martin pulled onto a straight section of back road, stopped, then floored it. My chest was crushed and I couldn\u2019t breathe in. And unlike my Elise, it didn\u2019t pause to change gear, it just kept going. It felt a lot faster off the line (they measured 3.6s 0\u201360mph), and my Elise is 4.3s 0\u201360mph, so I was impressed. The high pitch wailing noise of the drivetrain was very different, but I thought it was cool. The effect on people after their first experience of an electric launch later became known as the \u201cTesla Grin\u201d, and I\u2019m happy to have been one of the first recipients. It also converted me on the spot, and I started to follow their progress as they worked to get the Roadster designed and shipped.\nImage taken from an eaasv.org presentation I attended on Oct 18,\u00a02003\nAt the time they were being built, 2008\u20132011, I couldn\u2019t afford to order one, but in 2012, when Tesla launched the Model S, several early Roadster customers arranged a trade-in deal, and Tesla ended up with about 20\u201330 Roadsters that they needed to sell. The price was right, so I bought a low milage blue 2010 Roadster. I still have the car and have driven it a lot. It still generates the \u201cTesla grin\u201d. Nowadays I also have a Model 3 for everyday use.\nMy 2010 Tesla\u00a0Roadster",
        "metadata": {
          "word_count": 828,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Martin Eberhard and Marc Tarpenning, the original founders of Tesla, talk about the early days, and how they decided to base the Tesla Roadster on the Lotus Elise. I had a very small part in helping them along that path at the time, and decided to write down the story as I remember it. The Lotus Elise wasn\u2019t imported by Lotus until 2005, so I had one of just a handful of these cars in the USA for several years. A co-worker at Sun sent me an email, telling me that his wife was advising the founders of an electric car startup. Would I be prepared to meet them and take the founding CEO Martin Eberhard for a ride? Of course I said yes. Martin drove his T-Zero there, and explained to me that it had an electric drivetrain, a lithium-ion battery but the body was based on a race car chassis. They were in talks with Lotus, but they wanted to see the car and how it drove to build confidence before their trip to the UK. At the time they were being built, 2008\u20132011, I couldn\u2019t afford to order one, but in 2012, when Tesla launched the Model S, several early Roadster customers arranged a trade-in deal. Tesla ended up with about 20\u201330 Roadsters that they needed to sell. The price was right, so I bought one. My 2010 Tesla Roadster. I still have the car and have driven it a lot. It still generates the \u2018Tesla grin\u2019 Nowadays I also have a Model 3 for everyday use."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_8aa8afd0",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Part 3 eBay 2004 to 2007",
      "source": "Medium Archive",
      "published_date": "2023-02-01",
      "url": "https://medium.com/@adrianco/2023-02-01_What-Adrian-Did-Next---Part-3---eBay---2004-to-2007-90e313fe5a08",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aPart 3\u200a\u2014\u200aeBay\u200a\u2014\u200a2004 to\u00a02007\n\nI\u2019d left Sun (part 2 in this series), and had a few months off over the summer, so (of course) got married to @laurelco, bought a \u201cfixer upper\u201d house in the Los Gatos mountains, and worked on getting it tidied up.\n\nFrom a career point of view, I was looking for a change and decided to get out of the business of selling computers to big companies and figure out consumer oriented technology at eBay. I\u2019d helped eBay recover from capacity related outages in 1999 and had setup their capacity planning processes. I had also turned down a job offer at that time, and when I called back in 2004 they took me on as a Distinguished Engineer in their Operations Architecture team.\n\nAt the time eBay was one of the largest online businesses and was developing patterns that later became widely adopted. Their operations and deployment model was highly automated, pattern based, and horizontally scaled. Their database model later become known as NoSQL, although it was implemented on top of Oracle, each database held one table and indexes, and there were many sharded Oracle databases for each data set so it could be scaled horizontally as well. I learned a lot about building, operating and optimizing large scale global web services and made some good long term friends including Tom DiGrazia, Jeremy Edberg, and Randy Shoup.\n\nAfter a year or so, and a re-organization, eBay Research labs was formed by Eric Billingsley, and I joined to help recruit the team, including a good friend from Sun, Paul Strong, and build out the innovation programs. It was a great opportunity to explore new technologies and broaden my experience. We visited Xerox Parc and IBM Santa Theresa labs to meet with their leaders to get some ideas on what worked and what to avoid. We setup an innovation program to gather ideas from employees\u200a\u2014\u200awho kept suggesting things that already existed\u200a\u2014\u200aand ran advanced development projects. I got interested in programming mobile phones, and at the time, in 2005, the built-in email app on the Blackberry was state-of-the-art. We found one fairly advanced Nokia phone and built a flash based instant-deals app for it. I had to argue with people that it was a research project, so it didn\u2019t have to be deployable at scale, and it ended up looking cool and showing signs of the addictive/immersive nature of apps to come. That app concept was re-engineered and released as an eBay iPhone app many years later.\n\nThe eBay/PayPal duo was joined by Skype while I was working in the labs, and I turned my attention to figuring out how Skype worked and researching ways of leveraging it. My friend and colleague Josep Ferrandiz was working on capacity planning for PayPal, and came up with the idea of a peer-to-peer marketplace, that would scale with the end user nodes, the way Skype did, so we wouldn\u2019t need to bulk up datacenter capacity for peak loads. We prototyped and patented Skype Exchange, a marketplace (like eBay, but actually more like Craigslist), that used the Skype extensions API and transport as its platform (it transacted between Skype accounts), and which monetized via PayPal (you could PayPal money to a Skype account). We thought it was a neat idea, but one reaction from management was \u201cWe don\u2019t want people to think we bought Skype to do that\u2026\u201d and we didn\u2019t get support to productize it. My own theory for why companies kept buying Skype every few years was to to prevent Google from buying it\u2026 it was sold again and ended up at Microsoft.\n\nAt one point I was figuring out how to prototype a complex peer to peer multi-stage offer/bid/accept/pay transaction protocol and discovered that there was a business process standard (BPEL) based on Pi-Calculus, which I\u2019d learned about working on Occam in the 1980\u2019s, and decided to build a simulator using a port of the Occam language that I found was being maintained at the University of Kent. I remember telling Eric \u201cI just wrote 1000 lines of code in a language no-one\u2019s really used in 25 years to build this actor based simulator, and it works!\u201d, and he thought it was cool. I wrote it up as an academic paper, presented it at an IEEE conference, and along with a few patents, that\u2019s all that came out of the work. Many years later I re-wrote the simulator in Go and had some more fun with the concepts at Gophercon 2016.\n\nI also found out the main problem with research labs, which is that they create an us-and-them dynamic in a company, where teams outside the lab felt excluded, and when we wanted to productize ideas, they were seen as needing too much re-work to make them production ready. There were some good successful projects, like a new scalable search engine with real-time index updates, but it became frustrating. In addition, early in 2005 eBay, management decided to put up prices for sellers which drove an exodus to Amazon, and had also spent a lots of money buying adverts on Google (eBay was one of the earliest large revenue sources for Google) helping Google to become the new hot stock. The eBay stock price went from growing fast to going no-where, and the options I got when I joined were under-water. It was early 2007, and time to move on again.\n\nI talked to a few startups, and interviewed at Google, but they were fixated on repeating the operations job I had been doing before, not the job I wanted to do next. Then I got a call from Jessica Neal, who was recruiting for Netflix (and is currently their Chief Talent Officer), and decided to see what opportunities they had open. At the time Netflix was small and struggling, it\u2019s DVD rental business under siege from Blockbuster Total Access, and they were about to launch the first streaming service, that ran on Windows PCs only, and had a very small content catalog. They weren\u2019t known as a technical leader apart from the movie recommendation engine competition The Netflix Prize, which had launched recently. They were looking for people who could help them scale their infrastructure, and hired several people from eBay (a few miles up the road) around that time. The interview process was interesting, and their description of their corporate culture was fascinating (this is years before the \u201cculture deck\u201d was published in 2009). I joined in part to see if they really did run the company that way, in part because they wanted me to be a director for an engineering development team, which I hadn\u2019t done before, in part to learn how personalization algorithms work, and finally, because they were prepared to pay well to get a \u201cdistinguished\u201d scalability expert on their team. The whole company was a few hundred people. They convinced me that they had a better chance of succeeding than the markets thought (their split adjusted share price was about $3 then, and is hundreds of dollars now), and I would learn a lot, which I did. In fact, for the rest of my career, I\u2019ve been telling stories about how we did things at Netflix between 2007 and when I left in 2014. That\u2019s going to take more than one blog post to summarize\u2026",
        "metadata": {
          "word_count": 1235,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "At the time eBay was one of the largest online businesses and was developing patterns that later became widely adopted. Their operations and deployment model was highly automated, pattern based, and horizontally scaled. Their database model later become known as NoSQL. After a year or so, and a re-organization, eBay Research labs was formed by Eric Billingsley. I joined to help recruit the team, including a good friend from Sun, Paul Strong. It was a great opportunity to explore new technologies and broaden my experience. The eBay/PayPal duo was joined by Skype while I was working in the labs. We prototyped and patented Skype Exchange, a marketplace (like eBay, but actually more like Craigslist) At one point I was figuring out how to prototype a complex peer to peer multi-stage offer/bid/accept/pay transaction protocol and discovered that there was a business process standard (BPEL) based on Pi-Calculus. I decided to build a simulator using a port of the Occam language that I found was being maintained at the University of Kent. I remember telling Eric \u201cI just wrote 1000 lines of code in a language no-one\u2019s really used in 25 years to build this actor based simulator, and it works!\u201d, and he thoug it was a joke. I wrote it up as an academic paper, presented it at an IEEE conference, and along with a few patents, that\u2019s all that came out of the work. Many years later I re-wrote the simulator in Go and had some more fun with the concepts at Gophercon 2016. At the time Netflix was small and struggling, it\u2019s DVD rental business under siege from Blockbuster Total Access. They were about to launch the first streaming service, that ran on Windows PCs only, and had a very small content catalog. Netflix was founded in 2007 by Reed Hastings. Hastings left the company in 2014. He has written a book about his time at Netflix, which is published by Simon & Schuster."
      },
      "tags": [
        "architecture",
        "engineering",
        "infrastructure",
        "netflix",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_4990b0cd",
      "kind": "file",
      "subkind": "medium_post",
      "title": "AB Testing So You Know What Really Works",
      "source": "Medium Archive",
      "published_date": "2020-03-21",
      "url": "https://medium.com/@adrianco/2020-03-21_AB-Testing---So-You-Know-What-Really-Works-662f5c18b354",
      "content": {
        "text": "AB Testing\u200a\u2014\u200aSo You Know What Really\u00a0Works\n\u200e\u2068Castillo San Cristo\u0301bal\u2069, \u2068San Juan\u2069, picture by\u00a0Adrian\nWhen I joined Netflix in 2007 I was managing a team that built the personalized home page for the DVD shipping web site. The first thing I found was that every user visible change we made went through an A/B test. This was institutionalized across Netflix from the beginning, and it\u2019s part of the formation story told in Marc Randolph\u2019s book That Will Never Work. As an experienced product manager and the founding CEO, he wanted to be able to measure what was really working. An oft-quoted study by Microsoft Bing found that of the changes they test, one-third prove effective, one-third have neutral results, and one-third have negative results. The problem is that you have to do all the tests to find which third to keep.\n\nNow, when I\u2019m talking to companies about personalization, I find they are all excited about the latest machine learning algorithms, but very few seem to have got the basics in place to be able to tell what\u2019s working and what isn\u2019t.\n\nThere\u2019s quite a lot of online information about A/B testing and tools, but I want to concentrate on the basics.\n\nYou need an \u201cabtest\u201d database service. In a small system this may be a table in a central relational database, but it\u2019s best setup as a NoSQL data source using something like Amazon DynamoDB or Apache Cassandra, with a caching client library or at large scale a microservice data access layer. The table contains a row for every customer, that says what tests they are in and what test cell they are in, a timestamp for when they were added, and a flag for whether the test is active. Whenever a customer connects to your service, you read the entire row and see what test experiences they should be in. Each customer should be in a small number of tests. The abtest service should be used across your entire system, for all experiments that touch a customer.\n\nCustomer service should be able to tell what tests a customer is in, and report problems that seem to be correlated with tests. Individual customers can be dropped from a test and if it happens a lot the test should be shut down quickly.\n\nThere are three ways to populate the database. The best source of test data is new users who haven\u2019t seen the product before, so tests should be allocated as part of the signup flow for new customers. Excluding customers who are re-joining the service is a good idea. New customers provide the most sensitive and unbiased results, and are often most active as they explore the features of the product. The second way is feature specific, and is activated when a customer hits a specific condition, for example at Netflix, when a customer activates a specific device (e.g. an iPad) they may be allocated to a test that is specific for that device. The third way is to randomly select a group of existing customers. This helps make sure your changes aren\u2019t confusing people who were used to the previous functionality.\n\nSuccessful tests should be turned on for everyone by replacing the abtest with a feature flag, but in the case where there are some worries about the impact it\u2019s also useful to use a hold-back test. In this case everyone gets the new experience except a test cell that keeps the old experience to be sure that it really is better for everyone.\n\nThe number of users to allocate to a test depends on several factors (and without getting into the statistics theory) more users will find a smaller difference more quickly with higher confidence. The stream of new user signups to your service should be considered as a precious resource that is fully allocated to tests, according to discussions across product management about which test is highest priority for quick answers. If you think about the scale that Netflix operates at, and the number of new customers they get every day, they can run far more and bigger tests than their smaller competitors, so they have a scale advantage that they can use to tune their product, and maintain a higher rate of innovation.\n\nLet\u2019s say you have several million subscribers to a service and are adding about a million new users per quarter, that\u2019s about 10,000 new users per day. (For reference Netflix added almost 100,000 new users per day in Q4 2019). If you want to allocate a test that has four alternatives and a control group, you would split new users five ways at random and put 2000 per day in each test cell until you get to 10,000 per cell. Then you wait several weeks for statistically significant differences to emerge, and shut down the test. At that scale you can test a handful of new things each month. Existing user tests should have much larger allocations, perhaps 50,000 users per test cell, and should be selected from users that aren\u2019t currently in a test at all, or are in tests that are very unlikely to interact with each other.\n\nI think its good practice to have a big monthly A/B test meeting of all the product managers and engineering managers, including executives. That\u2019s where test results are presented, and there will be a lot of counter-intuitive results, head-scratching, and useful suggestions from the audience. Decisions on what needs a follow-on test, what to kill and what to turn on for all customers should be made there, and brand new ideas for tests shared and prioritized.\n\nThe mark of a good product manager is that they have useful intuition and judgement on what would be a good thing to test. The engineering manager\u2019s role is to find the fastest and easiest way to implement that test. The data scientist\u2019s role is to make sure the test results make sense, and were collected correctly, and determine whether there is a significant difference\u200a\u2014\u200aa non-overlapping confidence interval.\n\nThese practices are the basis for hypothesis driven development, as described in the book Lean Enterprise. Nowadays this should be a baseline capability if you are trying to figure out how to improve an online service, or tune a personalization algorithm.",
        "metadata": {
          "word_count": 1049,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Every user visible change we made at Netflix went through an A/B test. This was institutionalized across Netflix from the beginning. An oft-quoted study by Microsoft Bing found that of the changes they test, one-third prove effective. You need an \u201cabtest\u201d database service. Each customer should be in a small number of tests. The abtest service should be used across your entire system. The best source of test data is new users who haven't seen the product before. Excluding customers who are re-joining the service is a good idea. Individual customers can be dropped from a test and if it happens a lot the test should be shut down. The number of users to allocate to a test depends on several factors. More users will find a smaller difference more quickly with higher confidence. The stream of new user signups should be considered as a precious resource. If you add 10,000 new users per day, you can test a handful of new things each month. Existing user tests should have much larger allocations, perhaps 50,000 users per test cell. The mark of a good product manager is that they have useful intuition and judgement on what would be a good thing to test. These practices are the basis for hypothesis driven development, as described in the book Lean Enterprise."
      },
      "tags": [
        "engineering",
        "lean",
        "machine learning",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_8d0add50",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Historical Archive Cloud Camp September 2008",
      "source": "Medium Archive",
      "published_date": "2018-12-04",
      "url": "https://medium.com/@adrianco/2018-12-04_Historical-Archive---Cloud-Camp---September-2008-ee0ca1a26e24",
      "content": {
        "text": "Historical Archive\u200a\u2014\u200aCloud Camp\u200a\u2014\u200aSeptember 2008\n\n[I found this in an archive and thought it was worth sharing 10 years on. This is one of the first events I attended as we were starting to figure out our cloud transition at Netflix, and was where I first met Dave Nielsen, Reuven Cohen, Sam Charrington, and Sara Dornsife]\n\nFrom cloudcamp 9/30/08\n\nHello CloudCampers,\n\nAs you probably know, CloudCampSV is taking place tonite at Sun\u2019s Executive Briefing Center in Menlo Park. And it\u2019s sold out. We have an excellent lineup of attendees and we expect the unconference to be quite informative. See below for the schedule.\n\nWe are also happy to partner with SDForum\u2019s Cloud Computing and Beyond conference which takes place tomorrow in Santa Clara.\n\nAnd of course we\u2019d like to thank our sponsors for their generous support. Without them, we\u2019d have to charge a lot more than zero dollars for this wonderful event. We hope you will view their offerings.\n\nSponsors:\n\nSun\u200a\u2014\u200ahttp://www.sun.com\n\nParaScale\u200a\u2014\u200ahttp://www.parascale.com\n\nGoGrid\u200a\u2014\u200ahttp://www.gogrid.com\n\nAppistry\u200a\u2014\u200ahttp://www.appistry.com\n\nSOASTA\u200a\u2014\u200ahttp://www.soasta.com (pronounced So Sta)\n\nSnapLogic\u200a\u2014\u200ahttp://www.snaplogic.com\n\nGoogle App Engine\u200a\u2014\u200ahttp://code.google.com/appengine\n\nAlso, we\u2019d like to invite you to propose topics for sessions tonite (in the format of a group discussion). If you are interested in proposing a session, no need to respond. You will be given the opportunity to propose your session at 6:30pm. So don\u2019t be late! For those of you unfamiliar with the unconference session proposal process, please read about it here: http://en.wikipedia.org/wiki/Unconference\n\nOther than that, we hope you get a lot out of the event. And we thank you for helping to make CloudCamp a success. If you have any questions, feel free to let one of us know.\n\nBest,\n\nDave Nielsen\n\nReuven Cohen\n\nJesse Silver\n\nSam Charrington\n\nSara Dornsife\n\nTonites agenda for your convenience:\n\nAgenda:\n\nCloudCampSV Agenda:\n\nPreparation:\n\n4:30 Open up for setup\n\n- All are welcome to help setup\u00a0;-)\n\nNetworking:\n\n5:30 pm (60 mins)\n\n- Drinks & Food\n\nOpening Remarks:\n\n6:30 pm (10 mins)\n\n- Thank the sponsors\n\nOverview of Camp Agenda:\n\n6:40 pm (20 mins)\n\n- Intro to Open Space format\n\n- Session Creation Process\n\nSession Breakouts:\n\n7:00 pm (50 mins with 10 min break)\n\n8:00 pm (50 mins with 10 min break)\n\n9:00 pm (short session\u200a\u2014\u200a30 mins)\n\nClosing Remarks:\n\n9:40 pm (10 mins)\n\nLet\u2019s Party\n\n9:50 pm (40 mins)\n\nDate: Tuesday, September 30, 2008 from 06:00 PM\u200a\u2014\u200a10:00 PM (PT)\n\nLocation: [this is now Facebook HQ]\n\nSun EBC Briefing Center\n\n15 Network Circle\n\nMenlo Park, CA 94025\n\nFor more information click here: CloudCamp Silicon Valley 08\n\nThis email was sent by Eventbrite, your total online solution to publish, register, manage and promote events. Host Your Events With Eventbrite\n\nClick here to unsubscribe from events by \u201cCloudCamp.\u201d",
        "metadata": {
          "word_count": 465,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "CloudCampSV is taking place tonite at Sun\u2019s Executive Briefing Center in Menlo Park. We have an excellent lineup of attendees and we expect the unconference to be quite informative. CloudCamp will be held in San Francisco, California. The event will run from 4:30 to 6:30pm. We would like to invite you to propose topics for sessions. CloudCamp Silicon Valley will be held in Menlo Park, California on September 30, 2008 from 06:00 PM\u200a\u2014\u200910:00pm (PT) The event will take place at the Sun EBC Briefing Center."
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_0184ad9d",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Excellent summary",
      "source": "Medium Archive",
      "published_date": "2018-06-21",
      "url": "https://medium.com/@adrianco/2018-06-21_Excellent-summary--c172e4aafe49",
      "content": {
        "text": "Excellent summary. I published some ideas on cloud native cost optimization a few years ago. There\u2019s a reInvent video, slideshare deck and this ACM paper http://dl.acm.org/citation.cfm?id=2693197",
        "metadata": {
          "word_count": 26,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I published some ideas on cloud native cost optimization a few years ago. There\u2019s a reInvent video, slideshare deck and this ACM paper."
      },
      "tags": [
        "cloud",
        "cloud native",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_23dcc459",
      "kind": "file",
      "subkind": "medium_post",
      "title": "Thoughts from 10 years ago on the iPhone",
      "source": "Medium Archive",
      "published_date": "2017-07-01",
      "url": "https://medium.com/@adrianco/2017-07-01_Thoughts-from-10-years-ago-on-the-iPhone-c19214383e00",
      "content": {
        "text": "Thoughts from 10 years ago on the\u00a0iPhone\n\nI was blogging a lot about phones and trying to build my own one at the time. Migrated this post here unmodified from my old blog.\n\nWhat the iPhone doesn\u2019t do (yet) and thoughts on why\u00a0not.\n\nThere has been a lot of commentary, complaints and opinions on the iPhone. I haven\u2019t seen much discussion of its features and strategy from the perspective of the realities of product development. In my opinion, what Apple have done is the right set of strategic and tactical moves for the first product in a new family. What was announced and shown is not the final feature set for the initial device and it does not include the full vision of the product.\n\nLets look at the timing of the announcement. As a new entrant in the mobile phone marketplace, the correct strategy is to pre-announce. There is no existing product from Apple to cannibalize, and there is only partial overlap with the iPod market. The announcement was made after the holiday shopping season, and the timing is setup to get a volume ramp in place for the 2007 holiday shopping season. The initial launch is based on the minimum marketable features (MMF) required to address the Apple oriented consumer marketplace. Rather than wait until the full feature set is ready or create low quality solutions for a wide feature set, the Apple strategy is to develop a small number of features to an extremely high level of quality and integration, and focus on the needs of their core market of existing OSX and iPod users.\n\nLets look at disclosure related issues. For a phone to be released as a product it has to go through FCC testing that takes a month or two, and the FCC process is relatively open. All the new phones are scooped by Engadget Mobile before they turn up in stores. For the big splash product announcement, it needed to be scheduled before Apple turns over an iPhone to the FCC test process. In order to keep details on the product quiet for as long as possible, it is also much easier to do the initial launch before completing negotiations with key third party application developers like Adobe and Microsoft. I\u2019ve heard that there is no Adobe Flash support in the device at present, and there is a clear need to support Microsoft Office at some point in the future. These omissions are easily fixed, its just a matter of time.\n\nNew models in the iPod range are announced when they are basically in stock in the stores. If you take the iPhone package, and remove the phone parts, keeping the iTunes music and video functionality, and WiFi/web connectivity, you are left with a very nice looking wide-screen networked iPod. Its main issue would be the relatively small capacity flash, so that could be increased, or a hard disk could possibly be crammed into the package. I would not expect anything like this to be announced until it is completely ready and in-stock, but if it exists, it could end up being released this summer around the same time as the iPhone actually ships. Since it isn\u2019t a phone, its outside the Cingular agreement, but adding a WiFi only VOIP client like iChat or Skype would create a product that competes with the Sony Mylo.\n\nDuring the demo\u2019s no-one tried to show the iPhone\u2019s camera, this indicates to me that it isn\u2019t finished, and I hear elsewhere that they are still working on video capture. For use as a video-phone, the camera is on the wrong side, you can\u2019t see the display while you are on-camera. This makes it seem less likely that a full iChat function will be included in the initial package.Apple is getting a lot of criticism for its locked down and controlled approach to third party software on the iPhone, and lack of a developers program. Developer support falls outside the minimum marketable features required for initial launch into the consumer marketplace. By taking full control of the product, Apple can make sure that very high quality standards are in place, and that applications integrate with the iPhone experience. The reality of product development also makes it hard to build a stable developer API until the product is finished, so I fully expect a phased developer program. The initial phase included applications like Google Maps and Cingular Visual Voicemail from development partners (and I expect some kind of GPS location service to appear in the product soon\u200a\u2014\u200aperhaps even in the initial release). The second phase will be a closed private developer program including big partners like Microsoft and Adobe. The third phase will start to open up to the Apple developer community, with stable public APIs and developer tools. Extensibility is a MMF for the professional/consumer (prosumer) and enterprise marketplaces, along with Microsoft Office support. This may take a year or so to arrive, its inevitable, but I can see why its not a feature of the initial product launch.\n\nThere has also been a lot of grumbling about Cingular and the lack of 3G service. Apple have dropped hints that they will support 3G sooner rather than later. My guess is that 3G is considered an MMF for the European and Asian markets, so I wouldn\u2019t be surprised if the models launched in those markets in late 2007 and 2008 included 3G support, and as Cingular\u2019s own 3G network continues to roll out over the USA the timing would make sense here as well. The real alternative to Cingular for Apple would be to setup their own Mobile Virtual Network Operator (MVNO) like Helio or Virgin Mobile. This is a big complicated thing to do without any experience, so my guess is that they decided that the highest priority was to get the product launched with a big network partner like Cingular, and to decide later on whether it is worth creating an MVNO for a less compromised product. So when the exclusivity arrangement with Cingular expires, they may well focus on their own MVNO services.\n\nSo thats my opinion, as someone who has developed products and strategies in the past and understands the compromises, but with no inside information on their actual plans. My own plan is to avoid the initial release, and see what the product looks like for the Xmas 2007 shopping season.\n\nOriginally published at perfcap.blogspot.com on Jan 15, 2007.",
        "metadata": {
          "word_count": 1080,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Apple has done the right set of strategic and tactical moves for the first product in a new family. As a new entrant in the mobile phone marketplace, the correct strategy is to pre-announce. There is no existing product from Apple to cannibalize. The initial launch is based on the minimum marketable features (MMF) required to address the Apple oriented consumer marketplace. Rather than wait until the full feature set is ready or create low quality solutions, the Apple strategy is to develop a small number of features. New models in the iPod range are announced when they are basically in stock in the stores. If you take the iPhone package, and remove the phone parts, you are left with a very nice looking wide-screen networked iPod. Its main issue would be the relatively small capacity flash, so that could be increased, or a hard disk could possibly be crammed into the package. During the demo's no-one tried to show the iPhone\u2019s camera, this indicates to me that it isn\u2019t finished. This makes it seem less likely that a full iChat function will be included in the initial package. a WiFi only VOIP client like iChat or Skype would create a product that competes with the Sony Mylo. Apple is expected to have a phased developer program. The initial phase included applications like Google Maps and Cingular Visual Voicemail from development partners. The second phase will be a closed private developer program including big partners like Microsoft and Adobe. The third phase will start to open up to the Apple developer community. The real alternative to Cingular for Apple would be to setup their own Mobile Virtual Network Operator (MVNO) like Helio or Virgin Mobile. My own plan is to avoid the initial release, and see what the product looks like for the Xmas 2007 shopping seaso. Originally published at perfcap.blogspot.com on Jan 15, 2007. Back to Mail Online home. Back To the page you came from."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_c51084d0",
      "kind": "file",
      "subkind": "medium_post",
      "title": "What Adrian Did Next Joined OrionX to do some analyst and advisory work",
      "source": "Medium Archive",
      "published_date": "2022-08-09",
      "url": "https://medium.com/@adrianco/2022-08-09_What-Adrian-Did-Next---Joined-OrionX-to-do-some-analyst-and-advisory-work-843c86cbdd17",
      "content": {
        "text": "What Adrian Did Next\u200a\u2014\u200aJoined OrionX to do some analyst and advisory\u00a0work\nCastle Rock lit by an orange sunset\u200a\u2014\u200apicture by\u00a0Adrian\nI retired from Amazon and full time corporate work at the beginning of June 2022. However I\u2019m signing up to be a long term advisor at a few companies where I already have good connections, and also was introduced to an opportunity to do some consulting advisory work for a month or so, before taking vacation in August. So while I have retired from corporate life, I have ideas to share, as an analyst and presenter, and needed a framework to support consulting and analysis work. I reached out to my friend and former manager from Sun, Shahin Khan, who along with a few ex-Sun partners have been building OrionX into a commercial entity that is very much aligned with that objective. After catching up with Shahin, Stephen Perrenod, and Cindee Mock, we agreed that I would join them as a \u201cPartner and Analyst\u201d, help out with some of their projects, and use OrionX to setup my own consulting work. Together, we can also develop and share ideas around technology trends.\n\nOrionX has an ambitious agenda, to identify the biggest technology trends and track how those trends influence each other, and to translate that to business strategy and market execution in a very broad set of markets. Having worked with something like 70 large and small companies over the past decade, the business processes and best practices have been tested and work well. The group has focused on quality at the expense of size and that has led to an excellent reputation. After several years of studying tech trends and working with several clients in each area, OrionX has developed relatively deep expertise in IoT, AI, Cybersecurity, Cryptocurrencies, and Quantum technologies. The group has deep roots in High Performance Computing (that\u2019s what Shahin, Steve, and I did at Sun). So right from the beginning, I\u2019m happy to spend some time getting back up to speed and developing some of my ideas around new hardware capabilities. Amongst other things, HPC is becoming more cloud centric, and blending with both quantum computing, and AI/ML. Stephen and Shahin have also been analyzing Bitcoin and developments in crypto currencies. While most of the work only occasionally gets published in the form of papers, the group is very active with other media:\n\n@HPCpodcast, a weekly podcast on HPC/AI, in association with InsideHPC.com co-hosted by Shahin and Doug Black\n\nMktg_Podcast, a biweekly podcast on B2B and B2C marketing, co-hosted by Shahin and Doug Garnett\n\nOrionX Download, a quarterly (or so) podcast on quantum computing and cryptocurrencies, and other advanced tech\n\nQuantum Analyst Roundtable, a monthly video cast with four other analyst firms, a pretty unique project\n\nThe OrionX YouTube channel, which has become a collaboration with the Washington, DC Quantum Computing Meetup, and includes lots of academic and non-academic talks.\n\nMy own consulting work is initially in two areas. One is general technology advice on the state of the art in development practices, along the lines of the Cloud for CEOs booklet I published for AWS in 2019. The other is around monitoring and measuring carbon, and helping some startups figure out how to do that effectively. I\u2019m speaking at various events as I mentioned in this blog post, getting back up to speed with the state of art in HPC, and will attend the Supercomputing show with Shahin.\n\nI\u2019m happy with how things have worked out, with the flexibility to take time out, along with an established platform at OrionX for sharing ideas as an analyst, occasional consulting work, and some advisory positions that will be announced soon.",
        "metadata": {
          "word_count": 618,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian retired from Amazon and full time corporate work at the beginning of June 2022. Joined OrionX to do some analyst and advisory\u00a0work. OrionX has an ambitious agenda, to identify the biggest technology trends and track how those trends influence each other. The group has deep roots in High Performance Computing (that\u2019s what Shahin, Steve, and I did at Sun) Shahin and Black have been working on quantum computing for some time. The pair also run a podcast and a YouTube channel. The channel is called OrionX, which is a collaboration with the Washington, DC Quantum Computing Meetup. I\u2019m happy with how things have worked out, with the flexibility to take time out, along with an established platform at OrionX for sharing ideas. I\u2019ll be speaking at various events as I mentioned in this blog post, getting back up to speed with the state of the art in HPC."
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "monitoring",
        "performance",
        "platform",
        "podcast",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_f7d3a242",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Patents on Peer to Peer Trading Platform",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/03/patents-on-peer-to-peer-systems.html",
      "content": {
        "text": "While working at eBay Research Labs I did some work using Skype that resulted in four patents in the area of P2P trading and reputation systems. I just noticed that they are now public, as of late 2007, and here they are:<br /><br /><a href=\"http://www.freepatentsonline.com/20070214249.html\">Peer-to-peer trading platform</a><br /><br /><a href=\"http://www.freepatentsonline.com/20070214259.html\">Peer-to-peer trading platform with relative reputation-based item search and buddy rating</a><br /><br /><a href=\"http://www.freepatentsonline.com/20070214250.html\">Peer-to-peer trading platform with search caching</a><br /><br /><a href=\"http://www.freepatentsonline.com/20070211651.html\">Peer-to-peer trading platform with roles-based transactions</a><br /><br />The seed of the idea came from my good friend Josep Ferrandiz, who still works at PayPal. The ideas were developed with help from Zahid Ahmed who is now working for EMC, and with a lot of help from Neel Sundaresan who manages eBay Research labs. The patents are in all of our names.<br /><br />I think we came up with some cool ideas, we also stress tested the Skype API's in some novel ways and got some things fixed. The lasting legacy of this work is more robust application to application messaging API's and improvements in Skype4Java.",
        "metadata": {
          "word_count": 175,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "While working at eBay Research Labs I did some work using Skype that resulted in four patents in the area of P2P trading and reputation systems. The patents are now public. I think we came up with some cool ideas, we also stress tested the Skype API's in some novel ways and got some things fixed. The lasting legacy of this work is more robust application to application messaging API's and improvements in Skype4Java."
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_db909eeb",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "AT&T 3G Microcell doesn't support corporate account phones",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/at-3g-microcell-doesnt-support.html",
      "content": {
        "text": "I just got an ATT 3G Microcell for home, since our service is intermittent, sometimes so bad I can't send an SMS message successfully (although at times it works fine). We live in the mountains, and have several different poor signals to bond to, so that's probably why it is variable.<br /><br />In the store, they checked my phone number and said I couldn't activate a Microcell on my account, since it's a Netflix corporate account iPhone. However, my wife has an iPhone on a personal account, so I used that as the activation account.<br /><br />However, when I tried to add my own phone number to the Microcell's whitelist of up to 10 numbers, I got this error message<br /><br />\"FTC309: invalid subscriber information found\" - which got no hits on Google, however after this blog post it should get one..<br /><br />So the website told me to call the generic ATT support number. After a few short waits, a very polite first level support guy (who had probably never heard of the Microcell before I called) found a well informed business support specialist who was able to confirm that the error code was due to the Microcell only supporting consumer accounts. She agreed that it didn't make a lot of sense, found that they had been running a trial of corporate accounts a few months ago, but didn't know when/if it might change. She forwarded me an internal support document that discussed the limitations of the trial, including the lack of corporate support. Although it does say that it supports all ATT 3G devices.<br /><br />I installed it anyway, it takes a few hours to set itself up the first time. We will see how well it works for other iPhones, meanwhile I will just have to use Skype to make calls and send SMS... If it doesn't work out I will take it back.<br /><br />------- Update -------<br />I contacted the corporate account support team via work, and they over-rode the system to add me to the Microcell. So my corporate account iPhone is now working fine with the Microcell. They are investigating further to see if they can make this a more general fix for other people, but if you are in the same situation as I was, you should be able to escalate to support to get it fixed.",
        "metadata": {
          "word_count": 394,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I just got an ATT 3G Microcell for home, since our service is intermittent, sometimes so bad I can't send an SMS message successfully. We live in the mountains, and have several different poor signals to bond to, so that's probably why it is variable. The Microcell only supports consumer accounts. It takes a few hours to set itself up the first time. If it doesn't work out I will take it back. I. can make this a more general fix for other people, but if you are in the same situation as I was, you should be able to escalate to support to get it fixed."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_cccf778d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CPU Power Management",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/cpu-power-management.html",
      "content": {
        "text": "<p class=\"MsoNormal\" style=\"margin-right: 0.05in; text-align: left;\"><span style=\"font-size: 10pt; position: relative; top: -5pt;\">AMD PowerNow! for the Opteron series of server CPUs dynamically manages the CPU clock speed based on Utilization.<span style=\"\">  </span>The speed takes a few milliseconds to change, and it is not clear exactly what speeds are supported, but one report stated that the normal speed of 2.6GHz would reduce to as low as 1.2GHz under a light load. This report also shows CPU detailed configuration and power savings. <a href=\"http://www.gamepc.com/labs/view_content.asp?id=opteron285&page=3\">http://www.gamepc.com/labs/view_content.asp?id=opteron285&amp;page=3</a><o:p></o:p></span></p> <div style=\"text-align: left;\">  </div> <p class=\"MsoNormal\" style=\"margin-right: 0.05in; text-align: left;\"><span style=\"font-size: 10pt; position: relative; top: -5pt;\">The problem with this for capacity management is that there is no indication of the average clock rate in the standard system metrics collected by capacity planning tools. PowerNow! is described by AMD at <span style=\"\"> </span><a href=\"http://www.amd.com/us-en/0,,3715_12353,00.html\">http://www.amd.com/us-en/0,,3715_12353,00.html</a> and drivers for Linux and Windows are available from <a href=\"http://www.amd.com/us-en/Processors/TechnicalResources/0,,30_182_871_9033,00.html\">http://www.amd.com/us-en/Processors/TechnicalResources/0,,30_182_871_9033,00.html</a>. In the future, operating systems may be able to take the current speed into account, and estimate the capability utilization, but the service time is higher at low clock rates, so we will still see some confusing metrics.<o:p></o:p></span></p> <div style=\"text-align: left;\">    </div> <p class=\"MsoNormal\" style=\"margin-right: 0.05in; text-align: left;\"><span style=\"font-size: 10pt; position: relative; top: -5pt;\">The current PowerNow! implementation works on a per-chip basis, and Opteron\u2019s have two complete CPU cores per chip that share a common clock rate. In a multiprocessor system made up of several chips, each pair of processing cores could be running at a different speed, and their speed can change several times a second.<o:p></o:p></span></p> <div style=\"text-align: left;\">    </div> <p class=\"MsoNormal\" style=\"margin-right: 0.05in; text-align: left;\"><span style=\"font-size: 10pt; position: relative; top: -5pt;\"><o:p></o:p>Our basic assumption for well behaved workloads, that mean service time is a constant quantity, is invalidated in a very non-linear manner, and utilization measurements will also move in mysterious ways....<o:p></o:p></span></p>",
        "metadata": {
          "word_count": 298,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AMD PowerNow! for the Opteron series of server CPUs dynamically manages the CPU clock speed based on Utilization. The speed takes a few milliseconds to change. One report stated that the normal speed of 2.6GHz would reduce to as low as 1.2GHz under a light load. The current PowerNow! implementation works on a per-chip basis, and Opteron\u2019s have two complete CPU cores per chip that share a common clock rate. In the future, operating systems may be able to take the current speed into account, and estimate the capability utilization, but the service time is higher at low clock rates. Each pair of processing cores could be running at a different speed, and their speed can change several times a second. Our assumption that service time is a constant quantity is invalidated in a very non-linear manner."
      },
      "tags": [
        "metrics"
      ]
    },
    {
      "id": "virtual_adrianco_file_db853ce0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Free performance and capacity tools",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/07/free-performance-and-capacity-tools.html",
      "content": {
        "text": "What free tools do you use for performance and capacity planning work? I'm particularly interested in figuring out combinations of tools that work well together.<br /><br />Mario Jauvin and I are signed up to present a workshop this december at <a href=\"http://www.cmg.org\">CMG05</a> in Florida on performance monitoring and capacity planning with free tools. I know a lot about some of the tools, but it would be useful to get some input on the relative usage level of what is available.<br /><br />Here is a list of the kind of tools I'm interested in:<br /><br />Orca, <a href=\"http://www.orcaware.com/\">www.orcaware.com</a> data collection using SE toolkit etc. and web display using rrdtool<br />BigBrother <a href=\"http://www.bb4.org/\">www.bb4.org</a> and BigSister, <a href=\"http://bigsister.graeff.com/\">bigsister.graeff.com</a> which do you use and why?<br />OpenNMS, <a href=\"http://opennms.org/wiki\">opennms.org/wiki</a> Large scale network management system<br />Nagios, <a href=\"http://www.nagios.org/about\">www.nagios.org/about</a> Simpler and smaller scale network element management<br />SE Toolkit, <a href=\"http://www.sunfreeware.com/setoolkit.html\">www.sunfreeware.com/setoolkit.html</a> Solaris specific instrumentation and tools<br />R, <a href=\"http://www.r-project.org/\">www.r-project.org</a> statistical analysis and graphing package<br />MySQL, <a href=\"http://www.mysql.com/\">www.mysql.com</a> database for storing large amounts of performance data<br />PDQ, <a href=\"http://www.perfdynamics.com/\">www.perfdynamics.com</a> toolkit for building analytical queueing models in C or Perl<br />Ganglia, <a href=\"http://ganglia.sourceforge.net/\">ganglia.sourceforge.net</a> large scale distributed system monitoring<br /><br />Does the license model matter? There is a wide variety of licenses for free tools, from an absence of any license, through GPL to free use of commercially owned products. Do you use anything that is free, or does it have to meet some criteria?<br /><br />I'm inviting a lot more input than I have before, add comments to this post or send me messages, I'll summarize, and I'll describe the tools in more detail and how they inter-relate in future postings.<br /><br />Cheers Adrian",
        "metadata": {
          "word_count": 274,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Mario Jauvin and I are signed up to present a workshop this december on performance monitoring and capacity planning with free tools. I'm particularly interested in figuring out combinations of tools that work well together. Here is a list of the kind of tools I'm interested in. SE Toolkit, Solaris specific instrumentation and tools. R, statistical analysis and graphing package. PDQ, analytical queueing models in C or Perl. Ganglia, large scale distributed system monitoring. Adrian will describe the tools in more detail and how they inter-relate in future postings. re, add comments to this post or send me messages, I'll summarize."
      },
      "tags": [
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_bf915437",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "My Music 2009",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/01/my-music-2009.html",
      "content": {
        "text": "an eclectic list of the CDs that  I played most: by Amanda Palmer, The Duckworth Lewis Method, Fractal, Japonize Elephants, Zoe Keating, Tony Levin, Mirthkon, Muse, Queens of the Stone Age, Neil Young.<br /><br />Some of it was released in 2009, some of it was new to me in 2009, here are links to the albums.<br /><br /><a href=\"http://amandapalmer.net/\">Amanda Palmer - Who Killed Amanda Palmer</a><br /><a href=\"http://www.dlmethod.com/\">Neil Hannon - The Duckworth Lewis Method</a><br /><a href=\"http://itunes.apple.com/us/album/sequitur/id305514613\">Fractal - Sequitur</a><br /><a href=\"http://itunes.apple.com/us/album/40-years-of-our-family/id261066718\">Japonize Elephants - 40 Years of our Family</a><br /><a href=\"http://itunes.apple.com/us/album/one-cello-x-16-natoma/id97723501\">Zoe Keating - One Cello x 16: Natoma</a><br /><a href=\"http://itunes.apple.com/us/album/stick-man/id266688708\">Tony Levin - Stick Man</a><br /><a href=\"http://www.mirthkon.com/\">Mirthkon - Vehicle</a><br /><a href=\"http://itunes.apple.com/us/album/the-resistance/id326492721\">Muse - The Resistance</a><br /><a href=\"http://itunes.apple.com/us/album/era-vulgaris/id257091632\">Queens of the Stone Age - Era Vulgaris</a><br /><a href=\"http://itunes.apple.com/us/album/fork-in-the-road/id310330195\">Neil Young - Fork in the Road</a>",
        "metadata": {
          "word_count": 127,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Amanda Palmer, The Duckworth Lewis Method, Fractal, Japonize Elephants, Zoe Keating, Tony Levin, Mirthkon, Muse, Queens of the Stone Age, Neil Young. Muse - The Resistance. Queens of the Stone Age - Era Vulgaris. Neil Young - Fork in the Road. The Beatles - Sgt. Pepper."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fcd4b78e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Eventual Consistency of Cloud?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/08/eventual-consistency-of-cloud.html",
      "content": {
        "text": "Lori MacVittie wrote that eventually cloud standards will converge around the private cloud standards (RT @swardley). I disagree, and there are several examples I can think of that point to the opposite conclusion, that public cloud will be the standard, and AWS will continue to dominate. <a href=\"http://devcentral.f5.com/weblogs/macvittie/archive/2010/08/18/the-inevitable-eventual-consistency-of-cloud-computing.aspx\">The original article is here</a>.<br /><br />The first point of disagreement is the claim that public clouds aren't getting the input they need from customers to mature their services, because real enterprise customers aren't running in the public cloud. Well, here at Netflix, we are giving Amazon exactly that kind of input. We use every feature they have, we are driving them hard and Amazon is taking the input and improving their product rapidly in ways that benefit all the users of AWS.<br /><br />My main disagreement is the claim that lots of individual IT departments will converge on a single standard that will win out over public cloud standards. I find this highly implausible, there are a host of vendors feeding technology to enterprise clouds, and they will all do the usual vendor thing of looking for ways to lock in the customer, even if they base on the same standards their implementations will be different. Here's an example, the \"private\" Enterprise Unix variants Solaris, AIX, HP-UX, IRIX, OSF/1 etc. are all based on the same Unix standards, however the \"public\" alternatives are Linux and BSD, and Linux has won the mind share in this space. I see Linux as an analogy for the public cloud in the sense that there is a very low barrier to adoption. For Linux, you can download it to run on any old computer for nothing, learn to use it, then build very low cost solutions out of it. For AWS, for a few dollars on the existing Amazon account you use to buy books etc, you can explore all the features and learn to build very powerful systems in a few hours. This has produced a large population of very productive engineers, who know how to use AWS, Linux and other open source tools to solve problems rapidly at low cost using the same tools. In contrast, if every enterprise cloud moves ahead by solving their problems independently they will produce a variety of architectures, each optimized to their own problem, and with their own tooling, and a very small number of people who know how to run each variant. You will also find that every company also uses the public cloud to get stuff done more quickly and cheaply than the IT department, so that will become the common standard.<br /><br />Part of the thinking behind Netflix' move to the cloud is that large public cloud providers like Amazon will have far more engineers working on making their infrastructure robust, scalable, well automated and secure than any individual enterprise could afford. By using AWS we are leveraging a huge investment made by Amazon, and paying a small amount for it on a month by month basis. We also get to efficiently allocate resources, for example how much does it cost to provision a large cage of equipment in a new datacenter and how long does it take from deciding to do it, to having it running reliably in production? Let's say $10M and many months. Instead we could spend $10M on licensing more movies and TV shows to stream, and grow incrementally in the cloud. In a few months time we have more customers than if we spent the money up front on buying compute capacity and we just keep re-provisioning new instances in the cloud, so we never end up with a datacenter full of inappropriately sized or obsolete equipment. At present, Netflix' growth is accelerating, so it is difficult to guess in advance how much capacity to invest in, but we have already flat-lined our datacenter capacity, and have all incremental capacity happening on AWS. For example, we can just fire up a few thousand instances for a week to encode all the new movies we just bought the rights to, then stop paying for them until another big deal closes. Likewise on The Oscars Awards night, there is a big spike in web traffic, and we can grow on the day and shrink afterwards as needed without planning it and buying hardware a long time in advance.<br /><br />While the other public and private cloud vendors are competing to come up with standards, we are finding that resumes from the kind of engineers we want to hire already reference their experience with AWS as a de-facto cloud standard. It's also easier to attract the best people if they will learn transferable skills and work on the very latest technologies.<br /><br />That might sound like a lock-in, but a well designed architecture is layered, and the actual AWS dependencies are very localized in our code base. The bet on the end game is that in coming years, other cloud vendors produce large scale AWS compatible offerings (full featured, not just EC2 and S3), and a very large scale multi-vendor low cost public cloud market is created. Then even a large and fast growing enterprise like Netflix will be an insignificant and ever smaller proportion of the cloud. By definition, you can't be an insignificant proportion of your own private cloud....",
        "metadata": {
          "word_count": 888,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Lori MacVittie wrote that eventually cloud standards will converge around the private cloud standards. I disagree, and there are several examples that point to the opposite conclusion, that public cloud will be the standard. I see Linux as an analogy for the public cloud in the sense that there is a very low barrier to adoption. For Linux, you can download it to run on any old computer for nothing, learn to use it, then build very low cost solutions. For AWS, for a few dollars on the existing Amazon account you can explore all the features and learn to build very powerful systems in a few hours. Netflix is using Amazon's cloud to get stuff done more quickly and cheaply than the IT department. By using AWS we are leveraging a huge investment made by Amazon, and paying a small amount on a month by month basis. Netflix has already flat-lined its datacenter capacity, and have all incremental capacity happening on AWS. At present, Netflix' growth is accelerating, so it is difficult to guess in advance how much capacity to invest in. AWS is a de-facto cloud standard. It's easier to attract the best people if they will learn transferable skills. The bet on the end game is that in coming years, other cloud vendors produce large scale AWS compatible offerings. By definition, you can't be an insignificant proportion of your own private cloud. nd ever smaller proportion of the cloud. You can be a part of a cloud that is bigger than you are."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_ce73ca0c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cloud Outage Reports",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/11/cloud-outage-reports.html",
      "content": {
        "text": "The detailed summaries of outages from cloud vendors are comprehensive and the response to each highlights many lessons in how to build robust distributed systems. For outages that significantly affected Netflix, the Netflix techblog report gives insight into how to effectively build reliable services on top of AWS. I've included some Google and Azure outages here because they illustrate different failure modes that should be taken into account. Recent AWS and Azure outage reports have far more detail than Google outage reports.<br />\n<br />\nI plan to collect reports here over time, and welcome links to other write-ups of outages and how to survive them. My naming convention is {vendor} {primary scope} {cause}. The scope may be global, a specific region, or a zone in the region. In some cases there are secondary impacts with a wider scope but shorter duration such as regional control planes becoming unavailable for a short time during a zone outage.<br />\n<br />\nThis post was written while researching my AWS Re:Invent talk.<br />\nSlides:&nbsp;<a href=\"http://www.slideshare.net/AmazonWebServices/arc203-netflixha\" target=\"_blank\">http://www.slideshare.net/AmazonWebServices/arc203-netflixha</a><br />\nVideo:&nbsp;<a href=\"http://www.youtube.com/watch?v=dekV3Oq7pH8\" target=\"_blank\">http://www.youtube.com/watch?v=dekV3Oq7pH8</a><br />\n<br />\n<hr />\n<h2>\nNovember 18th, 2014 - Azure Global Storage Outage</h2>\n<h4>\nMicrosoft Reports</h4>\n<div>\n<a href=\"http://azure.microsoft.com/blog/2014/11/19/update-on-azure-storage-service-interruption/\">http://azure.microsoft.com/blog/2014/11/19/update-on-azure-storage-service-interruption/</a><br />\n<br />\n<a href=\"http://azure.microsoft.com/blog/2014/12/17/final-root-cause-analysis-and-improvement-areas-nov-18-azure-storage-service-interruption/\">http://azure.microsoft.com/blog/2014/12/17/final-root-cause-analysis-and-improvement-areas-nov-18-azure-storage-service-interruption/</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nJanuary 10th, 2014 - Dropbox Global Outage</h2>\n<h4>\nDropbox Report</h4>\n<div>\n<a href=\"https://tech.dropbox.com/2014/01/outage-post-mortem/\">https://tech.dropbox.com/2014/01/outage-post-mortem/</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nApril 20th, 2013 - Google Global API Outage</h2>\n<h4>\nGoogle Report</h4>\n<div>\n<a href=\"http://googledevelopers.blogspot.com/2013/05/google-api-infrastructure-outage.html\">http://googledevelopers.blogspot.com/2013/05/google-api-infrastructure-outage.html</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nFebruary 22nd, 2013 - Azure Global Outage Cert Expiry</h2>\n<h4>\nAzure Report</h4>\n<div>\n<a href=\"http://blogs.msdn.com/b/windowsazure/archive/2013/03/01/details-of-the-february-22nd-2013-windows-azure-storage-disruption.aspx\">http://blogs.msdn.com/b/windowsazure/archive/2013/03/01/details-of-the-february-22nd-2013-windows-azure-storage-disruption.aspx</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nDecember 24th, 2012 - AWS US-East Partial Regional ELB State Overwritten</h2>\n<h4>\nAWS Service Event Report</h4>\n<a href=\"http://aws.amazon.com/message/680587/\" target=\"_blank\">http://aws.amazon.com/message/680587/</a><br />\n<h4>\nNetflix Techblog Report</h4>\n<a href=\"http://techblog.netflix.com/2012/12/a-closer-look-at-christmas-eve-outage.html\" target=\"_blank\">http://techblog.netflix.com/2012/12/a-closer-look-at-christmas-eve-outage.html</a><br />\n<h3>\n</h3>\n<hr />\n<h2>\nOctober 26th, 2012 - Google AppEngine Network Router Overload</h2>\n<h4>\nGoogle Outage Report</h4>\n<div>\n<a href=\"http://googleappengine.blogspot.com/2012/10/about-todays-app-engine-outage.html\">http://googleappengine.blogspot.com/2012/10/about-todays-app-engine-outage.html</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nOctober 22, 2012 - AWS US-East Zone EBS Data Collector Bug</h2>\n<h4>\nAWS Outage Report</h4>\n<div>\n<a href=\"http://aws.amazon.com/message/680342/\">http://aws.amazon.com/message/680342/</a></div>\n<div>\n<h4>\nNetflix Techblog Report</h4>\n</div>\n<div>\n<a href=\"http://techblog.netflix.com/2012/10/post-mortem-of-october-222012-aws.html\">http://techblog.netflix.com/2012/10/post-mortem-of-october-222012-aws.html</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nJune 29th 2012 -&nbsp;AWS US-East Zone Power Outage During Storm&nbsp;</h2>\n<h4>\nAWS Outage Report</h4>\n<div>\n<a href=\"http://aws.amazon.com/message/67457/\">http://aws.amazon.com/message/67457/</a></div>\n<h4>\nNetflix Techblog Report</h4>\n<div>\n<a href=\"http://techblog.netflix.com/2012/07/lessons-netflix-learned-from-aws-storm.html\">http://techblog.netflix.com/2012/07/lessons-netflix-learned-from-aws-storm.html</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nJune 13th, 2012 - AWS US-East SimpleDB Region Outage</h2>\n<h4>\nAWS Outage Report</h4>\n<div>\n<a href=\"http://aws.amazon.com/message/65649/\">http://aws.amazon.com/message/65649/</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nFebruary 29th, 2012 - Microsoft Azure Global Leap-Year Outage</h2>\n<h4>\nAzure Outage Report</h4>\n<div>\n<a href=\"http://blogs.msdn.com/b/windowsazure/archive/2012/03/09/summary-of-windows-azure-service-disruption-on-feb-29th-2012.aspx\">http://blogs.msdn.com/b/windowsazure/archive/2012/03/09/summary-of-windows-azure-service-disruption-on-feb-29th-2012.aspx</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nAugust 17th, 2011 - AWS EU-West Zone Power Outage</h2>\n<h4>\nAWS Outage Report</h4>\n<div>\n<a href=\"http://aws.amazon.com/message/2329B7/\">http://aws.amazon.com/message/2329B7/</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nApril 2011 - AWS US-East Zone EBS Outage</h2>\n<div>\n<h4>\nAWS Outage Report</h4>\n</div>\n<div>\n<a href=\"http://aws.amazon.com/message/65648/\">http://aws.amazon.com/message/65648/</a></div>\n<div>\n<h4>\nNetflix Techblog Report</h4>\n</div>\n<div>\n<a href=\"http://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html\">http://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html</a><br />\n<h3>\n</h3>\n<hr />\n<h2>\nFebruary 24th, 2010 - Google App Engine Power Outage</h2>\n</div>\n<h4>\nGoogle Forum Report</h4>\n<div>\n<a href=\"https://groups.google.com/forum/#!topic/google-appengine/p2QKJ0OSLc8\">https://groups.google.com/forum/#!topic/google-appengine/p2QKJ0OSLc8</a></div>\n<h3>\n</h3>\n<hr />\n<h2>\nJuly 20th, 2008 - AWS Global S3 Gossip Protocol Corruption</h2>\n<h4>\nAWS Outage Report</h4>\n<div style=\"text-align: justify;\">\n<span style=\"color: #29303b; font-family: Georgia, Times New Roman, sans-serif; font-size: x-small;\"><a href=\"http://status.aws.amazon.com/s3-20080720.html\">http://status.aws.amazon.com/s3-20080720.html</a></span></div>\n<div style=\"text-align: justify;\">\n<span style=\"color: #29303b; font-family: Georgia, Times New Roman, sans-serif; font-size: x-small;\"><br /></span></div>\n<span style=\"background-color: #fff3db; color: #29303b; font-family: Georgia, 'Times New Roman', sans-serif; font-size: 13px; text-align: justify;\"><br /></span>",
        "metadata": {
          "word_count": 557,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The detailed summaries of outages from cloud vendors are comprehensive and the response to each highlights many lessons in how to build robust distributed systems. For outages that significantly affected Netflix, the Netflix techblog report gives insight into how to effectively build reliable services on top of AWS. I've included some Google and Azure outages because they illustrate different failure modes that should be taken into account. November 18th, 2014 - Azure Global Storage Outage. Dropbox Global Outage on January 10th,2014. Re:Invent talk on November 18th. February 22nd, 2013 - Azure Global Outage Cert Expiry. December 24th, 2012 - AWS US-East Partial Regional ELB State Overwritten. April 20th, 2013 -- Google Global API Outage. October 26th, 2012 - Google AppEngine Network Router Overload. October 22, 2012. - AWS US-East Zone EBS Data Collector Bug. October 26th 2012 - Netflix Techblog Report. AWS US-East Zone Power Outage During Storm. Netflix Techblog Report. Microsoft Azure Global Leap-Year Outage. Amazon Web Services outage. August 17th, 2011 - AWS EU-West Zone Power Outage. July 20th, 2008 - AWS Global S3 Gossip Protocol Corruption. February 24th, 2010 - Google App Engine Power outage. AWS Outage Report is available at: http://status.aws.com/s3-20080720.html. The outage was caused by a fault in the Amazon Web Services (AWS) cloud."
      },
      "tags": [
        "aws",
        "azure",
        "cloud",
        "distributed systems",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_e3d9574c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Slingbox-ing my TiVo",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/slingbox-ing-my-tivo.html",
      "content": {
        "text": "As I <a href=\"http://perfcap.blogspot.com/2006/11/slingbox-for-xmas.html\">mentioned before</a>, we got a Slingbox AV this Xmas, and I'm quite impressed. It was easy to setup, has been no trouble since, and now we can watch and control our TiVo from anywhere. The main use we have is to watch programs we have saved on our TiVo from anywhere in the house, and it runs a full TV resolution feed that uses a few Mbit/s to support this on our WiFi based laptop's. When running on a work PC laptop (Dell Inspiron with XP) the performance was very smooth, however on my three year old Apple Powerbook G4 I'm down on CPU power and running a beta-version of the Sling player that seems to need some more tuning. If I kill just about everything else on the Mac, it plays fairly well, but the picture keeps stalling if anything else is running.<br />I'm hoping that they will do some more tuning to fix this in the final release for the Mac...",
        "metadata": {
          "word_count": 166,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Slingbox AV was easy to setup, has been no trouble since. Main use is to watch programs we have saved on our TiVo from anywhere in the house."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_60cc2205",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CMG05 trip comments and \"utilization is useless...\"",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/12/cmg05-trip-comments-and-utilization-is.html",
      "content": {
        "text": "I have a good time at the <a href=http://www.cmg.org>Computer Measurement Group</a> meeting in Orlando recently. Mario Jauvin and I put together a tutorial on \"Capacity Planning and Performance Monitoring with Free Tools\" that was well attended, although relatively few attendees seem to be using free tools, mostly due to policy and support issues. We also attended James Holtman's workshop on using the free statistics package 'R' for performance data analysis and plotting.<br /><br />The main new theme at this year's conference seemed to be CPU virtualization. Many people using VMware, Zen, Solaris containers and other virtualization facilities are finding that their measurements of CPU utilization don't make sense any more. Both BMC and Teamquest are working on building some support for virtualization concepts into their tools.<br /><br />My observation is that utilization is useless as a metric and should be abandoned. It has been useless in virtualized disk subsystems for some time, and is now useless for CPU measurement as well. There used to be a clear relationship between response time and utilization, but systems are now so complex that those relationships no longer hold. Instead, you need to directly measure response time and relate it to throughput. Utilization is properly defined as busy time as a proportion of elapsed time. The replacement for utilization is headroom which is defined as the unused proportion of the maximum possible throughput. Dave Fisk calls this Capability Utilization.<br /><br />I'm thinking of writing a paper, maybe for next year's CMG on this topic.... <br /><br />Happy holidays everyone<br />Cheers Adrian",
        "metadata": {
          "word_count": 257,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The main new theme at this year's conference seemed to be CPU virtualization. CPU utilization is useless as a metric and should be abandoned. It has been useless in virtualized disk subsystems for some time, and is now useless for C. There used to be a clear relationship between response time and utilization, but systems are now so complex that those relationships no longer hold. Utilization is properly defined as busy time as a proportion of elapsed time. Headroom is defined as the unused proportion of the maximum possible throughput."
      },
      "tags": [
        "containers",
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_ef671c12",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Mobile disruptions and going mainstream",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/05/mobile-disruptions-and-going-mainstream.html",
      "content": {
        "text": "VC thoughts via <a href=\"http://petercranstone.blogspot.com/2008/05/top-10-tech-trends.html\">Peter Cranstone</a><br /><br />From Vinod Knosla: The mobile phone will be a mainstream personal computer.<br /><br />From Roger McNamee: Betting on smart phones: The mobile device migration to smart phones from features phones will produce even greater disruption than PC industry moving from character mode to graphical interface.<br /><br />Pretty much what I've been saying for the last year or two...",
        "metadata": {
          "word_count": 65,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Vinod Knosla: The mobile phone will be a mainstream personal computer. Roger McNamee: The migration to smart phones from features phones will produce even greater disruption than PC industry moving from character mode to graphical interface."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d364cb0a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Interesting Videos from AWS Re:Invent 2014",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/12/interesting-videos-from-aws-reinvent.html",
      "content": {
        "text": "There are too many, but here are the ones I've watched that I found particularly interesting. I'll add to this list as I find time to watch more of them.<br />\n<br />\nTo start with here's my own talk. I'm looking for feedback on it, as it's the first time I've tried this topic and I'm looking for more data to support the concepts I'm discussing.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/LZwlkqERv2g/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/LZwlkqERv2g&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/LZwlkqERv2g&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nThe \"must see\" talk for Re:Invent is James Hamilton on Innovation at Scale.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/JIQETrFC_SQ/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/JIQETrFC_SQ&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/JIQETrFC_SQ&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nThe new EC2 Container Service that supports Docker, described by Deepak Singh<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/2vJLS8qfhI0/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/2vJLS8qfhI0&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/2vJLS8qfhI0&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nAn excellent and entertaining talk on networking optimization by Becky Weiss - learn what SR-IOV means.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/JUw8y_pqD_Y/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/JUw8y_pqD_Y&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/JUw8y_pqD_Y&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nEC2 Instance Deep Dive by John Phillips explains HVM and storage path optimization.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/ujGx0tiI1L4/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/ujGx0tiI1L4&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/ujGx0tiI1L4&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nThe VPC Deep Dive by Kevin Miller talks about some long awaited features such as migration tools from EC2 Classic to VPC.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/HexrVfuIY1k/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/HexrVfuIY1k&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/HexrVfuIY1k&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nThere were lots of Netflix talks, but from Asgard to Zuul by Ruslan included the announcement of Docker images to make trying out NetflixOSS tools trivial.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/p7ysHhs5hl0/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/p7ysHhs5hl0&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/p7ysHhs5hl0&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nOne of the most popular talks at the event was Brendan Gregg of Netflix on EC2 Performance Tuning<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/7Cyd22kOqWc/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/7Cyd22kOqWc&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/7Cyd22kOqWc&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nPFC305 Embracing Failure Fault Injection and Service Reliability - Monkeys and the new FIT failure injection system that replaces Latency Monkey.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object width=\"320\" height=\"266\" class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/wrY7XoOnysg/0.jpg\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/wrY7XoOnysg&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/wrY7XoOnysg&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\nMore Netflix Talks<br />\nBDT403 Next Generation Big Data Platform at Netflix<br />\nARC317 Maintaining a Highly Available Front Door at Massive Scale<br />\nPFC304 Effective IPC for Microservices in the Cloud<br />\nENT209 Cloud Migration, DevOps and Distributed Systems<br />\nAPP310 Using Apache Mesos in the Cloud<br />\n<br />",
        "metadata": {
          "word_count": 563,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "To start with here's my own talk. I'm looking for feedback on it, as it's the first time I've tried this topic. There are too many, but here are the ones I've watched that I found particularly interesting. I'll add to this list as I find time to watch more of them. The \"must see\" talk for Re:Invent is James Hamilton on Innovation at Scale. The new EC2 Container Service that supports Docker is described by Deepak Singh. An excellent and entertaining talk on networking optimization by Becky Weiss. Learn what SR-IOV means and how to use it to your advantage. EC2 Instance Deep Dive by John Phillips explains HVM and storage path optimization. The video is part of a larger HVM Deep Dive on HVM. The VPC Deep Dive by Kevin Miller talks about some long awaited features such as migration tools from EC2 Classic to VPC. VPC is available for download from the Mac App Store. There were lots of Netflix talks, but from Asgard to Zuul by Ruslan included the announcement of Docker images to make trying out NetflixOSS tools trivial. PFC305 Embracing Failure Fault Injection and Service Reliability - Monkeys and the new FIT failure injection system that replaces Latency Monkey. an interview with Gregg of Netflix on EC2 Performance Tuning. The next generation big data platform at Netflix will be called Netflix Big Data. The platform will be built on top of Apache Mesos, a cloud-based software platform."
      },
      "tags": [
        "aws",
        "big data",
        "cloud",
        "devops",
        "distributed systems",
        "docker",
        "microservices",
        "netflix",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_a0a4aa38",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Books",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2004/07/books.html",
      "content": {
        "text": "Web Services Explained - Solutions and Applications for the Real World by Joe Clabby - Prentice Hall 2002\n<br />\n<br />An introductory primer suitable for less-technical managers, but with good coverage of what the technology is supposed to be for, issues and gotchas.\n<br />\n<br />\n<br />Developing Enterprise Web Services - An Architects Guide by Sandeep Chatterjee and James Webber - Prentice Hall 2004\n<br />\n<br />Much more in depth and up to date coverage. I'm still reading it, more comments later.\n<br />\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 94,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Developing Enterprise Web Services - An Architects Guide by Sandeep Chatterjee and James Webber - Prentice Hall 2004. Much more in depth and up to date coverage."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_11be0bd2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Build your own phone, any way you want it!",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/build-your-own-phone-any-way-you-want.html",
      "content": {
        "text": "As an antidote to all the grumbling about the lack of an open developers approach to the Apple iPhone I'm going to talk about some projects that are building open phone platforms.<br /><br />I'm a member of the Silicon Valley <a href=\"http://www.hbmobile.org/\">Homebrew Mobile Phone Club</a>, and there are <a href=\"http://www.opencellphone.org/index.php?title=Main_Page\">several</a> <a href=\"http://www.widgetry.org\">projects</a> in existence to create a free, open source, software stack and applications for mobile phones. The hardware parts are available off the shelf, so you can buy a <a href=\"http://www.trolltech.com/products/qtopia/greenphone\">complete open phone</a> or use whatever parts you like. All you need is an enclosure, and you are on your way to whatever spec you wanted, with whatever applications you wanted.<br /><br />Actually, if someone out there knows Autocad and wants to help design some enclosures, the assistance would be appreciated. The SVHMPC often meets at <a href=\"http://techshop.ws\">Techshop</a>, where they have a 3D printer that can make anything that can be described by a CAD file in ABS plastic.<br /><br />The phones use off the shelf GSM modems, that take a SIM card and deal with the network. They operate like any unlocked phone would work, and the GSM modem module prevents anything bad from happening to the network. The command set looks a lot like the old hayes modem AT codes, and Linux device drivers exist to manage some of the common ones. Some of the GSM modems include GPS location services as well, most of them are based on the relatively slow GPRS/EDGE standards for Internet connections.<br /><br />My own ideal phone starts with a 3.7\" VGA resolution touch screen and includes WiFi, bluetooth GPS and a 3G network, but I may have to put up with GPRS for now. The CPU is based on 400MHz ARM chips from Gumstix, and the <a href=\"http://www.widgetry.org/dokuwiki/doku.php?id=parts_lists\">parts list from the OpenCell project</a> gives you some idea of what's available.<br /><br />Its going to cost more than an iPhone, won't be as thin, and as cute, but I will be able to make it do whatever I want, and it will be runing as much of the standard Linux 2.6 distribution as I feel like.<br /><br />I built my first three home computers up from bare PCBs, I still have my soldering iron, and I'm not afraid to use it :-)",
        "metadata": {
          "word_count": 380,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There are several projects that are building open phone platforms. The hardware parts are available off the shelf. All you need is an enclosure, and you are on your way to whatever spec you wanted. The phones use off the shelf GSM modems, that take a SIM card and deal with the network. The command set looks a lot like the old hayes modem AT codes, and Linux device drivers exist to manage some of the common ones. I built my first three home computers up from bare PCBs, I still have my soldering iron, and I'm not afraid to use it :-) I will be able to make it do whatever I want. It will be runing as much of the standard Linux 2.6 distribution as I feel like."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f5dac67c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Extracct code posted on orcaware.com",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/07/extracct-code-posted-on-orcawarecom.html",
      "content": {
        "text": "Since the code I was writing to extract useful information from extended accounting logs is intended to end up as a data source for Orca, it made sense to host it on that site. I haven't had a chance to do any more development on it for a few months, so the source, including SPARC and x86 binaries is now posted there. Thanks to Blair Zajac for the space.<br /><br /><a href=http://www.orcaware.com/orca/pub/extracct/>extracct - extract useful data from extended Solaris accounting</a><br /><br />If anyone feels inspired to develop it further, please send me any changes you make.<br /><br />Cheers Adrian<br /><br />Update Feb 2006: I just downloaded it and untar'd it with no problem, so try downloading it again if you have any problems, should be about 54K long.<br /><br />Adrian",
        "metadata": {
          "word_count": 130,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Extracting useful information from extended accounting logs is intended to end up as a data source for Orca. I haven't had a chance to do any more development on it for a few months, so the source is now posted there."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_dcc4834d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Book: Release It! Design and Deploy Production Ready Software by Michael Nygard",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/01/book-release-it-design-and-deploy.html",
      "content": {
        "text": "Essential reading for anyone architecting web sites. It contains clear explanations and real-life anecdotes of many of the patterns and anti-patterns that I've run into over the years, and I'm learning some new ones. Now I just have to get all the engineers on my team to read it... <a href=\"http://www.amazon.com/Release-Production-Ready-Software-Pragmatic-Programmers/dp/0978739213/\">Here it is on Amazon</a>.",
        "metadata": {
          "word_count": 55,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Essential reading for anyone architecting web sites. Contains clear explanations and real-life anecdotes of many of the patterns and anti-patterns that I've run into over the years."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fff54ac1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Comments on Web Traffic",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/09/comments-on-web-traffic_03.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>This blog gets about 50 visitors a day, and most new visitors arrive as the result of a google search for my name, Thumper/ZFS or the SE toolkit. There is a very small number of yahoo and msn searches. The other main source of traffic is the <a href=\"http://community.sun.com\">Sun Community blogging site</a>, which links to Sun Alumni's blogs including this one. A few weeks ago I got a lot of traffic from Sun, and I think I traced it back to a <a href=\"http://blogs.sun.com/jonathan/entry/another_first\">blog entry from Jonathan Schwartz</a>, who talked about his General Counsel's blog, and <a href=\"http://blogs.sun.com/dillon/entry/new_blog_entry\">Mike Dillon the GC mentioned the Sun Community blog site</a>. This doubled my traffic for a week or two.</p>  <p>The other recent change is that I stopped showing adverts from <a href=\"http://www.ctxbay.com\">ctxbay</a>, which was created by an eBay developer as a side project, won a prize, but never really worked well enough to be useful. I've replaced them with the official eBay in-house AdContext system, which is being beta tested. AdContext looks at your page content, and matches keywords it with popular items from eBay. It can be configured to exclude certain keywords (in my case I exclude \"Adrian\", which was causing problems for ctxbay), and you can choose certain categories or stores to pick items from. I've picked Technology Books and some Storage hardware categories. I'm going to experiment with different formats and constraints, to see how well it works.</p>  <p>There are three formats, text only, pictures (which I started with) and flash (which scrolls multiple adds into the same amount of screen space). I'll switch it to flash when I get around to it...<br /></p>  <p><br /></p>  <p style=\"font-size: 10px; text-align: right;\">technorati tags:<a href=\"http://technorati.com/tag/sun\" rel=\"tag\">sun</a>, <a href=\"http://technorati.com/tag/thumper\" rel=\"tag\">thumper</a>, <a href=\"http://technorati.com/tag/traffic\" rel=\"tag\">traffic</a></p><p style=\"text-align: right; font-size: 8px;\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 300,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Most new visitors arrive as the result of a google search for my name, Thumper/ZFS or the SE toolkit. The other main source of traffic is the <a href=\"http://community.sun.com\">Sun Community blogging site. eBay's in-house AdContext system is being beta tested. AdContext looks at your page content, and matches keywords it with popular items from eBay. Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock. >traffic."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_8d68cd48",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "BIL Podcast: Garrett Lisi - Exceptionally Simple Theory of Everything",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/03/bil-podcast-garrett-lisi-theory-of.html",
      "content": {
        "text": "I published a BIL podcast from the recordings I made, first talk is my favorite, <a href=\"http://en.wikipedia.org/wiki/An_Exceptionally_Simple_Theory_of_Everything\">particle physics explained</a> by <a href=\"http://sifter.org/~aglisi/\">Garrett Lisi</a><br /><br /><a href=\"http://web.mac.com/adriancockcroft/BILcast/BILcast/BILcast.html\">http://web.mac.com/adriancockcroft/BILcast/BILcast/BILcast.html</a><br /><br />Subscribe to the podcast to pick up new talks as they are added.",
        "metadata": {
          "word_count": 40,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "BIL podcast from the recordings I made, first talk is my favorite. Particle physics explained by Garrett Lisi.Subscribe to the podcast to pick up new talks as they are added."
      },
      "tags": [
        "podcast"
      ]
    },
    {
      "id": "virtual_adrianco_file_c5fac61d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone Instant Queue Add for Netflix - get it now...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/11/iphone-instant-queue-add-for-netflix.html",
      "content": {
        "text": "I've released a second version of my very own iPhone app. I also reduced the price from 99c to FREE for the next few days. Please check it out and write a review for me on the app store. More info at <a href=\"http://www.millicomputer.com\">http://www.millicomputer.com</a>.",
        "metadata": {
          "word_count": 44,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I've released a second version of my very own iPhone app. I also reduced the price from 99c to FREE for the next few days. Please check it out and write a review for me on the app store."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_805e0ac2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Joost sends out a batch of beta invites",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/03/joost-sends-out-batch-of-beta-invites.html",
      "content": {
        "text": "I only got two, they are both already take and I don't have any spare, but several people are <a href=\"http://search.ebay.com/search/search.dll?from=R40&satitle=joost\">selling Joost invites on eBay</a> for a few bucks...",
        "metadata": {
          "word_count": 29,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Several people are selling Joost invites on eBay for a few bucks. I only got two, they are both already take and I don't have any spare."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3ff1d950",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Speeding Up Innovation - Cloud Expo Europe and QCon London 2014",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/03/speeding-up-innovation-cloud-expo.html",
      "content": {
        "text": "There is a huge transformation going on in enterprise IT at the moment. Decades old companies are discovering that IT which used to be a small side operation keeping track of things like employees and inventory, is now perhaps a thousand times more demanding and is central to their ability to compete.<br />\n<br />\n<ul>\n<li>From tracking inventory levels at sales outlets to handling web based sales directly.</li>\n<li>From managing a few employees to managing huge numbers of connected devices and mobile applications.</li>\n<li>From placing print adverts in newspapers and trade journals each month to using big data analytics to do real time bidding on internet adverts and sales force optimization.</li>\n</ul>\n<br />\nTechnologies and practices that have been developed in the leading web scale companies are finding their way into cloud based services that address the broadening of demand for these technologies, while packaging them to make them easier to implement. In addition, the pace of product development has greatly increased, and the ability to get faster feedback from customers and act on it conveys a significant competitive advantage.<br />\n<br />\nThe opening keynote talk I gave at Cloud Expo Europe on Feb 26th is aimed at setting the context and explaining why management needs to act. It ends with a few pointers towards next steps. The slideshare below doesn't include the builds that unfold with the explanation, and there isn't a video of the event, but I will be giving similar talks at other events in the coming months.<br />\n<br />\n<iframe allowfullscreen=\"\" frameborder=\"0\" height=\"356\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/31799721?rel=0\" style=\"border-width: 1px 1px 0; border: 1px solid #CCC; margin-bottom: 5px; max-width: 100%;\" width=\"427\"> </iframe> <br />\n<div style=\"margin-bottom: 5px;\">\n<strong> <a href=\"https://www.slideshare.net/adriancockcroft/speeding-up-31799721\" target=\"_blank\" title=\"Speeding Up Innovation\">Speeding Up Innovation</a> </strong> from <strong><a href=\"http://www.slideshare.net/adriancockcroft\" target=\"_blank\">Adrian Cockcroft</a></strong> </div>\n<br />\nOn Thursday next week I will be giving a developer oriented talk at <a href=\"http://qconlondon.com/\" target=\"_blank\">QCon London</a> \"Migrating to Microservices\". The talk goes into details on transitional techniques, architectures and tools that can be used to move a product development organization from the old world to a highly agile and innovative cloud based operation. This is an excellent conference and registration is still open, so I hope to see you there.<br />\n<br />\nIn my new role at Battery Ventures, we are looking for opportunities to fund companies in the enterprise IT space that can support the transition to this new world.<br />\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 412,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a huge transformation going on in enterprise IT at the moment. Decades old companies are discovering that IT which used to be a small side operation is now perhaps a thousand times more demanding. The opening keynote talk I gave at Cloud Expo Europe on Feb 26th is aimed at setting the context and explaining why management needs to act. eatly increased, and the ability to get faster feedback from customers and act on it conveys a significant competitive advantage. On Thursday next week I will be giving a developer oriented talk at QCon London. The talk goes into details on transitional techniques, architectures and tools that can be used to move a product development organization from the old world to a highly agile and innovative cloud based operation."
      },
      "tags": [
        "agile",
        "big data",
        "cloud",
        "microservices",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_f1e1588a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Permit - now the fun starts",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-permit-now-fun-starts.html",
      "content": {
        "text": "I went into the county office and picked up the permit, paid them some more money to cover their costs of inspecting the work as we do it. They gave me back one of the marked up copies of the final site and building plans, and the permit sheet, and I stopped off at a Fedex office to get several copies made. Next stop is Tuffshed, to talk to the foreman who gets it built, and the concrete contractor (Jose Amaya Concrete of Morgan Hill) who normally works with Tuffshed. Jose is available immediately so we may actually get going fairly quickly. First step is to mark out the foundation and pour it.<br /><br />The new septic tank connection pipe is going in today and being inspected tomorrow.",
        "metadata": {
          "word_count": 128,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The new septic tank connection pipe is going in today and being inspected tomorrow. The concrete contractor (Jose Amaya Concrete of Morgan Hill) who normally works with Tuffshed is available immediately. First step is to mark out the foundation and pour it."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_acfc55bf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal on iJigg",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/fractal-on-ijigg.html",
      "content": {
        "text": "Another music website, where users tag and vote for songs they like - kind of Digg for music. Worth a try, so I created an account and uploaded Fractal's <a href=\"http://www.ijigg.com/songs/C7CDEDPAA\">title track from their first CD - Continuum</a>.<br /><br />What do you think?<br /><br />Adrian",
        "metadata": {
          "word_count": 45,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian uploaded Fractal's title track from their first CD - Continuum. Another music website, where users tag and vote for songs they like."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d12073fd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Basic Instructions",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/02/whats-on-my-iphone-basic-instructions.html",
      "content": {
        "text": "One of my <a href=\"http://basicinstructions.net/\">favorite cartoons</a>, a collection of them is bunded as an app. Its not a live feed, but its good to be able to show people \"<a href=\"http://basicinstructions.net/?p=366\">how to wash a cat</a>\"... There is also a book out.",
        "metadata": {
          "word_count": 41,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "One of my favorite cartoons is bunded as an app. Its not a live feed, but its good to be able to show people how to wash a cat. There is also a book out."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ab5ca97c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Embedded songs from iJigg",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/embedded-songs-from-ijigg.html",
      "content": {
        "text": "This is a very nice embeddable player from iJigg, below is Continuum from my friends in Fractal, and also Smuggled Mutation by Estradasphere, which is from Palace of Mirrors, one of my most-played CDs in recent history. Its so varied and well played its impossible to get bored by it.<br /><br /><object width=\"315\" height=\"80\"><param name=\"movie\" value=\"http://www.ijigg.com/jiggPlayer.swf?songID=C7CDEDPAA&Autoplay=1\"><param name=\"scale\" value=\"noscale\" /><param name=\"wmode\" value=\"transparent\"><embed src=\"http://www.ijigg.com/jiggPlayer.swf?Autoplay=1&songID=C7CDEDPAA\" width=\"315\" height=\"80\"  scale=\"noscale\" wmode=\"transparent\"></embed></object> <br /><br /><object width=\"315\" height=\"80\"><param name=\"movie\" value=\"http://www.ijigg.com/jiggPlayer.swf?songID=D7E00P0&Autoplay=0\"><param name=\"scale\" value=\"noscale\" /><param name=\"wmode\" value=\"transparent\"><embed src=\"http://www.ijigg.com/jiggPlayer.swf?Autoplay=0&songID=D7E00P0\" width=\"315\" height=\"80\"  scale=\"noscale\" wmode=\"transparent\"></embed></object>",
        "metadata": {
          "word_count": 83,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This is a very nice embeddable player from iJigg. Below is Continuum from my friends in Fractal, and also Smuggled Mutation by Estradasphere, which is from Palace of Mirrors."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_10e07afd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "SunWorld Columns at ITworld",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/10/sunworld-columns-at-itworld.html",
      "content": {
        "text": "I found that the columns I wrote between 1995 and 1999 all seems to be online, but its hard to find, so I'm going to start by just listing everything I can find in its original form. BEWARE! Many of these articles were obsoleted by developments in later releases of Solaris, or refer to dead URL's so I'll work through them in subsequent blog entries and provide commentary and updates.<br /><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3380/UIR010329cockcroftletters/\" target=\"_blank\">2001/03: Collected Short Questions and Answers</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3744/UIR990801perf/\" target=\"_blank\">1999/08: What does 100 percent busy mean?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3745/UIR990701perf/\" target=\"_blank\">1999/07: Disk Error Detection</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/619/UIR990301perf/\" target=\"_blank\">1999/03: Digging into the details of WorkShop 5.0</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR990201perf/\" target=\"_blank\">1999/02: SyMON and SE get upgraded</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/14/UIR981201perf/\" target=\"_blank\">1998/12: Out and about at conferences</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR981101perf/\" target=\"_blank\">1998/11: What's new in Solaris 7?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3206/UIR981001perf/\" target=\"_blank\">1998/10: IOwait, what's the holdup?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/1748/UIR980901perf/\" target=\"_blank\">1998/09: Do collision levels accurately tell you the real story about your Ethernet?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3380/UIR980801perf/\" target=\"_blank\">1998/08: Unlocking the kernel</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR980701perf/\" target=\"_blank\">1998/07: Clearing up swap space confusion</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3603/UIR980601perf/\" target=\"_blank\">1998/06: How busy is the CPU, really?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR980501perf/\" target=\"_blank\">1998/05: Processor partitioning</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3138/UIR980401perf/\" target=\"_blank\">1998/04: Prying into processes and workloads</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR980301perf/\" target=\"_blank\">1998/03: Sizing up memory in Solaris</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR980201perf/\" target=\"_blank\">1998/02: Perfmeter unmasked</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR980101perf/\" target=\"_blank\">1998/01: SE Toolkit FAQ</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR971201perf/\" target=\"_blank\">1997/12:  At last! The updated SE release has arrived</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3215/UIR971101perf/\" target=\"_blank\">1997/11: Performance perplexities: Help! Where do I start?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/716/UIR971001perf/\" target=\"_blank\">1997/10: Learn to performance tune your Java programs</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/1120/UIR970901perf/\" target=\"_blank\">1997/09: Clarifying disk measurements and terminology</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR970801perf/\" target=\"_blank\">1997/08: How does Solaris 2.6 improve performance stats and Web performance?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/2784/UIR970701perf/\" target=\"_blank\">1997/07: Dissecting proxy Web cache performance</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/1427/UIR970601perf/\" target=\"_blank\">1997/06: Analysis of TCP transfer characteristics for Web servers made easier</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/1919/UIR970501perf/\" target=\"_blank\">1997/05: The memory go round</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR970401perf/\" target=\"_blank\">1997/04: Craving more books on Solaris? Look no further</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3400/UIR970301perf/\" target=\"_blank\">1997/03: How to optimize caching file accesses</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3144/UIR970201perf/\" target=\"_blank\">1997/02: Increase system performance by maximizing your cache</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR970101perf/\" target=\"_blank\">1997/01: Design your cache to match your applications</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/1427/UIR961201perf/\" target=\"_blank\">1996/12: Tips for TCP/IP monitoring and tuning</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3562/UIR961101perf/\" target=\"_blank\">1996/11: The right disk configurations for servers</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2402/UIR961001perf/\" target=\"_blank\">1996/10: Solving the iostat disk mystery</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Net/3138/UIR960901perf/\" target=\"_blank\">1996/09: Unveiling vmstat's charms</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3380/UIR960801perf/\" target=\"_blank\">1996/08: What's the best way to probe processes?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.javaworld.com/jw-07-1996/jw-07-javadev.html\" target=\"_blank\">1996/07  Monitor your Web server in realtime, Part 2</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2102/UIR960701perf/\" target=\"_blank\">1996/07: How can I optimize my programs for UltraSPARC?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/3743/UIR960601perf/\" target=\"_blank\">1996/06: How do disks really work?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/1936/UIR960501perf/\" target=\"_blank\">1996/05:  How much RAM is enough?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.javaworld.com/jw-04-1996/jw-04-javadev.html\" target=\"_blank\">1996/04: Monitor your Web server in real time, Part 1</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR960401perf/\" target=\"_blank\">1996/04: How does swap space work?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/AppDev/1609/UIR960301perf/\" target=\"_blank\">1996/03: Watching your Web server</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR960201perf/\" target=\"_blank\">1996/02: Which is better, static or dynamic linking?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR960101perf/\" target=\"_blank\">1996/01: What are the tunable kernel parameters for Solaris 2?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR951201perf/\" target=\"_blank\">1995/12: Performance Q&A Compendium</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/1057/UIR951101perf/\" target=\"_blank\">1995/11: When is it faster to have 64 bits?</a><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://www.itworld.com/Comp/2377/UIR951001perf/\" target=\"_blank\">1995/10 Help! I've lost my memory!</a>",
        "metadata": {
          "word_count": 571,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Many of these articles were obsoleted by developments in later releases of Solaris. I'll work through them in subsequent blog entries and provide commentary and updates. I found that the columns I wrote between 1995 and 1999 all seems to be online. D.js. d.js is a free, open-source, web-based version of the Java programming language. Use it to help you get the most out of WorkShop 5.0. 1998/08: Unlocking the kernel. 1998/06: How busy is the CPU, really? 1998/07: Clearing up swap space confusion. The SE Toolkit is a free, open-source Java software toolkit. It can be used to improve performance of Java programs. The toolkit can be downloaded from: http://www.itworld.com/Co. 1997/08: How does Solaris 2.6 improve performance stats and Web performance? 1997/09: Clarifying disk measurements and terminology. 1997/04: The memory go round. In 1997/03: How to optimize caching file accesses. In 1997/01: Design your cache to match your applications. In 1996/11: The right disk configurations for servers. In 1995/96: Tips for TCP/IP monitoring and tuning. 1996/10: Solving the iostat disk mystery. 1996/08: What's the best way to probe processes? 1996/07: How can I optimize my programs for UltraSPARC?  1996/04: Monitor your Web server in real time, Part 1. 1996/05: How much RAM is enough? 1996/06: How do disks really work? <a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http.itworld.com/Comp/2377/UIR960101perf/\" target=\"_blank\">1996/01: What are the tunable kernel parameters for Solaris 2?"
      },
      "tags": [
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_a0feed0b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - first step",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/06/solar-power-first-step.html",
      "content": {
        "text": "There is a rebate for solar power systems, and the first step is that the power company (PG&E) reserves a rebate and notifies me and SolarCity that if we install a system by a deadline we will get a certain amount of rebate. I have also got an appointment on Monday for the SolarCity engineer to come and spend a whole day taking measurements and designing the system installation.<br /><br />Last week we got the shingles replaced, since the old ones were getting near the end of their life, and its a good idea to put the solar panels over a roof that will last for a long time, so they won't need to be removed to fix anything.",
        "metadata": {
          "word_count": 119,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a rebate for solar power systems, and the first step is that the power company reserves a rebate and notifies me and SolarCity that if we install a system by a deadline we will get a certain amount of rebate. I have also got an appointment on Monday for the SolarCity engineer to come and spend a whole day taking measurements and designing the system installation."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7d80abee",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Bill Scott's Designing Web Interfaces book - and he's hiring...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/02/bill-scotts-designing-web-interfaces.html",
      "content": {
        "text": "Here is a (long) slideshare presentation of Bill's talk, <a href=\"http://looksgoodworkswell.blogspot.com/2009/02/oreilly-designing-web-interfaces.html\">there is a webcast of it</a> as well. Bill is also hiring for his group of UI engineers at Netflix, so if you are the <span style=\"font-style:italic;\">best of the best of the best</span> and want to work with the master himself, go for it!<br /><br /><br /><div style=\"width:425px;text-align:left\" id=\"__ss_986235\"><a style=\"font:14px Helvetica,Arial,Sans-serif;display:block;margin:12px 0 3px 0;text-decoration:underline;\" href=\"http://www.slideshare.net/billwscott/designing-web-interfaces-book-oreilly-webcast?type=presentation\" title=\"Designing Web Interfaces Book - O&#39;Reilly Webcast\">Designing Web Interfaces Book - O&#39;Reilly Webcast</a><object style=\"margin:0px\" width=\"425\" height=\"355\"><param name=\"movie\" value=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=dwi-1233708570866247-2&stripped_title=designing-web-interfaces-book-oreilly-webcast\" /><param name=\"allowFullScreen\" value=\"true\"/><param name=\"allowScriptAccess\" value=\"always\"/><embed src=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=dwi-1233708570866247-2&stripped_title=designing-web-interfaces-book-oreilly-webcast\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"355\"></embed></object><div style=\"font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;\">View more <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/\">presentations</a> from <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/billwscott\">Bill Scott</a>. (tags: <a style=\"text-decoration:underline;\" href=\"http://slideshare.net/tag/rich\">rich</a> <a style=\"text-decoration:underline;\" href=\"http://slideshare.net/tag/ajax\">ajax</a>)</div></div>",
        "metadata": {
          "word_count": 111,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Bill is also hiring for his group of UI engineers at Netflix. If you are the best of the best and want to work with the master himself, go for it! \" name=\"allowFullScreen\" value=\"true\"/><param name \"allowScriptAccess\" value \"always\"=\"always\" allowfullscreen\" \"width=\"425\" height=\"355\" \"padding-top:2px\" \"margin-bottom:1.5em; margin-top-right:0.25em; font-size:11px;font-family:tahoma,arial;height:26px;padding- top: 2px;\">View more <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/billwscott\">Bill Scott</a>. (tags: \"rich,\" \"ajax,\""
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_f68e1018",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Updated Ad Setup",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/09/updated-ad-setup.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>I just changed to the flash based ad sidebar. It lets you pick different keywords without reloading the page by clicking on the top tab.</p>  <p>I also continued to focus on Technology Books, picking some specific categories to try and exclude the certification and training books that I find less interesting. I then excluded Microsoft and mcse as keywords, and ended up with Cisco certification so I excluded them as well, and it looks like a more reasonable selection now.</p><p style=\"font-size:10px;text-align:right;\"><p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 91,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Flash based ad sidebar. It lets you pick different keywords without reloading the page by clicking on the top tab. Picking some specific categories to try and exclude the certification and training books that I find less interesting."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_afce7391",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Leaving eBay to join Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/leaving-ebay-to-join-netflix.html",
      "content": {
        "text": "I recently gave notice to eBay and start at <a href=\"http://www.netflix.com\">Netflix</a> on May 7th. I've had a lot of fun working at <a href=\"http://labs.ebay.com\">eBay Research Labs</a>, but I'm making a strategic move to a smaller company (about a tenth of the number of people as a technology organization) which makes it easier to take a broader role and develop skills and experience in new areas. I will be directing a few senior engineers who develop part of the Netflix web site as my primary role. Its an exciting new challenge!",
        "metadata": {
          "word_count": 90,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I recently gave notice to eBay and start at Netflix on May 7th. I will be directing a few senior engineers who develop part of the Netflix web site. Its an exciting new challenge!"
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_d67ecb11",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "I will be at Usenix 09 - San Diego - June 15-17",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/06/i-will-be-at-usenix-09-san-diego-june.html",
      "content": {
        "text": "I'm attending a tutorial on care and feeding of Hadoop on Monday, teaching my own tutorial on tuning and tools tuesday, and staying for the first day of papers on wednesday. See you there...<br /><br /><a href=\"http://www.usenix.org/usenix09/promote\"> <img src=\"http://www.usenix.org/events/usenix09/art/usenix09_button.jpg\" border=\"0\" width=\"125\" height=\"125\" alt=\"USENIX '09\"> </a>",
        "metadata": {
          "word_count": 45,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm attending a tutorial on care and feeding of Hadoop on Monday, teaching my own tutorial on tuning and tools t Tuesday, and staying for the first day of papers on Wednesday. See you there..."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9e29d978",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Parallel Application Performance and Tuning with OpenMP",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/11/parallel-application-performance-and.html",
      "content": {
        "text": "I just had dinner with my old friend Ruud van der Pas, who works for Sun's developer tools organization. He is a world class expert in making parallelized applications scale, and recently gave a talk with a very amusing <a href=\"http://www.thinkingparallel.com/2007/09/25/openmp-does-not-scale-or-does-it/\">performance conversation</a>.<br /><br /><a href=\"http://blogs.sun.com/ruud/\">Ruud is an advocate and expert</a> in OpenMP, and has just published an excellent book on the subject. \"Using OpenMP\". Published by MIT Press. The work was done together with Barbara Chapman and Gabriele Jost. The ISBN numbers are ISBN-10: 0-262-53302-2 and ISBN-13: 978-0-262-53302-7.<br /><br />Even if you aren't using OpenMP, the concepts for performance and scaling are the same for everyone...",
        "metadata": {
          "word_count": 106,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Ruud van der Pas is a world class expert in making parallelized applications scale. He has just published an excellent book on the subject, \"Using OpenMP\""
      },
      "tags": [
        "performance",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_b14a49a0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "what&#39;s on my iPhone: Bloomberg",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/what-on-my-iphone-bloomberg.html",
      "content": {
        "text": "The built in stock market app just gives prices but I use the Bloomberg app to get news and more detailed information on stocks.<div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div><br />",
        "metadata": {
          "word_count": 34,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The built in stock market app just gives prices but I use the Bloomberg app to get news and more detailed information on stocks. The Bloomberg app is free for iPhone users."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_490dfef5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "IPad makes it hard to embed YouTube in blog",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/ipad-makes-it-hard-to-embed-youtube-in.html",
      "content": {
        "text": "I'm trying to compose a blog post that embeds a YouTube video, but YouTube doesn't want to give me the HTML fragment to embed the video. If you browse to YouTube on an iPad it doesn't show any options to pick up the embed because it renders in \"touch\" mode. At the bottom of the page I found an option to switch to desktop mode and get the full page. This renders an embed option but doesn't let me select it to copy here. <br /><br />So I need to use my MacBook to compose posts. <br /><br />As the iPad moves the boundaries between content consumption and content generation, people need to rethink the functionality of the mobile and iPhone optimized web sites.",
        "metadata": {
          "word_count": 124,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "YouTube doesn't give me the HTML fragment to embed the video. If you browse to YouTube on an iPad it doesn't show any options to pick up the embed."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_ef0b323a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Making a case for Myphone",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/making-case-for-myphone.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiHI1jiQSi2jpbLG5JOLSC3YM01qkptZDfuBkONTiBYVNPxbAzNln-JK_ZRGW7ciF2FunsTjg1mFO5v36dAEjEghVq9FxofEdsUi81zy_b5JhJKDNLQWOe-ZmtHAkcSMXlA7Df/s1600-h/Finishedbezel.jpg\"><img style=\"margin: 0pt 10px 10px 0pt; float: left; cursor: pointer;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiiHI1jiQSi2jpbLG5JOLSC3YM01qkptZDfuBkONTiBYVNPxbAzNln-JK_ZRGW7ciF2FunsTjg1mFO5v36dAEjEghVq9FxofEdsUi81zy_b5JhJKDNLQWOe-ZmtHAkcSMXlA7Df/s400/Finishedbezel.jpg\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5026692173869394626\" border=\"0\" /></a><br />I mean that in both senses of the phrase \"making a case\". I have produced a lump of plastic, and I have good reasons why I want to build my phone.<br /><br />Here are the first parts I made, two copies of an LCD bezel with a rounded outside surface and a cutout to locate the LCD. The standing part is as it was made in the 3D printer, sideways on to get a smoother finish, and with additional \"support material\" that holds everything in place while the \"model material\" sets. The part lying flat is the same, but is face down and has the support material removed. The size is about 5\" by 3\" and the bezel is 0.1\" thick. It took about 7 hours to print the pair of bezels, each contains 0.6 cubic inches of model material and the total including support material was about 1 cubic inch. The bezels cost $10 each to make at <a href=\"http://techshop.ws\">Techshop in Menlo Park</a> ($100/month for membership, $10/cu inch to use the 3D printer).<br /><br />This experiment went quite well, and the next step was to extend the design to form a complete top and bottom case that fit together. This was kicked off using black rather than white plastic, and lying flat, face down. This prints more quickly, but uses more support material to create the base. The base will give the phone a textured surface, which can be made shiny by dipping in acetone, and I'm hoping it will look cool, like a carbon fibre finish. Pictures of the design, cad files, and a photo of the 3D printer in action can be found at the <a href=\"http://www.hbmobile.org/wiki/index.php?title=MyPhone_case\">SVHMPC Wiki</a>, I'm updating them as work progresses. The case walls are thicker than they need to be, so the final design will use less plastic and cost less than the $40 I was charged to make this complete case.<br /><br />The purpose of this build is to figure out where to put the contents, so I can mold support brackets into the design and make an attempt at a working prototype.<br /><br />Why am I doing this? Some of us are attending the <a href=\"http://conferences.oreillynet.com/etel2007/\">Emerging Telephony conference</a> in March, and I want to have something to show. Also its fun to make things, fun to hang out at Techshop, I'm learning a lot from the 3D printer instructor, and in the end I will have a phone that evolves rather than being thrown away every few years. If I want more memory, a bigger LCD, or different trade offs in features/size/battery life, I don't have to start from scratch or accept someone else's set of compromises. That's why I call it my phone.",
        "metadata": {
          "word_count": 473,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Here are the first parts I made, two copies of an LCD bezel with a rounded outside surface. The standing part is as it was made in the 3D printer, sideways on to get a smoo. The bezels cost $10 each to make at Techshop in Menlo Park. The size is about 5\" by 3\" and the bezel is 0.1\" thick. Each contains 0.6 cubic inches of model material. The case walls are thicker than they need to be, so the final design will use less plastic and cost less than the $40 I was charged to make this complete case. Pictures of the design, cad files, and a photo of the 3D printer in action can be found at the <a href=\"http://www.hbmobile.org/wiki/index.php?title=MyPhone_case\">SVHMPC Wiki. \"I don't have to start from scratch or accept someone else's set of compromises. That's why I call it my phone,\" he says."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_60f1d151",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - The Year in Review",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/09/solar-power-year-in-review.html",
      "content": {
        "text": "For our full story on solar <a href=\"http://perfcap.blogspot.com/search?q=solar\">click here</a>.<br /><br />We installed Solar panels in August 2009, and they were turned on during September. We have now had a whole year of power which is shown below. The time-of-use metering means that we get paid much more for the power we generate in the afternoon than the power we use overnight, so the \"zero point\" for billing is different than that for power consumption. PG&E recently sent us our annual bill, which was about $500. Added to the monthly bills for basic service this comes to about $700 for the whole year. Our previous electricity bill was about $2000 a year. Since we changed our hot water, clothes dryer and range from propane to electric, we saved over $1000 in propane cost as well. That puts payback at around ten years at todays prices, given the likelihood of increased propane and electricity prices over time, actual payback would be earlier than that.<br /><br />The panels have got a lot of dust on them (the roof is too high to get at easily to clean them), so aren't running at peak efficiency, the best day we saw was around 28KWh, with a lot of days of 26-27 KWh through the summer. The totals for each month are shown in the screenshots below.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgaNdKj0UQOcyxnY9q7Lyv_S6UN4Iqkpb2vDnWM7d-YNiz0mZgg97G462zp-TmoGgirJjNyH1g2VE1dVBmhNK9wyIes9EPtSNqIIy9HWk39kgpnfg2eWAgEMdjBxXS2fZuwWQm/s1600/IMG_0026.PNG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhgaNdKj0UQOcyxnY9q7Lyv_S6UN4Iqkpb2vDnWM7d-YNiz0mZgg97G462zp-TmoGgirJjNyH1g2VE1dVBmhNK9wyIes9EPtSNqIIy9HWk39kgpnfg2eWAgEMdjBxXS2fZuwWQm/s320/IMG_0026.PNG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5521375612110204242\" /></a><br /><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0M8Fgkkc2wrRnoGBHHLJbYi3HidvXefXty3IcFq0UgPuL9Lu7uDCvoDZVsMM4yE-abArnO95MvczmXRGRUN0Vbrh-BLZdM1JdxBMmMyG__EBO33w5ifPi3oz8m6EAajpUuGol/s1600/IMG_0025.PNG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg0M8Fgkkc2wrRnoGBHHLJbYi3HidvXefXty3IcFq0UgPuL9Lu7uDCvoDZVsMM4yE-abArnO95MvczmXRGRUN0Vbrh-BLZdM1JdxBMmMyG__EBO33w5ifPi3oz8m6EAajpUuGol/s320/IMG_0025.PNG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5521375406485595394\" /></a><br /><br />Since we have built a new garage, we now have a lot more roof area. So we are now looking to add another 4KW of panels, and to swap out our propane furnace for a heat pump that will give us heating and cooling (yay - its hot today...) sometime next spring. Then the only use for propane will be the emergency generator. It should also make the garage a bit cooler in the summer by shading half of the roof with panels.",
        "metadata": {
          "word_count": 338,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "We installed Solar panels in August 2009, and they were turned on during September. Added to the monthly bills for basic service this comes to about $700 for the whole year. Our previous electricity bill was about $2000 a year. Best day we saw was around 28KWh, with a lot of days of 26-27 KWh through the summer. (the roof is too high to get at easily to clean them), so aren't running at peak efficiency. Since we have built a new garage, we now have a lot more roof area. So we are now looking to add another 4KW of panels, and to swap out our propane furnace for a heat pump. This will give us heating and cooling (yay - its hot today...) sometime next spring. Then the only use for propane will be the emergency generator."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9709e30f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Night Camera",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-night-camera.html",
      "content": {
        "text": "Its very simple but it works amazingly well. When its dark, most non-flash digital pictures suffer from a lot of motion blur due to camera shake. The night camera app uses the accelerometer to wait until the camera has been still for long enough to get a clear picture.",
        "metadata": {
          "word_count": 49,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The night camera app uses the accelerometer to wait until the camera has been still for long enough to get a clear picture. When its dark, most non-flash digital pictures suffer from a lot of motion blur due to camera shake."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_eb9b1df9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "IEEE Conference Paper",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/07/ieee-conference-paper.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>I attended the IEEE E-Commerce conference in San Franscisco. The conference is known as CEC06/EEE06 and some other acronyms. It was a very interesting academic oriented event with a few hundred people from all over the world, I made some good contacts and learned some new stuff.</p>  <p>My own paper was about how I built a large scale simulation of a peer to peer network using a very efficient architecture based on the Occam language. I used to write a lot of Occam about 20 years ago, and it seemed appropriate to the problem I wanted to solve. I think most people are baffled by the language, but I like it. Unlike most recent languages where everything is an object with types and methods, in Occam everything is a process with protocols and messages. The other difference is that Occam was designed to run fast on a 10MHz CPU and on todays CPUs it is extremely fast and small compared to recent languages like Java.<br/> </p>  <p>What I found at the conference was that most of the simulation frameworks people were using were run overnight to generate results. My own example simulation of 1000 nodes ran for about three seconds to produce an interesting result.<br/> </p>  <p>The full paper can be obtained from <a href=\"http://doi.ieeecomputersociety.org/10.1109/CEC-EEE.2006.81\">http://doi.ieeecomputersociety.org/10.1109/CEC-EEE.2006.81</a></p>  <p>This is the official URL, and IEEE charges non-members for downloads.</p>  <br/> <p style=\"font-size:10px;text-align:right;\">technorati tags:<a href=\"http://technorati.com/tag/IEEE\" rel=\"tag\">IEEE</a>, <a href=\"http://technorati.com/tag/Occam\" rel=\"tag\">Occam</a>, <a href=\"http://technorati.com/tag/simulation\" rel=\"tag\">simulation</a></p><p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 248,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Occam was designed to run fast on a 10MHz CPU. Unlike most recent languages where everything is an object with types and methods, in Occam everything is a process with protocols. The full paper can be obtained from <a href=\"http://doi.ieeecomputersociety.org/10.1109/CEC-EEE.2006.81\"\u00a0\u00a0here. My own example simulation of 1000 nodes ran for about three seconds to produce an interesting result."
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_file_7e2fa310",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Enhanced headroom plot in R",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/07/enhanced-headroom-plot-in-r.html",
      "content": {
        "text": "For some reason I seem to find time to <a href=\"http://www.r-project.org\">write code in R</a> when I'm on an airplane. The last two trips I made resulted in significant enhancements and debugging of the code for my headroom plot. <a href=\"http://perfcap.blogspot.com/search?q=chp\">It started off simple</a> but it now has a lot of bells and whistles, including color coding.<br /><br />Main changes: the quantile used to remove outliers now only removes outliers that exceed the 95th percentile response time by default. It keeps all the throughput values unless you use qx=True.<br /><br />In each of the throughput bins used to draw the histogram, the maximum response time for that bin is now calculated and displayed as a staircase line unless you set max=False.<br /><br />The set of data is now split into ranges and color coded. The times series plot is coded so you can see the split, and the scatterplot shows how those points fall. I have been plotting weekly data at one minute intervals with split=7, which looks pretty good.<br /><br />I read in some data that has been extracted from vxstat into a csv format at a known URL and plotted it three ways.<br />I plot the first 2880 data points, picking the read data rather than write, two days at one minute intervals.<br /><br /><pre><br />> stime <- read.csv(url(\"http://somewhere/vxstat_response\"))<br />> sops <- read.csv(url(\"http://somewhere/vxstat_throughput\"))<br /><br />> names(sops)<br />[1] \"DateTime\"                             \"vxstat_dg_operationsRead\" <br />[3] \"vxstat_dg_operationsWrite\"<br /><br />> chp(sops[1:2880,2],stime[1:2880,2])<br />> chp(sops[1:2880,2],stime[1:2880,2],q=1.0)<br />> chp(sops[1:2880,2],stime[1:2880,2],q=1.0,splits=8)<br /></pre><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicozweOijXAXOq_MGRl-Eu_G0Vq0mcTa7ghReBBq8p2XfxMbcMPxBqRwETJydJxDFoDjhNO0xlCSxHRnKyPvTlR9EL2FmJ29lrrNAESD8Ig3UB_y2VW60b0P4Cg3pagq69EWd2/s1600-h/chpblog1.png\"><img style=\"cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEicozweOijXAXOq_MGRl-Eu_G0Vq0mcTa7ghReBBq8p2XfxMbcMPxBqRwETJydJxDFoDjhNO0xlCSxHRnKyPvTlR9EL2FmJ29lrrNAESD8Ig3UB_y2VW60b0P4Cg3pagq69EWd2/s400/chpblog1.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5223131801032379890\" /></a><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZa6eSR5LHNTdICbiuccSetn66WRiFZbpN9t2Kk63-zmzWfge7r99NKHGVdnGvY5HT0FsAdEZtPUH33RQ0UHRZ1d7cLkGcNrre4po9q5lg4rH93PfnmKvzZINPk72gNkNPrsgx/s1600-h/chpblog2.png\"><img style=\"cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZa6eSR5LHNTdICbiuccSetn66WRiFZbpN9t2Kk63-zmzWfge7r99NKHGVdnGvY5HT0FsAdEZtPUH33RQ0UHRZ1d7cLkGcNrre4po9q5lg4rH93PfnmKvzZINPk72gNkNPrsgx/s400/chpblog2.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5223131991156304722\" /></a><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8Yib6ijtSlSmUkOahFuIjwzPT06rBN5S-q1c-dCE4YLZMn8jJLQotrMlC8K-E1Fg6i0VZwbzFDxShoTAKv7DtJ_q-xUsaNVd6oUtX7PqEDuE8zqCp6TDv8tkyd8O70-4G17kn/s1600-h/chpblog3.png\"><img style=\"cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj8Yib6ijtSlSmUkOahFuIjwzPT06rBN5S-q1c-dCE4YLZMn8jJLQotrMlC8K-E1Fg6i0VZwbzFDxShoTAKv7DtJ_q-xUsaNVd6oUtX7PqEDuE8zqCp6TDv8tkyd8O70-4G17kn/s400/chpblog3.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5223132131633288482\" /></a><br /><br /><br /><br />Here is the code that generates the plot.<br /><br /><pre><br />> chp <-function(throughput,response, q=0.95, qx=F, xl=\"Throughput\",yl=\"Response\",tl=\"Throughput Over Time\",<br />ml=\"Headroom Plot\", fit=T, max=T, splits=0) {<br />      # remove zero throughput and response values<br />      nonzer <- (throughput != 0) & (response != 0)  # array of true/false<br />      y <- response[nonzer]<br />      x <- throughput[nonzer]<br />      # remove outliers, keep response time points inside 95% by default<br />      if (q != 1.0) {<br />              quant <- (y < quantile(y,q))<br />              # optionally trim throughput outliers as well<br />              if (qx) quant <- quant & (x < quantile(x, q))<br />              x <- x[quant]<br />              y <- y[quant]<br />      }<br />      # make histograms and record end points for scaling<br />      xhist <- hist(x,plot=FALSE)<br />      yhist <- hist(y,plot=FALSE)<br />      xbf <- xhist$breaks[1]                          # first<br />      ybf <- yhist$breaks[1]                          # first<br />      xbl <- xhist$breaks[length(xhist$breaks)]       # last<br />      ybl <- yhist$breaks[length(yhist$breaks)]       # last<br />      xcl <- length(xhist$counts)                     # count length<br />      ycl <- length(yhist$counts)                     # count length<br />      xrange <- c(0.0,xbl)<br />      yrange <- c(0.0,ybl)<br />      xlen <- length(x)<br />      # make a multi-region layout<br />      nf <- layout(matrix(c(1,3,4,2),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)<br />      layout.show(nf)<br />      # set plot margins for throughput histogram and plot it<br />      par(mar=c(0,4,3,0))<br />      barplot(xhist$counts, axes=FALSE,<br />              xlim=c(xcl*0.00-xbf/((xbl-xbf)/(xcl-0.5)),xcl*1.00),<br />              ylim=c(0, max(xhist$counts)), space=0, main=ml)<br />      # set plot margins for response histogram and plot it sideways<br />      par(mar=c(5,0,0,1))<br />      barplot(yhist$counts, axes=FALSE, xlim=c(0,max(yhist$counts)),<br />              ylim=c(ycl*0.00-ybf/((ybl-ybf)/(ycl-0.5)),ycl*1.00),<br />              space=0, horiz=TRUE)<br />      # set plot margins for time series plot<br />      par(mar=c(2.5,1.7,3,1))<br />      plot(x, main=tl, cex.axis=0.8, cex.main=0.8, type=\"S\")<br />      if (splits > 0) {<br />   step <- xlen/splits<br />   for(n in 0:(splits-1)) {<br />    lines((1+n*step):min((n+1)*step,xlen), x[(1+n*step):min((n+1)*step,xlen)], col=4+n)<br />   }<br />      }<br />      # set plot margins for main plot area<br />      par(mar=c(5,4,0,0))<br />      plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl, pch=20)<br />      if (max) {<br />        # max curve<br />        b <- xhist$breaks<br />        i <- b[2] - b[1] # interval<br />        maxl <- list(y[b[1] < x & x <= (b[1]+i)])<br />        for(n in b[c(-1,-length(b))]) maxl <- c(maxl,list(y[n < x & x <= (n+i)]))<br />        #print(maxl)<br />        maxv <- unlist(lapply(maxl,max)) # apply max function to elements of list<br />        #print(maxv)<br />        #lines(xhist$mids,maxv,col=2)  # join the dots<br />        #staircase plot showing the range for each max response<br />        lines(rep(b,1,each=2)[2:(2*length(maxv)+1)],rep(maxv,1,each=2),col=3)<br />        <br />      }<br />      if (fit) {<br />        # fit curve, weighted to predict high throughput<br />        # create persistent chpfit object using <<-<br />        chpfit <- glm(y ~ x, inverse.gaussian, weights=as.numeric(x))<br />        # add fitted values to plot, sorted by throughput<br />        lines(x[order(x)],chpfit$fitted.values[order(x)],col=2)<br />      }<br />      if (splits > 0) {<br />   step <- xlen/splits<br />   for(n in 0:(splits-1)) {<br />    Sys.sleep(1)<br />    points(x[(1+n*step):min((n+1)*step,xlen)],y[(1+n*step):min((n+1)*step,xlen)], xlim=xrange, ylim=yrange, col=4+n)<br />   }<br />      }<br />}<br /></pre>",
        "metadata": {
          "word_count": 730,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The headroom plot has been improved. The data is now split into ranges and color coded. The times series plot is coded so you can see the split. The scatterplot shows how those points fall. I read in some data that has been extracted from vxstat into a csv format at a known URL and plotted it three ways. I plot the first 2880 data points, picking the read data rather than write, two days at one minute intervals.  le=\"cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZa6eSR5LHNTdICbiuccSetn66WRiFZbpN9t2Kk63-zmzWfge7r99NKHGVdnGvY5HT0FsAdEZtPUH33RQ0UHRZ1d7cLkGcNrre4po9q5lg4rH93PfnmKvzZINPk72gNkNPrsgx/s1600- Here is the code that generates the plot. Chp <-function(throughput,response, q=0.95, qx=F, xl=\"Throughput\",yl=\"Response\",tl=\"Through put Over Time\",ml=\"Headroom Plot\", fit=T, max=T,. splits=0) Keep response time points inside 95% by default. If (q != 1.0) trim throughput outliers as well. Make histograms and record end points for scaling.  layout. nf <- layout(matrix(c(1,3,4,2),2,2,byrow=TRUE, c(3,1), c( 1,3), TRUE) layout.show(nf) # set plot margins for throughput histogram and plot it. barplot(xhist$counts, axes=FALSE, xlim=c(0,max(yhist $counts), space=0, main=ml) par(mar= c(5,0,0,.0), horiz=TROSE), plot(x, main =tl, cex.axis=0.8, type=\"S\") if (splits > 0  plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl, pch=20) if (max) { # set plot margins for main plot area par(mar=c(5,4,0,0), pch = 20) # create persistent chpfit object using >>-. # add fitted values to plot, sorted by throughput. # lines(x[order(x)],chpfit$fitted.values[ order(x),col=2) # points(x, y, xlim, ylim, col=4+n), if (splits > 0) {<br />   step <- xlen/splits.   for(n in 0:(splits-1), Sys.sleep(1)"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_44229377",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "NoSQL Netflix Use Case Comparison for Translattice",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/11/nosql-netflix-use-case-comparison-for_17.html",
      "content": {
        "text": "<div>[There is some discussion of this posting with comments by Michael at<a href=\"http://ask.slashdot.org/story/10/11/18/1947206/Horizontal-Scaling-of-SQL-Databases\"> Slashdot</a>]</div><div><blockquote></blockquote>Michael Lyle @mplyle CTO of <a href=\"http://www.translattice.com/\">Translattice</a> kindly provided a set of answers that I have interspersed with the questions below. Translattice isn't technically a NoSQL system, but it isn't a conventional database either. It's a distributed relational SQL database that supports eventual consistency, as Michael puts it:</div><div><span class=\"Apple-style-span\" style=\"border-collapse: collapse; \"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></span></div><blockquote><div><span class=\"Apple-style-span\" style=\"border-collapse: collapse; \"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>These answers are for the Translattice Application Platform (TAP)'s database component. Unlike other stores that have answered this question set, TAP contains a relational database that scales out over identical nodes.  TAP further allows applications written to the J2EE platform to scale out across the same collection of nodes.</i></span></span></span></div></blockquote><div>The original set of questions are <a href=\"http://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html\">posted here</a>. Each respondent will get their own blog post with answers, when there are enough to be interesting, I will write some summary comparisons.</div><div><br /></div><div>If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself.</div><div><br /></div><div><b><span class=\"Apple-style-span\"  style=\"font-size:large;\">Use Case Scenario for Comparison Across NoSQL Contenders </span></b></div>While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them, so that we understand the differences in behavior. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. I assume that is running on AWS and use that terminology, but the concepts are generic.<br /><br /><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><b>Use Case</b></span><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><br /></span>A TV based device calls the API to add a movie to its favorites list (like the Netflix instant queue, but I have simplified the concept here), then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list.<div><br /><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Favorites Storage<br /></b></span>Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads.</div><div><br /></div><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 1: Availability Zones<br /></b></span>When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones, is the initial write local or spread over the zones, is the write replication zone aware so data is repl<span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\">icated to more than one zone?</span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"   style=\"  border-collapse: collapse; font-family:arial, sans-serif;font-size:13px;\"><div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></div></div><blockquote><div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>In the Translattice Application Platform, data in relational tables is transparently sharded behind the scenes by the data store. These shards are stored redundantly across the nodes.  Reads are satisfied with the most local copy of data available on the network, unless that resource is currently overloaded in which case the system may fall back to reads from more distant locations. </i></span></span></div></div><div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><br /></i></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>When it comes to writes, applications have the choice on the durability and isolation levels for changes.  Each transaction may be made in a fully synchronous, serializable isolation level, or may be made in a locked eventually consistent mode that provides ACID serializable semantics except that durability may be sacrificed if an availability zone fails.  A further asynchronous mode allows potentially conflicting changes to be made and allows a user-provided reconciliation function to decide which change \"wins\".  A final commit requires a majority of nodes storing a shard to be available; in the case of the fully synchronous mode this would delay or prevent the return of success if a critical subset of the cluster fails.</i></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><br /></i></span></span></div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Policy mechanisms in the system allow administrators to specify how physical and cloud database instances correspond to administratively-relevant zones.  An administrator can choose to require, for instance, that each piece of information is replicated to at least three database nodes across a total of two availability zones.  An administrator may also use these mechanisms to require that particular tables or portions of tables must or must not be stored in a given zone (for instance, to meet compliance or security requirements).  Within the constraints set by policy, the system tracks usage patterns and places information in the most efficient locations.</i></span></span></div></blockquote><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></div></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p></span><blockquote><i><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></p></i></blockquote><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 2: Partitioned Behavior with Two Zones<br /></b></span>If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the NoSQL service support?<br /><ul><li>one zone decides that it is still working for reads and writes but half the size, and the other zone decide it is offline</li><li>both zones continue to satisfy reads, but refuse writes until repaired</li><li>data that has a master copy in the good zone supports read and write, slave copies stop for both read and write</li><li>both zones continue to accept writes, and attempt to reconcile any inconsistency on repair</li></ul><div><span class=\"Apple-style-span\" style=\"border-collapse: collapse; \"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><blockquote>Assuming that the SQL transaction in question is running in the fully synchronous or eventually consistent locked mode, writes will only be allowed in one of the two zones. Reads will continue in both zones, but will only be able to satisfy requests for which at least one replica of the requested data exists in the local zone (policy can be specified to ensure that this is always the case).  In the eventually consistent mode, multiple partitioned portions of the system can accept writes and reconcile later.  Essentially, any of the above desired modes can be used on a transaction-by-transaction basis depending on application and performance requirements.</blockquote></i></span></span></span></div><div><div></div></div><div><b>Question 3: Appending a movie to the favorites list</b></div>If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/values, and are queries by attribute/value supported?</div><div><span class=\"Apple-style-span\"   style=\"  border-collapse: collapse; font-family:arial, sans-serif;font-size:13px;\"><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></div><blockquote><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Because fully relational primitives are provided, there can easily be one row in the database per favorite.  Read-modify-write of the whole list is not required, and the only practical limits are application-defined.</i></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><br /></i></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Any SQL queries are supported against the store, and are transformed by the query planner into an efficient plan to execute the query across the distributed system.  Of course, how efficient a query is to execute will depend on the structure of the data and the indexes that an administrator has created.  We think this allows for considerable flexibility and business agility as the exact access methods that will be used on the data do not need to be fully determined in advance.</i></span></span></div></blockquote></span></div><span><span><b>Question 4: Handling Silent Data Corruption</b></span></span><div><span><span>When the storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously?</span></span></div><div><span><span><i><blockquote><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\">Network activity is protected by cryptographic hash authentication, which provides integrity verification as a side benefit.  Distributed transactions also take place through a global consensus protocol that uses hashes to ensure that checkpoints are in a consistent state (this is also how the system maintains transactional integrity and consistency when changes cross many shards).  Significant portions of the on-disk data are also presently protected by checksums and allow the database to \"fail\" a disk if corrupt data is read.</span></span></blockquote></i></span></span><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 5: Backup and Restore</b></span></div><div>Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?</div><div><span class=\"Apple-style-span\"   style=\"  border-collapse: collapse; font-family:arial, sans-serif;font-size:13px;\"><div></div><span><span><span><span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><blockquote>A good portion of this relational database's consistency model is implemented through a distributed multi-version concurrency control (MVCC) system.  Tuples that are in-use are preserved as the database autovacuum process will not remove tuples until it is guaranteed that no one could be looking at them anymore.  This allows a consistent version of the tables as of a point in time to be viewed from within a transaction; so BEGIN TRANSACTION; SELECT ... [or COPY FROM, to backup] ; COMMIT; works.  We provide mechanisms to allow database dumps to occur via this type of mechanism.<br />In the future we are likely to use this mechanism to allow quick snapshots of the entire database and rollbacks to previous snapshot versions (as well as to allow the use of snapshots to stage development versions of application code without affecting production state).</blockquote></i></span></span></span></span></span></span><div></div></span></div><i></i></div>",
        "metadata": {
          "word_count": 1636,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Translattice isn't technically a NoSQL system, but it isn't a conventional database either. It's a distributed relational SQL database that supports eventual consistency. The original set of questions were posted at http://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html. Each respondent will get their own blog post with answers. A TV based device calls the API to add a movie to its favorites list, then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads. A different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list. Is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones? Is the initial write local or spread across the zones? In the Translattice Application Platform, data in relational tables is transparently sharded behind the scenes by the data store. These shards are stored redundantly across the nodes. Reads are satisfied with the most local copy of data available on the network.  applications have the choice on the durability and isolation levels for changes. Each transaction may be made in a fully synchronous, serializable isolation level. A final commit requires a majority of nodes storing a shard to be available. Policy mechanisms in the system allow administrators to specify how physical and cloud database instances correspond to administratively-relevant zones. An administrator can choose to require, for instance, that each piece of information is replicated to at least three database nodes across a total of two availability zones. If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, what happens? In particular, which of these outcomes does the NoSQL service support? Assuming that the transaction in question is running in the fully synchronous or eventually consistent locked mode, writes will only be allowed in one of the two zones. In the eventually consistent mode, multiple partitioned portions of the system can accept writes and reconcil. If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/ values, and are queries by attribute/value supported? There can easily be one row in the database per favorite. Read-modify-write of the whole list is not required. The only practical limits are application-defined. Network activity is protected by cryptographic hash authentication, which provides integrity verification as a side benefit. Distributed transactions also take place through a global consensus protocol that uses hashes to ensure that checkpoints are in a consistent state. Significant portions of the on-disk data are also also also significant. l do not need to be fully determined in advance. Question 5: Backup and Restore. How can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen? A good portion of this relational database's consistency model is implemented through a distributed multi-version concurrency control (MVCC) system. Tuples that are in-use are preserved as the database autovacuum process will not remove tuples until it is guaranteed no one could be looking at them anymore."
      },
      "tags": [
        "aws",
        "cloud",
        "netflix",
        "performance",
        "platform",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_7c122b3f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "myPhone 2.0 Case comes off the 3D printer",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/myphone-20-case-comes-off-3d-printer.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRc_6sXgMkggQe2FhGy_dj4MpH86a8e1MvK6fNKt5Lq3IEHXGZ4Cb-hkHh_JSg8JUzg5rm_Fd1oFJ2bnzQZki6dwImJIB9drZf7bTjvUTvT5ucbHB79waB-17fSZHYM4upk_fm/s1600-h/Photo_041207_001.jpg\"><img style=\"cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiRc_6sXgMkggQe2FhGy_dj4MpH86a8e1MvK6fNKt5Lq3IEHXGZ4Cb-hkHh_JSg8JUzg5rm_Fd1oFJ2bnzQZki6dwImJIB9drZf7bTjvUTvT5ucbHB79waB-17fSZHYM4upk_fm/s400/Photo_041207_001.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5052784047560403234\" /></a><br /><br /><a href=\"http://s69.photobucket.com/albums/i79/fastspider/myPhone/\">More pictures of the latest myPhone 2.0 case design</a>.<br /><br />This version has a deeper case, and was printed at a simple angle, the previous attempt ended up warping, so I hope these changes will keep it flat. It also has a slot in the bottom end to take an iPod connector, which carries power, USB, stereo line level output etc. an antenna mounting hole at the top, and a retaining clip design to hold the LCD in place.<br /><br />This contains 2.6 cubic inches of ABS plastic, and used 1.4 cubic inches of support material, which costs about $40 in materials and took 8 hours to print at <a href=\"http://techshop.ws\">Techshop</a>.",
        "metadata": {
          "word_count": 125,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This version has a deeper case, and was printed at a simple angle, the previous attempt ended up warping. It also has a slot in the bottom end to take an iPod connector, which carries power, USB, ste. This contains 2.6 cubic inches of ABS plastic, and used 1.4 cubicinches of support material. It costs about $40 in materials and took 8 hours to print at Techshop."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2171cf18",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Slingbox for Xmas",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/slingbox-for-xmas.html",
      "content": {
        "text": "What new toys can we get this Xmas? I already have the stuff I need. I'd like a phone with Wifi and 3G network speeds and a touch screen, but my Treo 650 is OK until something better comes along. I'm curious to see what Apple may come up with next year, in the much rumoured iPhone.<br /><br />I've had a Tivo since 1999, and I'd like to be able to view the programs elsewhere in the house or further afield. The Slingbox does this, lets me control the Tivo remotely and stream the programs to a Windows or OSX laptop. The Slingbox AV was $179 list price on their web site, but I had a look on shopping.com and found it for sale from an out of state vendor for $140 with free shipping and no tax. So that's going to be the new toy this Xmas....",
        "metadata": {
          "word_count": 148,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Slingbox AV was $179 list price on their web site, but I had a look on shopping.com and found it for sale from an out of state vendor for $140. So that's going to be the new toy this Xmas...."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_948da49b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Memcached 500K ops/s on Niagara T2",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/05/memcached-500k-opss-on-niagara-t2.html",
      "content": {
        "text": "This is some nice work, scalability analysis to fix lock contention in memcached, then running on an out of the box Solaris T2 gets 500K ops/s or 9.6Gbits/s depending on payload size. The Niagara 2 based T2 has built-in 10Gbit network, and 8 cores with 8 threads in one chip. Sub-millisecond response.<br /><br /><a href=\"http://blogs.sun.com/zoran/entry/scaling_memcached_500_000_ops\">http://blogs.sun.com/zoran/entry/scaling_memcached_500_000_ops</a>",
        "metadata": {
          "word_count": 55,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Solaris T2 gets 500K ops/s or 9.6Gbits/s depending on payload size. Niagara 2 based T2 has built-in 10Gbit network, and 8 cores with 8 threads in one chip."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_77589e20",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Structure Completed",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/05/garage-structure-completed.html",
      "content": {
        "text": "After lots of stops and starts, all that is left to do is some exterior paint and to fit the main doors. Work has already started on the electrical installation and water for fire sprinklers. Since the walls got so wet, they warped and parts of the siding had to be replaced. For some reason Tuffshed did other jobs on days where the weather was nice, and turned up to work on this one whenever it rained. We had what was probably the last rain of the winter on Sunday and Monday, but the roof was already tiled by then. The last video was the trusses being installed, here is the rest of the sequence.<br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/iIIUrhMML-k&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/iIIUrhMML-k&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object><br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/iSVqgvPTg5M&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/iSVqgvPTg5M&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object><br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/PMuMgLJMoKQ&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/PMuMgLJMoKQ&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object><br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/HaOO9ihExhg&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/HaOO9ihExhg&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object><br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/DrosZv8C6DM&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/DrosZv8C6DM&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object><br /><br />Moving the viewpoint to catch trench digging around the back of the garage.<br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/Nj1YCcHTXRY&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/Nj1YCcHTXRY&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object>",
        "metadata": {
          "word_count": 224,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "After lots of stops and starts, all that is left to do is some exterior paint and to fit the main doors. Work has already started on the electrical installation and water for fire sprinklers. The last video was the trusses being installed, here is the rest of the sequence. object width=\"425\" height=\"344\"param name=\"movie\" value=\"http. youtube.com/v/iSVqgvPTg5M&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\" 425\" height \"344\" param name=\"allowFullScreen\" value \"true\" Param name \"allowscriptaccess\" value\" always \"always\" Moving the viewpoint to catch trench digging around the back of the garage. The view was changed to show the view from the front of the house. The viewpoint was also changed to reveal the view of the front. param name=\"allowscriptaccess\" value=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\" true. \"true\" \"always\" \"false\" \"allowfullscreen\""
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_5cdb6409",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cloud Application Architectures: GOTO Aarhus, Denmark, October",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/06/cloud-application-architectures-goto.html",
      "content": {
        "text": "Thanks to an invite via <a href=\"http://www.michaelnygard.com/\" target=\"_blank\">Michael Nygard</a>, I ended up as a track chair for the <a href=\"http://gotocon.com/aarhus-2012/\" target=\"_blank\">GOTO Aarhus conference</a> in Denmark in early October. The track subject is <a href=\"http://gotocon.com/aarhus-2012/schedule/tuesday.jsp\" target=\"_blank\">Cloud Application Architectures</a>, and we have three speakers lined up. You can <a href=\"https://secure.trifork.com/aarhus-2012/registration/\" target=\"_blank\">register with a discount</a> using the code <b>crof1000</b>.<br />\n<br />\nWe are starting out with a talk from the Citrix Cloudstack folks about how to architect applications on open portable clouds. These implement a subset of functionality but have more implementation flexibility than public cloud services. [Randy Bias of Cloudscaling was going to give this talk but had to pull out of the trip due to other commitments].<br />\n<br />\nTo broaden our perspective somewhat, and get our hands dirty with real code, the next talk is a live demonstration by Ido Flatow,\n<a href=\"http://gotocon.com/aarhus-2012/presentation/Building%20secured,%20scalable,%20low-latency%20web%20applications%20with%20the%20Windows%20Azure%20Platform\" target=\"_blank\">Building secured, scalable, low-latency web applications with the Windows Azure Platform</a>.<br />\n<blockquote class=\"tr_bq\">\n<span class=\"Apple-style-span\" style=\"font-family: Verdana, sans-serif; font-size: 11px; line-height: 17px;\">In this session we will construct a secured, durable, scalable, low-latency web application with Windows Azure - Compute, Storage, CDN, ACS, Cache, SQL Azure, Full IIS, and more. This is a no-slides presentation!</span></blockquote>\nFinally I will be giving my latest update on <a href=\"http://gotocon.com/aarhus-2012/presentation/Globally%20Distributed%20Cloud%20Applications%20at%20Netflix\" target=\"_blank\">Globally Distributed Cloud Applications at Netflix</a>.<br />\n<blockquote class=\"tr_bq\">\n<span class=\"Apple-style-span\" style=\"font-family: Verdana, sans-serif; font-size: 11px; line-height: 17px;\">Netflix grew rapidly and moved its streaming video service to the AWS cloud between 2009 and 2010. In 2011 the architecture was extended to use Apache Cassandra as a backend, and the service was internationalized to support Latin America. Early in 2012 Netflix launched in the UK and Ireland, using the the combination of AWS capacity in Ireland and Cassandra to create a truly global backend service. Since then the code that manages and operates the global Netflix platform is being released as a series of open source projects at netflix.github.com (Asgard, Priam etc.). The platform is structured as a large scale PaaS, strongly leveraging advanced features of AWS to deploy many thousands of instances. The platform has primary language support for Java/Tomcat with most management tools built using Groovy/Grails and operations tooling in Python. Continuous integration and deployment tooling leverages Jenkins, Ivy/Gradle, Artifactory. This talk will explain how to build your own custom PaaS on AWS using these components.</span></blockquote>\n<br />\nThere are many other excellent speakers at this event, which is run by the same team as the global series of &nbsp;QCon conferences, unfortunately, the cloud track runs at at the same time as Michael Nygard and Jez Humble on Continuous Delivery and Continuous Integration, however I'm doing another talk in the NoSQL track, (along with Martin Fowler and Coda Hale). <a href=\"http://gotocon.com/aarhus-2012/presentation/Running%20Netflix%20on%20Cassandra%20in%20the%20Cloud\" target=\"_blank\">Running Netflix on Cassandra in the Cloud</a>.<br />\n<blockquote class=\"tr_bq\">\n<span class=\"Apple-style-span\" style=\"font-family: Verdana, sans-serif; font-size: 11px; line-height: 17px;\">Netflix used to be a traditional Datacenter based architecture using a few large Oracle database backends. Now it is one of the largest cloud based architectures, with master copies of all data living in Cassandra. This talk will discuss how we made the transition, how we automated and open sourced Cassandra management for tens of clusters and hundreds of nodes using Priam and Astyanax, backups, archiving and performance and scalability benchmarks.</span></blockquote>\n<br />\nI'm looking forward to meeting old friends, getting to know some new people, and visiting Denmark for the first time. &nbsp;See you there!",
        "metadata": {
          "word_count": 567,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "GOTO Aarhus conference will take place in Denmark in early October. The track subject is Cloud Application Architectures. Citrix Cloudstack will give a talk on how to architect applications on open portable clouds. The next talk is a live demonstration by Ido Flatow. We will construct a secured, durable, scalable, low-latency web application with Windows Azure - Compute, Storage, CDN, ACS, Cache. This is a no-slides presentation! Netflix moved its streaming video service to the AWS cloud between 2009 and 2010. In 2011 the architecture was extended to use Apache Cassandra as a backend, and the service was internationalized to support Latin America. There are many other excellent speakers at this event, which is run by the same team as the global series of &nbsp;QCon conferences. Unfortunately, the cloud track runs at at the same time as Michael Nygard and Jez Humble on Continuous Delivery and Continuous Integration. I'm looking forward to meeting old friends, getting to know some new people, and visiting Denmark for the first time. sition, how we automated and open sourced Cassandra management for tens of clusters and hundreds of nodes. backups, archiving and performance and scalability benchmarks."
      },
      "tags": [
        "architecture",
        "aws",
        "azure",
        "cloud",
        "continuous integration",
        "netflix",
        "performance",
        "platform",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_11875e19",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Framing Walkaround",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/garage-framing-walkaround.html",
      "content": {
        "text": "The walls are up, but the trusses for the roof aren't ready, so it will be finished off next week. Luckily the weather looks good for the next week or so.<br /><br /><object width=\"420\" height=\"320\"><param name=\"movie\" value=\"http://www.youtube.com/v/HHiw3Mg74T4&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/HHiw3Mg74T4&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"320\"></embed></object>",
        "metadata": {
          "word_count": 47,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The walls are up, but the trusses for the roof aren't ready, so it will be finished off next week. Luckily the weather looks good for the next week or so."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f9ab8cd0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: my Pandora station",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-my-pandora-station.html",
      "content": {
        "text": "Pandora is one of my favorite apps. You can listen to <a href=\"http://www.pandora.com/?sc=sh59488715848528731\">Adrian's Prog Radio</a> station too.",
        "metadata": {
          "word_count": 17,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Pandora is one of my favorite apps. You can listen to Adrian's Prog Radio on the app too. It's a great way to stay up to date with the latest music."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_6ffb0255",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Skype downloads approach 500 Million",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/skype-downloads-approach-500-million.html",
      "content": {
        "text": "In the next few days, <a href=\"http://nyanyan.to/skype/40hr_chart.php\">Skype will pass the 500 million downloads mark</a>. I seem to remember someone saying that Kazaa was the most downloaded program ever, and looking at the Kazaa home page, they currently show 389 Million. Skype is at 498 Million as I write this, and is downloading at almost a million a day. The download rate was over 1M a day when they made a new release available and existing users were getting updates. Compared to Kazaa, Skype is downloading every day, what Kazaa is downloading every week.<br /><br />How are the competition doing? Vonage added a few hundred thousand users last quarter. Thats a few orders of magnitude away from being a competitor!<br /><br />Of course, Kazaa is based on an earlier version of the P2P technology that powers Skype, and which is about to come to the world again via Joost. Joost just released a new beta for Windows, and their first beta for Intel based OSX. I still can't justify upgrading from my own Powerbook G4 (it just works) so I'll have to wait for them to backport to the older systems, and use my work laptop.",
        "metadata": {
          "word_count": 195,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In the next few days, Skype will pass the 500 million downloads mark. Skype is downloading at almost a million a day. Compared to Kazaa, Skype is downloads every day, what KazaA is downloading every week. G4 will not work with the new software. I'll have to wait for them to backport to the older systems, and use my work laptop. G4 just works."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_1eac3035",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "The economics of Nissan Leaf ownership",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/09/economics-of-nissan-leaf-ownership.html",
      "content": {
        "text": "[Updated December 2012] We got our @NissanLeaf at the end of June 2011, and are very happy with it. It's fun to drive and is our first choice for any journey within range. Since the car is nearly silent and has over 200 lb-ft of torque, I often use full power for acceleration away from the lights, which quietly leaves the other cars behind. The low noise level is also great for listening to speech or music, and the steering wheel controls and bluetooth integration with my iPhone works well for phone calls and iPod or Pandora.<br />\n<br />\nLaurel usually takes it for her 66 mile round trip commute, and if not, I take it on my 20 miles round trip commute, and any shopping trips at the weekend. So far we have averaged about 1000 miles per month, mostly mountain roads and freeways, which is the worst case for electrical consumption. The Leaf collects its milage and power use, and we can go back to look at the activity.<br />\n<br />\nThe record shows 1003 miles at 3.8 miles/KWh and a total of 264.6 KWh. We pay 10c/KWh, so if we had charged at home that's $26.46. Since we charge at work for free it's more like $15. We have grid-tied solar power and time of use metering for a cheap overnight rate. The meter runs backwards during the day at a higher price, and the Leaf has a charging delay timer so that we can plug it in when we get home, then it starts charging when the cheapest rate starts at 9pm.<br />\n<br />\n1003 miles in the cars we would normally drive, which get about 20mpg, uses 50 gallons of premium gas at about $4.10 and about $206. So we actually saved $180 in August 2011.<br />\n<br />\nIf Laurel drives every day, 22 work days a month at 66 miles is 1452 miles, she tops up the charge at work each day and gets about 4 miles/KWh on that route so that's under a dollar a day. Our gas cost would be $298, so she could be saving about $280/month. That takes a big chunk out of the cost of buying the car in the first place. On top of that, the servicing costs are minimal, no engine oil changes, no gearbox, and the brakes last longer because the regenerative braking system takes a lot of the load. We have had to buy a new tire after popping it on a pothole, that was about $150, installed, but could happen to any car. We could have saved on gas costs by buying a hybrid, but they are less fun to drive and you are still paying to maintain a gas engine and a very complex transmission.<br />\n<br />\nThe icing on the cake is our white (for pure electric) car pool lane stickers, so Laurel can take the freeway in rush hour and zip silently past all the Prius drivers whose yellow (for hybrid) stickers no longer get them in the car pool lane. It took a total of ten weeks to get the license plates, then apply and get the white stickers.<br />\n<br />\nSo the value proposition for the Leaf is that it is much more fun to drive than a high mpg economy car like a Prius, gets you in the car pool lane (if you live in California), and the purchase cost is offset by ultra-low running costs if you use it regularly.<br />\n<br />\nWe aren't alone in figuring out that this is a good deal. At last count (end of 2012) there are more than ten Nissan Leaf owners at Netflix, along with several Volts (the latest version gets a green carpool lane sticker) and several Teslas. At Informatica, Laurel is among several Leafs and Volts sharing the chargers.<br />\n<br />\nWe leased our car on a three year 36,000 mile plan. We included the home charger installation in the payment (about $2K), put down a $2K initial payment, got a $7500 federal rebate bundled into the deal. Actual payment including taxes as one of the first Leaf owners was over $500/month, the current deals are much lower than this and Nissan is about to release the cost reduced US built Leaf in January 2013. We got a $2500 state rebate paid directly to us after signing up for it which covers the initial payment. We leased because we think that in three years time there may be big advances in electric car technology, we could decide to keep the Leaf, or give it back and get a the pick of the 2014 models.<br />\n<br />\nFor much more discussion about the car, the <a href=\"http://www.mynissanleaf.com/\">My Nissan Leaf forum</a> is quite active. One thing I found there is that upgraded springs and dampers are available, since we do a lot of mountain driving, we upgraded the suspension to be stiffer and better damped than stock.<br />\n<br />\nThe first question everyone asks is how far will the Leaf go, and the answer is between 60 and 100 miles per charge, but it depends on where you live and how hard you drive. The usable capacity of the battery pack is about 21KWh, the actual spec is 24KWh, so there is a little bit of extra capacity beyond it's \"I'm empty\" point. if you drive a lot of freeways at speed and climb mountains like we do, 3-4 miles/KWh gets you 60-80 miles. In a flat urban environment 4-5 miles/KWh is quite possible to get 80-100 miles.<br />\n<br />\nSince we live at the top of a mountain (2400ft) and work near sea level, it's a good idea to charge the car to 80% full at home, and 100% full at work. This way there is regenerative braking for the initial downhill run, which is free power and also saves the brake pads.<br />\n<br />\nOur \"carwings\" summary page for August 2011 is shown below.<br />\n<br />\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEip8DRL2xEkDSuPbQcgThI6JdawZ9gXPQ1a2jncQ0qz7g60PIGT5HITujDdlT13aVU1jBfWE_kwcaQEa7L8HFRPFIntH4AvDbv9_y18W5LL0Sq7czZx-wB3I4P2_dvd7jNJRAMW/s1600/Screen+shot+2011-09-22+at+12.54.35+AM.png\" onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\"><img alt=\"\" border=\"0\" id=\"BLOGGER_PHOTO_ID_5655102158133447218\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEip8DRL2xEkDSuPbQcgThI6JdawZ9gXPQ1a2jncQ0qz7g60PIGT5HITujDdlT13aVU1jBfWE_kwcaQEa7L8HFRPFIntH4AvDbv9_y18W5LL0Sq7czZx-wB3I4P2_dvd7jNJRAMW/s320/Screen+shot+2011-09-22+at+12.54.35+AM.png\" style=\"cursor: hand; cursor: pointer; height: 218px; width: 320px;\" /></a>",
        "metadata": {
          "word_count": 1030,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Leaf is nearly silent and has over 200 lb-ft of torque. The low noise level is also great for listening to speech or music. bluetooth integration with my iPhone works well for phone calls. If Laurel drives every day, 22 work days a month at 66 miles is 1452 miles. She tops up the charge at work each day and gets about 4 miles/KWh on that route so that's under a dollar a day. That takes a big chunk out of the cost of buying the car in the first place. The Leaf is much more fun to drive than a high mpg economy car like a Prius, gets you in the car pool lane (if you live in California) and is offset by ultra-low running costs if you use it regularly. At last count (end of 2012) there are more than ten Nissan Leaf owners at Netflix, along with several Volts (the latest version gets a green carpool lane sticker) and several Teslas. Laurel is among several Leafs and Volts sharing the chargers. The usable capacity of the battery pack is about 21KWh, the actual spec is 24KWh. 3-4 miles/KWh gets you 60-80 miles if you drive a lot of freeways at speed and climb mountains. In a flat urban environment 4-5 miles/ KWh is quite possible to get 80-100 miles. Our \"carwings\" summary page for August 2011 is shown below. The way there is regenerative braking for the initial downhill run, which is free power and also saves the brake pads."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_3bde76be",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Motivation",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/02/motivation.html",
      "content": {
        "text": "I started this blog, but didn't get far with it last summer. I just updated it and aim to be much more active, as I'm collecting information on free performance and capacity planning tools and want a place to share it.<br /><br />For those that know me from my books, I left Sun last summer and started work at eBay in the Operations Architecture group in September 2004.",
        "metadata": {
          "word_count": 68,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "For those that know me from my books, I left Sun last summer and started work at eBay in the Operations Architecture group in September 2004. I started this blog, but didn't get far with it last summer. I just updated it and aim to be much more active, as I'm collecting information on free performance and capacity planning tools."
      },
      "tags": [
        "architecture",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_29e9eb77",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Plan Detail",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-plan-detail.html",
      "content": {
        "text": "To get everything approved we had to survey the land and get a detailed plan drawn up by a Civil Engineer - we used<a href=\"http://tscivileng.com\"> Terry Szewczyk</a> and he was helpful, competent and kept the costs to a minimum. The picture below is a detail from one of the drawings, which were produced at 1inch to 20ft scale, and shows the important part where the garage goes. For some reason the contour lines altitude is shown based around 2000ft, whereas the real altitude is about 2400ft, but no-one seems to care if that aspect of it is accurate.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgI35S9fKm91kjlWV7AI_0kaKtA7uaAnLoogA7DjrbQG2kViewwpCiBEVl2GMALz8SsHkPgzZF_MAFH1Hw-KIOUzDIQSXWEs1IwfIY2G3WIDFo4taxHS9Rb7WGVZGgwzVIr3KlK/s1600-h/garageplandetail.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 315px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgI35S9fKm91kjlWV7AI_0kaKtA7uaAnLoogA7DjrbQG2kViewwpCiBEVl2GMALz8SsHkPgzZF_MAFH1Hw-KIOUzDIQSXWEs1IwfIY2G3WIDFo4taxHS9Rb7WGVZGgwzVIr3KlK/s320/garageplandetail.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5448317228110962322\" /></a><br /><br />Our first step is to replace the pipe that goes from the septic tank to the diversion valve and the remaining leach field, so we start by getting the tank emptied (which needs to be done every few years anyway). The pipe has to dive down a bit then go across under the concrete \"apron\" that will lead up to the garage, and it won't be buried very deep, so we will be putting down a 4\" pipe that is placed inside a 6\" pipe for the part that goes under the driveway, to give it extra protection from being crushed.<br /><br />The final detailed changes we need on the building design plan are now in hand (the layout plan shown above is already approved), so I'm back to the county office again next week to resubmit them and hopefully get the permit.<br /><br />The other details you can see on the plan are the electrical connection, which will be a 220V -200Amp dedicated circuit capable of charging electric cars one day. The propane tank will be relocated, and hooked up to a generator and the house. There is also a water connection to a fire sprinkler system.",
        "metadata": {
          "word_count": 313,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "To get everything approved we had to get a detailed plan drawn up by a Civil Engineer. The picture below is a detail from one of the drawings, which were produced at 1inch to 20ft scale. Our first step is to replace the pipe that goes from the septic tank to the diversion valve and the remaining leach field. The pipe has to dive down a bit then go across under the concrete \"apron\" that will lead up to the garage. The final detailed changes we need on the building design plan are now in hand. Propane tank will be relocated, and hooked up to a generator and the house. The electrical connection will be a 220V -200Amp dedicated circuit capable of charging electric cars."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_36dfa9f6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Looking back at 2013, with pointers to 2014",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/12/looking-back-at-2013-with-pointers-to.html",
      "content": {
        "text": "<br />\nI wrote a prediction post a year ago so I'm going to review and update it. Here's last year in full \"<a href=\"http://perfcap.blogspot.com/2013/01/looking-back-at-2012-with-pointers-to.html\" target=\"_blank\">Looking back at 2012, with pointers to 2013</a>\" .<br />\n<br />\nThe headlines from last year, with comments and updates:<br />\n<br />\n<i>Mobile Bandwidth Greater than Fixed Bandwidth</i><br />\nThis trend continues. LTE can get congested in cities, but the latest news is <a href=\"http://gigaom.com/2013/12/05/verizon-quietly-unleashes-its-lte-monster-tripling-4g-capacity-in-major-cities/\" target=\"_blank\">Verizon upgrading it's network</a> to have more capacity and speeds up to 100Mbit/s. I'm about to get an LTE MiFi for our house so that when we want higher speed or our local DSL gets congested, it's available, and when we go away we can take it with us. I'll keep the DSL for background connectivity, and to avoid hitting the MiFi bandwidth cap too much.<br />\n<br />\n<br />\n<i>Cutting The Cable/Satellite TV Feed</i><br />\n<br />\nWe still don't have cable/satellite. I got a Google Chromecast, but it's slow and fiddly to use compared to the AppleTV, and buggy for streaming Pandora, keeps dropping the stream. The picture quality is good though.<br />\n<br />\n<br />\n<i>The Netflix Open Source Cloud Platform Got Traction</i><br />\n<br />\nWe close out 2013 with 39 distinct projects at <a href=\"http://github.netflix.com/\">github.netflix.com</a>, a successful Netflix Cloud Prize contest, endorsements from many more companies including IBM, and growing acceptance that Cloud Native is an important concept that supports highly agile continuous delivery, and NetflixOSS is an onramp that accelerates transitioning to Cloud Native.<br />\n<br />\n<br />\n<i>Netflix Cloud Architecture Presentations</i><br />\n<br />\nI presented even more than in 2012, see the slides I posted at <a href=\"http://www.slideshare.net/adrianco\">http://www.slideshare.net/adrianco</a>. I've also got a permanent link to a full set of workshop slides at <a href=\"http://bit.ly/netflix-workshop\">bit.ly/netflix-workshop</a>&nbsp;which is easier to remember, and lets me update the workshop slides from time to time.<br />\n<br />\n<br />\n<i>The Concept of Anti-Fragility Took Off</i><br />\n<br />\n<a href=\"http://www.amazon.com/Antifragile-Things-That-Disorder-ebook/dp/B0083DJWGO/ref=tmm_kin_title_0\">Taleb's book</a>&nbsp;and concept&nbsp;became more accepted. Ariel Tseitlin wrote an ACM paper for Netflix on <a href=\"http://queue.acm.org/detail.cfm?id=2499552\" target=\"_blank\">The Antifragile Organization</a>.<br />\n<br />\n<br />\n<i>Cloud, Open Source, SaaS and the End of Enterprise Computing</i><br />\n<br />\n<div>\n\"<i>During 2013 we will see if Google manages to invest heavily and execute well enough to build up a big user base.</i>\"</div>\n<div>\nGoogle came out of beta and closed some gaps, but it's not clear that they are building up a big user base. They have their fans, and in some areas have some technical advantages, but still have a lot to prove. Other public cloud vendors didn't make much headway. Microsoft Azure remains strong in it's own ecosystem, but hasn't broken out into general use, and others are getting further behind or being bought.</div>\n<div>\n<br /></div>\n<div>\n\"<i>I personally think in 2014 we will likely see [...] the scale, features and price point of AWS and Google clouds make everyone else irrelevant.</i>\" I still think this is true. The 2013 Garner Magic Quadrant for IaaS didn't include Google as they were in beta, but showed AWS as dominant. It also included an estimate that AWS delivered capacity was five times bigger than the next 14 vendors combined. i.e. AWS was 85% of the market by delivered capacity (not by $ revenue). My tracking of AWS size by looking at their reserved IP address space <a href=\"http://bit.ly/awsiprange\">bit.ly/awsiprange</a> continues to show that AWS is doubling in size every year, and has grown 10x over the last three years, reaching 5.1 million IP addresses in September 2013.<br />\n<br />\nMost big enterprise companies are actively working on their AWS rollout now. Most of them are also trying to get an in-house cloud to work, with varying amounts of success, but even the best private clouds are still years behind the feature set of public clouds, which is has a big impact on the agility and speed of product development.<br />\n<br />\n<br />\n<i>Solar Powered Electric Cars Are For Real Now</i><br />\n<br />\nOur Nissan Leaf is getting towards the end of it's three year lease, and we're replacing it with a Fiat 500e. The Fiat is smaller and lighter, but has the same size battery, so gets a bit more range. It's also cheaper and more fun to drive. During 2013 a lot of people bought Tesla Model S, including people who traded in Tesla Roadsters. We picked up a second hand 2010 Tesla Roadster with full factory warranty, and although the technology is a bit older, it's a great fun car with over 200 miles real world range for longer trips. Even with two electric cars, we still generated more electricity than we used this year, so the marginal cost of energy is still zero for our household and our gasoline spend is way down.<br />\n<br />\n<br />\n<i>Global Warming Arrived in the USA in 2012</i><br />\n<br />\n\"<i>I'm going to try and re-balance my 401K retirement accounts to divest from oil companies. Many students are now<a href=\"http://gofossilfree.org/\">pressuring their colleges</a> to divest from oil as well.</i>\" I spent a few hours on Fidelity Investments web site and reduced my investments in the energy sector to a minimum. The divestment movement is also gathering momentum. The public conversation continues to shift, more extreme weather in the US and worldwide is helping, and the <a href=\"http://ipcc.ch/\" target=\"_blank\">IPCC</a> released an updated report.<br />\n<br />\n<br />\n<i>Twitter and Snapchat</i><br />\n<br />\nI had 6,500 followers on Twitter at the end of 2012, and I have 10,500 at the end of 2013. I correctly predicted that Snapchat would continue to grow in 2013, and it was reported that more photos are uploaded per day into Snapchat than into Facebook. Twitter had it's IPO, and is becoming part of the news and entertainment infrastructure with it's own ecosystem. I think they will figure out how to continue to grow and make money, so I bought a few shares to have skin in the game.</div>\n<div>\n<br /></div>\n<div>\nNew for 2014:</div>\n<h4>\nGoogle Glass will have a successful public launch</h4>\n<div>\nI got Glass last summer, and have been using it a bit and letting other people try it a lot. I just got a hardware update that makes my developer set compatible with the final consumer version, and it's clear that Google is getting much closer to having a real product to launch. No-one knows the price, and that will determine how widely people get Glass, but the feature set and support is now quite interesting. There is a generational divide, in that many younger people like and want Glass, and older people are more wary or bothered by it. Trying it out in person lets people understand what it does and doesn't do, and greatly increases acceptance.</div>\n<div>\n<br /></div>\n<div>\nThe Glass features that I was waiting for have mostly been addressed. The MyGlass app now supports iPhone, there is support for corporate Gmail accounts and multiple Gmail accounts on a single Glass. There is a headphone to supplement the built in speaker that was too quiet, and there will soon be prescription lens support. The last of those is the main reason I don't wear Glass every day, as I have to put in contact lenses to use it currently, and my contacts don't work as well as my glasses.</div>\n<div>\n<br /></div>\n<div>\nThe ability to get personal GPS directions while walking (or cycling) is one of my favorite features. \"First person\" hangout support has huge potential although it's still too fiddly to setup, and needs good network bandwidth. Video use drains the battery quickly, although in normal use it lasts long enough to be useful all day. The add-on applications I have installed include Twitter, so I get notified immediately if someone mentions or DMs me; Evernote for keeping track of shared to-do lists; and Translate where you look at a sign and it makes an English version of the sign for you.</div>\n<div>\n<br /></div>\n<div>\nVoice control works better than most people expect, directions to local places is remarkably good, but voice input dictation is very random. It needs quite a lot of practice to get messages recorded that contain more than \"on my way\" or other simple phrases. But then I don't use Siri on my iPhone either.</div>\n<div>\n<br /></div>\n<div>\nBest wishes to everyone for 2014.</div>\n<div>\n<br /></div>",
        "metadata": {
          "word_count": 1410,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I wrote a prediction post a year ago so I'm going to review and update it. Here's last year in full. The headlines from last year, with comments and updates. We close out 2013 with 39 distinct projects at <a href=\"http:// GitHub.com/ Netflix Cloud Prize contest, endorsements from many more companies including IBM, and growing acceptance that Cloud Native is an important concept. NetflixOSS is an onramp that accelerates transitioning to Cloud Native. I presented even more than in 2012, see the slides I posted at http://www.slideshare.net/adrianco. Ariel Tseitlin wrote an ACM paper for Netflix on The Antifragile Organization. He says Netflix still has a lot to prove. The 2013 Garner Magic Quadrant for IaaS didn't include Google as they were in beta. It also included an estimate that AWS delivered capacity was five times bigger than the next 14 vendors combined. Even the best private clouds are still years behind the feature set of public clouds. s are actively working on their AWS rollout now. Most of them are also trying to get an in-house cloud to work. \"I'm going to try and re-balance my 401K retirement accounts to divest from oil companies\" \"The divestment movement is also gathering momentum\" \"I had 6,500 followers on Twitter at the end of 2012, and I have 10,500\" I got Glass last summer, and have been using it a bit and letting other people try it a lot. I just got a hardware update that makes my developer set compatible with the final consumer version. No-one knows the price, and that will determine how widely people get Glass. There is a generational divide, in that many younger people like and want Glass, and older people are more wary or bothered by it. The ability to get personal GPS directions while walking (or cycling) is one of my favorite features. \"First person\" hangout support has huge potential although it's still too fiddly to setup. Video use drains the battery quickly, although in normal use it lasts long enough to be useful all day. Voice control works better than most people expect, directions to local places is remarkably good. Evernote for keeping track of shared to-do lists; and Translate where you look at a sign and it makes an English version of the sign for you."
      },
      "tags": [
        "agile",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "cloud native",
        "infrastructure",
        "netflix",
        "platform",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_a7312e55",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Skype Downloads Approach One Billion",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/07/skype-downloads-approach-one-billion.html",
      "content": {
        "text": "About eighteen months ago I<a href=\"http://perfcap.blogspot.com/2007/02/skype-downloads-approach-500-million.html\"> wrote a blog post about Skype</a> as it approached 500 million downloads. They have continued at around a million a day, and now, nearly 500 days later we are coming up on the one billion mark. I predict at the current rate that  the last 42 million downloads will happen in 37 days, so it will pass a billion around September 3rd.<br /><br />The usual caveats apply, most of the downloads are version upgrades, but there are over 300 million different registered skype names, and concurrent user counts peak at 12 million most days.<br /><br />At the time <a href=\"http://skypejournal.com/blog/2007/02/is_500_million_skype_downloads.html\">I got a nice mention from Skype Journal</a>, and you can find good analysis of the Skype usage levels at <a href=\"http://skypenumerology.blogspot.com/\">Jean Mercier's Skype Numerology</a> blog. As ever, you can track the actual numbers via an RSS feed that Skype provides, or <a href=\"http://nyanyan.to/skype/40hr_chart.php\">just view them at this graph</a>.<br /><br />Why does this matter? It matters because this is the kind of critical mass that the rest of the Telco industry and VOIP startups are only dreaming of. I'm about to turn off my long distance and international phone plans and just have local calls on my home phone. The home phone I just ordered will redirect long distance and international calls over Skype, and make local calls on the land line. This will save me hundreds of dollars a year in telco service costs, so it pays for itself in less than six months.<a href=\"http://accessories.skype.com/store/skypeeu/Content/pbPage.SubscriptionPilotPage?cm_sp=hw|philipsbundle-_-welcomeback|south1-_-save80_lnk|en_us\"> There is even a special offer on the hardware right now.</a> I fully expect that in a few years time I will also be making the bulk of my mobile calls over Skype as well.<br /><br />All I want from the Telco's is reliable unlimited raw bandwidth at a fixed low price, which is a terrifying prospect for the dinosaurs of this industry...",
        "metadata": {
          "word_count": 314,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "About eighteen months ago I wrote a blog post about Skype approaching 500 million downloads. They have continued at around a million a day, and now, nearly 500 days later we are coming up on the one billion mark. I predict at the current rate that  the last 42 million downloads will happen in 37 days. This is the kind of critical mass that the rest of the Telco industry and VOIP startups are only dreaming of. The home phone I just ordered will redirect long distance and international calls over Skype, and make local calls on the land line. This will save me hundreds of dollars a year in telco service costs. Low price,  low price, which is a terrifying prospect for the dinosaurs of this industry.  Low price, low price.  terrifying prospect."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_1b8c2d11",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Spring Equinox Update",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/solar-power-spring-equinox-update.html",
      "content": {
        "text": "Spring is here, and we just had a few clear days, shown below. The sunrise and sunset are about 12 hours apart. We installed the system just before the fall equinox, so we now have a good calibration of the output levels through the least sunny part of the year, from now on it will be interesting to track the output as we go to the longest day.<br /><br />Actual average output over the last 200 or so days is about 14.3 KWh/day. Yesterday was 22.5 KWh, and <a href=\"http://perfcap.blogspot.com/2009/12/solar-power-winter-solstice-and-new.html\">at the winter solstice low point it was 16 KWh on a clear day</a> as previously blogged.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh6_CrcVTL8zs4EsQ-RdoHUI-2QMYTOo_LuN2kN7xjxnv2lhGEKifyIdWy46igdvcIOx_9EKIN74UAcSw0fdawsicnHmqn6coHAI828N86zaW77a6FAT4p7jLHQfwcxhZS3uBG0/s1600-h/Screen+shot+2010-03-19+at+12.45.33+PM.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 158px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh6_CrcVTL8zs4EsQ-RdoHUI-2QMYTOo_LuN2kN7xjxnv2lhGEKifyIdWy46igdvcIOx_9EKIN74UAcSw0fdawsicnHmqn6coHAI828N86zaW77a6FAT4p7jLHQfwcxhZS3uBG0/s320/Screen+shot+2010-03-19+at+12.45.33+PM.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5450433986073121682\" /></a>",
        "metadata": {
          "word_count": 122,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Actual average output over the last 200 or so days is about 14.3 KWh/day. Yesterday was 22.5 KWh. At the winter solstice low point it was 16 KWh on a clear day. \"Screen+shot+2010-03-19+at+12.45.33+PM. eight: 158px;\" src=\"https://blogger.google.com/img/b/R29vZ2xl/AVvXsEh6_CrcVTL8zs4EsQ-RdoHUI-2QMYTOo_LuN2kN7xjxnv2lhGEKifyIdWy46igdvcIOx_9EKIN74UAcSw0fdawsicnHmqn6coHAI828N86zaW77a6FAT4p7jLHQfwcxhZ"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e93f3ff6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Apple iPhone Unanswered Questions...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/apple-iphone-unanswered-questions.html",
      "content": {
        "text": "Like everyone else, I think it looks great. The big difference between Apple and other vendors for just about everything they make is <span style=\"font-style: italic;\">craftsmanship</span>, everything else looks sloppy in design and execution in comparison.<br /><br />So the questions, they really are around whether this is really a pocket OSX machine or a locked down phone that happens to use some parts of OSX.<br /><ul><li>What CPU does it use? Can it run unmodified OSX applications, or do they have to be downsized and ported to a new CPU (Xscale/ARM is what Treo's use)</li><li>How much memory is there for running applications? The 8GB Flash is basically filesystem space, so there must be some RAM as well.</li><li>Is it open for more applications to be loaded? For example Firefox, Skype, Slingplayer, OSX Terminal, whatever..... or is it locked down?</li><li>What is the model for developers to extend the platform?</li></ul>I'm sure this will become clearer fairly soon, but all the commentary I'm reading is just going \"oh wow\" and no-one is asking or answering these fundamental questions that have big long term implications...<br /><br />Anyway, next year's Xmas present request list will have one of these on it!<br /><br />[Update...]<br />As an existing Treo650 user, the discussion on the iPhone at <a href=\"http://www.treocentral.com/content/Stories/1047-1.htm\">TreoCentral</a>  is very interesting. They imply that the CPU is an ARM architecture with an additional graphics processor, and that the iPhone is closed and controlled so you can't add applications to it. We'll see how long that lasts, either Apple will open it up or the hackers will have a lot of fun working out how to crack the design, just like they did for the games consoles, TiVo etc.",
        "metadata": {
          "word_count": 279,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The questions are around whether this is really a pocket OSX machine or a locked down phone that happens to use some parts of OSX. What CPU does it use? Can it run unmodified OSX applications, or do they have to be downsized and ported to a new CPU (Xscale/ARM is what Treo's use)? How much memory is there for running applications? Next year's Xmas present request list will have one of these on it. The iPhone is closed and controlled so you can't add applications to it. We'll see how long that lasts."
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_file_437cf8f2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal Performing Ellipsis at Hotel Utah",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/fractal-performing-ellipsis-at-hotel.html",
      "content": {
        "text": "<object width=\"480\" height=\"295\"><param name=\"movie\" value=\"http://www.youtube.com/v/HqCjJAGDZv4&hl=en&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/HqCjJAGDZv4&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"480\" height=\"295\"></embed></object>",
        "metadata": {
          "word_count": 15,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "param name=\"movie\" value=\"http://www.youtube.com/v/HqCjJAGDZv4&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"480\" height=\"295\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_0c766faf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Flow accounting printout",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/04/flow-accounting-printout.html",
      "content": {
        "text": "I went to edit the code and realized that I hadn't finished some previous work, so I added a printout of the flow accounting data, an example is shown below.<br /><br />The timestamp is the start of the flow, I translate to local timezone and show that as well. I difference the two timestamps to get the duration of the flow in seconds; add the uid and project id (which are only present for outbound flows); show the protocol; decode the IP addresses and look them up as names if possible, along with their port numbers; list the number of packets and bytes in the flow; and end with the filter name from the ipqos definition.<br /><br /><pre><span style=\"font-size:78%;\">timestamp  locltime dur   uid  proj prot           srcip:port            dstip:port  npkts  nbytes aname<br />1113850483 11:54:43  13     0     0  tcp   66.102.15.100:80               crun:55783    21    4148  acct<br />1113850483 11:54:43  13   100    10  tcp            crun:55783   66.102.15.100:80       13   15082  acct<br />1113850483 11:54:43  13     0     0  tcp            crun:55783   66.102.15.100:80        6     240  acct<br />1113850480 11:54:40  16   100    10  tcp            crun:55782   66.102.15.101:80        3    1253  acct<br />1113850480 11:54:40  16     0     0  tcp            crun:55782   66.102.15.101:80        7     280  acct</span><br /></pre>",
        "metadata": {
          "word_count": 185,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The code is written in Python. It shows the duration of the flow in seconds. It also shows the IP addresses and the number of packets in the flow. 1113850480 11:54:40  16   100    10  tcp  \u00a0 crun:55782   66.102.15.101:80 \u00a0 3   1253  acct. tcp \u00a0   crun   55783  66.102\u00a015.100:80\u00a0\u00a0 6   \u00a0   240  acc."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_6a2e2b86",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Variable Power and Performance in Enterprise Servers",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/06/variable-power-and-performance-in.html",
      "content": {
        "text": "Nice <a href=\"http://www.infoworld.com/article/08/06/09/Linux_beats_Windows_2008_powersaving_measures_1.html?source=rss&url=http://www.infoworld.com/article/08/06/09/Linux_beats_Windows_2008_powersaving_measures_1.html\">Infoworld article looking at the power savings</a> from variable speed CPUs in Enterprise servers running Windows and Linux.<br /><br />This feature messes with capacity planning tools and the calculation of headroom, as I described in past CMG papers (Utilization is Useless as a Metric).",
        "metadata": {
          "word_count": 46,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This feature messes with capacity planning tools and the calculation of headroom, as I described in past CMG papers (Utilization is Useless as a Metric)."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_c7c0879c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Hiring a researcher at Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/02/hiring-researcher-at-netflix.html",
      "content": {
        "text": "Here is the <a href=\"http://jobs.netflix.com/DetailFlix.asp?flix2028\">job specification for a research oriented position working for me at Netflix.</a><br /><br />We are looking for a very experienced engineer who can take the lead, researching, modeling, presenting, designing and building algorithms that run in a service oriented architecture to support millions of customers. The successful candidate will be a self-motivated, intellectually curious individual. A Doctorate or experience working in a research environment, work with personalization algorithms, machine learning, adaptive systems, and statistical analysis would be a bonus. <br /><br />I manage a sister group to the one that does the Cinematch star ratings predictions and hosts the Netflix Prize, we use that data as an input along with everything else we know about movies and customers to come up with the actual lists of movies that are shown all over the site. The research position in my group works on algorithms and mining for new data sources.<br /><br />Also, Netflix is a great place to work, big enough to be interesting and small enough to be fun.",
        "metadata": {
          "word_count": 173,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The successful candidate will be a self-motivated, intellectually curious individual. A Doctorate or experience working in a research environment, work with personalization algorithms, machine learning, adaptive systems, and statistical analysis would be a bonus. Also, Netflix is a great place to work, big enough to be interesting and small enough to have fun. Netflix is big enough for me to work at, but small enough for fun."
      },
      "tags": [
        "architecture",
        "machine learning",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_8c00ce78",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Time for some climate action on 10/10/10",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/time-for-some-climate-action-on-101010.html",
      "content": {
        "text": "organized by <a href=\"http://350.org\">350.org</a> there are events all over the world. The significance of 350 is that it is the safe level of CO2 in the atmosphere. <a href=\"http://www.350.org/en/about/science\">We are already at 390ppm</a>, which is why the climate is changing and extreme weather events are becoming common. The level is going up by 2ppm a year at the moment and the changes needed to reverse this haven't started, so its going to go much higher and have increasingly worse effects.<br /><br />Even if all you do is buy and start reading a copy of James Hansen's book <a href=\"http://www.stormsofmygrandchildren.com/\">Storms of my Grandchildren</a> (<a href=\"http://www.amazon.com/Storms-My-Grandchildren-Catastrophe-Humanity/dp/1608192008\">you can get it instantly from the Kindle store</a>) it will help.<br /><br />Here is a useful discussion of \"<a href=\"http://www.skepticalscience.com/The-value-of-coherence-in-science.html\">The Value of Coherence in Science</a>\" which is how you can tell what makes sense from what must be wrong. I'm a Physics graduate, and I was taught that science works by making predictions and testing them, and by eliminating incoherent propositions. i.e. if an argument contradicts itself it must be false. The denialist counter-arguments fail this test. To see why the denialist arguments are getting any air time at all, read<a href=\"http://www.merchantsofdoubt.org/\"> Merchants of Doubt</a> and <a href=\"http://www.amazon.com/Climate-Cover-Up-Crusade-Warming-ebook/dp/B002Z14KWQ/ref=pd_sim_kinc_1?ie=UTF8&m=AG56TWVU5XWC2\">Climate Cover Up</a>.<br /><br />For an example of incoherent and shoddy denialist work <a href=\"http://deepclimate.org/2010/09/26/strange-scholarship-wegman-report/\">John Mashey has systematically deconstructed the Wegman Report</a> which was presented to Congress as an impartial study when it was nothing of the sort.",
        "metadata": {
          "word_count": 239,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The significance of 350 is that it is the safe level of CO2 in the atmosphere. The level is going up by 2ppm a year at the moment. The changes needed to reverse this haven't started, so its going to go much higher and have increasingly worse effects. Science works by making predictions and testing them, and by eliminating incoherent propositions. The denialist counter-arguments fail this test. To see why the denialist arguments are getting any air time at all, read Merchants of Doubt."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_75a25363",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "MacWorld report: iZap, Apple MacBook Air, iPhone and AppleTV",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/01/macworld-report-izap-apple-macbook-air.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3ntJbwgcNInRuOutHdrkAqAz3atXUimsOH9gx-0e4Mj9hGQyR22siiNibM7r5He7P4FLVTiu3vQwzKl1-GYsybFKbWhvomJB9ysxktpjhMbnHFe1rEq7dFngnZMIiKxN-EdaH/s1600-h/iZap.JPG\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg3ntJbwgcNInRuOutHdrkAqAz3atXUimsOH9gx-0e4Mj9hGQyR22siiNibM7r5He7P4FLVTiu3vQwzKl1-GYsybFKbWhvomJB9ysxktpjhMbnHFe1rEq7dFngnZMIiKxN-EdaH/s400/iZap.JPG\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5155992449083995490\" /></a>The picture shows an electric car called the Zap, which someone appears to have decided that it needed a few upgrades to become the iZap. Jet motor, rotor blades and iPod interfaces....<br /><div><br />I tracked the MacWorld keynote via the <a href=\"http://www.macrumors.com/\">Macrumors</a> site on my iPhone this morning. It worked very well, with updates being pushed every few minutes. Over-all there was some good stuff released, but I think expectations were higher.</div><div><br /><div> </div><div>Later in the afternoon I dropped in on MacWorld for an hour or so, which was plenty of time to check out the interesting stuff.</div><div><br /></div><div> </div><div>The MacBook Air is another hit product, destined to become a geek status symbol. It looks good, feels good to use, and has the option of a solid state disk. I had some hands-on time, and I want one.... They are expensive, but the format is very well designed, and I expect it will come down in price and get new options over the coming years. To make the case strong enough despite being thin, there are more curves in the case (curved metal is much stronger than flat sheet) and it has no large holes in it. So there is no removable battery pack, and each key has its own small hole in the casing. It actually looks more mechanically robust than my MacBook pro.</div><div><br /></div><div> </div><div>I hoped to find some interesting third party iPhone applications but didn't see anything worth mentioning. I played with a demo iPhone with 1.1.3, it had a bunch of safari based games bookmarked on the second page of icons. Once I got home I upgraded my iPhone, its running fine and the new features work well. They were well leaked a few weeks ago, so there is nothing new to report. The iPod touch picked up the missing set of applications as I expected, but Apple want $20 for the privilege. This is the first example of optional applications, and charging for applications, and I expect the mechanism will get well tested by the Touch then it will be introduced for the iPhone and third party applications.</div><div><br /></div><div> </div><div>I have had an AppleTV for six months or so, and the upgraded software looks much better. I don't think I will be renting or buying many movies, but the user interface is better, and the ability to browse both audio and video based podcasts could be interesting. I mostly use my AppleTV for YouTube, playing music and sharing photo albums. One new feature is the ability to pull up photo albums over the Internet from the .mac galleries generated by iPhoto. I've been using .mac for years, and the latest (Leopard) version of iPhoto galleries is very slick and well integrated.</div><div><br /></div><div> </div><div>Apple also introduced Time Capsule, which is a network backup server combining a 500GB or 1TB of disk with an Airport. The hardware was not on show as far as I could tell, and it looks similar to the AppleTV package, but with more Ethernet ports and no video port. Personally, I don't want two separate products in this space, I want an AppleTV that operates as a Time Capsule and Airport. I want to backup my laptop and be able to play the music and video directly from the backup copy.</div><div><br /></div><div> </div><div>I'm disappointed that there was no new iPhone hardware or Leopard upgrade, and no business oriented iPhone software. I guess that's going to be featured in the next show....</div><div> </div><div> </div></div>",
        "metadata": {
          "word_count": 597,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The picture shows an electric car called the Zap, which someone appears to have decided that it needed a few upgrades to become the iZap. Jet motor, rotor blades and iPod interfaces... The MacBook Air is another hit product, destined to become a geek status symbol. It looks good, feels good to use, and has the option of a solid state disk. They are expensive, but the format is very well designed. The iPod touch picked up the missing set of applications as I expected, but Apple want $20 for the privilege. This is the first example of optional applications, and charging for applications. The ability to browse both audio and video based podcasts could be interesting. Apple also introduced Time Capsule, a network backup server combining a 500GB or 1TB of disk with an Airport. eTV for YouTube. One new feature is the ability to pull up photo albums over the Internet from the .mac galleries.  t show....</div><div> </div> <div> \u00a0\"I love you\" \"I'm so glad you're here. I love you so much. I'm so proud of you. Thank you for all you do for me. I appreciate you."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_10a92e68",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone, the good the bad and no ugly bits",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/07/iphone-good-bad-and-no-ugly-bits.html",
      "content": {
        "text": "I got my iPhone in the mail after 10 days, beating the 2-4 weeks delivery estimate. I've used it for a week and have some opinions I'd like to share.<br /><br />The biggest suprise is just how good video looks. Everyone I show it to goes \"wow\". This is something you have to see in person. The video quality for \"video iPod\" mode and for \"WiFi YouTube\" mode is basically perfect. When running on EDGE, YouTube downloads a lower quality version of the video that has visible artifacts, and looks more like typical video on a PC quality (e.g. similar to my <a href=\"http://www.slingbox.com\">Slingbox</a>). In any case this is a seriously good video player, and I happily watched several complete TV shows on it.<br /><br />The built-in speaker is also surprisingly good, considering the limitations of a mobile device. I left the iPhone lying on the table and used it as background music.<br /><br />The user interface is easy, no-one has any problem using it immediately, and typing on the keyboard is much easier and more accurate than I expected.<br /><br />My main disappointment is with Email. It works, but I've got used to the Gmail client application on the Treo and Blackberry. The generic pop client just doesn't work well enough since it doesn't understand archiving and threading, and the safari browser version is inconvenient and slow when using EDGE. I really hope that the Gmail application is in the works for a future update, it seems likely, given the relationship between Google and Apple, and the Google Maps application that is already included.<br /><br />For use at work, it can't yet replace my Blackberry. The iPhone VPN doesn't work with the VPN where I work. The LEAP security mode we use for wireless is also unsupported, although it works fine on my MacOS laptop. Using a simpler guest wireless setup I did see over 3Mbits/s in a speed test which is very impressive for a mobile device.<br /><br />I can use Safari to get at Outlook Web Access, and http redirect based secure access to internal web pages worked fine. I have had no crashes or problems with Safari, trying it out on several sites.<br /><br />For Netflix, heavy use of \"mouse-over\" in the user interface design is an issue since the touch screen doesn't generate mouse-over events. Although Safari on iPhone supports JavaScript, some work is needed to figure out how to use JavaScript effectively given the different input behaviors. In the meantime, the basic operations of browsing for movies and adding to the queue work fine, but rating a movie doesn't work since the star-setting method uses mouse-over.<br /><br />For detailed insight into the iPhone, go read <a href=\"http://www.roughlydrafted.com\">RoughlyDrafted</a>, its by far the most thoughtful and interesting blog on the subject.",
        "metadata": {
          "word_count": 465,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The biggest suprise is just how good video looks. Everyone I show it to goes \"wow\". This is something you have to see in person. The built-in speaker is also surprisingly good, considering the limitations of a mobile device. My main disappointment is with Email. It works, but I've got used to the Gmail client application on the Treo and Blackberry. The generic pop client just doesn't work well enough. I really hope that the Gmail application is in the works for a future update. For Netflix, heavy use of \"mouse-over\" in the user interface design is an issue. Although Safari on iPhone supports JavaScript, some work is needed to figure out how to use JavaScript effectively given the different input behaviors."
      },
      "tags": [
        "netflix",
        "security",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_74da157f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Emerging Communications Conference - EComm March 12-14",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/02/emerging-communications-conference.html",
      "content": {
        "text": "<a href=\"http://ecommmedia.com/\">I'm presenting at the EComm conference</a> on the subject of <a href=\"http://millicomputing.blogspot.com\">Millicomputing</a> on the morning of March 14th. There is <a href=\"http://hbmobile.org/wiki/index.php?title=Main_Page\">a Homebrew Mobile Club</a> meeting at the conference (at the Computer History Museum in Mountainview) the evening of March 12th.",
        "metadata": {
          "word_count": 42,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm presenting at the EComm conference on March 14th. There is a Homebrew Mobile Club meeting at the conference the evening of March 12th. I'll be at the Computer History Museum in Mountainview, California."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_86070070",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Jobs at Netflix: Capacity/Performance and Storage Admin",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/jobs-at-netflix-capacityperformance-and.html",
      "content": {
        "text": "Netflix is a great company to work for, there are lots of clever friendly and happy people here.<br />I just hired an engineer for my own group, but we have a lot more openings:<br /><br />We are looking for a performance and scalability oriented Java/Oracle developer<br /><b>Senior Performance and Scalability Engineer                 </b><br /><a href=\"http://jobs.netflix.com/DetailFlix.asp?flix1651\">http://jobs.netflix.com/DetailFlix.asp?flix1651</a><br /><br />My good friend Ed Wustenhof just joined Netflix to manage the System Administrator group, he's got two open positions, a storage oriented one:<b><br /><br /></b><b>Sr Unix Administrator - Storage Focus                       </b><br /><a href=\"http://jobs.netflix.com/DetailFlix.asp?flix1699\">http://jobs.netflix.com/DetailFlix.asp?flix1699</a><b><br /><br /><span style=\"font-weight: bold;\"></span></b>and a Capacity Planning and Performance Tuning one that I've copied the full info for below.<br /><b><span style=\"font-weight: bold;\"><br /></span>Senior Unix Administrator - Capacity Planning & Performance Tuning       <br /><br /></b><a href=\"http://jobs.netflix.com/DetailFlix.asp?flix1698\">http://jobs.netflix.com/DetailFlix.asp?flix1698</a><br /><br />RESPONSIBILITIES<br /><br />Design, Plan, Deploy and Support all aspects of Netflix's server and storage environment with a focus on Performance Tuning &amp; Capacity Planning.<br /><br />The SA team at Netflix is a crucial component to the success of the day to day operations of the company. We are a highly motivated and capable team that is seeking a new member that can take a key role in the provisioning and support of our unix/linux servers while ensuring performance stays within thresholds.<br /><br />Being the second line of support and subject matter expert, you will take individual responsibility for resolving system, storage and application issues reported via the Netflix Operations Center. You will be expected to resolve tactical issues while providing structural solutions. Your area of expertise is performance tuning and capacity planning.<br /><br />Part of the position will require travel to and from the data center (15-20min or ~10 miles), sometimes multiple times/week. On-call responsibilities, night and weekend projects are also to be expected.<br /><br />The candidate will often be the technical lead on short term projects. They will be the Subject Matter Expert with its associated responsibility. Some level of project planning and coordination of resources is to be expected. This includes vendor and contractor coordination.<br /><br />We are looking for someone with the following technical experience:<br />\u2022Deep experience of Unix with at least one of the following operating systems: AIX, Linux or Solaris.<br />\u2022Expert in capacity planning and performance tuning.<br />\u2022Strong experience in web based environments and supporting such architectures such as Java, Tomcat, Apache and other web applications.<br />\u2022Knowledge of IBM enterprise/midrange systems and storage virtualization is a plus.<br />\u2022Strong coordination and analytic skills.<br />\u2022Project planning experience and the ability to manage multiple projects simultaneously.<br />\u202210+ years IT experience with at least 8 years as System Administrator.<br />\u2022BS or MS degree in Computer science or equivalent experience.<br /><br />Exposure to the following highly desirable:<br />\u2022Experience with Symantec/Veritas product like VxVM and Netbackup<br />\u2022Strong expertise with SAN-based storage/application required.<br />\u2022Large Scale (5Tbyte +) Storage solutions deployment experience.<br />\u2022Unix shell and Perl programming.",
        "metadata": {
          "word_count": 468,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Netflix is a great company to work for, there are lots of clever friendly and happy people here. I just hired an engineer for my own group, but we have a lot more openings. We are looking for a performance and scalability oriented Java/Oracle developer. The SA team at Netflix is a crucial component to the success of the day to day operations of the company. We are seeking a new member that can take a key role in the provisioning and support of our unix/ Linux servers. The candidate will often be the technical lead on short term projects. They will be the Subject Matter Expert with its associated responsibility. Some level of project planning and coordination of resources is to be expected. 10+ years IT experience with at least 8 years as System Administrator. BS or MS degree in Computer science or equivalent experience. Experience with Symantec/Veritas product like VxVM and Netbackup."
      },
      "tags": [
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_3cdec670",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: GoSkyWatch",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/02/whats-on-my-iphone-goskywatch.html",
      "content": {
        "text": "Years ago when I had a Treo, a my friend Gred Oakes introduced me a to planetarium app, and I really liked being able to look up at night, and be able to identify the stars and planets. I live on top of a mountain, and have a great view of the night sky, and one of the first apps I paid for on my iPhone was GoSkyWatch. It has a cool feature that the accelerometer can tell you are holding it upside down snd it attempts to align its view with what you should be seeing. On a 2G iPhone with no GPS it doesn't know which way north is, so that doesn't work so well.",
        "metadata": {
          "word_count": 117,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "GoSkyWatch has a cool feature that the accelerometer can tell you are holding it upside down. It attempts to align its view with what you should be seeing. On a 2G iPhone with no GPS it doesn't know which way north is."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_a0e684d6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Patent on Performance and Capacity Planning",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/03/patent-on-performance-and-capacity.html",
      "content": {
        "text": "Here is another patent that I filed while working at Sun.<br /><br /><a href=\"http://www.freepatentsonline.com/20070124353.html\">System and method for generating a probability distribution of computer performance ratios</a><br /><br /><blockquote>A system and method for generating a probability distribution indicating the probability of a range of performance ratios between computer systems. Benchmark and/or other testing results are obtained for both computer systems. For each test, a ratio of one system's performance to the other is calculated. The ratios and/or testing results may be weighted. From the performance ratios, a histogram is produced to indicate the probabilities of achieving the various ratios. Based on the distribution of probabilities, a particular performance ratio can be predicted with a corresponding level of confidence, and used for capacity planning or configuring a computing environment.</blockquote>",
        "metadata": {
          "word_count": 126,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A system and method for generating a probability distribution of computer performance ratios. Based on the distribution of probabilities, a particular performance ratio can be predicted with a corresponding level of confidence."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_c2d1cb11",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Some New Performance Monitoring Tools",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/12/some-new-performance-monitoring-tools.html",
      "content": {
        "text": "There is a simple and extensible open source C based daemon called <a href=\"http://collectd.org/\">collectd</a> that writes to RRD files, an alternative to <a href=\"http://www.orcaware.com\">Orca/procallato</a>r for people who don't want the Perl based memory footprint of procallator. I'll check it out on my <a href=\"http://www.millicomputing.com\">Gumstix millicomputer</a> sometime.<div><br /></div><div>There is yet another open source full-function monitoring tool called <a href=\"http://www.zabbix.com/index.php\">Zabbix</a> that looks similar to <a href=\"http://www.cacti.net\">Cacti</a> in scope, possibly with more features, and with a SQL database backend. It has a commercial company backing it with support contracts etc, somewhat like the <a href=\"http://www.xetoolkit.com\">XE Toolkit</a>.</div><div><br /></div><div>The most interesting commercial tool I saw at CMG earlier this month is a capacity monitoring tool called PAWZ from <a href=\"http://perfcap.com/\">Perfcap Corporation</a>. The key thing they have worked on is taking the human out of the loop as much as possible with sophisticated capacity modelling algorithms and a simple and scalable operational model. It is very similar in concept to the capacity planning research I was working on and publishing in 2002-2004. The core idea is that you care about \"headroom\" in a service, and anything that limits that headroom is taken into account. Running out of CPU power, network bandwidth, memory, threads etc. will increase response time of the service, so monitor them all, track trends in headroom and calculate the point in time where lack of headroom will impact service response time. At eBay we used to call this the \"time to live\" for a service. You can easily focus on the services that have the shortest time to live, and proactively make sure that you have a low probability of poor response time. I'm going to take a closer look at this one...</div><div><br /></div><div><br /></div><div><br /></div><div><br /></div>",
        "metadata": {
          "word_count": 284,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a simple and extensible open source C based daemon called <a href=\"http://collectd.org/\">collectd</a> that writes to RRD files. There is yet another open source full-function monitoring tool called\u00a0Zabbix. The most interesting commercial tool I saw at CMG earlier this month is a capacity monitoring tool. Ked on is taking the human out of the loop as much as possible with sophisticated capacity modelling algorithms and a simple and scalable operational model. It is very similar in concept to the capacity planning research I was working on and publishing in 2002-2004."
      },
      "tags": [
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_b2c6efe0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Global Warming",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/12/global-warming.html",
      "content": {
        "text": "Two essential books to read: Climate Cover-Up: The Crusade to Deny Global Warming by James Hoggan - how the oil and coal industries are behind the denialist spin doctors - will make you angry. Storms of My Grandchildren: The Truth About the Coming Climate Catastrophe and Our Last Chance to Save Humanity by James Hansen - will make you scared.<br /><br />James Hansen is the NASA scientist who is probably the worlds top climate expert. Industry and politicians don't want to hear the message he has been trying to get through for years, that the situation is much worse than the IPCC projections already. He provides the research background for the safe limit of 350ppm of CO2 that is being promoted by some at Copenhagen at the moment. We are already at 387ppm, and increasing rapidly, and we are seeing irreversible changes in climate as the ice melts. James is calling for a worldwide moratorium on building coal fired power plants as a first step. The oil and coal has to be left underground.<br /><br />Human civilization has occurred during a time of very stable sea levels, over the last 7000 years. Before that sea levels have changed more rapidly and over a range of hundreds of feet. Human induced global warming is forcing change an order of magnitude stronger than the natural changes in climate, and is still accelerating. With very unstable sea levels, ports and coastal areas are going to flood, and it will not be possible to establish new ports on a permanent basis, as they will be flooded in turn within the lifetimes of our children and grandchildren. The huge numbers of refugees is an even bigger problem.<br /><br />What can we do?<br />Read those two books.<br />Sell coastal property while there are still people who will buy it.<br />Don't invest in oil, coal and insurance companies.<br />Put up solar panels and switch from gas to electric appliances.<br />Sign up at <a href=\"http://www.350.org/\">the 350.org web site</a>, <a href=\"http://www.facebook.com/350.org\">join the 350org facebook group</a>.<br /><br />I get books via the Kindle service and read them on my iPhone, instant delivery, and always with me to read.<br /><a href=\"http://www.amazon.com/Climate-Cover-Up-Crusade-Global-Warming/dp/1553654854/\">Climate Cover-Up: The Crusade to Deny Global Warming</a><br /><br /><a href=\"http://www.amazon.com/Storms-My-Grandchildren-Catastrophe-Humanity/dp/1608192008/\">Storms of My Grandchildren: The Truth About the Coming Climate Catastrophe and Our Last Chance to Save Humanity</a><br /><br />A good summary of the arguments is at <a href=\"http://www.skepticalscience.com/\">Skeptical Science</a><br /><br />I have read everything I can find on this subject over the last year or two, I have an Applied Physics degree and enough science and statistics that I can tell who and what makes sense and who doesn't. Every single denier and contrarian that I have heard of is backed by the coal and oil industries, often through opaque organizations and astroturf groups with innocent sounding names. They have been exposed as liars and cherry pickers, but really they are spin doctors sowing confusion. If you listen to them you are deluding yourself, and you should be investing in real estate in Florida...",
        "metadata": {
          "word_count": 504,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "James Hansen is the NASA scientist who is probably the worlds top climate expert. Industry and politicians don't want to hear the message he has been trying to get through for years, that the situation is much worse than the IPCC projections already. Human induced global warming is forcing change an order of magnitude stronger than the natural changes in climate. With very unstable sea levels, ports and coastal areas are going to flood. It will not be possible to establish new ports on a permanent basis. The huge numbers of refugees is an even bigger problem. I have an Applied Physics degree and enough science and statistics that I can tell who and what makes sense and who doesn't. Every single denier and contrarian that I have heard of is backed by the coal and oil industries, often through opaque organizations and astroturf groups with innocent sounding names. They have been exposed as liars and cherr. \"They are spin doctors sowing confusion. If you listen to them you are deluding yourself, and you should be investing in real estate in Florida,\" he says."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_415f4618",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Gumstix Goliath - myPhone system board",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/07/gumstix-goliath-myphone-system-board.html",
      "content": {
        "text": "The <a href=\"http://gumstix.com/store/catalog/product_info.php?products_id=194\">Gumstix Goliath open phone hardware</a> is now available for order. There are versions with and without the Global Positioning Systems (GPS) support.<br /><br /><table border=\"0\" cellpadding=\"2\" cellspacing=\"0\" width=\"93%\"><tbody><tr class=\"descrp\"><td height=\"20\" valign=\"top\" width=\"87\"><div align=\"right\"><strong>Features:</strong></div></td>  <td height=\"20\" valign=\"top\" width=\"280\"><p>GSM/GPRS/EDGE, GPS, analog audio in/out, LCD, touchscreen controller, USB host port, 3D-accelerometer, NiMH battery charger (batteries not included)<br />  <br />Provides full- speed USB access to the Siemens MC75 GPRS/EDGE module, the u-blox NEO-4S GPS module and one external USB device.</p></td> </tr> <tr class=\"descrp\">  <td height=\"20\" valign=\"top\"><div align=\"right\"><strong>Connection:</strong></div></td>  <td height=\"20\" valign=\"top\">60-pin Hirose connector to verdex motherboard only</td> </tr> <tr class=\"descrp\"> <td height=\"20\" valign=\"top\"><div align=\"right\"><strong>Dimensions:</strong></div></td> <td height=\"20\" valign=\"top\">105.5mm x 67.3mm</td></tr></tbody></table><br />I've placed my order (I already have a Gumstix Verdex CPU board), and I'm still working on myPhone case design for this board and its 4.3\" touchscreen LCD. Unlike the iPhone, the hardware and software is completely open, you can do anything you like with it, and thats also the problem, the software is extremely basic at the moment. However, its about the journey, not the destination, I'm going to have a lot of fun as we build this device.",
        "metadata": {
          "word_count": 187,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Gumstix Goliath open phone hardware is now available for order. There are versions with and without the Global Positioning Systems (GPS) support. I've placed my order (I already have a Gumstix Verdex CPU board), and I'm still working on myPhone case design for this board and its 4.3\" touchscreen LCD. Unlike the iPhone, the hardware and software is completely open, you can do anything you like with it. However, its about the journey, not the destination, I'm going to have a lot of fun as we build this device."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e8837f17",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Going Solar? Get a weather station...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/going-solar-get-weather-station.html",
      "content": {
        "text": "There's nothing line a 36 hour power outage (a tree fell and took the power lines with it) to make you think about where your energy comes from. We could get a generator, but it would only be used for a few days a year. If we get Solar power, it pays for itself over time and we would get daytime power at least....<br /><br />We have a very sunny high altitude microclimate, on top of the Santa Cruz mountains with a south facing roof. It seems ideal, but the economics are quite dependent on how much Sun we actually get. We called up some local solar power companies to get an idea of what would be involved, but in the short term we decided that it would be good to measure our local weather.<br /><br />I've been using the <a href=\"http://www.wunderground.com/weatherstation/index.asp\">Weather Underground</a> web site for a while, to track actual local conditions such as wind speed at a station a few miles away. This site contains all you need to know about setting up your own station, and there is plenty of low cost hardware and software to do it.<br /><br />I think the amount of technology available for the price is amazing. I picked the Oregon Scientific WMR100 wireless weather station. It comes with a full set of sensors for temperature, humidity, barometric pressure, rainfall, wind speed and direction, that connect to the base station via wireless. It has a USB connection for uploading to a PC. List price is over $200, but I bought it on eBay for $129.99+18 shipping, which was the best overall deal at the time.<br /><br />I plan to add the optional UV sensor to get an indication of Sunlight (the UV sensor for this model is not yet available, I got an answer that it should be out in a couple of months), and monitor two temperature sensors, one in the shade, and one in full sunlight. I should be able to figure out which days are clear and sunny, and for how long.<br /><br />[Update]<br />It arrived, I corrected a few details about the product features above, and I've assembled it and got the sensors to connect over the wireless, ready to install outside. Its nicely made, everything seems to work fine.",
        "metadata": {
          "word_count": 381,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "We have a very sunny high altitude microclimate, on top of the Santa Cruz mountains with a south facing roof. It seems ideal, but the economics are quite dependent on how much Sun we actually get. If we get Solar power, it pays for itself over time and we would get daytime power at least. The Oregon Scientific WMR100 is a wireless weather station. It comes with a full set of sensors for temperature, humidity, barometric pressure, rainfall, wind speed and direction. It has a USB connection for uploading to a PC. The wireless system is ready to be installed outside. It's nicely made, everything seems to work fine. The wireless system has a built-in camera to monitor the weather."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_338ae876",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Are any (ex-)PAE Snorclers looking for a new career?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/08/are-any-ex-pae-snorclers-looking-for.html",
      "content": {
        "text": "I'm looking for someone who has been benchmarking and testing web services/cloud performance to work in Los Gatos CA, no remote work, sorry.<br /><br />For the uninitiated, PAE = Performance Applications Engineering, one of the groups I was in years ago at Sun that built benchmarks and did performance analysis for a living. Snorcle = Sun + Oracle sinking beneath the waves...<br /><br />This is a great time to join Netflix, we are growing fast, there are a lot of open positions and we pay very well for star players.",
        "metadata": {
          "word_count": 90,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm looking for someone who has been benchmarking and testing web services/cloud performance to work in Los Gatos CA, no remote work, sorry. For the uninitiated, PAE = Performance Applications Engineering."
      },
      "tags": [
        "engineering",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_9aa1e819",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Strange Contextual Ads | by Adrian Cockcroft | 11th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/strange-contextual-ads-by-adrian.html",
      "content": {
        "text": "The eBay contextual ads seems to be fixated on the word \"margin\" which is not present in the content of my blog. However the CSS template that is used to host this blog contains the word margin over and over again. I think Alex needs to do a better job of filtering out formatting words before he does his context analysis....",
        "metadata": {
          "word_count": 61,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The eBay contextual ads seems to be fixated on the word \"margin\" which is not present in the content of my blog. The CSS template that is used to host this blog contains the word margin over and over again. I think Alex needs to do a better job of filtering out formatting words before he does his context analysis."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_348678f0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Data logged by flow accounting",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/data-logged-by-flow-accounting.html",
      "content": {
        "text": "The data comes in two forms, outgoing traffic is tagged with the userid and project of the initiating process, but incoming traffic is missing this information. Since TCP flows are captured in pairs they need to be matched up. The output from the provided demo program /usr/demo/libexacct/exdump -v is shown below.<br /><br />These match if the src and dest address and ports are reversed<br /><br /><pre><br />    ff  group-header                    [group of 4 object(s)]<br />     1   version                        1<br />     2   filetype                       \"exacct\"<br />     3   creator                        \"SunOS\"<br />     4   hostname                       \"crun\"<br />   109  group-flow                      [group of 11 object(s)]<br />  3000   src-addr-v4                    a.b.c.d  <br />  3001   dest-addr-v4                   e.f.g.h  crun<br />  3004   src-port                       80<br />  3005   dest-port                      43727<br />  3006   protocol                       6              tcp<br />  3007   diffserv-field                 0<br />  300a   creation-time                  1110482732     03/10/05 11:25:32<br />  300b   last-seen                      1110482734     03/10/05 11:25:34<br />  3008   total-bytes                    3447<br />  3009   total-packets                  10<br />  300e   action-name                    \"acct\"<br />   109  group-flow                      [group of 13 object(s)]<br />  3000   src-addr-v4                    e.f.g.h  crun<br />  3001   dest-addr-v4                   a.b.c.d<br />  3004   src-port                       43727<br />  3005   dest-port                      80<br />  3006   protocol                       6              tcp<br />  3007   diffserv-field                 0<br />  300a   creation-time                  1110482732     03/10/05 11:25:32<br />  300b   last-seen                      1110482734     03/10/05 11:25:34<br />  3008   total-bytes                    4561<br />  3009   total-packets                  5<br />  300c   projid                         10<br />  300d   uid                            100<br />  300e   action-name                    \"acct\"<br /></pre>",
        "metadata": {
          "word_count": 213,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The data comes in two forms, outgoing traffic is tagged with the userid and project of the initiating process, but incoming traffic is missing this information. Since TCP flows are captured in pairs they need to be matched up. The output from the provided demo program /usr/demo/libexacct/exdump -v is shown below. 300a   creation-time    1110482732   03/10/05 11:25:32. 300b   last-seen  \u00a0 1110 482734   01/09/09 11:00:00. 3008   total-bytes \u00a0   3447. 3009  total-packets   \u00a0   10. 300e  action-name   \"acct\"   109  group-flow  . 109   [group of 13 object(s)],. 300d   uid:    100. 300e   action-name: \"acct\" 300e action-verb: \"I am\" 300d action-phrase: \"You are\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_853afa28",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What&#39;s on my iPhone: eBay and Amazon",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/what-on-my-iphone-ebay-and-amazon.html",
      "content": {
        "text": "Great for looking up how much stuff costs and how easy is it to find something rare. <br/><div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a>]</p><br/></div><br />",
        "metadata": {
          "word_count": 25,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Great for looking up how much stuff costs and how easy is it to find something rare. Can also be used to find out how much it costs to buy a particular item."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_59a8510d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone 1.1.1 and the iBrick",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/10/iphone-111-and-ibrick.html",
      "content": {
        "text": "This blog just got mentioned by a <a href=\"http://www.wired.com/gadgets/mac/commentary/cultofmac/2007/10/cultofmac_1003?currentPage=2\">Wired News posting about the iPhone</a>. Other than some random spellings of my name I think it picked up on my points quite well.<br /><br />I haven't hacked my iPhone, and it upgraded to 1.1.1 cleanly. The new features and upgrades are welcome, but the upgrade process didn't clear out the ever increasing amount of \"Other Memory\". I had resorted to removing videos from my iPhone to make space and avoid out of memory errors. You can see 1.6GB in the display below.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2MKU8ibssz-5Ode1VwgJt_mWqyCFojF8ALHOKvnp3AbqhOi46pOIkxROFyZVUVCH_K42sdb3RyMyeOk4R41-9ZuzNzkv0djdfTOdcdh6q9F_UXAbuvpcTcTvr22UEIeHVLZpU/s1600-h/Picture+12.png\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi2MKU8ibssz-5Ode1VwgJt_mWqyCFojF8ALHOKvnp3AbqhOi46pOIkxROFyZVUVCH_K42sdb3RyMyeOk4R41-9ZuzNzkv0djdfTOdcdh6q9F_UXAbuvpcTcTvr22UEIeHVLZpU/s400/Picture+12.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5117337194005869842\" /></a><br /><br />I went through the settings, cleared out deleted mail and safari history and it made no difference. I deleted all the content and it made no difference, so I re-loaded the iPhone from scratch, and it dropped to 14MB after I had put my content back.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1E5p47ZOpPYHTWwY5osAWgmRrWMyfPtH916G5vmZEDoplkY-tRjGYOPvkgctQBBjv2yFwKY8gNEw7OW1URPnISw8NTCRetbOise1IbsFw6SIdo-AxO4s013opMddzJDrpYTac/s1600-h/Picture+16.png\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi1E5p47ZOpPYHTWwY5osAWgmRrWMyfPtH916G5vmZEDoplkY-tRjGYOPvkgctQBBjv2yFwKY8gNEw7OW1URPnISw8NTCRetbOise1IbsFw6SIdo-AxO4s013opMddzJDrpYTac/s400/Picture+16.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5117337610617697570\" /></a><br /><br />There are some posts on the iPhone support forums about this problem, but no mention of an official fix from Apple. One user claims that it could be related to Google Maps, which I use quite often. I'm going to watch this \"other memory\" usage to try to figure out which apps are increasing it.<br /><br />I haven't seen a clear explanation of what is going on inside the iPhone that makes hacking applications very different from hacking the carrier, and from my experience building myPhone I can hopefully clarify this issue.<br /><br />There are two CPUs inside just about all phones of any type. One is part of a GSM modem module that interfaces to the phone network, the other is the application CPU that runs the user interface (e.g. OSX on the iPhone). For the Gumstix Goliath a Siemens MC75 module interfaces to the SIM card, and the GSM antenna directly, and has USB, serial and audio interfaces. The way all these GSM modems are managed is by sending it command sequences that would be familiar to any old-timers, its a derivative of the Hayes AT command set. Inside the iPhone is a similar module. The GSM modem has a low speed low power CPU with its own flash memory. The service providers don't like having people mess with this firmware, because it controls the wireless signaling, power levels and cellular communication protocol directly, and they have to be certified for connection to the GSM network. For our homebrew phones we buy Telit or Siemens modules off the shelf and don't mess with their internals, and they just work. Thats why we can build usable homebrew phones.<br /><br />Since this firmware interfaces directly with the SIM card, it can contain code that locks the phone to a particular service provider. The iPhone's GSM modem firmware is locked to AT&T. In order to modify this firmware the unlock applications such as iPhoneSIMFree had to take advantage of a debug feature or security flaw to load modified firmware into the GSM modem. Since the GSM modem is a standard component, hackers were probably able to find out its supplier and part number and get hold of a lot of documentation on how it works.<br /><br />With the 1.1.1 release it appears that Apple closed off some security holes that were being used to hack into the iPhone, but also digitally signed the code inside the phone. This means that any attempt to modify OSX or the GSM firmware breaks the signature, and since the CPUs have hardware support for digital signatures, its extremely difficult to get around this form of protection. Its now several weeks since 1.1.1 was released on the iPhone and iPod Touch and it hasn't been broken into yet. The hackers have to find and exploit a completely new security flaw to get into the application CPU first, then find a way into the GSM modem. It is then easy for Apple to figure out how they are getting in, and fix it in their next release. With Apple releasing monthly updates, there isn't a viable business proposition based on hacking iPhones.<br /><br />I still think Apple has a lot of unreleased iPhone software in the pipeline and will open up to developers when it has pushed out a few more updates. I think the ideal audience for an iPhone developer launch is MacWorld San Francisco in January. The highest density of startups and developers is in the Bay Area, and its the anniversary of the iPhone launch. Thats as near as I'm getting to a prediction....<br /><br />In the meantime, the developers who want something to do could join the <a href=\"http://www.openmoko.org/\">OpenMoko project</a>, help build applications for it and maybe even port the Linux based OpenMoko to the iPhone hardware. Thats what I will be working on when I get myPhone up and running....",
        "metadata": {
          "word_count": 823,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I haven't hacked my iPhone, and it upgraded to 1.1.1 cleanly. New features and upgrades are welcome, but the upgrade process didn't clear out the ever increasing amount of \"Other Memory\" I had resorted to removing videos from my iPhone to make space and avoid out of memory errors. I went through the settings, cleared out deleted mail and safari history and it made no difference. I deleted all the content and it makes no difference, so I re-loaded the iPhone from scratch, and it dropped to 14MB after I had put my content back. There are some posts on the iPhone support forums about this problem. One user claims that it could be related to Google Maps. I'm going to watch this \"other memory\" usage to try to figure out which apps are increasing it. The iPhone's GSM modem firmware is locked to AT&T. The service providers don't like having people mess with this firmware. For our homebrew phones we buy Telit or Siemens modules off the shelf and don't mess with their internals. Hackers have been able to get into the iPhone and iPod Touch since 1.1.1 was released. Apple has digitally signed the code inside the phone. This means any attempt to modify OSX or the GSM firmware breaks the signature. With Apple releasing monthly updates, there isn't a viable business proposition based on hacking iPhones. I think the ideal audience for an iPhone developer launch is MacWorld San Francisco in January. The highest density of startups and developers is in the Bay Area, and its the anniversary of the iPhone launch."
      },
      "tags": [
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_b00e7947",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Thoughts from ETech - Tutorial Day - FLORWAX? | by Adrian Cockcroft | 7th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/thoughts-from-etech-tutorial-day.html",
      "content": {
        "text": "I'm at the O'ReillyEmerging Technology conference in San Diego, today was \"Tutorial Day\" and I decided to attend \"Designing the next generation of Web Applications\" in the morning and \"Next Generation Flash Development with Flex\" in the afternoon.<br /><br />The morning talk was very nicely presented, <a href=\"http://feeds.feedburner.com/jjg\">Jesse James Garrett</a> coined the term AJAX amongst other things, and <a href=\"http://veen.com/jeff/rss.xml\">Jeff Veen</a> worked on Hotwire, Blogger and <a href=\"http://www.measuremap.com\">Measuremap</a>, they provided a structured set of best practices for designing web applications with lots of great examples and anecdotes.<br /><br />AJAX acts as a convergence point for browser based applications because almost all current browsers support the same set of technologies and there is a highly functional lowest common denominator. There is now also a large body of applications that provide the intertia or value that constrains the browser writers from diverging with incompatible functionality. This is the same effect that occurred in the PC marketplace, when MSDOS and Windows developed enough application value that neither Intel nor Microsoft could diverge in an incompatible manner. The collective self interest of the end user reaches a tipping point that blocks radical innovation, and slow incremental evolution takes over.<br /><br />The interesting area for me is how this maps to the mobile/wireless space. There is no AJAX for wireless applications, the market is huge, but the platform diversity is also huge and is growing. One estimate I heard was that there are 1000 separate platforms to target and that this number is growing, not shrinking.<br /><br />So what we need, is the equivalent of AJAX for Wireless, something like Flash Lite On Rails Wireless Asynchronous XML - FLORWAX - which also has a household cleaning connotation :-)<br />If enough people standardize on a common set of technologies, then the handset vendors will start to build to a common profile as well, and we could end up with a decently functional lowest common denominator.<br /><a href=\"http://technorati.com/tag/etech06\" rel=\"tag\">etech06</a>, <a href=\"http://technorati.com/tag/etech\" rel=\"tag\">etech</a>, <a href=\"http://technorati.com/tag/florwax\" rel=\"tag\">florwax</a>",
        "metadata": {
          "word_count": 328,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "AJAX acts as a convergence point for browser based applications. Almost all current browsers support the same set of technologies. There is now also a large body of applications that provide the intertia or value that constrains the browser writers from diverging. If enough people standardize on a common set of technologies, then the handset vendors will start to build to a common profile as well. This is the same effect that occurred in the PC marketplace, when MSDOS and Windows developed enough application value that neither Intel nor Microsoft could diverge in an incompatible manner. We could end up with a decently functional lowest common denominator. and we could endup with a\u00a0decently\u00a0functional\u00a0lowest\u00a0common\u00a0 denominator\u00a0and we could be in business soon."
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_f62f1890",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "MacOS X Leopard, iPhone and Stereo Bluetooth Headphones - A2DP",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/12/macos-x-leopard-and-stereo-bluetooth.html",
      "content": {
        "text": "Over a year ago I bought stereo bluetooth headphones on eBay, its a multifunction unit the OMIZ OMS600, and it includes an MP3 player (with MicroSD slot), FM radio, Stereo A2DP headphone and Headset with Mono audio/Microphone. When I tried to configure it on MacOS X Tiger it didn't work because there was no A2DP headphone support.<div><br /></div><div>MacOS X Leopard now supports A2DP and \"just works\" with this headset. The Mac sees both the Headset (Mono audio/mic for Skype etc) and Headphone (A2DP Stereo) as separate devices. After the usual Bluetooth device wizard setup, simply put the OMS600 into the headphone mode and pick \"Use Headphone\" from the Bluetooth dropdown menu on the Mac, the Mac's internal speakers mute, and the Headphones play. It worked over a 10 foot range walking around a room, crackled a bit at the limit and dropped the connection if I went too far away.</div><div><br /></div><div>I don't see the OMS600 for sale any more, but there are plenty of A2DP headphones out there now, and its nice to see that Apple finally got the devices to work, and made it \"Just Work\" as usual.</div><div><br /></div><div>The headset mode also works on my iPhone, but the A2DP mode doesn't. I paired the headset with the iPhone and was able to make calls and receive them. When using the FM radio or the MP3 player built-into the headset it paused and resumed for incoming calls. However when the Headphone mode was paired with my Leopard machine the iPhone didn't route calls to the Headset. The OMS600 headset has the microphone built into the left side earpiece with no voice tube or boom down nearer my mouth, and it doesn't pick up very clearly. The noise cancelling Jawbone headset works far better.</div><div><br /></div><div>I listened to some music on iTunes (Radiohead In Rainbows and Gorillaz D-Sides are my current albums for serial listening) and waited forever for XP to start up in Parallels so I could fire up IE7 and see a Netflix Watch Instantly show about the Pixies reunion tour (called LoudQuietLoud). [Yes I know it would be nice if it worked natively on the Mac, but the studios only approve Windows DRM, and the alternatives all have issues that are taking way too much time to sort out].</div><div><br /></div><div>Happy new year...</div><div><br /></div>",
        "metadata": {
          "word_count": 384,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "MacOS X Leopard now supports A2DP and \"just works\" with this headset. After the usual Bluetooth device wizard setup, simply put the OMS600 into the headphone mode and pick \"Use Headphone\" The headset mode also works on my iPhone, but the A2DP mode doesn't. When using the FM radio or the MP3 player built-into the headset it paused and resumed for incoming calls. The OMS600 headset has the microphone built into the left side earpiece with no voice tube or boom down nearer my mouth. The noise cancelling Jawbone headset works far better. It would be nice if it worked natively on the Mac, but the studios only approve Windows DRM, and the alternatives all have issues that are taking way too much time to sort out]."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_79c1e7af",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Free Communication is Disrupting Hierarchies",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/11/free-communication-is-disrupting.html",
      "content": {
        "text": "Jeff Jarvis <a href=\"http://buzzmachine.com/2013/08/08/give-up-on-the-net/\" target=\"_blank\">recently blogged</a>:<br />\n<blockquote class=\"tr_bq\">\n<span style=\"background-color: #fdfdfd; font-family: 'Open Sans', 'Helvetica Neue', Helvetica, Arial, sans-serif; font-size: 15px; line-height: 25px;\"><i>I doubt the net\u2019s creators realized how subversive it was to connect anyone to anyone, bypassing the institutions that mediated those connections: from media to government, universities to retailers. These institutions are now circling wagons to protect their prerogatives: copyright for media, secrecy for government.</i></span></blockquote>\n<br />\nSomewhat related discussions are taking place at the <a href=\"http://defragcon.com/2013/bigger-networks-of-smaller-things-part-2/\" target=\"_blank\">Defrag conference</a> and on <a href=\"http://switchandshift.com/the-death-of-workplace-hierarchy\" target=\"_blank\">blogs talking about</a> corporate organizations and <a href=\"http://www.theguardian.com/commentisfree/2013/sep/05/government-betrayed-internet-nsa-spying\" target=\"_blank\">government surveillance</a>.<br />\n<br />\nHowever this is nothing new, over centuries there has been a gradual reduction in the cost of communication between people, which has caused social changes, disrupted old industries, created new industries and led to political revolutions. The end point that we have reached in the last few years is that the cost of communication is now zero. There is no incremental cost for me to send a tweet that is delivered to thousands of people a few seconds later. If the tweet contains novel ideas or information that people want to share, then in a few seconds, millions of people could see it. With about 10,000 direct followers my own twitter mention reach was in the millions last week.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPpHk-fbNvwKpQDzh-NAJLozT2AWOlirXSKUUtuGCLY38PWQVGriUaV0gXjyn7lajXgXDkyxo5fidf8u4p-ETaBHB2J7BuR_Jt_m3e_veT2MvFqGMHJFBCMxAs3hv8eUg3qzwy/s1600/Screen+Shot+2013-11-24+at+11.56.23+AM.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"80\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgPpHk-fbNvwKpQDzh-NAJLozT2AWOlirXSKUUtuGCLY38PWQVGriUaV0gXjyn7lajXgXDkyxo5fidf8u4p-ETaBHB2J7BuR_Jt_m3e_veT2MvFqGMHJFBCMxAs3hv8eUg3qzwy/s320/Screen+Shot+2013-11-24+at+11.56.23+AM.png\" width=\"320\" /></a></div>\n<br />\n<br />\nWhen I see an industry or organization that was built on assumptions that are no longer true, I expect to see disruption follow, and opportunities to create new industries and organizations. <b>The assumption that communication is expensive is baked into our world. The new industries and organizations that take advantage of this change are emerging now.</b><br />\n<br />\nTo see how subversive this is, we need to look at some history. Every step in the reduction of communication cost has enabled changes that have been resisted by the hierarchy, led to revolutions, and in most cases have been co-opted and controlled by the elite.<br />\n<br />\nThe ability to control, coordinate and influence a large group of people depends upon effective communication. Early examples include the creation of sea-going ships and roads to sustain empires, from the Romans to the British Empire as examples. The leaders of these empires could gather information via messengers and send out decrees to manage at a distance, by using a coordinated hierarchical communication network. The cost of communication for the workers was prohibitive, so all information trickled down through the local authorities, whether civil, military or religious. The only low cost communication took place within the local village.<br />\n<br />\nMost workers were illiterate, and taking England in the middle ages as an example, a series of disruptions were caused by the invention of the printing press. The loss of control that resulted caused major problems for authorities, and allowed ideas to spread in new ways.<br />\n<br />\nAs cities got bigger, local communication got faster, newsletters and newspapers started to spread ideas more quickly and the creation of postal services brought the cost of peer to peer communication down far enough that everyone could communicate over larger distances. Governments co-opted these new mechanisms, regulating the news, running the postal services. The industrial revolution created commercial entities with their own hierarchies of control and communication. The ability of ideas to spread rapidly and reach many people allowed groups to self-organize, leading to political upheaval. Revolution was enabled by the ability to spread ideas outside the incumbent ruler's hierarchy of communication and control.<br />\n<br />\nToday we live in societies where effectively everyone is literate, connected and mobile. However we still have the same hierarchical institutions that were developed hundreds of years ago. Governments to manage the hierarchies of society, Companies to manage the hierarchies of commerce, Religions to manage the hierarchies of belief, Universities to manage the hierarchies of education.<br />\n<br />\nThese hierarchies still have value, they continue to represent ownership and have responsibilities but they no longer have control. They were sustained in the past by imposing their will on people, and carefully managed what information people are exposed to. The hierarchies are now very large and deeply nested, so rather than a small ruling elite and a large peasant population, there are very many levels in the middle, which makes many of us have a vested interest in the continuation of parts of the hierarchies.<br />\n<br />\nWe are now all connected by a series of relatively flat peer to peer networks. Facebook for a curated network of family, friends and interest groups, twitter for real-time news and a much more dense ad-hoc interconnection of interests, email for direct communication, and many other services. The \"Arab Spring\", the \"Occupy Movement\" and other recent political revolutions were able to find and coordinate large numbers of people extremely rapidly (hours or days) without forming a hierarchy first.<br />\n<br />\nThe new world is a blend of hierarchies with many peer to peer links that reach across and between the old hierarchies. The problem for the elite leaders is that they are used to having control, owning the flow of information and managing deeply nested organizations. In this new world, the leaders have ownership, responsibility, influence but much less control, they control and manage a small part of the flow of information, and their organizations are flatter and less well defined. At the extreme there are <a href=\"https://github.com/rtomayko/distributed-management-talk\" target=\"_blank\">organizations like Github,</a> which are figuring out how to make an emergent self organizing structure work. To scale a flat organization there needs to be a very strong sense of culture and unity of purpose. I think Netflix has found a productive balance between a hierarchy of responsibility for aspects of the business and freedom at each level to self organize the delivery of what the business needs. Broad and open communication across the business, and many skip-level one-on-one meetings flattens the organization and minimizes \"politics\".<br />\n<br />\nLooking beyond business, Religion only really works if you don't think too hard about any other variant of religion. Religions all claim to be true, but claim different incompatible and incoherent things, so that exposure to ideas such as the <a href=\"http://debunkingchristianity.blogspot.com/2007/03/outsider-test-for-faith.html\" target=\"_blank\">Outsider Test for Faith</a> causes deep problems for the faithful. Religion in general is being disrupted by science and free communication, and polarization into atheism and reactionary fundamentalism is one of the outcomes in society.<br />\n<br />\nEducation is based on the communication of skills and information from teachers to students, and the big institutions of education are being disrupted. They no longer control the flow of education, and the costs are increasing beyond the ability of the students to repay them, so companies like Coursera and the Khan Academy are building alternatives. For software developers, the body of code you own and contributed to on github matters far more than where (or even if) you studied. The state of the art isn't being created and taught in University research departments any more, the commercial ecosystems that form around technologies are where the bleeding edge of innovation is being disseminated to new recruits.<br />\n<br />\nThese disruptions are on the way, but change tends to occur rapidly when a tipping point is reached rather than gradually. A point made by Nicholas Taleb in his discussion of <a href=\"http://en.wikipedia.org/wiki/Black_swan_theory\" target=\"_blank\">Black Swan events</a>, is that its easier to point out that a change could happen and plan for what to do next, than to predict when exactly it will come. So like a cartoon Wily Coyote who has run off a cliff, we ignore the change until we look down and realize that nothing is holding us up.<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVPEENiVLjPdKaSxac49Hcr-4617KBKh8UGctCzylT3XS2toe-I8g5CcAMunuzgRoj9rZrfSyatYYt3hFLU-hZbfQUfmLsCElKKn3zDbJylt-eDqUpru9kGP6FOlx3urRakGEe/s1600/Unknown.jpeg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhVPEENiVLjPdKaSxac49Hcr-4617KBKh8UGctCzylT3XS2toe-I8g5CcAMunuzgRoj9rZrfSyatYYt3hFLU-hZbfQUfmLsCElKKn3zDbJylt-eDqUpru9kGP6FOlx3urRakGEe/s1600/Unknown.jpeg\" /></a></div>\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 1327,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Somewhat related discussions are taking place at the <a href=\"http://defragcon.com/2013/bigger-networks-of-smaller-things-part-2/\" target=\"_blank\">Defrag conference. Jeff Jarvis recently blogged about giving up on the net. Over centuries there has been a gradual reduction in the cost of communication between people. This has caused social changes, disrupted old industries, created new industries and led to political revolutions. The end point that we have reached in the last few years is that the Cost of communication is now zero. The assumption that communication is expensive is baked into our world. The new industries and organizations that take advantage of this change are emerging now. Every step in the reduction of communication cost has enabled changes that have been resisted by the hierarchy. The ability to control, coordinate and influence a large group of people depends upon effective communication. Early examples include the creation of sea-going ships and roads to sustain empires, from the Romans to the British Empire as examples. As cities got bigger, local communication got faster, newsletters and newspapers started to spread ideas more quickly. Governments co-opted these new mechanisms, regulating the news, running the postal services. Industrial revolution created commercial entities with their own hierarchies of control and communication. Hierarchies still have value, they continue to represent ownership and have responsibilities but they no longer have control. The hierarchies are now very large and deeply nested, so rather than a small ruling elite and a large peasant population, there are very many levels in the middle.  revolutions were able to find and coordinate large numbers of people extremely rapidly (hours or days) without forming a hierarchy first. The new world is a blend of hierarchies with many peer to peer links that reach across and between the old hierarchies. Open communication across the business, and many skip-level one-on-one meetings flattens the organization and minimizes \"politics\" Religion only really works if you don't think too hard about any other variant of religion. The state of the art isn't being created and taught in University research departments any more. The commercial ecosystems that form around technologies are where the bleeding edge of innovation is being disseminated to new recruits. The author of this article is a self-confessed cynic. He believes that people who run off a cliff ignore the change until they look down and realize nothing is holding them up."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_632fc67f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Capture Ratio and measurement overhead",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/capture-ratio-and-measurement-overhead.html",
      "content": {
        "text": "For performance monitoring applications, we often want to know the process related information, but collecting it from /proc is very expensive compared to collecting other performance data, and the amount of work increases as the number of processes increases.<br /><br />The <b>capture ratio</b> is defined as the amount of CPU time that is gathered by looking at process data versus the total systemwide CPU used. The difference is made up by processes that stop during the intervals between measurements. Since short lived processes may start and stop between measurements, and we don't know whether a process stopped immediately before a measurement or just after a measurement, there is always an error in sampled process measures. The error is reduced by using a short measurement interval, but that increases overhead. Bypassing the /proc interface, and reading process data directly from the kernel is very implementation dependent but is used by BMC's <a href=http://www.bmc.com>PATROL\u00ae for Unix - Perform & Predict</a> data collector, so that they can collect process data efficiently on large systems at high data rates.<br /><br />By watching traditional SysV accounting records that fall between measures, some heuristics can be used to improve the capture ratio. However the SysV accounting record does not include the process id, so this is an inexact technique. The <a href=http://www.teamquest.com>Teamquest\u00ae View and Model</a> data collector uses this trick.<br /><br />With extended accounting, we can use <b>wracct</b> to force a record of all current processes to be written to the accounting file, along with the processes that terminate. This gives us a perfect capture ratio, even at infrequent measurement intervals, so the overhead of process data collection is extremely low.<br /><br />There is no need for a performance collection agent, a cron script can invoke wracct at the desired measurement interval. Another cron script can use acctadm to switch logs to a new file and process locally or ship the old file to a central location as required.<br /><br />That in a nutshell is why extended accounting is interesting, very good quality data, perfect capture ratio and very low measurement overhead.",
        "metadata": {
          "word_count": 346,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The capture ratio is defined as the amount of CPU time that is gathered by looking at process data versus the total systemwide CPU used. The difference is made up by processes that stop during the intervals between measurements. Bypassing the /proc interface, and reading process data directly from the kernel is very implementation dependent. Traditional SysV accounting records that fall between measures, some heuristics can be used to improve the capture ratio. This is an inexact technique as it does not include the process id. With extended accounting, we can use wracct to force a record of all current processes to be written to the accounting file. This gives us a perfect capture ratio, even at infrequent measurement intervals. That in a nutshell is why extended accounting is interesting, very good quality data, perfect capture ratio and very low measurement overhead. entral location as required."
      },
      "tags": [
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_a1519528",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Benchmarks",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2004/06/benchmarks.html",
      "content": {
        "text": "Google of [\"web services\" benchmark] comes up with a SPEC benchmark development, controversial because price/perf was voted out of the spec, and a <a href=\"http://www.hanselman.com/blog/PermaLink.aspx?guid=294\">Doculabs @Bench evaluation of several platforms</a>\n<br />\n<br /><a href=http://www.sarvega.com/xslt-benchmark.php>\n<br />An XML benchmark</a>\n<br />\n<br /><a href=http://www.spec.org/appPlatform/>\n<br />SPEC appPlatform</a>",
        "metadata": {
          "word_count": 47,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Google's benchmark is controversial because price/perf was voted out of the spec. Google of [\"web services\" benchmark] comes up with a SPEC benchmark development."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7fba5798",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix and Silverlight and Joost",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/05/netflix-and-silverlight-and-joost.html",
      "content": {
        "text": "Very interesting <a href=\"http://www.hackingnetflix.com/2007/05/netflix_demos_s.html\">demo of Microsoft Silverlight as a Netflix application</a> from MIX07.<br /><br />This seems like a nice, high performance multiplatform way to build web based user interfaces. It should give Flash some competition...<br /><br />I'm handing back eBay's Dell D600 with Windows XP and getting a MacBook when I start at Netflix, it will be nice to have XP running only when I need it, and when the Silverlight version of Netflix web video ships, it will support Mac.<br /><br />Joost has also released a new beta build, however I don't have a machine I can run it on during this transition. They don't support my old PowerBook G4 yet....",
        "metadata": {
          "word_count": 112,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Silverlight is a new way to build web based user interfaces. It should give Flash some competition. Joost has also released a new beta build, however I don't have a machine I can run it on."
      },
      "tags": [
        "netflix",
        "performance",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_f03d6954",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solaris Internals and Performance 2nd Edition",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/08/solaris-internals-and-performance-2nd.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\">Richard and Jim have finally finished their updated book and got it published. Rush out and buy a copy! I just <a href=\"http://blogs.sun.com/roller/page/rmc?entry=dtrace_mdb_solaris_internals_podcast\">listened to a podcast</a> where they talked about it and mentioned that you can get it for 30% off from<a href=\"http://www.sun.com/books\"> http://www.sun.com/books.</a>  Strangely, my own Sun Performance and Tuning book isn't listed there, although my BluePrint books on Capacity Planning and Resource Management are.<br /><p>I was also amused to see that i<a href=\"http://blogs.sun.com/roller/page/rmc?entry=fun_at_the_opensolaris_users\">n the slide deck</a> they use to launch the book they reference <a href=\"http://perfcap.blogspot.com/2006/03/how-to-finish-writing-book-by-adrian.html\">Adrian's Rule of book writing</a> (book size grows faster than you can write).<br /></p>  <p>  </p><p>Congratulations!</p>  <p>Now do I have time to start another book? I'm not sure... maybe.<br /></p>  <p style=\"font-size: 10px; text-align: right;\">technorati tags:<a href=\"http://technorati.com/tag/Solaris\" rel=\"tag\">Solaris</a>, <a href=\"http://technorati.com/tag/performance\" rel=\"tag\">performance</a></p><p style=\"text-align: right; font-size: 8px;\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 138,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Richard and Jim have finally finished their updated book and got it published. Strangely, my own Sun Performance and Tuning book isn't listed there, although my BluePrint books on Capacity Planning and Resource Management are. 'Solaris' and 'Flock' are Technorati tags. They are used to tag performance and performance-related content. 'Solaris', 'Flocks' are used for the performance tag, respectively."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_52783af4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Power Usage Logging with the WELserver",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/09/power-usage-logging-with-welserver.html",
      "content": {
        "text": "Reading some forums about ground source heat pump installations, we noticed references to a logging system called the Web Energy Logger that collects data in your house and provides you with a detailed and customized view of how the systems in your house are operating. I bought a <a href=\"http://welserver.com/\">WELserver</a> kit and have started installing some of the data collectors.<br /><br /><a href=\"http://welserver.com/\">Here's the summary of what it does</a>:<br /><br /><blockquote>Simply put, the Web Energy Logger (or WEL) is the most economical way to monitor lots of temperature sensors, and a host of other energy related devices in your home or office. It's ideal to verify the performance or a solar thermal system, or to calculate the efficiency of a geothermal installation, or even to prove that your home really is Net Zero.  Not only does the WEL give you a window into your energy environment, but it also provides a live snapshot and a range of trend charts that you can share on the WEB in real-time.  Finally, the WEL records all your sensor data in monthly log files that are easy to download and import into programs like Excel.      It's compact, versatile, and extremely configurable.<br /><br />The minimal WEL in a Box is ideal for a DIY'er, and costs $375.  In contrast, the more complete WEL Starter Kit is ready to run out-of-the-box with 10 temperature sensors and costs $525.  All WEL's come with full monitoring software on the board, and a dedicated WEL logging site included in the price.</blockquote><br /><br />The monitors are all strung together in a daisy-chain configuration on a single twisted pair wire using the Dallas 1-Wire signaling technology. I got one of the optional current sensors and installed it in the electric hot water tank. Here is the sensor, one of the power wires needs to be threaded through the hole so it can detect current flow.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYP_6eIC8uyAqKmvKTgGeev5SN7-dcUCLXxSHLbbHY6vBrHGKYnAAy9ZrTGBSWHN38wwOzf3oApSpns8yPvzbsTT16OpqcnQNQyGCiSlz7UE7LX6KBPN7Jtixtp57jjFMOQnWy/s1600-h/IMG_0274.JPG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYP_6eIC8uyAqKmvKTgGeev5SN7-dcUCLXxSHLbbHY6vBrHGKYnAAy9ZrTGBSWHN38wwOzf3oApSpns8yPvzbsTT16OpqcnQNQyGCiSlz7UE7LX6KBPN7Jtixtp57jjFMOQnWy/s320/IMG_0274.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5386170524649077682\" /></a><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWzUYCCdmDAQ-UlFVngsMVMdDIOnk-gF1rwDc55xZDjN-I60twgaB8K1DtqtOJJrvVsofwaxsgEKF9dfUMVhkv6-5-ETrYbpeCmSTOuQWFWRbgYEBUBZ46_B3ma7pnM-I4zKTX/s1600-h/IMG_0273.JPG\"><img style=\"cursor:pointer; cursor:hand;width: 240px; height: 320px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhWzUYCCdmDAQ-UlFVngsMVMdDIOnk-gF1rwDc55xZDjN-I60twgaB8K1DtqtOJJrvVsofwaxsgEKF9dfUMVhkv6-5-ETrYbpeCmSTOuQWFWRbgYEBUBZ46_B3ma7pnM-I4zKTX/s320/IMG_0273.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5386170517308446386\" /></a><br /><br />Here is the water tank. The orange wire had been installed without its protective shielding, so when that was fitted, we took the opportunity to add the sensor.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7tweJc3DctYw6XHA7Yh39Z4Xhhxhn0mzh2I0mbPnwUKK2EYl5x7hq0HQs6mJJYWHGP5gGvfZNy42LzGEi7lXOr71Q_UGHsL8L33VjpDBWhvZvOXP_8xRroHXraf0FgMmHx-Z2/s1600-h/IMG_0277.JPG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi7tweJc3DctYw6XHA7Yh39Z4Xhhxhn0mzh2I0mbPnwUKK2EYl5x7hq0HQs6mJJYWHGP5gGvfZNy42LzGEi7lXOr71Q_UGHsL8L33VjpDBWhvZvOXP_8xRroHXraf0FgMmHx-Z2/s320/IMG_0277.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5386170535916904866\" /></a><br /><br />The recess in the top of the water tank has enough space to hold the sensor, with the black wire threaded through the hole.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqf-iJN0pcyQsY7h9JGTeD-5l_p4Xufzf8xAEfL7bOGKUDJgkVUjgFHlR2bSqmfAt6hlsuNDjRba0GBChqW3-Vo4D3C4CjQ5gbDO4kUZV7DqIGIRaWJyhBhXY5m02P4TasxqJI/s1600-h/IMG_0278.JPG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqf-iJN0pcyQsY7h9JGTeD-5l_p4Xufzf8xAEfL7bOGKUDJgkVUjgFHlR2bSqmfAt6hlsuNDjRba0GBChqW3-Vo4D3C4CjQ5gbDO4kUZV7DqIGIRaWJyhBhXY5m02P4TasxqJI/s320/IMG_0278.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5386170542513278802\" /></a><br /><br />The cover is in two parts, which gives easy access to the wiring again if needed.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_SWzBek_9UOj4WhD_foP-_sjORCZscMq7BCim20TzJVTBk85X1UhSdWiqfT_xL5UyDG2_3NxqKI2k-Jxu6vBZ18eMfImfQ9w7S9v1YC3jBBPIMODkT6v0kg7aM9H-lRKBEhnK/s1600-h/IMG_0280.JPG\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj_SWzBek_9UOj4WhD_foP-_sjORCZscMq7BCim20TzJVTBk85X1UhSdWiqfT_xL5UyDG2_3NxqKI2k-Jxu6vBZ18eMfImfQ9w7S9v1YC3jBBPIMODkT6v0kg7aM9H-lRKBEhnK/s320/IMG_0280.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5386170551723029090\" /></a><br /><br />At this point, I don't have the WEL configured. I need to add temperature sensors to the water pipes, and work out how to monitor the heat pump once we get it installed in place of the propane furnace. The WEL control box has an ethernet connection, and I'm not sure whether to run ethernet into the basement, or run the 1-wire cable around the house to monitor all the rooms. Spare wires in the existing telephone cabling can be used for this purpose.",
        "metadata": {
          "word_count": 546,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Web Energy Logger is the most economical way to monitor lots of temperature sensors, and a host of other energy related devices in your home or office. It's ideal to verify the performance or a solar thermal system, or to calculate the efficiency of a geothermal installation. The minimal WEL in a Box is ideal for a DIY'er, and costs $375. The more complete WEL Starter Kit is ready to run out-of-the-box with 10 temperature sensors. All WEL's come with full monitoring software on the board, and a dedicated WEL logging site. <a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/ img/b/R29vZ2xl/AVvXsEiYP_6eIC8uyAqKmvKTgGeev5SN7-dcUCLXxSHLbbHY6vBrHGKYnAAy9ZrTGBSWHN38wwOzf3oApSpns8yPvzbsTT16OpqcnQNQyGCiSlz7UE7LX6KBPN7Jtixtp57jj Here is the water tank. The orange wire had been installed without its protective shielding. When that was fitted, we took the opportunity to add the sensor. The cover is in two parts, which gives easy access to the wiring again if needed. The black wire is threaded through the hole to hold the sensor. At this point, I don't have the WEL configured. I need to add temperature sensors to the water pipes, and work out how to monitor the heat pump. I'm not sure whether to run ethernet into the basement, or run the 1-wire cable around the house to monitor all the rooms. Spare wires in the existing telephone cabling can be used for this purpose."
      },
      "tags": [
        "logging",
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_e38eeddc",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Sequencing exacct logs",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/04/sequencing-exacct-logs.html",
      "content": {
        "text": "I decided to simplify the next step and to just get raw data logged in a useful manner. To do this there needs to be a periodic process that generates readable log files. After thinking about this for a while, I think I have a simple and effective way to do it.<br /><br />I will modify the exdump code to add another option \"-a\". This will invoke acctadm to switch to a new set of log files, with a datestamped name. Before it does the switch, it will need to cause all current processes to write accounting entries, so \"-a\" will normally be used in conjunction with the \"-w\" option I added already. After the log switch, the old log files will be processed from their binary form to a text file with one record per line, ready for consumption by futher processing steps.<br />When \"-a\" is specified, a directory can be specified on the command line to hold output files, the input filename does not need to be specified as it was for the original version. This makes it easy to invoke exdump directly from cron without needing a wrapper script.<br /><br />The sequence is:<br /><pre><br />Obtain existing log files by running acctadm or reading /etc/acctadm.conf<br />Note which logs are being generated and their filenames<br />Run wracct on all existing processes<br />Run acctadm to switch to new log files with date-time-stamped names<br />Process existing log files to text form in the current directory or a <br />   specified directory using same base filenames<br /></pre><br />To start exacct the user needs to manually run acctadm once to enable the logs of interest (flow, task or proc) then exdump will generate the output logs at the cron interval. A find command can be used from cron to remove old logs and keep disk space managed.<br /><br />Since many commands run on the hour, it might be worth starting exdump a few seconds early, and have the file timestamp round up to the next minute so that any command sequences started from cron on the hour are less likely to get split up.<br /><br />This still leaves the problem of processing individual accounting records into useful data, but at least the one line per record text form with interval data will be more tractable than the binary file.",
        "metadata": {
          "word_count": 386,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I decided to simplify the next step and to just get raw data logged in a useful manner. There needs to be a periodic process that generates readable log files. After thinking about this for a while, I think I have a simple and effective way to do it. I will modify the exdump code to add another option \"-a\". This will invoke acctadm to switch to a new set of log files, with a datestamped name. To start exacct the user needs to manually run acctadm once to enable the logs of interest. Then exdump will generate the output logs at the cron interval. A find command can be used from cron to remove old logs and keep disk space managed. At least the one line per record text form with interval data will be more tractable than the binary file. cessing individual accounting records into useful data."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b1a4caa7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "More solar power",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/more-solar-power.html",
      "content": {
        "text": "We just signed up for almost 8KW on the garage roof. It's justified based on replacing our propane furnace with a heat pump that also gives us aIr conditioning, and also running an electric car (e.g. Nissan Leaf). We can use our existing monitoring system, but unlike last time, when we bought the system, we plan to lease it from Solar City this time. There Is a four month lead time, so we can pick the exact terms of the lease next February before it is installed. There are several options,  zero down with a monthly charge that increases slightly each year, or options to pay various amounts up front with a fixed monthly payment. The monthly payments are all less than a typical electricity bill.<br /><br />The goal for next year is to only use propane to run the backup generator (usually for a few days a year), and to make a dent in our petrol usage for the daily commute. We are on the Nissan Leaf list, but not in the first wave of owners. We should be able to get a test drive soon, I will report on any progress as it happens.<br /><br />If you are interested in electric cars, look for Robert Llewellyn's Fully Charged video show on YouTube. He has been test driving everything and he's good fun to watch. You may recognize him as the actor who played Kryton in Red Dwarf.",
        "metadata": {
          "word_count": 239,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "We just signed up for almost 8KW on the garage roof. The goal for next year is to only use propane to run the backup generator. We are on the Nissan Leaf list, but not in the first wave of owners. If you are interested in electric cars, look for Robert Llewellyn's Fully Charged video show on YouTube. You may recognize him as the actor who played Kryton in Red Dwarf."
      },
      "tags": [
        "monitoring",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_816acd3a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What I did for earth day...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/04/what-i-did-for-earth-day.html",
      "content": {
        "text": "Update: the electric mower works well, cuts better than my old gas mower, and the drive option works very well. Since the batteries make it heavier than a gas mower, the power drive makes a big difference. Hacking through very long grass, the batteries last about half an hour, keeping previously cut grass trimmed short I think it would last a lot longer. Its enough for me]<br /><br />Actually, what really happened was that I tried to mow the grass last weekend and decided that my old gas lawnmower wasn't up to the job, so I did some research and bought an electric one. Its quiet, doesn't need gas, the throttle and air intake don't need cleaning out and just starts when I want it to.<br /><br />I ordered it from Amazon.com on Sunday evening, as usual, with the free 5-7 day shipping option, and it arrived on Tuesday. I guess Amazon had nothing better to do than ship stuff... but it was nice to save $67 for shipping a 100 lb package and still get it very fast.<br /><br />I got the latest biggest electric I could find, the <a href=\"http://www.amazon.com/21-Inch-Cordless-Propelled-Discharge-EP21H/dp/B001PKTRTO/\">Epic EP21H 24Volt cordless self propelled 21 inch</a>. The rear wheels are belt driven from the main motor via a gearbox. It has a high efficiency brushless motor that drives the blade directly and two 12V lead acid batteries and should run for 40-50 minutes per charge. This is plenty of time for me to get fed up with mowing and charge it overnight. Its a bit bigger and more rugged than my old 4HP 20inch gas mower, and has a side discharge option so won't get clogged trying to mulch tall grass. I'll try it out after work.<br /><br />I've also been looking forward to getting solar power installed on my roof, and I'm finalizing plans to make a commitment to get it done this year. I will blog my way through the process as it happens...",
        "metadata": {
          "word_count": 329,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The electric mower cuts better than my old gas mower, and the drive option works very well. Since the batteries make it heavier than a gas mowers, the power drive makes a big difference. Hacking through very long grass, the batteries last about half an hour. Epic EP21H 24Volt cordless self propelled 21 inch. The rear wheels are belt driven from the main motor via a gearbox. It has a high efficiency brushless motor that drives the blade directly."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ba77f44e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solid State Disks - 1 TB in a 2.5 inch announced",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/solid-state-disks-1-tb-in-25-inch.html",
      "content": {
        "text": "Like I said... the next thing for SSD's is to have higher storage capacity in each of the form factors than spinning rust. Here's another step in that direction. <a href=\"http://www.engadget.com/2009/01/10/puresilicon-introduces-worlds-first-1tb-2-5-inch-ssd/\">http://www.engadget.com/2009/01/10/puresilicon-introduces-worlds-first-1tb-2-5-inch-ssd/</a>. SSD's already have disks beaten on all the performance metrics. This one maxes out the SATA interface and does 300MBytes/s transfer rates.<br /><br />I've also been seeing prices dropping as volumes increase for laptops with SSDs. I think the end of the line for rust will be 2010, as SSD's will no longer have a worse cost/GB and they already have higher capacity and performance.",
        "metadata": {
          "word_count": 96,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The next thing for SSD's is to have higher storage capacity in each of the form factors. I think the end of the line for rust will be 2010, as SSD's will no longer have a worse cost/GB."
      },
      "tags": [
        "metrics",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_e0b75e82",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - \"City\" Inspection",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/08/solar-power-city-inspection.html",
      "content": {
        "text": "We passed last week (we live outside the city, so it was actually Santa Clara County), and how have another 12 days for PG&E to put in a new time-of-use power meter and bless the installation so we can turn it on.<br /><br />It took a bit of prodding for SolarCity to get the inspections setup, at least that's the way it seemed. After the install, they sent me an email saying please pay the invoice, but they hadn't sent me an invoice. I waited a few days then asked them if they were going to send me one, then they sent it. After they get paid they are supposed to schedule the inspection. They say it can take a month or so to complete the inspections. After a week or two I emailed and asked when the inspection might happen, and a few days later it was scheduled for the following week. They sent someone to wait at my house for the inspector, so we didn't need to be there, other than making sure they had access to the basement where the inverter is installed.<br /><br />The ground source heat pump is taking a bit longer than hoped. The supplies have been hard to source on the West Coast, it seems that there are relatively few GSHP installations in California so far. I'm hoping to get the detailed planning approval and install done in the coming week.",
        "metadata": {
          "word_count": 238,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "After the install, SolarCity sent me an email saying please pay the invoice. After they get paid they are supposed to schedule the inspection. After a week or two I emailed and asked when the inspection might happen. A few days later it was scheduled for the following week. The supplies have been hard to source on the West Coast, it seems that there are relatively few GSHP installations in California so far. I'm hoping to get the detailed planning approval and install done in the coming week."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f70bf68c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Aftermath is finally released...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/aftermath-is-finally-released.html",
      "content": {
        "text": "After many years of listening to this song evolve from an initial concept to its final released form, its now time to share it with the world and find out what everyone else thinks of it. Its a moving reaction to the events and aftermath of 911, sounds like nothing else, and has an odd but catchy hook.<br /><br />Its available on Fractal's site at <a href=http://fractal.bebo.com>fractal.bebo.com</a> and via the <a href=\"http://www.ijigg.com/songs/V2CFF4DPD\">iJigg widget below</a>.<br /><br /><object width=\"315\" height=\"80\"><param name=\"movie\" value=\"http://www.ijigg.com/jiggPlayer.swf?songID=V2CFF4DPD&Autoplay=0\"><param name=\"scale\" value=\"noscale\" /><param name=\"wmode\" value=\"transparent\"><embed src=\"http://www.ijigg.com/jiggPlayer.swf?Autoplay=0&songID=V2CFF4DPD\" width=\"315\" height=\"80\"  scale=\"noscale\" wmode=\"transparent\"></embed></object>",
        "metadata": {
          "word_count": 90,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Its a moving reaction to the events and aftermath of 911, sounds like nothing else, and has an odd but catchy hook. Its available on Fractal's site at <a href=http://fractal.bebo.com/songs/V2CFF4DPD\">iJigg widget below."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7c957c7a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Maslow's Hierarchy of NoSQL Reads (and Writes)",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/03/maslows-hierarchy-of-nosql-reads-and.html",
      "content": {
        "text": "I tried out Prezi to create this presentation, it was more fun to create than powerpoint but has a few limitations. I would like better drawing tools. The talk itself puts some of the main NoSQL solutions into context.<div><br /></div><div><br /><div class=\"prezi-player\"><style type=\"text/css\" media=\"screen\">.prezi-player { width: 550px; } .prezi-player-links { text-align: center; }</style><object id=\"prezi_veagqhsz38u8\" name=\"prezi_veagqhsz38u8\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" width=\"550\" height=\"400\"><param name=\"movie\" value=\"http://prezi.com/bin/preziloader.swf\"><param name=\"allowfullscreen\" value=\"true\"><param name=\"allowscriptaccess\" value=\"always\"><param name=\"bgcolor\" value=\"#ffffff\"><param name=\"flashvars\" value=\"prezi_id=veagqhsz38u8&amp;lock_to_path=0&amp;color=ffffff&amp;autoplay=no&amp;autohide_ctrls=0\"><embed id=\"preziEmbed_veagqhsz38u8\" name=\"preziEmbed_veagqhsz38u8\" src=\"http://prezi.com/bin/preziloader.swf\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\" allowscriptaccess=\"always\" width=\"550\" height=\"400\" bgcolor=\"#ffffff\" flashvars=\"prezi_id=veagqhsz38u8&amp;lock_to_path=0&amp;color=ffffff&amp;autoplay=no&amp;autohide_ctrls=0\"></embed></object><div class=\"prezi-player-links\"><p><a title=\"There are too many ways to read and write data, and polyglot persistence can get confusing, here is a guide to help...\" href=\"http://prezi.com/veagqhsz38u8/nosql-maslows-hierarchy-of-reads-and-writes/\">NoSQL - Maslow's Hierarchy of Reads (and Writes)</a> on <a href=\"http://prezi.com/\">Prezi</a></p></div></div><br /></div>",
        "metadata": {
          "word_count": 113,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I tried out Prezi to create this presentation, it was more fun to create than powerpoint but has a few limitations. I would like better drawing tools. The talk itself puts some of the main NoSQL solutions into context. There are too many ways to read and write data, and polyglot persistence can get confusing, here is a guide to help. NoSQL - Maslow's Hierarchy of Reads (and Writes) on Prezi."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_eb2aa883",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone SDK, Leopard and 1.1.2",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/10/iphone-sdk-leopard-and-112.html",
      "content": {
        "text": "<div><br /></div><div>As I suspected, Apple plans to release an SDK for native apps on the iPhone next year. The official word is on <a href=\"http://www.apple.com/hotnews/\">Apple's Hot News site</a>. They appear to have pre-announced the SDK which is likely to have its official announcement at MacWorld in January, and they say it will ship in February. I'm sure that this was the plan all along, and the developer frenzy has managed to flush out a pre-announcement to give some guidance to the market, which I think is a good move at this point.</div><div><br /></div><div>There has also been speculation that the iPhone \"runs Leopard\". I think that the truth is likely to be a bit more subtle than that. There has to be a master code base for MacOSX, and efficient engineering management practices should try to minimize branches in that code base. The master code base has a release branch called \"Leopard\" and another one called \"iPhone\", but the differences between them should be as small as possible. Bug fixes and features get checked back into the master code base whenever possible. So the work put into Leopard is leveraged for the iPhone, and we should see some of the feature set that was announced for Leopard transfer to the iPhone.</div><div><br /></div><div>I think there is a good chance that Apple will release iPhone 1.1.2 immediately after they release Leopard, and that some Leopard features will be included. One obvious feature would be localized versions of Mail and other apps for the iPhone and iPod Touch, bringing the iPod Touch more in line with the iPhone's application set. There is a new and upgraded iChat in Leopard, and it would also make sense to release this for the iPhone/Touch.</div><div><br /></div><div>I also noticed that the DTrace analysis/monitoring framework from Solaris 10 <a href=\"http://www.apple.com/macosx/features/300.html\">is included in Leopard</a>. This is very cool, it provides a huge upgrade in observability for performance analysis of both desktop and server applications.</div>",
        "metadata": {
          "word_count": 323,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Apple plans to release an SDK for native apps on the iPhone next year. The official word is on Apple's Hot News site. It is likely to have its official announcement at MacWorld in January. Some Leopard features will be included. localized versions of Mail and other apps for the iPhone and iPod Touch. DTrace analysis/monitoring framework from Solaris 10 is included in Leopard."
      },
      "tags": [
        "engineering",
        "observability",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_5de24954",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Data from process accounting",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/data-from-process-accounting.html",
      "content": {
        "text": "The process accounting record is far more detailed than the standard sysV acct record used by most Unix based systems. For a start it includes the pid of the process and the pid of the parent process so you can stitch the records together properly. The Solaris project and task id's let you manage and control workloads effectively, and since microstate accounting is on by default in Solaris 10, the CPU usage numbers are accurate and high resolution. By default everything is in the global zone. Zones are the virtual machine containers used for fault isolation and resource management in Solaris 10, so data needs to be separated by zone as well as workload.<br /><br /><pre><br />    ff  group-header                    [group of 4 object(s)]<br />     1   version                        1<br />     2   filetype                       \"exacct\"<br />     3   creator                        \"SunOS\"<br />     4   hostname                       \"crun\"<br />   100  group-proc                      [group of 34 object(s)]<br />  1000   pid                            1748<br />  1001   uid                            0              root<br />  1002   gid                            0              root<br />  1004   projid                         1              user.root<br />  1003   taskid                         56<br />  100b   cpu-user-sec                   0<br />  100c   cpu-user-nsec                  417258<br />  100d   cpu-sys-sec                    0<br />  100e   cpu-sys-nsec                   989267<br />  1007   start-sec                      1109813617     03/02/05 17:33:37<br />  1008   start-nsec                     139893535<br />  1009   finish-sec                     1109813617     03/02/05 17:33:37<br />  100a   finish-nsec                    152386048<br />  1006   command                        \"acctadm\"<br />  100f   tty-major                      24<br />  1010   tty-minor                      4<br />  1011   faults-major                   0<br />  1012   faults-minor                   0<br />  1014   msgs-snd                       0<br />  1013   msgs-recv                      0<br />  1015   blocks-in                      0<br />  1016   blocks-out                     5<br />  1017   chars-rdwr                     481<br />  1018   ctxt-vol                       2<br />  1019   ctxt-inv                       0<br />  101a   signals                        0<br />  101b   swaps                          0<br />  101c   syscalls                       94<br />  101d   acctflags                      2              SU<br />  101f   ppid                           1568<br />  1020   wait-status                    0              exit<br />  1021   zone                           \"global\"<br />  1022   memory-rss-avg-k               996<br />  1023   memory-rss-max-k               29636<br /></pre>",
        "metadata": {
          "word_count": 292,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Solaris project and task id's let you manage and control workloads effectively. Microstate accounting is on by default in Solaris 10. By default everything is in the global zone. \"Pid\" is used to track a computer's status. \"Projid\" represents a project. \"Taskid\" refers to a task. \"Gid\" means a user. 1013   msgs-recv  \u00a0    1020   wait-status   \u00a0 1023   memory-rss-max-k\u00a0\u00a0\u00a0 29636\u00a0\u00a0 <br />\u00a0 1021   zone   \"global\"\u00a0\u00a0 1022   memories-avg-k   \u201c   996\u201d\u00a0\u00a0  1023\u00a0 memory- RSSK-max\u00a0\u00a0 \u201c\u00a0  29636\u201c\u00a0 1023\u201d \u201cmemory-rss\u2019savg\u2019-k\u2019\u00a0\u00a0 996 \u201c\u2013\u201d 1023 \u201cMemory- RSSS\u2019Saves\u2019\u2019"
      },
      "tags": [
        "containers"
      ]
    },
    {
      "id": "virtual_adrianco_file_bdc17e29",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Help! I've lost my memory! Updated Sunworld Column",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/10/help-ive-lost-my-memory-updated.html",
      "content": {
        "text": "<p><span style=\"font-size:78%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">Originally published in Unix Insider 10/1/95 <span style=\"color:#dc2300;\"><br />Stripped of adverts, url references fixed and comments added in red type to bring it up to date ten years later.</span></span></span></p><p><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">Dear Adrian, After a reboot I saw that most of my computer's memory was<br />free, but when I launched my application it used up almost all the<br />memory. When I stopped the application the memory didn't come back!<br />Take a look at my <code><span style=\"font-family:courier;\">vmstat</span></code><em> </em></span></span>output:<span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\"> </span></span><br /></p><pre><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">% vmstat 5</span></span><br /><span style=\"font-family:courier;\">procs     memory            page            disk          faults      cpu</span><br /><span style=\"font-family:courier;\">r b w   swap  free  re  mf pi po fr de sr s0 s1 s2 s3   in   sy   cs us sy id</span><br /></pre><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><em>This is before the program starts:</em></span></span></p><pre><span style=\"font-family:courier;\">0 0 0 330252 80708   0   2  0  0  0  0  0  0  0  0  1   18  107  113  0  1 99</span><br /><span style=\"font-family:courier;\">0 0 0 330252 80708   0   0  0  0  0  0  0  0  0  0  0   14   87   78  0  0 99</span><br /></pre><p><em><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">I start the program and it runs like this for a while:</span></span></em></p><pre><span style=\"font-family:courier;\">0 0 0 314204  8824   0   0  0  0  0  0  0  0  0  0  0  414  132   79 24  1 74</span><br /><span style=\"font-family:courier;\">0 0 0 314204  8824   0   0  0  0  0  0  0  0  0  0  0  411   99   66 25  1 74</span><br /></pre><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><em>I stop it, then almost all the swap space comes back, but the free memory does not:</em></span></span></p><pre><span style=\"font-family:courier;\">0 0 0 326776 21260   0   3  0  0  0  0  0  0  1  0  0  420  116   82  4  2 95</span><br /><span style=\"font-family:courier;\">0 0 0 329924 24396   0   0  0  0  0  0  0  0  0  0  0  414   82   77  0  0 100</span><br /><span style=\"font-family:courier;\">0 0 0 329924 24396   0   0  0  0  0  0  0  0  2  0  1  430   90   84  0  1 99</span><br /></pre><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><em>I checked that there were no application processes running. It looks like a huge memory leak in the operating system. How can I get my memory back?<br />--RAMless in Ripon</em> </span></span><br /></p><p><strong><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"color:#dc2300;\">Update</span></span></span></strong></p><p><strong style=\"font-weight: normal;\"><span style=\"\"><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"color:#dc2300;\">This remains one of the most frequently asked questions of all time. The<br />original answer is still true for many Unix variants. However while<br />writing his book on Solaris Internals, Richard McDougall worked out<br />how to fix Solaris to make it work better, and to make this aparrent<br />problem go away. The result was one of the most significant<br />performance improvements in Solaris 8, but the first edition of his<br />book was written before Solaris 8 came out, so doesn't describe the<br />fix! </span></span></span></span></strong><br /></p><p><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\"><strong>The short answer</strong><br /><br />Launch your application again. Notice that it starts up more quickly than it did the first time, and with less disk activity. The application code and its data files are still<br />in memory, even though they are not active. The memory they occupy is<br />not \"free.\" If you restart the same application it finds<br />the pages that are already in memory. The pages are attached to the<br />inode cache entries for the files. If you start a different<br />application, and there is insufficient free memory, the kernel will<br />scan for pages that have not been touched for a long time, and \"free\"<br />them. Once you quit the first application, the memory it occupies is<br />not being touched, so it will be freed quickly for use by other<br />applications. </span></span><br /></p><p><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">In 1988, Sun introduced this feature in SunOS 4.0. It still applies to<br />all versions of Solaris 1 and 2. The kernel is trying to avoid disk<br />reads by caching as many files as possible in memory. Attaching to a<br />page in memory is around 1,000 times faster than reading it in from<br />disk. The kernel figures that you paid good money for all of that<br />RAM, so it will try to make good use of it by retaining files you<br />might need. </span></span><br /></p><p><span style=\"color:#dc2300;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">Since Solaris 8, the memory in the file cache is actually also on the free<br />list, so you do see vmstat free memory reduce when you quit a<br />program. You also should expect large amounts of file I/O to cause<br />high scan rates on older Solaris releases, and for there to be no<br />scanning at all on Solaris 8 systems. If Solaris 8 scans at all, then<br />it has truly run out of memory and is overloaded. </span></span></span><br /></p><p><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">By contrast, Memory leaks appear as a shortage of swap space after the<br />misbehaving program runs for a while. You will probably find a<br />process that has a larger than expected size. You should restart the<br />program to free up the swap space, and check it with a debugger that<br />offers a leak-finding feature (<span style=\"color:#dc2300;\">run it with the libumem version of the malloc library that instruments memory leaks</span>).</span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><strong>The long (and technical) answer</strong><br /><br />To understand how Sun's operating systems handle memory, I will explain how the inode cache works, how the buffer cache fits into the picture, and how the life<br />cycle of a typical page evolves as the system uses it for several<br />different purposes. </span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><strong>The inode cache and file data caching</strong><br /><br />Whenever you access a file, the kernel needs to know the size, the access permissions,<br />the date stamps and the locations of the data blocks on disk.<br />Traditionally, this information is known as the inode for the file.<br />There are many filesystem types. For simplicity I will assume we are<br />only interested in the Unix filesystem (UFS) on a local disk. Each<br />filesystem type has its own inode cache. </span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">The filesystem stores inodes on the disk; the inode must be read into<br />memory whenever an operation is performed on an entity in the<br />filesystem. The number of inodes read per second is reported as<br /><code><span style=\"font-family:courier;\">iget/s</span></code> by the <code><span style=\"font-family:courier;\">sar<br />-a</span></code> command. The inode read from disk is cached in case<br />it is needed again, and the number of inodes that the system will<br />cache is influenced by a kernel parameter called <em>ufs_ninode</em>.<br />The kernel keeps inodes on a linked list, rather than in a fixed-size<br />table. </span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">As I mention each command I will show you what the output looks like. In<br />my case I'm collecting <code><span style=\"font-family:courier;\">sar</span></code><br />data automatically using <code><span style=\"font-family:courier;\">cron</span></code>.<br /><code><span style=\"font-family:courier;\">sar</span></code>, which defaults to<br />reading the stored data for today. If you have no stored data,<br />specify a time interval and <code><span style=\"font-family:courier;\">sar</span></code><br />will show you current activity. </span></span><br /></p><pre><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">% sar -a</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">SunOS hostname 5.4 Generic_101945-32 sun4c    09/18/95</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">00:00:01  iget/s namei/s dirbk/s</span></span><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">01:00:01       4       6       0</span></span><br /></pre><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">All reads or writes to UFS files occur by paging from the filesystem. All<br />pages that are part of the file and are in memory will be attached to<br />the inode cache entry for that file. When a file is not in use, its<br />data is cached in memory, using an inactive inode cache entry. When<br />the kernel reuses an inactive inode cache entry that has pages<br />attached, it puts the pages on the free list; this case is shown by<br /><code><span style=\"font-family:courier;\">sar -g</span></code> as <code><span style=\"font-family:courier;\">%ufs_ipf</span></code>.<br />This number is the percentage of UFS inodes that were overwritten in<br />the inode cache by <code><span style=\"font-family:courier;\">iget</span></code> and<br />that had reusable pages associated with them. The kernel flushes the<br />pages, and updates on disk any modified pages. Thus, this %ufs_ipf<br />number is the percentage of igets with page flushes. Any non-zero<br />values of %ufs_ipf reported by <code><span style=\"font-family:courier;\">sar -g</span></code><br />indicate that the inode cache is too small for the current workload. </span></span><br /></p><pre><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">% sar -g</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">SunOS hostname 5.4 Generic_101945-32 sun4c    09/18/95</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">00:00:01  pgout/s ppgout/s pgfree/s pgscan/s %ufs_ipf</span></span><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">01:00:01     0.02     0.02     0.08     0.12     0.00</span></span><br /></pre><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">For SunOS 4 and releases up to Solaris 2.3, the number of inodes that the<br />kernel will keep in the inode cache is set by the kernel variable<br /><em>ufs_ninode</em>. To simplify: When a file is opened, an inactive<br />inode will be reused from the cache if the cache is full; when an<br />inode becomes inactive, it is discarded if the cache is over-full. If<br />the cache limit has not been reached then an inactive inode is placed<br />at the back of the reuse list and invalid inodes (inodes for files<br />that longer exist) are placed at the front for immediate reuse. It is<br />entirely possible for the number of open files in the system to cause<br />the number of active inodes to exceed <em>ufs_ninode</em>; raising<br /><em>ufs_ninode</em> allows more inactive inodes to be cached in case<br />they are needed again. </span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">Solaris 2.4 uses a more clever inode cache algorithm. The kernel maintains a<br />reuse list of blank inodes for instant use. The number of active<br />inodes is no longer constrained, and the number of idle inodes<br />(inactive but cached in case they are needed again) is kept between<br /><em>ufs_ninode</em> and 75 percent of <em>ufs_ninode</em> by a new<br />kernel thread that scavenges the inodes to free them and maintains<br />entries on the reuse list. If you use <code><span style=\"font-family:courier;\">sar<br />-v</span></code> to look at the inode cache, you may see a larger<br />number of existing inodes than the reported \"size.\" </span></span><br /></p><pre><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">% sar -v</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">SunOS hostname 5.4 Generic_101945-32 sun4c    09/18/95</span></span><br /><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">00:00:01  proc-sz    ov  inod-sz    ov  file-sz    ov   lock-sz</span></span><br /><span style=\"font-family:courier;\"><span style=\"font-size:85%;\">01:00:01   66/506     0 2108/2108    0  353/353     0    0/0   </span></span><br /></pre><p><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\"><strong>Buffer cache</strong><br /><br />The buffer cache is used to cache filesystem<br />data in SVR3 and BSD Unix. In SunOS 4, generic SVR4, and Solaris 2,<br />it is used to cache inode, indirect block, and cylinder group blocks<br />only. Although this change was introduced in 1988, many people still<br />incorrectly think the buffer cache is used to hold file data. Inodes<br />are read from disk to the buffer cache in 8-kilobyte blocks, then the<br />individual inodes are read from the buffer cache into the inode<br />cache. </span></span><br /></p><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><strong>Life cycle of a typical physical memory page</strong><br /><br />This section provides additional insight into the way memory is used. The sequence<br />described is an example of some common uses of pages; many other<br />possibilities exist. </span></span><br /> </p><dl><dt style=\"margin-bottom: 0.2in;\"><a name=\"Step 1\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">1. Initialization -- A page is born </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">When the system boots, it forms all free memory into pages, and allocates<br />a kernel data structure to hold the state of every page in the<br />system. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 2\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">2. Free -- An untouched virgin page </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">All the memory is put onto the free list to start with. At this stage the<br />content of the page is undefined. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 3\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">3. ZFOD -- Joining an uninitialized data segment </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">When a program accesses data that is preset to zero for the very first<br />time, a minor page fault occurs and a Zero Fill On Demand (ZFOD)<br />operation takes place. The page is taken from the free list,<br />block-cleared to contain all zeroes, and added to the list of<br />anonymous pages for the uninitialized data segment. The program then<br />reads and writes data to the page. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 4\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">4. Scanned -- The pagedaemon awakes </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">When the free list gets below a certain size, the pagedaemon starts to<br />look for memory pages to steal from processes. It looks at all pages<br />in physical memory order; when it gets to the page, the page is<br />synchronized with the memory management unit (MMU) and a reference<br />bit is cleared. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 5\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">5. Waiting -- Is the program really using this page right now? </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">There is a delay that varies depending upon how quickly the pagedaemon<br />scans through memory. If the program references the page during this<br />period, the MMU reference bit is set. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 6\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">6. Pageout Time -- Saving the contents </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">The pageout daemon returns and checks the MMU reference bit to find that<br />the program has not used the page so it can be stolen for reuse. The<br />pagedaemon checks to see if anything had been written to the page;<br />if it contains no data, a page-out occurs. The page is moved to the<br />pageout queue and marked as I/O pending. The swapfs code clusters<br />the page together with other pages on the queue and writes the<br />cluster to the swap space. The page is then free and is put on the<br />free list again. It remembers that it still contains the program<br />data. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 7\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">7. Reclaim -- Give me back my page! </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">Belatedly, the program tries to read the page and takes a page fault. If the<br />page had been reused by someone else in the meantime, a major fault<br />would occur and the data would be read from the swap space into a<br />new page taken from the free list. In this case, the page is still<br />waiting to be reused, so a minor fault occurs, and the page is moved<br />back from the free list to the program's data segment. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 8\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">8. Program Exit -- Free again </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">The program finishes running and exits. The data segments are private to<br />that particular instance of the program (unlike the shared-code<br />segments), so all the pages in the data segment are marked as<br />undefined and put onto the free list. This is the same state as <a href=\"http://www.blogger.com/post-edit.g?blogID=7434008&postID=113077969542636161#Step%202\">Step<br />2</a>. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 9\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">9. Page-in -- A shared code segment </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">A page fault occurs in the code segment of a window system shared library.<br />The page is taken off the free list, and a read from the filesystem<br />is scheduled to get the code. The process that caused the page fault<br />sleeps until the data arrives. The page is attached to the inode of<br />the file, and the segments reference the inode. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 10\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">10. Attach -- A popular page </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">Another process using the same shared-library page faults in the same place.<br />It discovers that the page is already in memory and attaches to the<br />page, increasing its inode reference count by one. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"><a name=\"Step 11\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">11. COW -- Making a private copy </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">If one of the processes sharing the page tries to write to it, a<br />copy-on-write (COW) page fault occurs. Another page is grabbed from<br />the free list, and a copy of the original is made. This new page<br />becomes part of a privately mapped segment backed by anonymous<br />storage (swap space) so it can be changed, but the original page is<br />unchanged and can still be shared. Shared libraries contain jump<br />tables in the code that are patched, using COW as part of the<br />dynamic linking process. </span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 12\"></a><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">12. File Cache -- Not free </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">The entire window system exits, and both processes go away. This time<br />the page stays in use, attached to the inode of the shared library<br />file. The inode is now inactive but will stay in the inode cache<br />until it is reused, and the pages act as a file cache in case the<br />user is about to restart the window system again. <span style=\"color:#dc2300;\">The<br />change made in Solaris 8 was that the file cache is the tail of the<br />free list, and any file cache page can be reused immediately for<br />something else without needing to be scanned first.</span></span></span><br /></dd><dt style=\"margin-bottom: 0.2in;\"> <a name=\"Step 13\"></a><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">13.<em> fsflush</em> -- Flushed by the sync </span></span><br /></dt><dd style=\"margin-bottom: 0.2in;\"> <span style=\"font-family:verdana, arial, geneva, sans-serif;\"><span style=\"font-size:85%;\">Every 30 seconds all the pages in the system are examined in physical page<br />order to see which ones contain modified data and are attached to a<br />vnode. The details differ between SunOS 4 and Solaris 2, but<br />essentially any modified pages will be written back to the<br />filesystem, and the pages will be marked as clean. </span></span><br /><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\">This example sequence can continue from <a href=\"http://www.blogger.com/post-edit.g?blogID=7434008&postID=113077969542636161#Step%204\">Step 4</a> or<br /><a href=\"http://www.blogger.com/post-edit.g?blogID=7434008&amp;postID=113077969542636161#Step%209\">Step 9</a> with minor variations. The <em>fsflush</em><br />process occurs every 30 seconds by default for all pages, and<br />whenever the free list size drops below a certain value, the<br />pagedaemon scanner wakes up and reclaims some pages. <span style=\"color:#dc2300;\">A<br />recent change in Solaris 10, backported to Solaris 8 and 9 patches,<br />makes fsflush run much more efficiently on machines with very large<br />amounts of memory. However, if you see fsflush using an excessive<br />amount of CPU time you should increase \u201cautoup\u201d in /etc/system<br />from its default of 30s, and you will see fsflush usage reduce<br />proportionately.</span></span></span><br /></dd></dl><p><span style=\"font-size:85%;\"><span style=\"font-family:verdana, arial, geneva, sans-serif;\"><strong>Now you know</strong><br /><br />I have seen this missing-memory question<br />asked about once a month since 1988! Perhaps the manual page for<br /><em>vmstat</em> should include a better explanation of what the<br />values are measuring. This answer is based on some passages from my<br />book <em><a href=\"http://www.amazon.com/gp/product/0130952494/002-4502333-9850410\">Sun Performance and Tuning</a></em>. The book explains in detail how the<br />memory algorithms work and how to tune them. <span style=\"color: rgb(255, 0, 0);\">However the book doesn't cover the changes made in Solaris 8.</span></span></span><br /></p>",
        "metadata": {
          "word_count": 3104,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Originally published in Unix Insider 10/1/95. Stripped of adverts, url references fixed and comments added in red type to bring it up to date. This is before the program starts. The program runs like this for a while:\u00a0\u00a0\u00a0\u201cr b w   swap  free  re  mf pi po fr de sr s0 s1 s2 s3   in   sy  cs us sy id\u201d \" RAMless in Ripon\" is a weekly, free-to-use, web-based guide to all things RAMless. This week, we look at a memory leak in the operating system and how to fix it. Richard McDougall worked out how to fix Solaris to make it work better. The result was one of the most significant improvements in Solaris 8. In 1988, Sun introduced this feature in SunOS 4.0. It still applies to all versions of Solaris 1 and 2. The kernel is trying to avoid disk reads by caching as many files as possible in memory. Since Solaris 8, the memory in the file cache is actually also on the free list. You also should expect large amounts of file I/O to cause high scan rates on older Solaris releases. A process that has a larger than expected size should be restarted to free up the swap space. The inode cache and file data caching is the key to Sun's operating systems. The number of inodes read per second is reported as iget/s. The inode read from disk is cached in case it is needed again. The kernel keeps inodes on a linked disk. As I mention each command I will show you what the output looks like. In my case I'm collecting data automatically using <code><span style=\"font-family:courier;\">cron</span></code>. All reads or writes to UFS files occur by paging from the filesystem. All pages that are part of the file and are in memory will be attached to the inode cache entry for that file. When a file is not in use, its data is cached in memory, using an inactive inode Cache entry. %ufs_ipf is the percentage of igets with page flushes. The kernel flushes the pages, and updates on disk any modified pages. Any non-zero values of this number indicate that the inode cache is too small. Solaris 2.4 uses a more clever inode cache algorithm. When a file is opened, an inactive inode will be reused from the cache if the cache is full. Inodes for files that no longer exist are placed at the front for immediate reuse. The number of active inodes is no longer constrained. idle inodes are kept between 75 percent and 75 percent of the total inodes. A new kernel thread scavenges the inodes to free them and maintains a reuse list. The buffer cache is used to cache filesystem data in SVR3 and BSD Unix. In SunOS 4, generic SVR4, and Solaris 2, it is used only to cache inode, indirect block, and cylinder group blocks. This section provides additional insight into the way memory is used. When the system boots, it forms all free memory into pages, and allocates a kernel data structure to hold the state of every page in the system.described is an example of some common uses of pages; many other possibilities exist. When a program accesses data that is preset to zero for the very first time, a Zero Fill On Demand (ZFOD) operation takes place. The page is taken from the free list, block-cleared to contain all zeroes, and added to the list of anonymous pages. The program then reads and writes data to the page. When the free list gets below a certain size, the pagedaemon starts to look for memory pages to steal from processes. The page ischronized with the memory management unit (MMU) and a reference is cleared. The pageout daemon returns and checks the MMU reference bit to find that the program has not used the page so it can be stolen for reuse. The page is moved to the pageout queue and marked as I/O pending. The program tries to read the page and takes a page fault. If the page had been reused by someone else in the meantime, a major fault would occur and the data would be read from the swap space into a new page taken from the free list. The program finishes running and exits. The data segments are private to that particular instance of the program. All the pages in the data segment are marked asundefined and put onto the free list. sleeps until the data arrives. The page is attached to the inode of the file, and the segments reference that inode. It discovers that the page is already in memory and attaches to the page, increasing its inode reference count. If one of the processes sharing the page tries to write to it, a copy-on-write (COW) page fault occurs. Another page is grabbed from the free list and a copy of the original is made. This new page becomes part of a privately mapped segment backed by anonymous storage. The inode is now inactive but will stay in the inode cache until it is reused. The pages act as a file cache in case the user is about to restart the window system. This example sequence can continue from <a href=\"http://www.blogger.com/post-edit.g?blogID=7434008&amp;postID=113077969542636161#Step%209\">Step 9. Thefsflush process occurs every 30 seconds by default for all pages.  n /etc/system\u00a0from its default of 30s, and you will see fsflush usage reduce. This answer is based on some passages from my book Sun Performance and Tuning."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_7e69b677",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Pesky style sheet format.....",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/04/pesky-style-sheet-format.html",
      "content": {
        "text": "Sorry that some of the lines are wrapped and others run off the edge, I'm still figuring out how to make the composition tools and template do what I want, rather than what they want....",
        "metadata": {
          "word_count": 35,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Some of the lines are wrapped and others run off the edge. I'm still figuring out how to make the composition tools and template do what I want, rather than what they want."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9b7babba",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Part 3: Disruptive Innovation viewed as a Maturity Model | by Adrian Cockcroft | June 6th, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/part-3-disruptive-innovation-viewed-as.html",
      "content": {
        "text": "This time I'll take a more abstract view of a maturing market as each phase evolves, and refer to the development of in-home movie watching as an example.<br /><br /><ol>   <li>An emerging market is characterized by competition on the basis of technology. Early adopters like to play with new technology and are able to cope with its issues. Many different products are competing for market share on the basis of \"my features are better\". Think of the early days of the VCR, with VHS vs. Betamax. In a mature market, few people worry about features, most VCR or DVD players have the same feature set and very good picture quality at a very low price. If you want to be sure you get a good one, you are most likely to buy using brand name (e.g. Sony) rather than poring over detailed specifications. Margins are low, but volume is high and margins can be better if you won the brand battle.<br /></li>   <li>The next phase in the market is characterized by competition on the basis of service. Think of the video rental store as a service. You visit the store and pay rental according to how much you use the service. As an emerging service, anyone could setup to rent videos and DVDs. As the market matured, larger stores with a bigger selection and more centralized buying power provided a better service, and video rental chains such as Blockbuster took over the market. Again, the power of a dominant brand became the primary differentiator as the service market matured.</li>   <li>The third phase in the market is the evolution of a service into a utility. A utility provides a more centralized set of resources, and a regular subscription or monthly bill. It can provide similar services, but in a more automated manner. NetFlix is my example of a utility based DVD provider service. You pay a monthly fee which encourages steady consumption, and NetFlix have automated the recommendation system, which replaces asking the counter clerk in a video rental store for advice. The recommendations are the result of many peoples opinions, so are likely to be less biased and better informed, but the most important difference in the utility approach is that it doesn't need people to provide the service directly to the customer. This makes it fundamentally cheaper. Many traditional services were transformed into utilities by the arrival of the Internet, which allows consumers to access information based utilities in a generic and efficient manner. The network effect benefit of having a large user base also causes dominant brand names to emerge. NetFlix leads mindshare in this space, despite attempts by BlockBuster to copy their business model, NetFlix can grow faster with fewer people as a pure utility.<br /></li>   <li>The final phase in the evolution of a market occurs as the cost of replication and distribution of the product approaches zero. For digital content the end customer already has a computer and an Internet connection. There is no additional cost to use it to download a movie. A central utility such as YouTube can use a mixture of advertising and premium services (for a minority of power users) to offset their own costs. Peer to peer systems distribute the load so that there is no central site and no incremental cost in the system. The only service that is needed is some kind of search, so that peers can find each other's content to exchange it. PirateBay is primarily a search engine, and search engines become dominant when the brand gets well known, and they find what you are looking for because they have a comprehensive index.</li> </ol> So the evolution of a marketplace goes from competing on the basis of technology, to competing on service, to competing as a utility, to competing for free. In each step of the evolution, competitors shake out over time and a dominant brand emerges.<br /><br />To use this as a maturity model, take a market and figure out whether the primary competition is on the basis of technology, service, utility or search, and consider whether a dominant brand has emerged in that phase. The model should then indicate what the next step is likely to be, so you can try to find the right disruptive innovation to get you there. Good luck!",
        "metadata": {
          "word_count": 719,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In a mature market, few people worry about features, most VCR or DVD players have the same feature set and very good picture quality at a very low price. If you want to be sure you get a good one, you are most likely to buy using brand name (e.g. Sony) The third phase in the market is the evolution of a service into a utility. A utility provides a more centralized set of resources, and a regular subscription or monthly bill. NetFlix is my example of a utility based DVD provider service. You pay a monthly fee which encourages steady consumption. Many traditional services were transformed into utilities by the arrival of the Internet. NetFlix leads mindshare in this space, despite attempts by BlockBuster to copy their business model. PirateBay is primarily a search engine, and search engines become dominant when the brand gets well known. Peer to peer systems distribute the load so that there is no central site and no incremental cost in the system. The next step is likely to be to try to find the right disruptive innovation to get you there. Good luck and have a great day."
      },
      "tags": [
        "netflix",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_321bacd6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone 2.0 upgrade and applications - Pandora",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/07/iphone-20-upgrade-and-applications.html",
      "content": {
        "text": "I waited until Saturday evening to attempt an upgrade on my iPhone, and it completed with no problems, so I guess Apple fixed their site and the rush had died down a bit.<br /><br />I looked through some of the applications and loaded Pandora, eBay, Paypal (for old times sake), Weatherbug, Remote (I also upgraded my AppleTV so this will come in useful) and Facebook. Enough to get me started. I also saw a nice star mapping application, I used to have one of these on my Palm/Treo but it wasn't free, so I decided to leave it for later.<br /><br />The Pandora application is my favorite. I am an intermittent user of Pandora, as I didn't like being tethered to my laptop to listen. However the iPhone version works fine even over a fairly poor EDGE signal. There are some pauses between songs and the occasional pause during a song, but overall its a nice way to listen to the channels I had already setup, and discover new music that is similar to my favorites. I suspect that this is going to get a lot of usage since I can listen to it on the go.<br /><br />There are over 500 applications, so plenty of stuff to play with if I get bored.<br /><br />I applied for iPhone developer program membership when it was announced, and got an email yesterday saying that I have been accepted, so went through the purchase process, and downloaded the final version of the iPhone SDK....",
        "metadata": {
          "word_count": 252,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Pandora, eBay, Paypal (for old times sake), Weatherbug, Remote (I also upgraded my AppleTV so this will come in useful) and Facebook. The Pandora application is my favorite. I applied for iPhone developer program membership when it was announced, and got an email yesterday saying that I have been accepted. There are over 500 applications, so plenty of stuff to play with if I get bored."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4d00bfd4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Winter Solstice and New Ducting Update",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/12/solar-power-winter-solstice-and-new.html",
      "content": {
        "text": "I've previously shared the first few months of output from our solar system, we turned it on at the start of September, and on a clear day we were getting around 24KWh. As the days got shorter and the Sun got lower in the sky, the clear day output dropped off, and earlier this week we hit the winter solstice, the shortest day. Wednesday was a clear day, and we generated about 16KWh, as shown in the plot below.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpNQpM9AESrpt6NoMIhJliag2bebCOMxwX0yAlxLxBbP2VgodXoJPtVbwV2IiyD8KjScDMwBrUcowR4WJ6NMQAwLLeH0LH-c9MM_pZss5bd7BVxEETy7p4SuTzqky8vFBhmBqk/s1600-h/winter-solstice.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 157px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgpNQpM9AESrpt6NoMIhJliag2bebCOMxwX0yAlxLxBbP2VgodXoJPtVbwV2IiyD8KjScDMwBrUcowR4WJ6NMQAwLLeH0LH-c9MM_pZss5bd7BVxEETy7p4SuTzqky8vFBhmBqk/s320/winter-solstice.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5418893208625307026\" /></a><br /><br />We have also seen the benefit of our E6-time of use rate plan. We generate electricity during the week while we are mostly out of the house at the highest rate, and use it at the cheapest rate. So we continue to be net consumers of electricity, generating between 50% and 70% of what we use, but our electricity bill is basically zero at this point. In addition, PG&E just gave us a rebate of $67 due to lower energy costs, so the standing charge of $11.50/month is zeroed out for almost six months.<br /><br />We converted our gas appliances to electric, for heating water, drying clothes and cooking, and currently only use gas for the furnace that heats the house. We looked into replacing the furnace with a ground source heat pump (GSHP) but got wildly varying designs and costs from everyone we talked to so have postponed that idea. The one thing everyone agreed on was that our ducting was sub-standard, with insufficient flow for A/C and barely enough for the furnace, and it was poorly constructed using the lowest quality materials.<br /><br />So we have just replaced the ducts throughout the house, changing the configuration to include separate zones for upstairs and downstairs, adding a new return path from upstairs and one extra outlet. The ducts are insulated to R8, rather than R4.2, so they don't lose heat, and they have fewer leaks and a properly designed balance of air flow into each room. This is done using something called a Manual-J calculation, and there is also an independent analysis service that verifies that everything is working correctly. The main effect of this is that we will use less propane, and have much better control over temperature upstairs. We often want to heat the bedrooms without wanting to heat downstairs to the same level, and we now have a thermostat in the main bedroom for upstairs, as well as one downstairs.<br /><br />I'm planning to blog some more about GSHP options, its  quite complicated....",
        "metadata": {
          "word_count": 433,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "On a clear day we were getting around 24KWh. As the days got shorter and the Sun got lower in the sky, the clear day output dropped off. Earlier this week we hit the winter solstice, the shortest day. Wednesday was aclear day, and we generated about 16KWh, as shown in the plot below. We generate electricity during the week while we are mostly out of the house at the highest rate. PG&E just gave us a rebate of $67 due to lower energy costs, so the standing charge of $11.50/month is zeroed out. The ducts are insulated to R8, rather than R4.2, so they don't lose heat. They have fewer leaks and a properly designed balance of air flow into each room. The main effect of this is that we will use less propane."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9d5790df",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Sun ZFS and Thumper (x4500)",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/08/sun-zfs-and-thumper-x4500.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>I was one of the beta testers for Sun's new <a href=\"http://www.sun.com/servers/x64/x4500/\">x4500</a> high density storage server, and it turned out pretty well. I was able to hire <a href=\"http://www.ortera.com\">Dave Fisk</a> as a consultant to help me do the detailed evaluation using his in-depth tools, and it turned into a fascinating investigation of the detailed behavior of the <span>ZFS</span> <span>file system</span>.</p>  <p><span><a href=\"http://www.sun.com/software/solaris/zfs_learning_center.jsp\">ZFS</a></span> is simple to use, has lots of extremely useful features, and the price is right (bundled with <span>Solaris</span> 10 6/06 or <span>OpenSolaris</span>). However its doing lots of clever things under the hood and it behaves like nothing else. Its far more complicated to predict its performance than any other <span>file system</span> we've looked at. It even baffled Dave at first, he had to change his tools to support ZFS, but he's got it pretty well figured out now.<br/> </p>  <p>For a start, its a write <span>anyware</span> <span>file system</span> layout (<span>WAFL</span>) which is similar in some ways to a <span>NetApp</span> filer. This means that random writes are batched up, sorted by file, <span>file system</span> etc. and every few seconds a big burst of sequential writes commits the data to disk as a transaction. Since sequential writes to disk are always much more efficient than random writes, this mean that it gets much more performance per disk than <span>UFS</span>/<span>VxFS</span> etc for random writes.</p>  <p>The combination of the x4500 and <span>ZFS</span> works well, since <span>ZFS</span> knows that the firmware on the 48 <span>SATA</span> drives in the x4500 have a write cache that can safely be enabled and flushed on demand. This greatly improves performance and fixes an issue that I have been complaining about for years. Finally a safe way to use the write caches that exist in every modern drive.<br/> </p>  <p>Its actually easier to list the things that <span>ZFS</span> on the x4500 <i>doesn't have</i>.</p>  <ul> <li>No extra cost - its bundled in a free OS<br/> </li> <li>No volume manager - its built in</li> <li>No space management - <span>file systems</span> use a common pool<br/> </li> <li>No long wait for <span>newfs</span> to finish - we created a 3TB <span>file system</span> in a second</li> <li>No <span>fsck</span> - its transactional commit means its consistent on disk</li> <li>No <span>rsync</span> - snapshots can be differenced and replicated remotely</li> <li>No silent data corruption - all data is checksummed as it is read</li> <li>No bad archives - all the data in the <span>file system</span> is scrubbed regularly</li> <li>No penalty for software RAID - RAID-Z has a clever optimization</li> <li>No downtime - mirroring, RAID-Z and hot spares</li> <li>No immediate maintenance - double parity disks if you need them</li> <li>No hardware failures in our testing - we didn't get to try out some of these features!<br/> </li> </ul> <p>and finally, on the downside<br/> </p>  <ul> <li>No way to know how much performance headroom you have<br/> </li> <li>No way to get at the disks without taking the top off the x4500<br/> </li> <li>No clustering support - I guess they couldn't put everything on the wish list...</li> </ul> <p>The performance is actually very good, and in normal use its going to be fine, but when we tried to drive <span>ZFS</span> to its limit, we found that the results were less consistent or predictable than more conventional <span>file systems</span>. Some of the issues we ran into are present in the <span>Solaris</span> 10 6/06 release, but when the x4500 ships it will have an update to <span>ZFS</span> that includes performance fixes to speed things up in general and reduce the impact of the worst case issues, so it should be more consistent.</p>  <p>We've put <span>ZFS</span> on some of our internal file servers, to see how it goes in light usage. However, it always takes a while to build up confidence in a large body of new code, especially if its storage related. If we can add this one to the list:</p>  <ul> <li>No nasty bugs or surprises?<br/> </li> </ul> <p>Then <span>ZFS</span> looks like a good way to take a lot of cost out of the storage tier.</p>  <p>I'm interested to hear how other people are getting on with <span>ZFS</span>, especially mission critical production uses.<br/> </p>  <p/>  <p style=\"font-size:10px;text-align:right;\">technorati tags:<a href=\"http://technorati.com/tag/ZFS\" rel=\"tag\">ZFS</a>, <a href=\"http://technorati.com/tag/Solaris\" rel=\"tag\">Solaris</a>, <a href=\"http://technorati.com/tag/x4500\" rel=\"tag\">x4500</a>, <a href=\"http://technorati.com/tag/Thumper\" rel=\"tag\">Thumper</a>, <a href=\"http://technorati.com/tag/Ortera\" rel=\"tag\">Ortera</a></p><p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 710,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "ZFS is simple to use, has lots of extremely useful features, and the price is right. However its doing lots of clever things under the hood and it behaves like nothing else. Its far more complicated to predict its performance than any other file system we've looked at. The combination of the x4500 and ZFS works well. ZFS knows that the firmware on the 48 <span>SATA drives in the x 4500 have a write cache that can safely be enabled and flushed on demand. This greatly improves performance and fixes an issue that I have been complaining about for years. ZFS on the x4500 has no extra cost - its bundled in a free OS. No volume manager - its built in. No silent data corruption - all data is checksummed as it is read. No downtime - mirroring, RAID-Z and hot spares. The performance is actually very good, and in normal use its going to be fine. We found that the results were less consistent or predictable than more conventional <span>file systems. Some of the issues we ran into are present in the Solaris 10 6/06 release. When the x4500 ships it will have an update to ZFS that includes performance fixes. ZFS looks like a good way to take a lot of cost out of the storage tier. I'm interested to hear how other people are getting on with ZFS."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_26cf97e1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "MyPhone Making Progress and ETel Conference",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/03/myphone-making-progress-and-etel.html",
      "content": {
        "text": "I'm at the O'Reilly ETel conference this week, lots of new and interesting things happening. I've met the OpenMoko people and held the phone (nice hardware, quite compact).<br /><br />Several people from the SVHMPC are also attending, we have been showing off the cases I made a few weeks ago, and I've been spending way too long on a new and much cooler case design. I've posted my design including all the cad files (this is a completely open source project, including hardware designs) to a page on the <a href=\"http://www.hbmobile.org/wiki/index.php?title=Portrait_oriented_case\">Homebrew Mobile Wiki</a>.<br /><br />Thats the main reason there have been relatively few postings in the last week...",
        "metadata": {
          "word_count": 108,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm at the O'Reilly ETel conference this week, lots of new and interesting things happening. I've been spending way too long on a new and much cooler case design."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3defff1e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "extracct 1.0",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/05/extracct-10.html",
      "content": {
        "text": "I cleaned up the build directory and renamed the program extracct - extract useful info from exacct - and its been running fine on Solaris 10/x64 for a while. If anyone wants to try it out let me know and I'll email a tarfile, I haven't yet got around to testing it on SPARC. I built it on Opteron using gmake/gcc from /usr/sfw/bin.<br /><br />I'm also wondering how best to make it more generally available, perhaps via orcaware.com or sunfreeware.com, or I could post it to BigAdmin or the OpenSolaris source code as an updated demo. Any opinions out there?",
        "metadata": {
          "word_count": 100,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I cleaned up the build directory and renamed the program extracct - extract useful info from exacct - and its been running fine on Solaris 10/x64 for a while. If anyone wants to try it out let me know and I'll email a tarfile, I haven't yet got around to testing it on SPARC."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_70dc0461",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "SEtoolkit and XEtoolkit releases",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/setoolkit-and-xetoolkit-releases.html",
      "content": {
        "text": "The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_0\">SEtoolkit</span> was developed in 1993 by Rich <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_1\">Pettit</span>, and I used it as a way to prototype many new tools and ideas over the years. Its a Solaris specific performance tool scripting language that supports very rapid development of new tools. The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_2\">SEtoolkit</span> has been widely deployed as the Solaris collector for the popular system performance monitor <a href=\"http://www.orcaware.com\"><span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_3\">Orca</span></a>. Rich gave up development of the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_4\">SEtoolkit</span> a few years ago, put the code into open source  under <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_5\">GPL</span>, and <a href=\"http://www.setoolkit.org/cms/\">its now available via <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_6\">sourceforge</span></a>, where it is being maintained by <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_7\">Dagobert</span> <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_8\">Michelsen</span>. A bug in the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_9\">SEtoolkit</span> was causing it to crash when used with complex disk subsystems, and this has now been fixed in the SE3.4.1 release (April 10<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_10\">th</span>, 2007).<br /><br />Meanwhile, Rich has been trying to make a multi-platform (Solaris, Windows, Linux, FreeBSD, <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_11\">OSX</span>, HP-<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_12\">UX</span>, <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_13\">AIX</span>) version of SE for a long time, and finally gave up trying to implement his own language, and based his latest development, the <a href=\"http://www.xetoolkit.com\"><span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_14\">XEtoolkit</span>, on Java 5. The first full release <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_15\">XEtoolkit</span> 1.0 came out on April 15<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_16\">th</span>, 2007</a>. The code is released and supported under both open source and commercial licenses, by Rich's new company - <a href=\"http://www.captivemetrics.com/\">Captive Metrics.</a> The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_17\">GPL</span> license allows full free use of the provided tools, and development of new  and derived tools that are also contributed to the community. The commercial license allows custom <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_18\">XEtoolkit</span> development for proprietary tools, with a higher level of support.<br /><br />The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_19\">XEtoolkit</span> 1.0 release doesn't support HP-<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_20\">UX</span> or <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_21\">AIX</span>, but <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_22\">AIX</span> support is coming soon. I encourage you to try it out, give Rich some feedback and make it worth his while to continue. He's one of the very best programmers and performance tool architects I've ever met....",
        "metadata": {
          "word_count": 337,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "SEtoolkit is a Solaris specific performance tool scripting language that supports very rapid development of new tools. It has been widely deployed as the Solaris collector for the popular system performance monitor. A bug in the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_9\">SEtoolkit. was causing it to crash when used with complex disk subsystems. Rich has been trying to make a multi-platform. version of SE for a long time, and finally gave up trying to. implement his own language, and based his latest development, the. XEtoolkit, on Java 5. Full release of XEtoolkit 1.0 came out on April 15, 2007. The code is released and supported under both open source and commercial licenses. \" sp-spelling-error\" id=\"SPELLING_ERROR_22\">AIX</span> support is coming soon. I encourage you to try it out, give Rich some feedback and make it worth his while to continue."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_4e31ac79",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - More panels and Nissan Leaf",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/12/solar-power-more-panels-and-nissan-leaf.html",
      "content": {
        "text": "Solar City will triple our solar power output in February, and we just ordered a Leaf, which they say is deliverable in the April-July timeframe.<div><br /></div><div>We are adding 6.5KW of net delivered AC capacity (gross is about 7.5KW DC) to the garage roof. This time we are leasing it, and there are several options for how much downpayment vs. recurring payment to make.</div><div><br /></div><div>There was a power cut last night for about two hours, and the automatic generator that runs off our propane tank kicked in after about 20 seconds. When the power came back, there wasn't a glitch, everything kept running. That was the first real test. We have a Generac 17KW unit that powers most of the house. With hindsight, it would have been better to get the 20KW generator that can power a whole 200Amp distribution panel rather than routing individual circuits. The extra cost would be offset by a simpler installation. We hope to replace the propane furnace with a heat pump at some point, then the generator will be the only thing running on propane, and since our solar system is grid-tied, it needs to work well to keep an all electric house going.</div><div><br /></div><div>We were originally looking a geothermal heat pumps, but the extra cost of digging wells to get a more efficient heat pump is not cost effective versus adding extra solar panels to make up the difference. In climates where it is colder and there is less solar irradiance available, it makes more sense to use geothermal.</div><div><br /></div><div>During 2010 we generated about 60% of the electricity we consumed, and also greatly reduced our propane consumption by switching appliances from propane to electric. In 2011 we should be almost 100% solar powered electric, and reduce our gasoline usage as well.</div>",
        "metadata": {
          "word_count": 297,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Solar City will triple our solar power output in February. We just ordered a Leaf, which they say is deliverable in the April-July timeframe. We hope to replace the propane furnace with a heat pump at some point. During 2010 we generated about 60% of the electricity we consumed. We also greatly reduced our propane consumption by switching appliances from propane to electric. In 2011 we should be almost 100% solar powered electric, and reduce our gasoline usage."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fc7cbfac",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Thoughts on iPhone 1.1.3, Macworld, 3G and business users",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/12/thoughts-on-iphone-113-macworld-3g-and.html",
      "content": {
        "text": "Reports indicate the the 1.1.3 update is likely to ship before MacWorld in January, it includes an update to Google Maps that adds the same location feature as recently shipped on other non-GPS platforms (basically locating to the nearest cell-tower). It also provides mechanisms to select and arrange the applications installed on the iPhone. This is a key new feature, since the iPhone has been updated by Apple as a monolithic all-or-nothing set of applications so far. It greatly reduces the need to hack into the iPhone to customize it.<div><br /></div><div>The updates themselves are fairly small, and to me it makes sense to get them installed before MacWorld. We will then see announcements at MacWorld of a set of optional applications that users can pick and choose to install on their iPhones. This could include the ability to add the Mail application and other \"missing\" iPhone apps to the iPod Touch. It could include brand new applications (I'm going to nominate iChat as a candidate yet again...). The public announcement of the SDK is due at MacWorld, with shipping in February, which will open up anyone to build officially sanctioned applications. However key vendors will have been testing the SDK over the last few months so I expect a bunch of third party applications to be announced or ready to ship at MacWorld.</div><div><br /></div><div>The other leaks and rumors indicate that there is likely to be a second generation iPhone with 3G support shipping in the spring, and announced at MacWorld. This would also support launching the iPhone in Asian markets like Japan, where there is no GSM support.</div><div><br /></div><div>I also expect that Apple will start to make moves towards business use of the iPhone, with some tools and upgrades provided by Apple, and others by key third parties.</div><div><br /></div><div>I currently carry a Verizon Blackberry 8703e for work use, and my iPhone for personal use and iPod functionality. In order to use the iPhone as my work phone I need a few key features.</div><div><br /></div><div><ul><li>Firewall support - the BB is inside the corporate firewall, the iPhone can't access it. We use Juniper Network Connect which is a Java based VPN solution on MacOS/XP.</li><li>WiFi support - we use LEAP to login to WiFi at work, need support for LEAP on iPhone, it works fine on my MacOS X laptop, should be a simple feature to add.</li><li>Exchange support - I can't use the IMAP workaround due to firewall issues, properly integrated Exchange email and calendar support is what everyone is asking for.</li><li>Ideally RIM will port the Blackberry application suite to the iPhone, like they did for the Treo...</li></ul><div>The two other biggest missing features are Flash and Java support. I know there are lots of issues with CPU/memory/battery life. Perhaps the next generation iPhone will be based on a more advanced ARM CPU (e.g. the ARM Cortex based Qualcomm Scorpion) with more performance and more memory so it can run Flash and Java apps alongside the existing apps?</div><div><br /></div><div>We'll find out in a few weeks...</div><div>Happy new year.</div></div><div><br /></div>",
        "metadata": {
          "word_count": 504,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Reports indicate the the 1.1.3 update is likely to ship before MacWorld in January. It includes an update to Google Maps that adds the same location feature as recently shipped on other non-GPS platforms. It also provides mechanisms to select and arrange the applications installed on the iPhone. Apple expected to announce iPhone SDK at MacWorld in February. SDK will allow anyone to build officially sanctioned applications. Apple expected to make moves towards business use of the iPhone. Ideally RIM will port the Blackberry application suite to the iPhone. The two other biggest missing features are Flash and Java support. Perhaps the next generation iPhone will be based on a more advanced ARM CPU.  e'll find out in a few weeks...</div><div>Happy new year.</div></div><br><br /></div>"
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_e34d97cb",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix in the cloud and HTML5",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/05/netflix-in-cloud-and-html5.html",
      "content": {
        "text": "One of the fun things about working at Netflix is that we are always \"leaning forward\" and looking for new technologies to leverage. As we roll out our website to run on the Amazon cloud, we are also re-architecting the code base, adding internationalization support and bringing in the latest technologies. One of these is HTML5, which is raising the bar for cross browser support for advanced user interface features, and is now supported by a large and rapidly growing percentage of the visitors to netflix.com. In addition many TV based devices now embed webkit, which is the HTML5 compatible technology that underpins the Safari and Chrome browsers. The user interface engineering team is looking to hire the best and brightest to work on these cutting edge technologies. Here's a write-up <a href=\"http://jobs.netflix.com/DetailFlix.asp?jobid=flix3666\">for one of the open positions</a>:<br /><br /><blockquote>Are you passionate about building great website experiences used by millions of visitors each day?  Come to Netflix where we are using HTML5 based web technologies to move ecommerce directly onto televisions in our customers\u2019 living rooms.   As part of our Customer Acquisition team, you will lead the way to our internationalized television user interface designed to help new customers find Netflix and start streaming movies in seconds.  This new experience will be deployed to HTML5 capable embedded browsers and served from our cutting edge cloud based backend service.</blockquote><br /><br />---------<br />Update - <a href=\"http://www.hackingnetflix.com/2010/05/netflix-to-support-html-5-in-browsers-and-tvbased-devices.html\">Hacking Netflix</a> and <a href=\"http://techcrunch.com/2010/05/19/netflix-html5/\">TechCrunch</a> picked up on this posting. MG Siegler at TechCrunch decided that I was talking about streaming video and Silverlight, which I wasn't. I was thinking of HTML5 features that let us build very cool user interfaces with drag-and-drop, canvas transforms etc. for the web site, and for embedded TV devices specifically. The Silverlight player is used for PC/Mac playback only, and the basic HTML5 Video doesn't have a viable DRM solution at this point. I'm the Cloud Architect for Netflix, so my involvement is to architect robust and scalable support in the cloud for these new user interfaces.",
        "metadata": {
          "word_count": 336,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Netflix is rolling out its website to run on the Amazon cloud. The company is also re-architecting the code base, adding internationalization support. The user interface engineering team is looking to hire the best and brightest. Netflix is using HTML5 based web technologies to move ecommerce directly onto televisions in our customers\u2019 living rooms. This new experience will be deployed to HTML5 capable embedded browsers and served from our cutting edge cloud based backend service. Netflix's Cloud Architect is working on a cloud-based DRM solution. The Silverlight player is used for PC/Mac playback only. The basic HTML5 Video doesn't have a viable DRM solution at this point."
      },
      "tags": [
        "cloud",
        "engineering",
        "netflix",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_2b888a48",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Comment on How Netflix Is Ruining Cloud Computing",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/03/comment-on-how-netflix-is-ruining-cloud.html",
      "content": {
        "text": "<br />\nI wrote a long comment response to&nbsp;<a href=\"http://www.informationweek.com/cloud-computing/platform/how-netflix-is-ruining-cloud-computing/240151650\" target=\"_blank\">how-netflix-is-ruining-cloud-computing</a>&nbsp;on Information Week, but they don't seem in a hurry to post it. Luckily I saved a copy so here it is:<br />\n<br />\n<br />\n<br />\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nThere should be a http://techblog.netflix.com post in the next day or so that will give more context to the Cloud Prize and clarify most of the points above. However I will address some of the specific issues here.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nCloud 1.0 vs. 2.0?</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nI would argue that the way most people are doing cloud today is to forklift part of their existing architecture into a cloud and run a hybrid setup. That's what I would call Cloud 1.0. What Netflix has done is show how to build much more agile green field native cloud applications, which might justify being called Cloud 2.0. The specific IaaS provider used underneath, and whether you do this with public or private clouds is irrelevant to the architectural constructs we've explained.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nOutages</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nThe outages that have been mentioned were regional, they didn't apply to Netflix operations in Europe for example. Our current work is to build tooling for multi-regional support on AWS (East cosat/West coast), including the DNS management that was mentioned. This removes the failure mode with the least effort and disruption to our existing operations.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nPortability</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nOther cloud vendors have a feature set and scale comparable to AWS in 2008-2009. We're still waiting for them to catch up. There are many promises but nothing usable for Netflix itself. However there is demand to use NetflixOSS for other smaller and simpler applications, in both public and private clouds, and Eucalyptus have demonstrated Asgard, Edda and Chaos Monkey running, and will ship soon in Eucalyptus 3.3. There are signs of interest from people to add the missing features to OpenStack, CloudStack and Google Compute so that NetflixOSS can also run on them.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nEdda</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nYou've completely missed the point of Edda. It does three important things. 1) if you run at large scale your automation will overload the cloud API endpoint, Edda buffers this information and provides a query capability for efficient lookups. 2) Edda stores a history of your config, it's a CMDB that can be used to query for what changed. 3) Edda cross integrates multiple data sources, the cloud API, our own service registry Eureka, Appdynamics call flow information and can be extended to include other data sources.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nAMInator</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nIf you want to spin up 500 identical instances, having them each run Chef or Puppet after they boot creates a failure mode dependency on the Chef/Puppet service, wastes startup time, and if anything can go wrong with the install you end up with an inconsistent set of instances. By using AMInator to run Chef once at build time, there is less to go wrong at run time. It also makes red/black pushes and roll-backs trivial and reliable.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nCloud Prize</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nThe prize includes a portability category. It's a broad category and might be won by someone who adds new language support to NetflixOSS (Erlang, Go, Ruby?) or someone who makes parts of NetflixOSS run on a broader range of IaaS options. The reality is that AWS is actually dominating cloud deployments today, so contributions that run on AWS will have the greatest utility by the largest number of people. The alternatives to AWS are being hyped by everyone else, and are showing some promise, but have some way to go.</div>\n<div style=\"font-family: Helvetica; font-size: 12px; min-height: 14px;\">\n<br /></div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\nWe hope that NetflixOSS provides a useful driver for higher baseline functionality that more IaaS APIs can converge on, and move from 2008-era EC2 functionality to 2010-era EC2 functionality across more vendors. Meanwhile Netflix itself will be enjoying the benefits of 2013 AWS functionality like RedShift.</div>\n<div style=\"font-family: Helvetica; font-size: 12px;\">\n<br /></div>",
        "metadata": {
          "word_count": 760,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There should be a http://techblog. Netflix.com post in the next day or so that will give more context to the Cloud Prize. I will address some of the specific issues here. Netflix is building tooling for multi-regional support on AWS (East cosat/West coast) This removes the failure mode with the least effort and disruption to our existing operations. Other cloud vendors have a feature set and scale comparable to AWS in 2008-2009. There are many promises but nothing usable for Netflix itself. There is demand to use NetflixOSS for other smaller and simpler applications. Edda stores a history of your config, it's a CMDB that can be used to query for what changed. Edda cross integrates multiple data sources, the cloud API, our own service registry Eureka, Appdynamics call flow information. The prize includes a portability category. It's a broad category and might be won by someone who adds new language support to NetflixOSS (Erlang, Go, Ruby?) Netflix will be using 2013 AWS functionality like RedShift. C2 functionality to 2010-era EC2 functionality across more vendors. Meanwhile Netflix itself will be enjoying the benefits of 2013 AWS."
      },
      "tags": [
        "agile",
        "architecture",
        "automation",
        "aws",
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_2465be38",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - plan submitted",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-plan-submitted.html",
      "content": {
        "text": "solarcity.com emailed me a very detailed pdf of the plan for 4.5KW on my roof, and submitted it to the County for planning approval today. Meanwhile the house is being re-wired this week for 220V and a 200Amp feed to replace the 110V/100A setup. Also putting in 220V outlets for the kitchen range, hot tub, dryer and hot water heater. Propane tank is now disconnected and I have to go shopping for new appliances...",
        "metadata": {
          "word_count": 74,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Solarcity.com emailed me a very detailed pdf of the plan for 4.5KW on my roof, and submitted it to the County for planning approval today. Meanwhile the house is being re-wired this week for 220V and a 200Amp feed to replace the 110V/100A setup."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5956d700",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "NoSQL Netflix Use Case Comparison for MongoDB",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/nosql-netflix-use-case-comparison-for_31.html",
      "content": {
        "text": "<span class=\"Apple-style-span\"  style=\"font-size:medium;\">Roger Bodamer @rogerb from </span><a href=\"http://www.10gen.com/\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">10gen.com</span></a><span class=\"Apple-style-span\"  style=\"font-size:medium;\"> kindly provided a set of answers for MongoDB  that I have interspersed with the questions below. The original set of questions </span><a href=\"http://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">are posted here</span></a><span class=\"Apple-style-span\"  style=\"font-size:medium;\">. Each NoSQL contender will get their own blog post with answers, when there are enough to be interesting, I will write some summary comparisons. If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself.<br /><br /></span><div><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Use Case Scenario for Comparison Across NoSQL Contenders </span></b></div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them, so that we understand the differences in behavior. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. I assume that is running on AWS and use that terminology, but the concepts are generic.<br /><br /></span><span class=\"Apple-style-span\"><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Use Case</span></b></span><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span><span class=\"Apple-style-span\"  style=\"font-size:medium;\">A TV based device calls the API to add a movie to its favorites list (like the Netflix instant queue, but I have simplified the concept here), then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list.</span><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Favorites Storage<br /></span></b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads.<br /><br /></span><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Question 1: Availability Zones<br /></span></b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones, is the initial write local or spread over the zones, is the write replication zone aware so data is replicated to more than one zone?<br /></span></div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Let's assume for discussion purposes that we are using MongoDB across </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">three availability zones in a region.  We would have a replica set </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">member in each of the three zones.  One member will be elected primary </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">at a given point in time.</span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">All writes will be sent to the primary, and then propagate to </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">secondaries from there.  Thus, writes are often inter-zone.  However </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">availability zones are fairly low latency (I assume the context here </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">is EC2).</span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Reads can be either to the primary, if immediate/strong consistency </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">semantics are desired, or to the local zone member, if eventually </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">consistent read semantics are acceptable.</span></i></span></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></p><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Question 2: Partitioned Behavior with Two Zones<br /></span></b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the NoSQL service support?<br /></span><ul><li><span class=\"Apple-style-span\"  style=\"font-size:medium;\">one zone decides that it is still working for reads and writes but half the size, and the other zone decide it is offline</span></li><li><span class=\"Apple-style-span\"  style=\"font-size:medium;\">both zones continue to satisfy reads, but refuse writes until repaired</span></li><li><span class=\"Apple-style-span\"  style=\"font-size:medium;\">data that has a master copy in the good zone supports read and write, slave copies stop for both read and write</span></li><li><span class=\"Apple-style-span\"  style=\"font-size:medium;\">both zones continue to accept writes, and attempt to reconcile any inconsistency on repair</span></li></ul><b><blockquote><span class=\"Apple-style-span\" style=\"font-weight: normal;\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\">Let's assume again we are using three zones - we could use two but three is more interesting.  To be primary in a replica set, the primary must be visible to a majority of the members of the set: in this case, two thirds of the members, or two thirds of the zones.  If one zone is partitioned from the other two, what will happen is: a member in the 2 zone side of the partition will become primary, if not already.  It will be available for reads and writes.<br /><br />The minority partition will not service writes.  Eventually consistent reads are still possible in the minority partition.<br /><br />Once the partition heals, the servers automatically reconcile.<br /><br /></span></span><a href=\"http://www.mongodb.org/display/DOCS/Replica+Set+Design+Concepts\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"> http://www.mongodb.org/display/DOCS/Replica+Set+Design+Concepts</span></span></a><span class=\"Apple-style-span\" style=\"font-style: normal; font-weight: 800;\"><span class=\"Apple-style-span\" style=\"font-weight: normal;\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></i></span></span></i></span></blockquote><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Question 3: Appending a movie to the favorites list<br /></span></b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions?</span></div><div><p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 13px/normal Consolas; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"></span></span></span></i></p><blockquote><p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 13px/normal Consolas; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">MongoDB supports atomic operations on single documents via both its $ </span></span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">operators ($set, $inc) and also by compare-and-swap operations.  In </span></span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">MongoDB one could model the list as a document per favorite, or, put </span></span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">all the favorites in a single BSON object.  In both cases atomic </span></span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">operations free of race conditions are possible. </span></span></span></i><span class=\"Apple-style-span\" style=\"color: rgb(23, 56, 245); \"><span style=\"text-decoration: underline; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><a href=\"http://www.mongodb.org/display/DOCS/Atomic+Operations\">http://www.mongodb.org/display/DOCS/Atomic+Operations</a></span></span></i></span></span></p><p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 13px/normal Consolas; min-height: 15px; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></span></i></p><p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 13px/normal Consolas; min-height: 15px; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">This is why mongodb elects a node primary: to facilitate these </span></span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">atomic operations for use cases where these semantics are required.</span></span></span></i></p></blockquote><p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 0px; margin-left: 0px; font: normal normal normal 13px/normal Consolas; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"></span></span></span></i></p></div><div><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></span></i></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? </span></div><div><span class=\"Apple-style-span\"  style=\"font-family:Consolas;\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"><blockquote><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Yes.</span></blockquote></span></span></i></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">What limits exist on the size of the value or number of attribute/values?</span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-family:Georgia, serif;\"><span class=\"Apple-style-span\" style=\" font-style: normal;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></span></i></span></div><div> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></i></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">A single BSON document must be under the limit -- currently that </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">limit is 8MB.  If larger than this, one should consider modeling as </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">multiple documents during schema design.</span></span></i></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></i></p><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></p><blockquote></blockquote><span class=\"Apple-style-span\"  style=\"font-size:medium;\">and are queries by attribute/value supported?</span><p></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></i></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Yes. For performance, MongoDB supports secondary (composite) indices.</span></span></i></p><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></i></p></div><div><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Question 4: Handling Silent Data Corruption</span></b></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">When the storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously?</span></div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">The general assumption is that the storage system is reliable.  Thus, </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">one would normally use a RAID with mirroring, or a service like EBS </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">which has intrinsic mirroring.</span></span></i></p><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></i></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">However, the BSON format has a reasonable amount of structure to it. </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">It is highly probable, although not certain, that a corrupt object </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">would be detected and an error reported.  This could then be correct </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">with a database repair operation.</span></span></i></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></i></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Note: the above assumes an actual storage system fault.  Another case </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">of interest is simply a hard crash of the server.  MongoDB 1.6 </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">requires a --repair after this.  MongoDB v1.8 (pending) is crash-safe </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">in its storage engine via journaling.</span></span></i></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></p></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><b><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Question 5: Backup and Restore</span></b></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? </span></div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></i></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">The most used method is to have a replica which is used for backups </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">only; perhaps an inexpensive server or VM.  This node can be taken </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">offline at any time and any backup strategy used.  Once re-enabled, it </span></span></i><span class=\"Apple-style-span\" style=\"color: rgb(23, 56, 245); \"><span style=\"color:#000000;\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">will catch back up.  </span></span></i></span><span style=\"text-decoration: underline\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><a href=\"http://www.mongodb.org/display/DOCS/Backups\">http://www.mongodb.org/display/DOCS/Backups</a></span></span></i></span></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></i></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">With something like EBS, quick snapshotting is possible using the </span></span></i><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">fsync-and-lock command.</span></span></i></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></span></i></p></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?</span></div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">One can stop the server(s), restore the old data file images, and restart.</span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">MongoDB supports a slaveDelay option which allows one to force a </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">replica to stay a certain number of hours behind realtime.  This is a </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">good way to maintain a rolling backup in case of someone </span></i></span><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">\"fat-fingering\" a database operation.</span></i></span></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:large;\"><i></i></span></span></p></div>",
        "metadata": {
          "word_count": 2133,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Roger Bodamer @rogerb from 10gen.com kindly provided a set of answers for MongoDB. If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself. While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin. The second request goes to a different API server that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list. Is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones? is the initial write local or spread across the zones? Is the write replication zone aware so data is replicated to more than one zone? member in each of the three zones.  One member will be elected primary. All writes will be sent to the primary, and then propagate to the zone. Reads can be either to the primary, if immediate/strong consistenc. pple-style-span\"  style=\"font-size:medium;\">secondaries from there. Thus, writes are often inter-zone.  <blockquote class=\"Apple-style-span\" style=\"font-family:'times new roman';\">semantics are desired, or to the local zone member, if eventually. y </span></i></span></p></blockquote. If a NoSQL service. fails, and a partition occurs so that external traffic coming into and staying within a zone. continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the No.SQL service support? To be primary in a replica set, the primary must be visible to a majority of the members. If one zone is partitioned from the other two, what will happen is: a member in the 2 zone side of the partition will become primary. If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? p://www.mongodb.org/display/DOCS/Replica+Set+Design+Concepts. MongoDB supports atomic operations on single documents via both its $. operators ($set, $inc) and also by compare-and-swap operations. All the favorites in a single BSON object. In both cases atomic. \"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class \"font-size:medium;\">operations free of race conditions are possible. This is why mongodb elects a node primary: to facilitate these atomic operations for use cases where these semantics are required. \"><p style=\"margin-top: 0px; margin-right: 0.0px, margin-bottom: 0%.0px;margin-left: 0%; font: normal normal normal 13px/normal Consolas; min-height: 15px; \"><i><span class=\"Apple-style-span\"  style=\"font-family:'times new roman'  \u00a0\"\u00a0\"><br /></span></span></ span></span>span></i></p></p> If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? \"Yes\" \"What limits exist on the size of the value or number of attribute/ values?\" A single BSON document must be under the limit -- currently that limit is 8MB. If larger than this, one should consider modeling as a roman. <blockquote style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style\"font-size:medium;\">multiple documents during Schema design. le=\" fonts: 13.0px Consolas; min-height: 15.0%; font-family:TimesNewRoman; font-size-12.0; font. size: 12.0\"; font-weight:bold; margin-bottom:1.0, margin-right:1, font.size:12.5,. font.family: TimesNewRoman, font-style: sans-serif; font.-size:16.0,. font-letter:bold.bold; font MongoDB supports secondary (composite) indices. When storage or network subsystem corrupts data, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously? The general assumption is that the storage system is reliable. One would normally use a RAID with mirroring, or a service like EBS. The BSON format has a reasonable amount of structure to it. It is highly probable, although not certain, that a corrupt object would be detected. MongoDB v1.8 (pending) is crash-safe. The above assumes an actual storage system fault. Another case is simply a hard crash of the server. Question 5: Backup and Restore. Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? The most used method is to have a replica which is used for backups. Once re-enabled, it will catch back up to the original node. With something like EBS, quick snapshotting is possible using the fsync-and-lock command. <p style=\"margin: 0.0%.0.0px; font: 13.0,.0px Consolas; min-height: 15.0%; min-width: 1.5m; margin-bottom: 1m; font-family: TimesNewRoman'; min-size: 2m; min.height: 1em; font.font.size: medium; margin.width: 2.0m; margins-bottom : 1m, 1m,.0m, 0.1m, margin-top: 1s; font.-size: 3m, font.family: For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored? What is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen? MongoDB supports a slaveDelay option which allows one to force a replica to stay behind realtime. This is a good way to maintain a rolling backup in case of someone. \"fat-fingering\" a database operation. times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">\" fat-fingered\" a Database operation.</span>"
      },
      "tags": [
        "aws",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_7858cbc2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Workplace Myths - Career Advice",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/05/workplace-myths-career-advice.html",
      "content": {
        "text": "Some <a href=\"http://feeds.feedburner.com/%7Er/guykawasaki/Gypm/%7E3/117643688/the_nine_bigges.html\">very good career advice</a> on Guy Kawasaki's blog today. The Nine Biggest Workplace Myths, by Penelope Trunk. I've spent a lot of time mentoring people and helping with career development and this is pretty much what I've been telling people for many years.<br /><br />Its not easy to follow the advice, but it does give a clear vision of what to aim for, and how to know when you get there...",
        "metadata": {
          "word_count": 73,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Nine Biggest Workplace Myths, by Penelope Trunk. Some good career advice on Guy Kawasaki's blog today. Its not easy to follow the advice, but it does give a clear vision of what to aim for."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7836ed2d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "New music: MirthKon's Vehicle",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/06/new-music-mirthkons-vehicle.html",
      "content": {
        "text": "Some of the most inventive new music is coming from \"unknown\" bands in the greater San Francisco region. Through my connections with Fractal I have been a fan of Headshear for many years and now MirthKon have released their first full CD, \"Vehicle\". Fractal and Headshear have played many gigs together, and MirthKon founder Wally Scharold played in Headshear for a while a few years ago.<br /><br /><a href=\"http://www.mirthkon.com/\">I have seen MirthKon play a few times, and they are currently on a west coast tour</a>, having recently played SF and Berkeley, they are in Hollywood on June 5th and San Diego on June 6th, then Chico on June 18th, and Eugene and Portland on June 19th and 20th. They have a conceptual video extravaganza to enhance the musical experience at live shows.<br /><br />So, what do they sound like? Well, lets start by saying that if you like Frank Zappa, and wished he was still putting out new music, then you will be very happy to take this Vehicle for a ride to Joe's Garage. Its impossible to really say what Zappa sounds like, since the mix of styles and influences is so diverse, but MirthKon have some of the same elements, along with their own unique character. There is a strong dose of humor, very complex rhythms and time signatures, some jazzy vocals on a few tracks, but a largely instrumental mix that combines clarinet and saxophone with guitar, bass and drums<br /><br />They dub it \"The first full-length CD from the Bay Area's most hyphenated thrash-jazz-prog-chamber-core ensemble\", which probably doesn't help much. I have been happily playing it over and over again in my car for the last week, expand your musical horizons and support local music by getting your own copy....<br /><br />miRthkon is: <br />Wally Scharold - guitars, vocals, composition, sound design, conceptual design, text, video, producer <br />Rob Pumpelly - guitars, composition <br />Nat Hawkes - bass guitar, vocals <br />Carolyn Walter - piccolo, flute, clarinet, bass clarinet, alto saxophone, tenor saxophone <br />Jamison Smeltz - alto saxophone, baritone saxophone, vocals <br />Matt Guggemos - drums and percussion <br />Jarred McAdams - video, text, conceptual collaborator",
        "metadata": {
          "word_count": 361,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "MirthKon have released their first full CD, \"Vehicle\" Fractal and Headshear have played many gigs together. They are currently on a west coast tour. They have a conceptual video extravaganza to enhance the musical experience. MirthKon is a Bay Area-based thrash-jazz-prog-chamber-core ensemble. The group combines clarinet and saxophone with guitar, bass and drums.  n Smeltz - alto saxophone, baritone saxophone and vocals. Matt Guggemos - drums and percussion. Jarred McAdams - video, text, conceptual collaborator."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_d838244e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "See you at Techshop at Maker Faire this weekend",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/05/see-you-at-techshop-at-maker-faire-this.html",
      "content": {
        "text": "I've been using <a href=\"http://techshop.ws\">Techshop</a> to make cases for my homebrew phone on their 3D Printer, and I signed up for annual membership since I have a few other projects to work on (e.g. TIG welding some modifications to my <a href=\"http://www.abarth.itgo.com/pic/124rally/124rally05.jpg\">Fiat 124 Abarth Rally replica</a> :-)<br /><br />I'm going to be at Maker Faire in San Mateo this weekend, helping explain the 3D printer to attendees at the Techshop pavillion, and hanging out with Matt on the <a href=\"http://hbmobile.org\">SVHMPC</a> booth. I'm on duty from 2pm onwards both Saturday and Sunday for Techshop so I should be easy to find and I'd be happy to chat about how to make mobile phone cases or <a href=\"http://millicomputing.blogspot.com\">millicomputing</a> or whatever.",
        "metadata": {
          "word_count": 118,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm going to be at Maker Faire in San Mateo this weekend, helping explain the 3D printer to attendees at the Techshop pavillion. I'm on duty from 2pm onwards both Saturday and Sunday for Techshop so I should be easy to find. I'd be happy to chat about how to make mobile phone cases or <a href=\"http://millicomputing.blogspot.com\">millimomputing</a> or whatever."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b46b4403",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal Zoom video with Ellipsis",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/fractal-zoom-video-with-ellipsis.html",
      "content": {
        "text": "Here is a collaboration between Eric Bigas, who makes Fractal Zoom videos and the band Fractal who have provided Ellipsis as the soundtrack to this video.<br /><br /><object width=\"480\" height=\"295\"><param name=\"movie\" value=\"http://www.youtube.com/v/eXoSIN3HNUs&hl=en&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/eXoSIN3HNUs&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"480\" height=\"295\"></embed></object>",
        "metadata": {
          "word_count": 42,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Eric Bigas makes Fractal Zoom videos and Fractal provide Ellipsis as the soundtrack to this video. Here is a collaboration between Eric Bigas and the band Fractal."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_e43af619",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal iPhone app source code",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/04/fractal-iphone-app-source-code.html",
      "content": {
        "text": "<a href=\"http://code.google.com/p/fractal-band-iphone-app/\">I have posted the source to Fractal's iPhone app to google code</a>. I'm hoping that other bands can replace the artwork and URLs and quickly get their own promotional apps out there. If we end up with bug-fixes and improvements, then everyone gains....",
        "metadata": {
          "word_count": 44,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I have posted the source to Fractal's iPhone app to google code. I'm hoping that other bands can replace the artwork and URLs and quickly get their own promotional apps out there."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_dedf60c1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Bloglines, OPML, Blogger and Flock",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/10/bloglines-opml-blogger-and-flock.html",
      "content": {
        "text": "I aggregate 50 or so blog feeds using Bloglines, its a very useful way to keep track of infrequent blogs in particular, and it strips off the adverts and other decoration from the power bloggers.<br /><br />I just did some tidying up of my blog list, exported it in OPML format and uploaded it to <a href=\"http://share.opml.org\">http://share.opml.org/</a><br /><br />This is an interesting way to contribute to the \"Top 100\" blogs list on that site, and it also has some useful features like seeing who else reads the same blogs, and who has the most similar list of blogs.<br /><br />I seem to have added to the \"long tail\" since many of the blogs I read were new to the site, so I'm the only reader. My blog had one other reader \"Ian\" (hello!) who has a very long list of blogs, that I may poke around in if I get some free time...<br /><br />Meanwhile, Flock is working well as my cross-platform browser. I'm writing blog entries with it, although I did lose an entry I was writing a week or so ago, when I upgraded Flock and didn't save an almost complete entry first. The new version of Flock appears to automatically save entries every few minutes, but I hate re-writing things, so that entry may not be re-created for a while.<br /><br />Flock seems to have some issues writing entries to blogger.com at the moment. I'm not using the updated blogger.com, but Flock fails to write the blog entry most of the time, then randomly works. I gave up and used cut and paste to post this entry directly....",
        "metadata": {
          "word_count": 271,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I aggregate 50 or so blog feeds using Bloglines. It strips off the adverts and other decoration from the power bloggers. I seem to have added to the \"long tail\" since many of the blogs I read were new to the site, so I'm the only reader. Flock seems to have some issues writing entries to blogger.com at the moment. New version of Flock appears to automatically save entries every few minutes, but I hate re-writing things."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_c24933fd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Extended Accounting in Solaris 8-10",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/02/extended-accounting-in-solaris-8-10.html",
      "content": {
        "text": "<p class=\"mobile-post\">This is a very under-used facility introduced in Solaris 8. Solaris 10<br />extends it so that you can get TCP flow accounting as well as extended<br />process accounting info. It also adds a perl interface.</p><p class=\"mobile-post\">Unlike traditional unix accounting - which is low overhead but stores<br />very poor quality information - extended accounting can save data at<br />fixed time intervals (as well as at process termination). It can be<br />configured to store detailed information, and in Solaris 10 since<br />microstate accounting is enabled for all processes by default you can<br />get the microstate details for everything with no extra overhead. I'll<br />be digging into the details of how to configure and process this data<br />soon.</p>",
        "metadata": {
          "word_count": 116,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Extended accounting is a very under-used facility introduced in Solaris 8. Solaris 10 extends it so that you can get TCP flow accounting as well as extended process accounting info."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_df32e8ff",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Classic car events in August",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/classic-car-events-in-august.html",
      "content": {
        "text": "I go to the Monterey Historic Races every year, and this year I'm also taking my Citroen SM to a new event<a href=\"http://www.montereybayconcours.com/\"> La Dolce Vita Automobili</a> - it's set up to be a smaller and more fun alternative to the Concorso Italiano, which is on the same day Friday August 14th.<br /><br />Here is the SM at Crater Lake a few weeks ago.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRhPnB6CopJbKuAu4Vjmlefo1kH1fHsDy0TUhuBGmzJYsJMQTtIP4fKvLp3IvRqMRN8L24ByxmpN1yItDmzju8Psn8gh4XhJFqeUEDmcD3cR5_ekmykp6qWIoPdbBz2InGNB3/s1600-h/SANY0312.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgxRhPnB6CopJbKuAu4Vjmlefo1kH1fHsDy0TUhuBGmzJYsJMQTtIP4fKvLp3IvRqMRN8L24ByxmpN1yItDmzju8Psn8gh4XhJFqeUEDmcD3cR5_ekmykp6qWIoPdbBz2InGNB3/s320/SANY0312.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5363891429504607922\" /></a>",
        "metadata": {
          "word_count": 79,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "La Dolce Vita Automobili is set up to be a smaller and more fun alternative to the Concorso Italiano, which is on the same day Friday August 14th. Here is the SM at Crater Lake a few weeks ago."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_923671df",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Installation Day 4",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-installation-day-4.html",
      "content": {
        "text": "All the panels are on the roof, the trench has been re-filled, the wiring is done, it was tested and span the meter backwards. Next step is inspections that can take a few weeks before we can turn it on for real....<br /><br />I installed the Solarcity Internet monitoring system base station by plugging it into a spare Ethernet port on my DSL modem. It flashed some lights that told me that it had connected to Solarcity successfully, then started scanning to find the monitored devices. It talks to the inverter to track how much power is being generated, and I will have an optional consumption monitor that tells me how much power the house is using. I have to wait for the install to be complete before I can <a href=\"http://solarcity.com/residential/solarguard-system-monitoring.aspx\">see any output on the solarcity.com web site</a>.<br />Also, they were out of stock of the consumption monitor, so it will need to be added on later, but it doesn't affect the operation of the system.<br /><br />I have also figured out the Ground Source Heat Pump, I'm going for a vertical well based installation, <a href=\"http://www.residential.carrier.com/products/geothermalheatpumps/index.shtml\">its a 3-ton A/C unit and heater made by Carrier</a> that hooks up to my existing propane furnace ducting, and includes a water heater that replaces my propane hot water tank. It exchanges heat with the ground using three 200ft deep wells, and for every KW of electricity it uses, it pumps around 4KW of heat in or out of the house. This is over twice as efficient as a conventional air based A/C unit. It costs more, but there is a 30% federal tax credit on the full price, just like the solar power installation, thanks to Obama's stimulus package (this credit started in February 2009). I'm hoping to have enough solar power to zero out the running costs, we will see how that goes over time.",
        "metadata": {
          "word_count": 315,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "All the panels are on the roof, the trench has been re-filled, the wiring is done. Next step is inspections that can take a few weeks before we can turn it on for real. Ground Source Heat Pump uses three 200ft deep wells. For every KW of electricity it uses, it pumps around 4KW of heat in or out of the house."
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_bd248906",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Public Cloud Instance Pricing Wars - Detailed Context and Analysis",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/04/public-cloud-instance-pricing-wars.html",
      "content": {
        "text": "As part of my opening keynote at <a href=\"http://www.interop.com/lasvegas/conference/cloud-connect.php?_mc=we_cc_le_intlv14_cportalbtn\" target=\"_blank\">Cloud Connect in Las Vegas</a> I summarized the latest moves in cloud, the slides are available via the new <a href=\"http://www.battery.com/powered/\" target=\"_blank\">Powered by Battery</a> site as \"<a href=\"http://www.battery.com/powered/the-whole-stack/2014/03/18/you-need-to-act-now-tapping-the-cloud-to-more-quickly-deliver-it-services-and-lessons-from-netflix/\" target=\"_blank\">The Good the Bad and the Ugly: Critical Decisions for the Cloud Enabled Enterprise</a>\". This blog post is a detailed analysis of just part of what happened.<br />\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nSummary points<o:p></o:p></div>\n<div class=\"MsoNormal\">\n</div>\n<ul>\n<li>AWS users should migrate from obsolete m1, m2, c1, c2 to the\nnew m3, r3, c3 instances to get better performance at lower prices with the\nlatest Intel CPUs.</li>\n<li>Any cloud benchmark or cost comparison that uses the AWS m1 family\nas a basis should be called out as bogus benchmarketing.</li>\n<li>AWS and Google instance prices are essentially the same for\nsimilar specs.</li>\n<li>Microsoft doesn\u2019t appear to have the latest Intel CPUs generally available\nand only matches prices for obsolete AWS instances.</li>\n<li>IBM Softlayer pricing is still higher, especially on small\ninstance types</li>\n<li>Google's statement that prices should follow <a href=\"http://en.wikipedia.org/wiki/Moore%27s_law\" target=\"_blank\">Moore\u2019s law</a> implies that we should expect prices to halve every 18-24 months</li>\n<li>Pricing pages by <a href=\"http://aws.amazon.com/ec2/pricing/\" target=\"_blank\">AWS</a>, <a href=\"https://cloud.google.com/products/compute-engine/\" target=\"_blank\">Google Compute Engine</a>, <a href=\"http://blogs.msdn.com/b/windowsazure/archive/2014/03/31/microsoft-azure-innovation-quality-and-price.aspx\" target=\"_blank\">Microsoft Azure</a>, <a href=\"http://www.softlayer.com/virtual-servers\" target=\"_blank\">IBM Softlayer</a></li>\n<li>Adrian\u2019s spreadsheet summary of instances from the above vendors at <a href=\"http://bit.ly/cloudinstances\">http://bit.ly/cloudinstances</a></li>\n<li>Analysis of the prices by <a href=\"http://www.rightscale.com/blog/cloud-cost-analysis/aws-responds-price-cuts-google-vs-aws-pricing-round-2\" target=\"_blank\">Rightscale</a></li>\n</ul>\n<br />\n<div class=\"MsoNormal\">\nOn Tuesday 25<sup>th</sup> March 2014 Google announced some\nnew features and steep price cuts, the next day Amazon Web Services also announced new features and matching price cuts. On Monday 31<sup>st</sup> March Microsoft Azure also reduced prices. Many\npundits repeated talking points from press releases in blog posts but unfortunately\nthere was little attempt to understand what really happened, and explain the context and outcome. When I wrote up a summary for my opening keynote at Cloud\nConnect on 31<sup>st</sup> March I looked at the actual end result and came up\nwith a different perspective and a list of gaps.</div>\n<div class=\"MsoNormal\">\n<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nI\u2019m only going to discuss instance types and on-demand\nprices here. There was a lot more in the announcements that other people have\ndone a good job of summarizing. The <a href=\"http://www.rightscale.com/blog/cloud-cost-analysis/aws-responds-price-cuts-google-vs-aws-pricing-round-2\" target=\"_blank\">Rightscale blog</a> linked above also gives an\naccurate and broader view on what was announced. I will discuss other pricing\nmodels beyond on-demand in future blog posts.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nThere are some things you need to know to get the right\nbackground context for the instance price cuts. The most important is to\nunderstand that AWS has two generations of instance types, and is in a\ntransition from Intel CPU technology they introduced five or more years ago to\na new generation introduced in the last year. The new generation CPUs are based\non an architecture known as <a href=\"http://en.wikipedia.org/wiki/Sandy_Bridge\" target=\"_blank\">Sandybridge</a>. The latest tweak is called Ivybridge\nand has incremental improvements that give more cores per chip and slightly\nhigher performance. Since Google is a recent entrant to the public cloud\nmarket, all their instances types are based on Sandybridge. To correctly\ncompare AWS prices and features with Google, there is a like-for-like\ncomparison that can be made. AWS is encouraging the transition by pricing its\nnewer faster instances at a lower cost than the older slower ones. In the\nrecent announcement, AWS cut the prices by obsolete instance type families by a\nsmaller percentage than the newer instance type families, so the gap has just widened.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nOld AWS instance types have names starting with m1, m2 and\nc1, c2. They all have newer replacements known as m3, r3 and c3 except the\nsmallest one \u2013 the m1.small. The newer instances have a similar amount of RAM\nand CPU threads, but the CPU performance is significantly higher. The new\nequivalents also replace small slow local disks with smaller but far faster and\nmore reliable solid-state disks, and the underlying networks move from 1Gbit/s\nto 10Gbit/s. The newer instance families should also have lower failure rates.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nMost people are much more familiar with the old generation\ninstance types, and competitors write their press releases they are able to get\naway with claiming that they are both faster and cheaper than AWS, by comparing\nagainst the old generation products. This is an old \u201cbenchmarketing\u201d trick \u2013\ncompare your new product against the competitions older and more recognizable\nproduct.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nFor the most commonly used instance types there is a close specification\nmatch between the AWS m3 and the Google n1-standard. They are also exactly the\nsame price per hour. Since AWS released its changes after Google, this implies\nthat AWS deliberately matched Google\u2019s price. The big architectural difference\nbetween the vendors is that Google instances are diskless, all their storage is\nnetwork attached, while AWS have various amounts of SSD included. The AWS\nhypervisor also makes slightly more memory available per instance, and ratings\nfor the c3 imply that AWS is supplying a slightly higher CPU clock rate for that\ninstance type. I think that this is because AWS has based its compute intensive\nc3 instance types on a higher clock rate Ivybridge CPU rather than the earlier\nSandybridge specification. For the high memory capacity instance types it is a\nlittle different. The Google n1-himem instances have less memory available than\nthe AWS r3 equivalents, and cost a bit less. This makes intuitive sense as this\ninstance type is normally bought for its memory capacity.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nMicrosoft previously committed to match AWS prices, and in\ntheir announcement their comparisons matched the m1 range exactly at it\u2019s new\nprice, and they compared their memory oriented A5 instance as cheaper than an\nold m2.xlarge, but the A5 is an older slower CPU type, more expensive ($0.22 vs\n$0.18) and has less memory (14GB vs. 15GB) than the AWS r3.large. The common\nCPU options on Azure are aligned with the older AWS instance types. Azure does have Intel Sandybridge CPUs for compute use cases as the A8 and A9 models, but I couldn't find list pricing for them and they appear to be a low volume special option. The\nAzure pricing strategy ignores the current generation AWS product, so the price\nmatch guarantee doesn\u2019t deliver. In addition the Google and AWS price changes were effective from April 1<sup>st</sup>, but Azure takes effect May 1<sup>st</sup>.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nIBM Softlayer has a choose-what-you-want model rather than a\nspecific set of instance types. The smaller instances are $0.10/hr where AWS\nand Google n1-standard-1 are $0.07/hr. As you pick a bigger instance type on\nSoftlayer the cost doesn\u2019t scale up linearly, while Google and AWS double the\nprice each time the configuration doubles. The Softlayer equivalent of the\nn1-standard-16 is actually slightly lower cost than Google. Softlayer pricing\non most instances is in the same ballpark as AWS and Azure were before the cuts,\nso I expect they will eventually have to cut prices to match the new level.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nGaps and Missing Features<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nThe remaining anomaly in AWS pricing is the low-end\nm1.small. There is no newer technology equivalent at present, so I wouldn\u2019t be\nsurprised to see AWS do something interesting in this space soon. Generally AWS\nhas a much wider range of instances than Google, but AWS is missing an\nm3.4xlarge to match Google's n1-standard-16, and the Google hicpu range has\ndouble the CPU to RAM ratio of the AWS c3 range so they aren\u2019t directly\ncomparable.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nGoogle has no equivalent to the highest memory and CPU AWS\ninstances, and has no local disk or SSD options. Instead they have\nbetter attached disk performance than AWS Elastic Block Store, but attached\ndisk adds to the instance cost, and can never be as fast as local SSD inside the instance.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nMicrosoft Azure needs to refresh its instance type options,\nit has a much smaller range, older slower CPUs, and no SSD options. It doesn\u2019t\nlook particularly competitive.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nConclusion<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nIf you buy hardware and capitalize it over three years, and later on there is a price cut; you don\u2019t get to reduce your monthly costs. Towards the\nend your CPUs are getting old, leading to less competitive response times and\nhigher failure rates. With public cloud vendors driving the costs down several\ntimes a year and upgrading their instances, your model of public vs. private\ncosts needs to factor in something like Moore\u2019s law for cost reductions and a\ntechnology refresh more often than every three years. Google actually said we\nshould expect Moore\u2019s law to apply in their announcement, which I interpret to\nmean that we can expect costs to halve about every 18-24 months. This isn\u2019t a race\nto zero; it\u2019s a proportional reduction every year. Over a three-year period the\ncost at the end is a third to a quarter of the cost at the start.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<div class=\"MsoNormal\">\nI still hear CIOs worry that cloud vendor lock-in would let\nthem raise prices. This ruse is used to justify private cloud investments. Even\nwithout switching vendors, you will see repeated price reductions for the public\ncloud systems you are already using. This was the 42<sup>nd</sup> price cut for\nAWS, the argument is ridiculous.<o:p></o:p></div>\n<div class=\"MsoNormal\">\n<br /></div>\n<!--[if gte mso 9]><xml>\n <o:DocumentProperties>\n  <o:Revision>0</o:Revision>\n  <o:TotalTime>0</o:TotalTime>\n  <o:Pages>1</o:Pages>\n  <o:Words>1298</o:Words>\n  <o:Characters>7403</o:Characters>\n  <o:Company>Battery</o:Company>\n  <o:Lines>61</o:Lines>\n  <o:Paragraphs>17</o:Paragraphs>\n  <o:CharactersWithSpaces>8684</o:CharactersWithSpaces>\n  <o:Version>14.0</o:Version>\n </o:DocumentProperties>\n <o:OfficeDocumentSettings>\n  <o:AllowPNG/>\n </o:OfficeDocumentSettings>\n</xml><![endif]-->\n\n<!--[if gte mso 9]><xml>\n <w:WordDocument>\n  <w:View>Normal</w:View>\n  <w:Zoom>0</w:Zoom>\n  <w:TrackMoves/>\n  <w:TrackFormatting/>\n  <w:PunctuationKerning/>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:DoNotPromoteQF/>\n  <w:LidThemeOther>EN-US</w:LidThemeOther>\n  <w:LidThemeAsian>JA</w:LidThemeAsian>\n  <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript>\n  <w:Compatibility>\n   <w:BreakWrappedTables/>\n   <w:SnapToGridInCell/>\n   <w:WrapTextWithPunct/>\n   <w:UseAsianBreakRules/>\n   <w:DontGrowAutofit/>\n   <w:SplitPgBreakAndParaMark/>\n   <w:EnableOpenTypeKerning/>\n   <w:DontFlipMirrorIndents/>\n   <w:OverrideTableStyleHps/>\n   <w:UseFELayout/>\n  </w:Compatibility>\n  <m:mathPr>\n   <m:mathFont m:val=\"Cambria Math\"/>\n   <m:brkBin m:val=\"before\"/>\n   <m:brkBinSub m:val=\"--\"/>\n   <m:smallFrac m:val=\"off\"/>\n   <m:dispDef/>\n   <m:lMargin m:val=\"0\"/>\n   <m:rMargin m:val=\"0\"/>\n   <m:defJc m:val=\"centerGroup\"/>\n   <m:wrapIndent m:val=\"1440\"/>\n   <m:intLim m:val=\"subSup\"/>\n   <m:naryLim m:val=\"undOvr\"/>\n  </m:mathPr></w:WordDocument>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\"\n  DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\"\n  LatentStyleCount=\"276\">\n  <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\"/>\n  <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\"/>\n  <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\"/>\n  <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\"/>\n  <w:LsdException Locked=\"false\" Priority=\"1\" Name=\"Default Paragraph Font\"/>\n  <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\"/>\n  <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\"/>\n  <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\"/>\n  <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Table Grid\"/>\n  <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\"/>\n  <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\"/>\n  <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\"/>\n  <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\"/>\n  <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\"/>\n  <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\"/>\n  <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\"/>\n  <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\"/>\n  <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\"/>\n  <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\"/>\n  <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\"\n   UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\"/>\n  <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\"/>\n  <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\"/>\n </w:LatentStyles>\n</xml><![endif]-->\n\n<!--[if gte mso 10]>\n<style>\n /* Style Definitions */\ntable.MsoNormalTable\n {mso-style-name:\"Table Normal\";\n mso-tstyle-rowband-size:0;\n mso-tstyle-colband-size:0;\n mso-style-noshow:yes;\n mso-style-priority:99;\n mso-style-parent:\"\";\n mso-padding-alt:0in 5.4pt 0in 5.4pt;\n mso-para-margin:0in;\n mso-para-margin-bottom:.0001pt;\n mso-pagination:widow-orphan;\n font-size:12.0pt;\n font-family:Cambria;\n mso-ascii-font-family:Cambria;\n mso-ascii-theme-font:minor-latin;\n mso-hansi-font-family:Cambria;\n mso-hansi-theme-font:minor-latin;}\n</style>\n<![endif]-->\n\n\n\n<!--StartFragment-->\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<!--EndFragment--><br />\n<div class=\"MsoNormal\">\nI\u2019ve <a href=\"http://www.slideshare.net/adrianco/yowworkshop-131203193626phpapp01-1\" target=\"_blank\">previously published presentation materials on costoptimization with AWS</a>. I\u2019m researching this area and over the coming months\nwill publish a series of posts on all aspects of cloud optimization. <o:p></o:p></div>",
        "metadata": {
          "word_count": 2900,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "As part of my opening keynote at Cloud Connect in Las Vegas I summarized the latest moves in cloud. The slides are available via the new <a href=\"http://www.battery.com/powered/the-whole-stack/2014/03/18/you-need-to-act-now-tapping-the-cloud-to more-quickly-deliver-it-services-and-lessons-from-Netflix\" target=\"_blank\">Powered by Battery. This blog post is a detailed analysis of just part of what happened. Pricing pages by Amazon, Google, Microsoft and IBM show prices for AWS and Google instances. Prices are essentially the same for similar specs. Microsoft doesn't appear to have the latest Intel CPUs generally available. Google, Amazon and Microsoft all announced price cuts for cloud services in March. Many pundits picked up on talking points from press releases, but there was little attempt to understand what really happened, and explain the context and outcome. I looked at the end result and came up with a different perspective and a list of gaps. I\u2019m only going to discuss instance types and on-demand \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0prices here. There was a lot more in the announcements that other people have done a good job of summarizing. The most important is to understand that AWS has two generations of instance types.  AWS is encouraging the transition by pricing its newer faster instances at a lower cost than the older slower ones. Since Google is a recent entrant to the public cloud, all their instances types are based on Sandybridge.  AWS m3 and Google n1 are the most commonly used instance types. They are also exactly the same in price and performance. The new versions replace small slow local disks with smaller but far faster solid-state disks. Google n1-himem instances have less memory available than their AWS r3 equivalents, and cost a bit less. I think that this is because AWS has based its compute intensivec3 instance types on a higher clock rate Ivybridge CPU rather than the earlier Sandybridge specification. IBM Softlayer has a choose-what-you-want model rather than a specific set of instance types. Google and AWS price changes were effective from April 1, but Azure takes effect May 1. Azure does have Intel Sandybridge CPUs for compute use cases. As you pick a bigger instance type on Softlayer the cost doesn\u2019t scale up linearly. The Softlayer equivalent of the Google n1-standard-16 is actually slightly lower cost than Google. Softlayer pricing on most instances is in the same ballpark as AWS and Azure were before the cuts. Google has no equivalent to the highest memory and CPU AWS instances. Microsoft Azure has a much smaller range, older slower CPUs, and no SSD options. Google says we can expect costs to halve about every 18-24 months. CIOs worry that cloud vendor lock-in would let them raise prices. This was the 42<sup>nd</sup> price cut for \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0AWS, the argument is ridiculous. We are using. dy using dy using mso 9. The full document can be found at the bottom of the page. The full document is available in English, Spanish, and Arabic. The document can also be read in German, French, and Japanese. \"LsdException Locked=\"false\" Priorit. alse\" Priority=\"0\" SemiHidden=\" false\" UnhideWhenUsed=\"false,\" Q Format=\"true\" Name=\"Normal,\" Priority=\"9\" Semi hidden=\"false\", Priority=\"39\" \"Lsd Exception Locked\" Priority \"39\" Priority\u00a0=\"0,\" SemiHidden \"false\" \" LsdException locked\" Priority \"#39,\" Priority\u00a0 \"#39\", Priority\u00a0\"Normal\", SemiHidden\u00a0\"false,\" Priority \"Normal\" \" y=\"39\" Name=\"toc 3\" \" y=\"38\" \"y=\"39,\" \"y\" \"toc\" \"Y\" \"o\" \"O\" \"T\" \"C\" \"t\" \"c\" \"i\" \"I\" \"3\" \"2\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\" \"11\" \"12\" \"13\" \"14\" \"15\" \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\" \"31\" \"32\" The author uses the term \"Table Grid\" to refer to a table of text. He also uses \"Light Shading\" and \"Light List\" in the title of this article. \u00a0\u00a0   UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" \u00a0\u00a0UnhideWhen used=\" false\" Name= \"Medium List 2\" Name\u00a0\"Medium List 3\" Name \"Dark List\" Name  \"Colorful Shading\"   \"Medium Grid\" \"Light Shading Accent 1\" \"Light Grid Accent\" \"Medium Shading 2\" \"Heavy Shading 1\"  \"Light List\" \"Revision\" iHidden=\"false\" UnhideWhenUsed=\" false\" Q Format=\"true\" Name=\"Intense Quote\" iHidden=\" false,\" UnhideWh=\"false,\" iHidden=false, iHidden. iHidden = \"iHidden\", iHidden\u00a0= \"0\", i hidden = \"0,\" i hidden= \"1\", i Hidden = \"1,\" i Hidden= \"2\", ihidden= \"3,\" ihidden = \"2\" i hidden=\"false\",    Unhidewh=\"false\"; iHidden='false', iHidden={\"iHidden\": \"1\" }); \" enUsed=\"false\" Name=\"Colorful Grid Accent 1\" \" enUsed\" \" Name=\"Light Shading 1 Accent 2\" \"En used\" \"Light Grid\" \"Colorful Shading\" \"Medium Shading 2 Accent\" \"Dark Shading 3\" \"Shading 4\" \"Bright Shading 5\" \"Deep Shading 6\" \"Mixed Shading 7\" \"Black Shading 8\" \"White Shading 9\" \"Gray Shading 10\" \"Green Shading 11\" \"\u00a0 UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" \"Medium Grid 3 Accent\" \"Dark List Accent 1\" \"Light Shading Accent 3\" \"Colorful Grid Accent2\" \"Shading\" \"No. 1\" \u00a0\"Dark List\"  \"Light List\" \" ked=\"false\" Priority=\"62\" SemiHidden=\"false,\" \"Ked=\" false\" Priority = \"62,\" \"SemiHidden\" = \"63\", \"SemI\" \"SemII\" \"KED\" \"Priority\": \"62\" \"Preferred\": \"63\" \"Median\": \"64,\" \"Priced\": \"65,\" \"Medial\": \"66,\" \"Precious\": \"68\" \"Paid\": \"69\" \"UnhideWhenUsed\" \"Light Grid Accent 3\" \"Medium Shading 1\" and \"Light Shading 2\" \"Shading 3\" iHidden=\"false\" UnhideWhenUsed=\" false\" Name=\"Dark List Accent 3\" iHidden=\"true\" Un hideWhenUsed='false' Name=\"Light Shading 1 Accent 4\" i hidden=\"false' Unhide when used: i hidden: i hid: iHidden. iHidden: 0. Unhide When used: 1. \" Name=\"Medium Shading 2 Accent 4\" \"Medium Grid 1 Accent 3\" \"Dark List Accent\" \"Colorful Shading Accent 5\" \"Shading 1\" and \"Shader 2\" \"LsdException Locked=\"false\"    UnhideWhenUsed=\" false\" Name=\"Colorful Grid Accent 4. nt 4\" \"Lsd Exception Locked=\"true\" \u00a0\"priority=\"73\" \"priority=\"60\" \"prioritization=\"61\" \"preparation\u00a0=\"62\" \"premier=\"63\" \"semiHidden\" \"hidden\" \"false\" \"non-zero\" \"true\" \"none\" \"no\" \"undefined\" \"un-expressed\" \"n-exposed\" \"N.E.N.T\" \"Non-Expressed\" \"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\" false\" Name=\"Medium Grid 1 Accent 5\" \"false\"   Unhide when used to mean \"dark list\" or \"light list\" \"False\" \u00a0UnhideWhen used to refer to a colorful grid. UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" UnhideWhen used to mean \"Light List\" or \"Light Shading\" \"QFor. e\" is a shorthand for \"Medium Grid 3 Accent 6\" \"QFor\" is an abbreviation for \"Quantum Grid 3\" \"Quad\" is for \"quad\" or \"square\" Table. mat=\"true\" Name=\"Intense Reference\" Table. mat = \"Table Normal\"; Table. mso-padding-alt:0in 5.4pt; Table. MsoNormalTable: \"MsoNormal\"; TableTable. table = 'Table Normal' Table. font-family:Cambria;. table-size:12.0pt; table-font-family-family. \"M soNormalTable\"; table-style-name: \"Table. Normal\"; table. style-parent: \"mso.style-parent:\"; table. font: \"Hansi\", table. size: \"5.4\", table-margin-bottom: 0.0001pt; I\u2019ve previously published presentation materials on costoptimization with AWS. I\u2019m researching this area and over the coming months will publish a series of posts."
      },
      "tags": [
        "architecture",
        "aws",
        "azure",
        "cloud",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_6619a340",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone Tethering Review",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/07/iphone-tethering-review.html",
      "content": {
        "text": "Along with iOS4 and the new ATT data plans they now also support tethering. I setup my wife\u2019s personal iPhone 3GS for tethering, and it worked fine on her work laptop with Windows XP over the USB cable (she could work from the passenger seat while we were driving freeways). You just need to have iTunes installed on the XP machine, even if that isn\u2019t the machine that sync\u2019s the iPhone.<br /><br />I also got Netflix to switch on tethering on my company iPhone 3GS, and I\u2019m using it via bluetooth to my MacBook Air, easy setup and it also worked well for me.<br /><br />The downside is that you lose the unlimited data plan, but for a few dollars a month, it can be very useful.",
        "metadata": {
          "word_count": 127,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The downside is that you lose the unlimited data plan, but for a few dollars a month, it can be very useful. You just need to have iTunes installed on the XP machine, even if that isn't the machine that sync\u2019s the iPhone."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_4c197673",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix in the Cloud",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/netflix-in-cloud.html",
      "content": {
        "text": "I'm presenting this talk on Thursday at the Cloud Computing Meetup, and again on Nov 3rd at QConSF. So far I have posted a \"teaser\" <a href=\"http://www.slideshare.net/adrianco/netflix-oncloudteaser\">summary on slideshare.</a> After QCon I will post the full slide deck [update: <a href=\"http://www.slideshare.net/adrianco/netflix-on-cloud-combined-slides-for-dev-and-ops\">combined deck from both talks posted here</a>].<br /><br /><a href=\"http://www.meetup.com/cloudcomputing/calendar/14476942/\">The meetup is the \"beta test\" of the presentation</a>. It's in Mountain View at \"Hacker Dojo\", and at the time of writing 437 people have signed up to attend. If everyone turns up it's going to be crazy and over-flowing trying to park and get in, so get there early.... I will focus the meetup talk more on the operational aspects of the cloud architecture, and migration techniques.<br /><br /><a href=\"http://qconsf.com/sf2010/presentation/Running+Netflix+in+the+Cloud\">At QConSF, the presentation is in the \"Architectures you've always wondered about\" track</a>, and I will spend more time talking about the software architecture.<br /><br />Why give these talks? We aren't trying to sell cloud to CIOs, <a href=\"http://www.netflix.com/jobs\">it's all about hiring</a>, we are talking at engineer focused events in the Bay Area. Netflix is growing fast, pathfinding new technologies in the public cloud to support a very agile business model, and is trying to attract the best and brightest people. We use LinkedIn a lot when we search for positions, so <a href=\"http://www.linkedin.com/in/adriancockcroft\">feel free to connect to me</a> if you think this could be interesting, or <a href=\"http://twitter.com/adrianco\">follow @adrianco</a> on Twitter.",
        "metadata": {
          "word_count": 231,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm presenting this talk on Thursday at the Cloud Computing Meetup, and again on Nov 3rd at QConSF. After QCon I will post the full slide deck. At the time of writing 437 people have signed up to attend. Netflix is growing fast, and is looking to hire engineers. The company is using the cloud to support a very agile business model. We use LinkedIn a lot when we search for positions, so feel free to connect to me."
      },
      "tags": [
        "agile",
        "architecture",
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_f66fd332",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "An exercise in complexity....",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/exercise-in-complexity.html",
      "content": {
        "text": "Time for a grumble....<br /><br />My plan was to take the libexacct.so API and expose it as an SE toolkit class. After looking at what it would take to do this I have come to the conclusion that the data structure definitions and API for reading the data are too complex.<br /><br />The design is so abstract that it seems that reading meaningful data out of the log file is some obscure side effect of the code. You can read the data, but there is no guarantee that any specific item of data will be present. The accounting system has various options to send more or less data to the file, so it needs to be flexible, but the important thing is the meaning of the data being logged. I care about the semantic and informational content of the data source. What I get from exacct is \"there are some tagged typed objects in this file\". I can't consume the data without making assumptions about it, and the API doesn't embody those assumptions.<br /><br />Some of the data being reported is useless (blocks in and blocks out are archaic measures that are always zero) and other stuff is missing - like the good microstate information on CPU wait, page-in wait etc.<br /><br />I'm going to take the exdump.c code and turn it into a library that exports a sane and simple set of abstractions (like the data structures I defined in earlier posts).",
        "metadata": {
          "word_count": 243,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The design is so abstract that it seems that reading meaningful data out of the log file is some obscure side effect of the code. You can read the data, but there is no guarantee that any specific item of data will be present. The accounting system has various options to send more or less data to the file. I'm going to take the exdump.c code and turn it into a library that exports a sane and simple set of abstractions (like the data structures I defined in earlier posts). Other stuff is missing like microstate information on CPU wait, page-in wait etc."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7b53ce4d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Comparing NoSQL Availability Models",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html",
      "content": {
        "text": "<div>let's risk feeding the <a href=\"http://codahale.com/you-cant-sacrifice-partition-tolerance/\">CAP</a> trolls, and try to get some insight into the differences between the many NoSQL contenders. I have circulated an earlier version of this to a few people and got at least one good response. If you have answers, or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself.</div><div><br /></div><div><ul><li><a href=\"http://perfcap.blogspot.com/2010/10/nosql-netflix-use-case-comparison-for.html\">Riptano/Cassandra's response is here</a></li><li><a href=\"http://perfcap.blogspot.com/2010/10/nosql-netflix-use-case-comparison-for_31.html\">10gen/MongoDB's response is here</a></li><li><a href=\"http://perfcap.blogspot.com/2010/11/nosql-netflix-use-case-comparison-for.html\">Basho/Riak's response is here</a></li><li><a href=\"http://perfcap.blogspot.com/2010/11/nosql-netflix-use-case-comparison-for_17.html\">Translattice's response is here</a></li></ul></div><div><br /></div><div><b><span class=\"Apple-style-span\" style=\"font-size: large;\">Use Case Scenario for Comparison Across NoSQL Contenders </span></b></div>While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them, so that we understand the differences in behavior. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. I assume that is running on AWS and use that terminology, but the concepts are generic.<br /><br /><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Use Case</b></span><span class=\"Apple-style-span\" style=\"font-size: large;\"><br /></span>A TV based device calls the API to add a movie to its favorites list (like the Netflix instant queue, but I have simplified the concept here), then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list.<div><br /><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Favorites Storage<br /></b></span>Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads.<br /><br /><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Question 1: Availability Zones<br /></b></span>When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones, is the initial write local or spread over the zones, is the write replication zone aware so data is replicated to more than one zone?&nbsp;<br /><br /><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Question 2: Partitioned Behavior with Two Zones<br /></b></span>If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the NoSQL service support?<br /><ul><li>one zone decides that it is still working for reads and writes but half the size, and the other zone decide it is offline</li><li>both zones continue to satisfy reads, but refuse writes until repaired</li><li>data that has a master copy in the good zone supports read and write, slave copies stop for both read and write</li><li>both zones continue to accept writes, and attempt to reconcile any inconsistency on repair</li></ul><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Question 3: Appending a movie to the favorites list<br /></b></span>If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/values, and are queries by attribute/value supported?<br /><br /></div><div><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Question 4: Handling Silent Data Corruption</b></span></div><div>When the storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously?</div><div><br /></div><div><span class=\"Apple-style-span\" style=\"font-size: large;\"><b>Question 5: Backup and Restore</b></span></div><div>Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?</div><div><br /></div>",
        "metadata": {
          "word_count": 695,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I have circulated an earlier version of this to a few people and got at least one good response. If you have answers, or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself. While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin. The second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list. If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, what happens? In particular, which of these outcomes does the NoSQL service support?  e list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/ values, and are queries by attribute/value supported? W is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?"
      },
      "tags": [
        "aws",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_d26fdd7f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Skype on Solaris",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/skype-on-solaris.html",
      "content": {
        "text": "<p><a href=\"http://blogs.sun.com/darren/entry/skype_1.3.0.53_on_solaris_via\">http://blogs.sun.com/darren/entry/skype_1.3.0.53_on_solaris_via</a></p> <p><!-- PProtector -->Solaris has a Linux compatible subsystem called BrandZ for running Linux binaries that don't have Solaris builds (like Skype). Darren figured out how to get the Linux build of Skype to run on Opensolaris.</p> <p><!-- PProtector -->Thanks to <a href=\"http://www.crypticide.com/dropsafe\">Alec</a> for pointing this out.</p> <p><br /></p>",
        "metadata": {
          "word_count": 50,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Solaris has a Linux compatible subsystem called BrandZ for running Linux binaries that don't have Solaris builds. Darren figured out how to get the Linux build of Skype to run on Opensolaris."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_320af855",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Ops, DevOps and PaaS (NoOps) at Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/03/ops-devops-and-noops-at-netflix.html",
      "content": {
        "text": "There has been a sometimes heated discussion on twitter about the term NoOps recently, and I've been quoted extensively as saying that NoOps is the way developers work at Netflix. However, there are teams at Netflix that do traditional Operations, and teams that do DevOps as well. To try and clarify things I need to explain the history and current practices at Netflix in chunks of more than 140 characters at a time.<br /><br />When I joined Netflix about five years ago, I managed a development team, building parts of the web site. We also had an operations team who ran the systems in the single datacenter that we deployed our code to. The systems were high end IBM P-series virtualized machines with storage on a virtualized Storage Area Network. The idea was that this was reliable hardware with great operational flexibility so that developers could assume low failure rates and concentrate on building features. In reality we had the usual complaints about how long it took to get new capacity, the lack of consistency across supposedly identical systems, and failures in Oracle, in the SAN and the networks, that took the site down too often for too long.<br /><br />At that time we had just launched the streaming service, and it was still an experiment, with little content and no TV device support. As we grew streaming over the next few years, we saw that we needed higher availability and more capacity, so we added a second datacenter. This project took far longer than initial estimates, and it was clear that deploying capacity at the scale and rates we were going to need as streaming took off was a skill set that we didn't have in-house. We tried bringing in new ops managers, and new engineers, but they were always overwhelmed by the fire fighting needed to keep the current systems running.<br /><br />Netflix is a developer oriented culture, from the top down. I sometimes have to remind people that our CEO Reed Hastings was the founder and initial developer of Purify, which anyone developing serious C++ code in the 1990's would have used to find memory leaks and optimize their code. Pure Software merged with Atria and Rational before being swallowed up by IBM. Reed left IBM and formed Netflix. Reed hired a team of very strong software engineers who are now the VPs who run developer engineering for our products. When we were deciding what to do next Reed was directly involved in deciding that we should move to cloud, and even pushing us to build an aggressively cloud optimized architecture based on NoSQL. Part of that decision was to outsource the problems of running large scale infrastructure and building new datacenters to AWS. AWS has far more resources to commit to getting cloud to work and scale, and to building huge datacenters. We could leverage this rather than try to duplicate it at a far smaller scale, with greater certainty of success. So the budget and responsibility for managing AWS and figuring out cloud was given directly to the developer organization, and the ITops organization was left to run its datacenters. In addition, the goal was to keep datacenter capacity flat, while growing the business rapidly by leveraging additional capacity on AWS.<br /><br />Over the next three years, most of the ITops staff have left and been replaced by a smaller team. Netflix has never had a CIO, but we now have an excellent VP of ITops Mike Kail (@mdkail), who now runs the datacenters. These still support the DVD shipping functions of Netflix USA, and he also runs corporate IT, which is increasingly moving to SaaS applications like Workday. Mike runs a fairly conventional ops team and is usually hiring, so there are sysadmin, database,, storage and network admin positions. The datacenter footprint hasn't increased since 2009, although there have been technology updates, and the over-all size is order-of-magnitude a thousand systems.<br /><br />As the developer organization started to figure out cloud technologies and build a platform to support running Netflix on AWS, we transferred a few ITops staff into a developer team that formed the core of our DevOps function. They build the Linux based base AMI (Amazon Machine Image) and after a long discussion we decided to leverage developer oriented tools such as Perforce for version control, Ivy for dependencies, Jenkins to automate the build process, Artifactory as the binary repository and to construct a \"bakery\" that produces complete AMIs that contain all the code for a service. Along with AWS Autoscale Groups this ensured that every instance of a service would be totally identical. Notice that we didn't use the typical DevOps tools Puppet or Chef to create builds at runtime. This is largely because the people making decisions are development managers, who have been burned repeatedly by configuration bugs in systems that were supposed to be identical.<br /><br />By 2012 the cloud capacity has grown to be order-of-magnitude 10,000 instances, ten times the capacity of the datacenter, running in nine AWS Availability zones (effectively separate datacenters) on the US East and West coast, and in Europe. A handful of DevOps engineers working for Carl Quinn (@cquinn - well known from the Java Posse podcast) are coding and running the build tools and bakery, and updating the base AMI from time to time. Several hundred development engineers use these tools to build code, run it in a test account in AWS, then deploy it to production themselves. They never have to have a meeting with ITops, or file a ticket asking someone from ITops to make a change to a production system, or request extra capacity in advance. They use a web based portal to deploy hundreds of new instances running their new code alongside the old code, put one \"canary\" instance into traffic, if it looks good the developer flips all the traffic to the new code. If there are any problems they flip the traffic back to the previous version (in seconds) and if it's all running fine, some time later the old instances are automatically removed. This is part of what we call NoOps. The developers used to spend hours a week in meetings with Ops discussing what they needed, figuring out capacity forecasts and writing tickets to request changes for the datacenter. Now they spend seconds doing it themselves in the cloud. Code pushes to the datacenter are rigidly scheduled every two weeks, with emergency pushes in between to fix bugs. Pushes to the cloud are as frequent as each team of developers needs them to be, incremental agile updates several times a week is common, and some teams are working towards several updates a day. Other teams and more mature services update every few weeks or months. There is no central control, the teams are responsible for figuring out their own dependencies and managing AWS security groups that restrict who can talk to who.<br /><br />Automated deployment is part of the normal process of running in the cloud. The other big issue is what happens if something breaks. Netflix ITops always ran a Network Operations Center (NOC) which was staffed 24x7 with system administrators. They were familiar with the datacenter systems, but had no experience with cloud. If there was a problem, they would start and run a conference call, and get the right people on the call to diagnose and fix the issue. As the Netflix web site and streaming functionality moved to the cloud it became clear that we needed a cloud operations reliability engineering (CORE) team, and that it would be part of the development organization. The CORE team was lucky enough to get Jeremy Edberg (@jedberg - well know from running Reddit) as its initial lead engineer, and also picked up some of the 24x7 shift sysadmins from the original NOC. The CORE team is still staffing up, looking for Site Reliability Engineer skill set, and is the second group of DevOps engineers within Netflix. There is a strong emphasis on building tools too make as much of their processes go away as possible, for example they have no run-books, they develop code instead,<br /><br />To get themselves out of the loop, the CORE team has built an alert processing gateway. It collects alerts from several different systems, does filtering, has quenching and routing controls (that developers can configure), and automatically routes alerts either to the PagerDuty system (a SaaS application service that manages on call calendars, escalation and alert life cycles) or to a developer team email address. Every developer is responsible for running what they wrote, and the team members take turns to be on call in the PagerDuty rota. Some teams never seem to get calls, and others are more often on the critical path. During a major production outage con call, the CORE team never make changes to production applications, they always call a developer to make the change. The alerts mostly refer to business transaction flows (rather than typical operations oriented Linux level issues)  and contain deep links to dashboards and developer oriented Application Performance Management tools like AppDynamics which let developers quickly see where the problem is at the Java method level and what to fix,<br /><br />The transition from datacenter to cloud also invoked a transition from Oracle, initially to SimpleDB (which AWS runs) and now to Apache Cassandra, which has its own dedicated team. We moved a few Oracle DBAs over from the ITops team and they have become experts in helping developers figure out how to translate their previous experience in relational schemas into Cassandra key spaces and column families. We have a few key development engineers who are working on the Cassandra code itself (an open source Java distributed systems toolkit), adding features that we need,  tuning performance and testing new versions. We have three key open source projects from this team available on github.com/Netflix. Astyanax is a client library for Java applications to talk to Cassandra, CassJmeter is a Jmeter plugin for automated benchmarking and regression testing of Cassandra, and Priam provides automated operation of Cassandra including creating, growing and shrinking Cassandra clusters, and performing full and incremental backups and restores. Priam is also written in Java. Finally we have three DevOps engineers maintaining about 55 Cassandra clusters (including many that span the US and Europe), a total of 600 or so instances. They have developed automation for rolling upgrades to new versions, and sequencing compaction and repair operations. We are still developing our Cassandra tools and skill sets, and are looking for a manager to lead this critical technology, as well as additional engineers. Individual Cassandra clusters are automatically created by Priam, and it's trivial for a developer to create their own cluster of any size without assistance (NoOps again). We have found that the first attempts to produce schemas for Cassandra use cases tend to cause problems for engineers who are new to the technology, but with some familiarity and assistance from the Cloud Database Engineering team, we are starting to develop better common patterns to work to, and are extending the Astyanax client to avoid common problems.<br /><br />In summary, Netflix stil does Ops to run its datacenter DVD business. we have a small number of DevOps engineers embedded in the development organization who are building and extending automation for our PaaS, and we have hundreds of developers using NoOps to get their code and datastores deployed in our PaaS and to get notified directly when something goes wrong. We have built tooling that removes many of the operations tasks completely from the developer, and which makes the remaining tasks quick and self service. There is no ops organization involved in running our cloud, no need for the developers to interact with ops people to get things done, and less time spent actually doing ops tasks than developers would spend explaining what needed to be done to someone else. I think that's different to the way most DevOps places run, but its similar to other PaaS enviroments, so it needs it's own name, NoOps. [Update: the DevOps community argues that although it's different, it's really just a more advanced end state for DevOps, so lets just call it PaaS for now, and work on a better definition of DevOps].",
        "metadata": {
          "word_count": 2050,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There has been a sometimes heated discussion on twitter about the term NoOps. I've been quoted extensively as saying that NoOps is the way developers work at Netflix. However, there are teams at Netflix that do traditional Operations, and teams that do DevOps as well. Netflix is a developer oriented culture, from the top down. We tried bringing in new ops managers, and new engineers, but they were always overwhelmed by the fire fighting needed to keep the current systems running. I sometimes have to remind people that our CEO Reed Hastings was the founder and initial developer of Purify. Reed was involved in deciding that Netflix should move to cloud. Part of that decision was to outsource the problems of running large scale infrastructure to AWS. Over the next three years, most of the ITops staff have left and been replaced by a smaller team. Netflix has never had a CIO, but we now have an excellent VP of ITops Mike Kail. The Amazon cloud is ten times the capacity of the datacenter, running in nine AWS Availability zones (effectively separate datacenters) on the US East and West coas. The cloud capacity has grown to be order-of-magnitude 10,000 instances. Several hundred development engineers use these tools to build code, run it in a test account in AWS, then deploy it to production themselves. They never have to have a meeting with ITops, or file a ticket asking someone from ITops to make a change. Code pushes to the datacenter are rigidly scheduled every two weeks, with emergency pushes in between to fix bugs. There is no central control, the teams are responsible for figuring out their own dependencies and managing AWS security groups. Netflix has a cloud operations reliability engineering team. The CORE team is looking for Site Reliability Engineer skill set. There is a strong emphasis on building tools too make as much of their processes go away as possible. CORE team members take turns to be on call in the PagerDuty rota. Some teams never seem to get calls, and others are more often on the critical path. The alerts mostly refer to business transaction flows. Netflix moved from Oracle to Apache Cassandra, which has its own dedicated team. Astyanax is a client library for Java applications to talk to Cassandra. CassJmeter is a Jmeter plugin for automated benchmarking and regression testing. Priam is written in Java. We have three DevOps engineers maintaining about 55 Cassandra clusters (including many that span the US and Europe) They have developed automation for rolling upgrades to new versions, and sequencing compaction and repair operations. NoOps removes many operations tasks completely from the developer. There is no ops organization involved in running our cloud. No need for the developers to interact with ops people to get things done. PaaS is a form of DevOps, but we don't yet know what DevOps is. PaaS can be used to build software for DevOps. We're working on a better definition."
      },
      "tags": [
        "agile",
        "architecture",
        "automation",
        "aws",
        "cloud",
        "devops",
        "distributed systems",
        "engineering",
        "infrastructure",
        "netflix",
        "performance",
        "platform",
        "podcast",
        "security",
        "site reliability"
      ]
    },
    {
      "id": "virtual_adrianco_file_8b8167d3",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "GP Musumeci joins Netflix....",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/09/adrian-cockcrofts-blog-are-any-ex-pae.html",
      "content": {
        "text": "<a href=\"http://perfcap.blogspot.com/2009/08/are-any-ex-pae-snorclers-looking-for.html\">Adrian Cockcroft&#39;s Blog: Are any (ex-)PAE Snorclers looking for a new career?</a><br /><br />My call was answered.... GP Musumeci started work last week at Netflix. He's the author of O'Reilly System Performance Tuning, and we worked together at Sun PAE in 2000-2002. Since then he worked on the operations side, building Time Warner AoL's video streaming service, and Salesforce.com's infrastructure, then moved to become a developer of recommendation systems and distributed databases. The perfect mix...",
        "metadata": {
          "word_count": 76,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "GP Musumeci started work last week at Netflix. He's the author of O'Reilly System Performance Tuning. We worked together at Sun PAE in 2000-2002."
      },
      "tags": [
        "infrastructure",
        "netflix",
        "performance",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_fbd7e5cb",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "The IT swamp draining manual for anyone who is neck deep in alligators",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/01/the-it-swamp-draining-manual-for-anyone.html",
      "content": {
        "text": "I've spent the last year or so reviewing Gene Kim's <a href=\"http://www.amazon.com/gp/product/B00AZRBLHO/\" target=\"_blank\">new book - the Phoenix Project </a>and encouraging him to get it finished. It came out this week, is the top business book on Amazon as I write this, and I got a nice back-cover quote shown below with Gene's actual finger in the photo.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv2LGOtFyUVu26pbETWfqEV18ygMT0XtSVdOFfjEPWghasfUi3TT5p7gaT40QestNfqNlD0tJY3wGm0icZFCe83Lqd5kYPOHoTV9AT2cvTIyW-7eel6-PaMfN13wcszjF5yJ_D/s1600/PastedGraphic-4.tiff\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"320\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjv2LGOtFyUVu26pbETWfqEV18ygMT0XtSVdOFfjEPWghasfUi3TT5p7gaT40QestNfqNlD0tJY3wGm0icZFCe83Lqd5kYPOHoTV9AT2cvTIyW-7eel6-PaMfN13wcszjF5yJ_D/s320/PastedGraphic-4.tiff\" width=\"240\" /></a></div>\n<br />\nMany years ago <a href=\"http://www.amazon.com/The-Goal-Process-Improvement-ebook/dp/B002LHRM2O/\" target=\"_blank\">someone gave me a copy of The Goal</a>, which is the inspiration for The Phoenix Project. In both cases the book is a novel about a company that is dysfunctional and on the verge of going out of business. The lead character is dropped into the job of figuring out how to dig their way out of the problem, and in the case of The Phoenix Project, the company fumbles its way from legacy enterprise where IT isn't regarded as central to their success, into the modern world of agile development practices and DevOps deployments where IT becomes a competitive advantage.<br />\n<br />\nDon't just read it, give copies of it to your friends in management. It should be on every CxO's bookshelf.<br />\n<br />",
        "metadata": {
          "word_count": 212,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I've spent the last year or so reviewing Gene Kim's new book - the Phoenix Project. It came out this week, is the top business book on Amazon. I got a nice back-cover quote shown below with Gene's actual finger in the photo. The Goal is a novel about a company that is dysfunctional and on the verge of going out of business. In the case of The Phoenix Project, the company fumbles its way from legacy enterprise to the modern world of DevOps deployments."
      },
      "tags": [
        "agile",
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_file_63b1849a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "See you at the developers conference? | by Adrian Cockcroft | May 15th, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/05/see-you-at-developers-conference-by.html",
      "content": {
        "text": "The combined eBay, PayPal and Skype developer conference is coming up, June 10-12 in Las Vegas. I missed the event last year, but I will be staffing it this year! A few of us are being let out of the mysterious eBay Research Labs for the occasion. They told us to get to work on <span style=\"font-style: italic;\">the future of e-commerce</span>, and have kept us locked up for months, shipping in occasional supplies of Starbucks and fresh interns.  I've been writing serious amounts of code for the first time in years, if fact I'm too busy writing Java to have time to go to JavaOne this week.<br /><br />The conference is supposed to illuminate questions such as:<br /><br />-          What will the next technology revolution be<br /><br />-          How will it impact commerce and communications on the web<br /><br />-          And what opportunities will it provide for developers and technology innovators<br /><br />-          How will the Long Tail Theory play out<br /><br />-          Web 2.0 and how to build revenue streams<br /><br />Its become a common joke to keep incrementing this: Web 2.1, Web 3.0 etc. but personally I think the most interesting developments aren't even Web based. e.g. Skype isn't a Web application, it defines its own virtual private peer to peer fabric that overlays the Internet.<br /><br />See you in Vegas!",
        "metadata": {
          "word_count": 223,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The combined eBay, PayPal and Skype developer conference is coming up, June 10-12 in Las Vegas. I missed the event last year, but I will be staffing it this year! A few of us are being let out of the mysterious eBay Research Labs for the occasion. Most interesting developments aren't even Web based. e.g. Skype isn't a Web application, it defines its own virtual private peer to peer fabric that overlays the Internet."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2881d65b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "IPQoS Configuration",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/ipqos-configuration.html",
      "content": {
        "text": "I found a config file that logs data to the accounting system without<br />filtering it much, it basically filters by protocol, I just take tcp<br />and pass it to the flow accounting module. You could add udp and other protocols if needed.<br /><br />This is installed using<br /><br /><pre><font face=\"courier\"><br /># ipqosconf -a exacct.qos<br /></pre></font><br />Where exacct.qos is written as follows<br /><pre><font face=\"courier\"><br />fmt_version 1.0<br /><br />action {<br />        module ipgpc<br />        name ipgpc.classify<br />        params {<br />                global_stats true<br />        }<br />        filter {<br />                name tcpfilter<br />                protocol tcp<br />                class allclass<br />        }<br />        class {<br />                name allclass<br />                next_action acct<br />                enable_stats true<br />        }<br />}<br />action {<br />        module flowacct<br />        name acct<br />        params {<br />                global_stats true<br />                timer 10000<br />                timeout 10000<br />                max_limit 2048<br />                next_action continue<br />        }<br />}<br /></font></pre>",
        "metadata": {
          "word_count": 141,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I found a config file that logs data to the accounting system withoutfiltering it much. It basically filters by protocol, I just take tcp and pass it to the flow accounting module. You could add udp and other protocols if needed.  lobal_stats true<br />  \u00a0    timer 10000, max_limit 2048, next_action continue, timeout 10000. lobal\u00a0stats true, max\u00a0limit\u00a0 2048, timer\u00a0 10000,\u00a0 max_action\u00a0 continue,\u00a0 timeout 10000, next\u00a0action\u00a0continue,\u00a0max_limit\u00a0\u00a0 2048."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e5d6f0e7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "My iPhone apps 2009",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/01/my-iphone-apps-2009.html",
      "content": {
        "text": "the apps that I use most frequently: Echofon, Skype, Newsstand, Instant Queue Add for Netflix, Amazon, Kindle, Pandora, WootWatch, CoPilot.<br /><br />In particular I get most of my news via rss feeds using Newsstand, which synchronizes very well with Google Reader on my laptop. I don't own a kindle, but whenever possible I buy books to read on my iPhone using the kindle app.",
        "metadata": {
          "word_count": 64,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In particular I get most of my news via rss feeds using Newsstand, which synchronizes very well with Google Reader on my laptop. I don't own a kindle, but whenever possible I buy books to read on my iPhone using the kindle app."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_91c51567",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Open letter to my Sun friends at Oracle",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/08/open-letter-to-my-sun-friends-at-oracle.html",
      "content": {
        "text": "I recently heard about Illumos via a tweet from Alec Muffett, and responded with my own tweet \"I predict that #illumos will be just as irrelevant as Solaris has been for the last few years. Legacy.\" - personally I haven't logged into a Solaris or SPARC machine for about four years now. There are none at Netflix.<br /><br />I have also been talking to a few friends who stayed at Sun and are now at Oracle, and there is a common thread that I decided to put out there in this blog post.<br /><br />This week I presented at a local Computer Measurement Group meeting, talking about how easy it is to use the Amazon cloud to run Hadoop jobs to process terabytes of data for a few bucks [<a href=\"http://www.slideshare.net/adrianco\">slideshare</a>]. I followed a talk on optimizing your Mainframe software licensing costs by tweaking workload manager limits. There are still a lot of people working away on IBM Mainframes, but it's not where interesting new business models go to take over the world.<br /><br />The way I see the Oracle/Sun merger is that Oracle wanted to compete more directly with IBM, and they will invest in the bits of Sun that help them do that. Oracle has a very strong focus on high margin sales, so they will most likely succeed in making good money with help from Solaris and SPARC to compete with AIX, z/OS and P-series, selling to late-adopter industries like Banking, Insurance etc. Just look where the Mainframes are still being used. Sun could never focus on just the profitable business on its own, because it had a long history of leading edge innovation that is disruptive and low margin. However, what was innovative once is now a legacy technology base of Solaris and SPARC, and it's not even a topic of discussion in the leading edge of disruptive innovators, who are running on x64 in the cloud on Linux and a free open source stack. There is no prospect of revenue for Oracle in this space, so they are right to ignore it.<br /><br />That is what I meant when I tweeted that Illumos is as irrelevant as Solaris, and it is legacy computing. I don't mean Solaris will go away, I'm sure it will be the basis of a profitable business for a long time, but the interesting things are happening elsewhere, specifically in public cloud and \"infrastructure as code\".<br /><br />You might point to Joyent, who use Solaris, and now have Bryan Cantrill on board, but they are a tiny bit-player in cloud computing and Amazon are running away with the cloud market, and creating a set of de-facto standard APIs that make it hard to differentiate and compete. You might point to enterprise or private clouds, but as @scottsanchez tweeted: \"Define: Private Cloud ... 1/2 the features of a public cloud, for 4x the cost\", that's not where the interesting things are happening.<br /><br />So to my Sun friends at Oracle, if you want to work for a profitable company and build up your retirement fund Oracle is an excellent place to be. However, there are a lot of people who joined Sun when it was re-defining the computer industry, changing the rules, disrupting the competition. If you want some of that you need to re-tool your skill set a bit and look for stepping stones that can take you there.<br /><br />When Sun shut down our HPC team in 2004 I deliberately left the Enterprise Computing market, I didn't want to work for a company that sold technology to other companies, I wanted to sell web services to end consumers, and I had contacts at eBay who took me on. In 2007 I joined Netflix, and it's the best place I've ever worked, but I needed that time at eBay to orient myself to a consumer driven business model and re-tool my skill set, I couldn't have joined Netflix directly.<br /><br />There are <a href=\"http://www.netflix.com/Jobs\">two slideshare presentations on the Netflix web site</a>, one is on the company culture, the other on the business model. It is expected that anyone who is looking for a job has read and inwardly digested them both (its basically an interview fail if you haven't). These aren't aspirational puff pieces written by HR, along with everyone else in Netflix management (literally, at a series of large offsites), I was part of the discussion that helped our CEO Reed Hastings write and edit them both.<br /><br />What can you do to \"escape\"? The tools are right there, you don't need to invest significant money, you just need to carve out some spare time to use them. Everything is either free open source, or available for a few cents or dollars on the Amazon cloud. The best two things you can have on your resume are hands on experience with the Amazon Web Services tool set, and links to <a href=\"http://wiki.github.com/adrianco/Actors-for-Netflix-on-iPad/\">open source projects</a> that you have contributed to. There isn't much demand for C or C++ programmers, but ObjectiveC is an obvious next step, it's quite fun to code in and you can develop user interfaces for iPhone/iPad in a few lines of code, that back-end into cloud services. Java code (for app servers like Tomcat) on Android phones, Ruby-on-Rails, and Python are the core languages that are being used to build innovative new businesses nowadays. If you are into data or algorithms, then you need to figure out how to use Hadoop, which as I describe in one of my <a href=\"http://www.slideshare.net/adrianco\">slideshare</a> decks is trivially available from Amazon. You can even get an HPC cluster on a 10Gbit ethernet interconnect from Amazon now. There is hadoop based <a href=\"http://mahout.apache.org/\">open source algorithm project called Mahout</a> that is always looking for contributors.<br /><br />To find the jobs themselves, spend time on <a href=\"http://www.linkedin.com\">LinkedIn</a>. I use it to link to anyone I think might be interesting to hire or work with. Your connections have value since it is always good to hire people that know other good people. Keep your own listing current and join groups that you find interesting, like Java Architecture or Cloud Computing, and Sun Alumni. At this point LinkedIn is the main tool used by recruiters and managers to find people.<br /><br />Good luck, and keep in touch (you can find me on <a href=\"http://www.linkedin.com\">LinkedIn</a> or twitter @adrianco :-)",
        "metadata": {
          "word_count": 1062,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I predict that Illumos will be just as irrelevant as Solaris has been for the last few years. I haven't logged into a Solaris or SPARC machine for about four years now. There are none at Netflix. Oracle wanted to compete more directly with IBM, and will invest in the bits of Sun that help them do that. Sun could never focus on just the profitable business on its own, because it had a long history of leading edge innovation that is disruptive and low margin. Solaris will be the basis of a profitable business for a long time. The interesting things are happening in public cloud and \"infrastructure as code\" When Sun shut down our HPC team in 2004 I deliberately left the Enterprise Computing market. In 2007 I joined Netflix, and it's the best place I've ever worked. But I needed that time at eBay to orient myself to a consumer driven business model and re-tool my skill set. The tools are right there, you don't need to invest significant money, you just need to carve out some spare time to use them. Everything is either free open source, or available for a few cents or dollars on the Amazon cloud. ObjectiveC is an obvious next step, it's quite fun to code in. If you are into data or algorithms, then you need to figure out how to use Hadoop. There is an HPC cluster on a 10Gbit ethernet interconnect from Amazon now. Adrianco is a freelance writer and blogger based in London. He is best known for his work with the London 2012 Paralympic Games. Follow Adrian on Twitter and LinkedIn."
      },
      "tags": [
        "architecture",
        "cloud",
        "hpc",
        "infrastructure",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_9e6465bf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "FON: World WiFi Domination?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/fon-world-wifi-domination.html",
      "content": {
        "text": "Now if there ever was a company working on world domination, it would be FON. They aim to build a global network of Wifi hotspots by selling subsidized routers that have custom firmware. The routers have a private secure WPA based network for you to use, and a public network that has a signup page, for anyone else to use. There is also a control that limits how much bandwidth the public can use. They have a few hundred thousand users, and are now the largest WiFi network by far, but the location of their base stations is much more random and less useful than T-Mobile HotSpots.<br /><br />The incentive in the short term is that you can get a high quality router at a very low price. FON has been giving routers away to seed markets and to surround Starbucks stores. The incentive in the longer term is that if you have a FON router at home, you can use any FON router you find on your travels for free.<br /><br />I've had a FON router for some time. I met CEO Martin Varsavsky when he gave a talk at eBay last year, and got one of the initial Linksys units with custom firmware. When their own hardware solution came out it had a much nicer feature set, so I upgraded to La Fonera as they call it. I've been using it as my primary home router for about six months now, and its an excellent design. Its been much more reliable than the other low cost routers I've owned (Netgear and Linksys) and works as well as the much more expensive Apple Airport Express that I've had for a long time.<br /><br />FON just released a booster antenna \"La Fontenna\", it is designed to give 6.5dB of extra signal strength and project further out than the standard antenna. The idea is that you mount it on the window facing the street, and it covers a larger area. I just got one, and while it seems to work fine, it doesn't appear to be giving me as much signal strength or range as I expected. I wandered around outside with my MacBook using iStumbler's WiFi monitoring widget to track signal strength, and it showed a bit more gain than my Airport. I think I need to experiment a bit more with positioning La Fontenna.<br /><br />So next time you need a router, don't just buy a normal one, join Fon and support world domination!",
        "metadata": {
          "word_count": 415,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "FON aim to build a global network of Wifi hotspots by selling subsidized routers that have custom firmware. The routers have a private secure WPA based network for you to use, and a public network that has a signup page, for anyone else to use. FON just released a booster antenna \"La Fontenna\", it is designed to give 6.5dB of extra signal strength and project further out than the standard antenna. I just got one, and while it seems to work fine, it doesn't appear to be giving me as much signal strength or range as I expected. So next time you need a router, don't just buy a normal one, join Fon and support world domination. nk I need to experiment a bit more with positioning La Fontenna."
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_9fefe3dd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "XE Toolkit 1.2 - new portable performance tools from Rich Pettit",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/04/xe-toolkit-12-new-portable-performance.html",
      "content": {
        "text": "Rich just updated the portable Java based XE tookit, it even runs on mainframes now :-)<br /><br />Rich (with some contributions from me) wrote many versions of the SE toolkit for Solaris. This is his latest project, please try it out, give him feedback and buy a support contract to encourage him...<br /><br />Cheers Adrian<br /><br />Subject: [Xetoolkit-interest]  XE Version 1.2 Available<br />To: xetoolkit-interest@xetoolkit.com<br /><br /><br />XE Version 1.2 is available in pre-packaged form through the Captive<br /> Metrics online store, http://store.captivemetrics.com.<br /><br /> New platforms for 1.2:<br />        AIX (5.3 base)<br />        Linux on Power (IBM pSeries & iSeries)<br />        Linux on s390  (IBM zSeries)<br /><br /> Please report bugs to support@captivemetrics.com.<br /><br /> Changes since Version 1.1<br /><br /> Bug fixes<br />   Solaris locking problems<br />   Performance improvements in mdr<br />   Locale problems in non-US-English Windows installs<br /> Support for AIX (5.3 base)<br /> Support for Linux on Power (IBM pSeries & iSeries)<br /> Support for Linux on s390 (IBM zSeries)<br /> Code restructuring and use of new \"snap\" utility<br />   and command-line build for Windows<br /> VirtualGuru rotates log files when the day changes<br /> bbClient groovy script for monitoring BigBrother clients<br /><br /> The source code is available at<br /> http://sourceforge.net/projects/xe-toolkit as an SVN tree.<br /> There will be a web site with \"tarballs\" eventually. It's not<br /> high on the priority list.<br /><br /> The build process uses \"snap\", a /bin/sh utility written by me that<br /> generates Makefiles on the fly.<br /><br /> Please support future development by purchasing the package.<br /><br /> Thank you for your continued interest.<br /><br /> Rich<br /> --<br /> Richard Pettit<br /> Captive Metrics Software Corporation<br /> Em: richp@captivemetrics.com<br /> Ph: 8-MONITOR-01",
        "metadata": {
          "word_count": 284,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "XE Version 1.2 is available in pre-packaged form through the Captive Metrics online store, http://store.captivemetrics.com. New platforms for 1. 2: AIX (5.3 base) and Linux on Power (IBM pSeries & iSeries) Source code is available at http://sourceforge.net/projects/xe-toolkit as an SVN tree. There will be a web site with \"tarballs\" eventually. The build process uses \"snap\", a /bin/sh utility."
      },
      "tags": [
        "metrics",
        "monitoring",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_078f8cd9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Using iPhone 3.0 beta",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/03/using-iphone-30-beta.html",
      "content": {
        "text": "So far its working well, all the previously installed apps that I have tried work. I did a cut and paste from notes to Facebook, listened to some Pandora, downloaded a new app from the AppStore. I was a bit concerned about jumping on the release so early, but haven't found any downsides yet.<br /><br />I have an older Mac that my first gen iPhone treats as its home machine, I upgraded that one to SDK3.0beta and also used it to do the full backup and restore and re-sync needed to get my iPhone running 3.0. I have a newer MacBook Air that is my work machine, and I have left that with SDK2.2.1, so I can work on current apps and make sure they run on 3.0.",
        "metadata": {
          "word_count": 128,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I was a bit concerned about jumping on the release so early, but haven't found any downsides yet. All the previously installed apps that I have tried work. I did a cut and paste from notes to Facebook, listened to some Pandora, downloaded a new app from the AppStore."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_bfaea06d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Battery Ventures at Strata, Santa Clara",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/02/battery-ventures-at-strata-santa-clara.html",
      "content": {
        "text": "<br />\n<div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"color: black;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"http://strataconf.com/strata2014\">Strata</a>\u2019s theme this year is \u201cMaking Data Work\u201d. The conference is sold out, but some of the sessions will be available&nbsp;<a href=\"http://strataconf.com/strata2014/public/content/video\">as a live stream</a>&nbsp;so you can follow along at home.<o:p></o:p></span></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\">Battery Ventures kicked off the conference the night before, with friends and portfolio companies getting together nearby. &nbsp;For me attending Strata is a great way to keep track of what is going on in the rapidly developing Big Data space.<o:p></o:p></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"color: black;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"x-apple-data-detectors://2\" x-apple-data-detectors-result=\"2\" x-apple-data-detectors-type=\"calendar-event\" x-apple-data-detectors=\"true\">On Wednesday at 1:30pm</a>&nbsp;Mike Dauber of Battery Ventures is moderating&nbsp;<a href=\"mailto:http://strataconf.com/strata2014/public/schedule/detail/33146\">\u201cNot your typical VC panel\u201d</a>. (Follow Mike&nbsp;<a href=\"http://www.twitter.com/dauber\">@dauber</a><span class=\"MsoHyperlink\" style=\"text-decoration: underline;\">)</span><o:p></o:p></span></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt 0.5in;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\"><i>A group of VCs who invest from very early, through later stage investments talk about all things Big Data. There will be no \u201c3 Vs\u201d discussion here. The Panelists are committed to making this a lively discussion about topics ranging from the typical (what sectors do they want to invest in) to the atypical (what\u2019s out there that they don\u2019t like? Which Hadoop Distros are in trouble? Should any entrepreneur care about Data Scientists?).</i><i><o:p></o:p></i></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"color: black;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"x-apple-data-detectors://3\" x-apple-data-detectors-result=\"3\" x-apple-data-detectors-type=\"calendar-event\" x-apple-data-detectors=\"true\">On Thursday at 4:00pm</a>&nbsp;I\u2019m a panelist in<a href=\"http://strataconf.com/strata2014/public/schedule/detail/33544\">\u201cThe Great Debate: Technology Creates More Jobs than it Destroys\u201d</a>. &nbsp;(Follow me<a href=\"http://www.twitter.com/adrianco\">@adrianco</a><span class=\"MsoHyperlink\" style=\"text-decoration: underline;\">)</span><o:p></o:p></span></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt 0.5in;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\"><i>Is technology creating new jobs, and ridding us of drudgery? Or is it spawning an era of rampant unemployment and class divides? That\u2019s what we\u2019ll be debating.</i><i><o:p></o:p></i></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\">Strata also features Battery Ventures portfolio companies as sponsors:<o:p></o:p></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt 0.5in;\">\n<span style=\"color: black;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"http://www.platfora.com/\">Platfora</a>&nbsp;masks the complexity of Hadoop, making it easy for customers to understand all the facts in their business across events, actions, behaviors and time.<o:p></o:p></span></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt 0.5in;\">\n<span style=\"color: black;\"><span style=\"background-color: rgba(255, 255, 255, 0);\"><a href=\"http://www.continuuity.com/\">Continuuity</a>&nbsp;makes it easy for any Java developer to build, deploy, scale and manage Apache Hadoop and HBase applications in the cloud or on-premise.<o:p></o:p></span></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<br /></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\">We look forward to catching up with everyone and finding new and interesting people and products to discuss.<o:p></o:p></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\"><br /></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\nComing up next for me is a trip to the UK. I'm presenting at Cloud Expo Europe and QCon London.&nbsp;</div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\"><br /></span></div>\n<div class=\"MsoNormal\" style=\"margin: 0in 0in 0.0001pt;\">\n<span style=\"background-color: rgba(255, 255, 255, 0);\"><br /></span></div>\n</div>\n<span id=\"OLK_SRC_BODY_SECTION\" style=\"background-color: rgba(255, 255, 255, 0);\"></span><br />\n<div style=\"-webkit-composition-fill-color: rgba(175, 192, 227, 0.235294); -webkit-tap-highlight-color: rgba(26, 26, 26, 0.301961); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; color: black; font-family: Calibri, sans-serif; font-size: 14px; font-style: normal; font-variant: normal; font-weight: 300; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px;\">\n<br /></div>",
        "metadata": {
          "word_count": 622,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Strata conference is sold out, but some of the sessions will be available as a live stream. The theme this year is \u201cMaking Data Work\u2019. A group of VCs who invest from very early, through later stage investments talk about all things Big Data. There will be no \u201c3 Vs\u201d discussion here. The Panelists are committed to making this a li I\u2019m a panelist in \u201cThe Great Debate: Technology Creates More Jobs than it Destroys\u201d on Thursday at 4:00pm. Follow me on Twitter at @adrianco. Strata also features Battery Ventures portfolio companies as sponsors:\u00a0Platfora. Stratfora makes it easy for customers to understand all the facts in their business through Hadoop. Continuuity makes it easy for any Java developer to build, deploy, scale and manage Apache Hadoop and HBase applications in the cloud or on-premise.  across events, actions, behaviors and time. Coming up next for me is a trip to the UK. I'm presenting at Cloud Expo Europe and QCon London. I'll be presenting at QCon in London."
      },
      "tags": [
        "big data",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_d007a2b8",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Looking for a star/ninja/guru developer....",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/08/looking-for-starninjaguru-developer.html",
      "content": {
        "text": "<a href=\"http://jobs.netflix.com/DetailFlix.asp?flix2297\">I'm hiring, looking for someone who regularly gets a \"wow\" response from co-workers:</a><br /><br /><blockquote>As a key engineer/ researcher for our web application development team, you will be working on data mining and algorithm development to build the world\u2019s best movie finding software with some of the brightest people in the E-commerce arena. Our goal is to know our customers\u2019 movie tastes well enough to create the perfect store, always presenting amazing titles just for them.<br /><br />We are looking for a very experienced engineer who can take the lead, researching, modeling, presenting, designing and building algorithms that run in a service oriented architecture to support millions of customers. The successful candidate will be a self-motivated, intellectually curious individual. A Doctorate or experience working in a research environment, work with personalization algorithms, machine learning, adaptive systems, and statistical analysis would be a bonus. <span style=\"font-weight:bold;\">Experience with large scale multithreaded Java applications is essential</span>.<br /><br />This is a hands-on individual contributor role, for example you may need to prototype or model a new concept, communicate it, architect a Hadoop cluster, write the code to load and analyze interesting data and build high performance scalable Java and SQL code to incorporate the results into the web site.</blockquote><br /><br />Netflix is a <span style=\"font-style:italic;\">performance culture</span>. Everyone is located in Los Gatos CA, no teleworking, contractors or offshore development. Pay is at the top end of the scale for the SF Bay Area. Netflix is scaling the company by hiring the very best people in the industry. There are no interns or junior staff, minimal need for processes, we hire fully formed adults and give them <span style=\"font-style:italic;\">freedom with responsibility</span>. Netflix is very focused, small enough to be fun and to know everyone and everything that is going on, yet big enough to be interesting, with a big impact in a very entertaining industry.",
        "metadata": {
          "word_count": 311,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Netflix is looking for a key engineer/ researcher for our web application development team. The successful candidate will be a self-motivated, intellectually curious individual. A Doctorate or experience working in a research environment is a plus. This is a hands-on individual contributor role, for example you may need to prototype or model a new concept, communicate it, architect a Hadoop cluster, write the code to load and analyze interesting data. Netflix is scaling the company by hiring the very best people in the industry. There are no interns or junior staff. \"It's a very entertaining industry. thing that is going on, yet big enough to be interesting, with a big impact,\" he says. \"It's not a big thing, but it's a big enough thing to have an impact\""
      },
      "tags": [
        "architecture",
        "machine learning",
        "netflix",
        "performance",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_97d872e0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "HPTS at Asilomar",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/10/hpts-at-asilomar.html",
      "content": {
        "text": "I'm attending the High Performance Transaction Systems workshop http;//hpts,ws over the next few days. I'm chairing a session on moving to the cloud. Quality crowd - Bechtolsheim, Stonebraker, Patterson, and so on, see the Agenda... Second time for me, it's an honor to be invited back. <div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div><br />",
        "metadata": {
          "word_count": 57,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm attending the High Performance Transaction Systems workshop http;//hpts,ws over the next few days. I'm chairing a session on moving to the cloud. Second time for me, it's an honor to be invited back."
      },
      "tags": [
        "cloud",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_ba7ea7c7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Running Netflix in the (public #aws) cloud",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/08/running-netflix-in-public-aws-cloud.html",
      "content": {
        "text": "my talk at #qconsf in Nov http://qconsf.com/sf2010/presentation/Running+Netflix+in+the+Cloud",
        "metadata": {
          "word_count": 7,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_c07f7f2b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "How busy is your CPU, really?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/10/how-busy-is-your-cpu-really.html",
      "content": {
        "text": "Just in case you thought that you could compare your CPU utilization data across Solaris releases I have a few words of caution.<br /><br />To start with there is the whole problem of interrupts, do they count as system time, or do they just make whatever they interrupted take longer?<br /><br />Then there is the question of wait-for-io, and who is waiting for which io? This is a form of idle time that tends to confuse people as it doesn't really mean anything once you have more than one CPU.<br /><br />There is one mechanism used to report the systemwide CPU utilization data. This data is reported in a per-cpu kstat data structure and is used by every tool that ever reports CPU usr, sys, wio, idle etc. Most tools sum the data over all the CPUs, mpstat gives you the per CPU data. The form of the data is a number of ticks of CPU time that accumulates, starting at zero at boot time. To measure CPU utilization over a time interval, you measure the difference in the number of ticks and divide by the time, and the tick rate. The tick rate is set by the clock interrupt, it defaults to 100Hz, but can be set to 1000Hz.<br /><br />There are two mechanisms used to measure CPU usage, one is the old method of using the clock interrupt to see what is running every 100Hz. This is low resolution, and since the clock wakes up jobs, its possible for jobs to hide between the ticks, so its a statistically biased measure.<br />The other mechanism is microstate accounting, where every change of state is timed with a hires clock. This was only used for tracking CPU usage by processes, and needed special calls to get the data in releases up to Solaris 9.<br /><br />So how do those nice numbers you get from vmstat or your favourite tool vary?<br /><br /><span style=\"font-weight: bold;\">Solaris 8</span> and earlier releases: interrupt time does not get classified as system time<br /><br /><span style=\"font-weight: bold;\">Solaris 9</span>: Interrupt time is counted as system time.<br /><br /><span style=\"font-weight: bold;\">Solaris 10</span>: wait-for-io time will always be zero. The ratio of confusion to enlightenment was too high, so the entire concept has been removed from Solaris. This is a good thing.<br /><br /><span style=\"font-weight: bold;\">Solaris 10</span> initial release: systemwide CPU time is now measured using microstates, this is far more accurate than ticks, but somehow the interrupt time ended up spread out rather than in system time.<br /><br />This is unfortunate, but I'm told that an update of <span style=\"font-weight: bold;\">Solaris 10</span> will again classify interrupts as system time, and then it will all be just about as accurate as it can be.<br /><br />You may be curious about the size of errors in these measurements, the answer is that \"it depends\". There is an SE toolkit script called cpuchk.se that looks at the process data and compares the ticks with the microstate data. However, I don't have any measures of interrupt time differences.",
        "metadata": {
          "word_count": 506,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is one mechanism used to report the systemwide CPU utilization data. Most tools sum the data over all the CPUs, mpstat gives you the per CPU data. To measure CPU utilization over a time interval, you measure the difference in the number of ticks and divide by the time. The tick rate is set by the clock interrupt, it defaults to 100Hz, but can be set to 1000Hz. This was only used for tracking CPU usage by processes, and needed special calls to get the data in Solaris 9. The ratio of confusion to enlightenment was too high, so the entire concept has been removed from Solaris. CPU time is now measured using microstates, this is far more accurate than ticks."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3c7504f5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Interesting hardware for database servers",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/01/interesting-hardware-for-database.html",
      "content": {
        "text": "I've been too occupied on other things to keep posting regularly in the last month. The good news is that I'm learning a lot about some new areas.<br /><br />So what is new in hardware? I think there are some interesting trends in server hardware for running databases. The cost base of a mid-sized Solaris/Oracle server with 32-64GB of RAM is dropping fast. A very common platform in this space has been the 8-way UltraSPARC III based V880, moving to the 12-way V1280 and currently the E2900 (a 24 core V1280 chassis with UltraSPARC IV) over the last few years. Prices vary by configuration, and newer systems give you more performance per $, but are of the order of magnitude of $100K (plus the disk subsystem and software licenses - but thats another topic).<br /><br />The two new entrants in this space are Niagara based systems and Opteron based systems, each has its strengths. When loaded up with RAM the costs are largely dominated by the price of RAM rather than the CPU itself, however both these systems use commonly available DIMMs, rather than the more specialized and expensive memory of the older generation systems.<br /><br />Niagara has everything on one chip, 8 cores and 32 execution threads. The cool thing about this for database is not the low power consumption touted by Sun (which is dwarfed by the disk subsystem for a database application) , but is that any inter-thread locking will be blindingly fast since the signals do not have to go off-chip. Badly behaved applications that are sensitive to high memory latency (the kind that don't scale well on physically bigger systems) will run relatively well. However the cores themselves are not particularly fast and are atrociously slow for anything that does floating point, so single stream performance is not a strength. With 2GB DIMMs you can get 32GB RAM connected to a single Niagara chip, this should move to 64GB using 4GB DIMMs eventually. The performance of a Niagara seems to be a bit better than a V1280, but the cost is much much lower. Software support for SPARC Solaris 10 doesn't seem to be an issue at this point. Most things are supported and the system is compatible with earlier releases of SPARC/Solaris products.<br /><br />The common Opteron systems are two socket/four core with a maximum of 16GB with 2GB DIMMs. There are some four socket and eight socket systems available from several vendors (including Sun), with 32-64GB, moving to 128GB with 4GB DIMMs. The Opteron seems to have performance per GHz in the same ballpark as UltraSPARC systems, so 8 cores at 2.4GHz would be between the performance of a V1280 and E2900. The 32GB 8-Core Opteron systems are in the same order of magnitude for performance and price as a 32GB Niagara, but far faster for single stream work and floating point, and relatively slower for lock intensive workloads where the signals have to move between the Opteron chips. On Opteron the software situation is a little different, there are three possible operating systems - Solaris 10, Linux and Windows 64. Solaris support isn't as good as it is on SPARC, for example Oracle 10g is the only option, the earlier releases of Oracle don't seem to be available. Linux probably has the widest choice for support, but 64bit Linux on larger systems doesn't seem to scale as well as Solaris 10 in my experience. Linux tends to be more efficient than Solaris on 32bit systems (your milage will vary, it depends greatly on what features of the OS your workload hits hard). I don't know anything about Windows 64, but I expect these large Opteron systems will be good SQLserver platforms.<br /><br />Thats what the landscape looks like to me as we go into 2006. I hope to be doing some testing later this year to compare all the options, including Intel's next generation servers, to get my performance and price comparisons to be more precise than the general comments above. I'd be interested to swap experiences with other people moving in this direction.",
        "metadata": {
          "word_count": 682,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The cost base of a mid-sized Solaris/Oracle server with 32-64GB of RAM is dropping fast. Niagara based systems and Opteron based systems, each has its strengths. Niagara has everything on one chip, 8 cores and 32 execution threads. With 2GB DIMMs you can get 32GB RAM connected to a single Niagara chip. The performance of a Niagara seems to be a bit better than a V1280, but the cost is much much lower. The common Opteron systems are two socket/four core with a maximum of 16GB with 2GB DIMMs. There are some four socket and eight socket systems available from several vendors (including Sun), with 32-64GB, moving to 128GB with 4GB D IMMs. 64bit Linux on larger systems doesn't seem to scale as well as Solaris 10 in my experience. Linux tends to be more efficient than Solaris on 32bit systems. I don't know anything about Windows 64, but I expect these large Opteron systems will be good SQLserver platforms."
      },
      "tags": [
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_c5d850e1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Starting a new role at Battery Ventures",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/01/starting-new-role-at-battery-ventures.html",
      "content": {
        "text": "Careful what you wish for. Take some side interests and activities that\naren't central to your real job, have a bit too much fun with them,\nmention to <a href=\"http://www.battery.com/our-team/member/mike-dauber/\" target=\"_blank\">a friend</a> that it would be cool to do them for real, and have\nhim convince his company to create a completely new position tailored for\nyou that they've never had before, and haven't heard of anyone else doing. In a week.<br />\n<br />\nOver the last few years I've given a lot of talks, and the industry&nbsp;reaction to Netflix's all-in public cloud strategy has evolved from <i>\"It&nbsp;won't work\"</i> through <i>\"It only works for unicorns like Netflix\"</i> to <i>\"How do&nbsp;we get there?\"</i> My answer to that question is the Netflix Open Source&nbsp;Software (NetflixOSS) platform, consisting of around forty distinct&nbsp;projects shared at <a href=\"http://netflix.github.com/\" target=\"_blank\">netflix.github.com</a> that provides an on-ramp for&nbsp;applications to move to a cloud native architecture.<br />\n<br />\nI have presented aspects of this work to different audiences at public&nbsp;conferences, private events and directly within companies. For developers,&nbsp;I concentrated on explaining the NetflixOSS components, DevOps and&nbsp;continuous delivery patterns. For managers I concentrated on how an open&nbsp;source and cloud native strategy delivers rapid product evolution,&nbsp;scalability and high availability. As interest from individual companies&nbsp;grew it became clear that there was an opportunity to take what I had&nbsp;learned and apply it directly to a wider audience.<br />\n<div>\n<br /></div>\n<div>\nIn my new role at&nbsp;Battery Ventures I will be continuing my conversations with large&nbsp;enterprise companies planning a move to cloud native, and SaaS vendors who&nbsp;are re-architecting for scale, to understand the gaps and demands of this&nbsp;market. With that context I will work in the Battery Ventures team that is&nbsp;looking for opportunities to fund companies who are enabling the&nbsp;transformation of enterprise IT, and I will also provide advice and&nbsp;mentoring for portfolio companies.<br />\n<br />\nThe role is called <i>Technology Fellow</i>. It's different to the short term&nbsp;Entrepreneur in Residence (EIR) position, and is similar to full time investing staff.&nbsp;Battery just raised their tenth fund, and have a total of $900M to invest.&nbsp;My job is to help spend that wisely and get the best return. That also&nbsp;means even more conference presentations, more international coverage, and&nbsp;a broader range of subjects to discuss.<br />\n<br />\nI will no longer explain how Netflix works, so the starting point for my&nbsp;new presentation materials is going to be something like <a href=\"http://perfcap.blogspot.com/2013/12/velocity-and-volume-speed-wins-keynote.html\" target=\"_blank\">my Flowcon talk</a>&nbsp;with the Netflix specific parts removed. If you still want a Netflix&nbsp;Architecture overview talk, start with <a href=\"https://twitter.com/jedberg\" target=\"_blank\">@jedberg</a>, he's a very highly rated&nbsp;speaker, and he can also connect you to the many excellent speakers at&nbsp;Netflix that cover their own specific areas.<br />\n<br />\nIt\u00b9s going to take me a little while to figure out the new role, but a big&nbsp;part of the job is meeting lots of people. I'm taking bookings for&nbsp;February...<br />\n<br />\n<h4>\nPress Coverage</h4>\n<a href=\"http://www.marketwired.com/press-release/Adrian-Cockcroft-Joins-Battery-Ventures-as-Technology-Fellow-1866835.htm\" target=\"_blank\">Press Release</a> by Battery Ventures<br />\n<a href=\"http://thewholestack.com/2014/01/07/cold-pizza-and-aws-adrian-cockcroft-joins-battery-from-netflix/\" target=\"_blank\">The Full Stack blog post</a> by Mike Dauber of Battery Ventures<br />\n<a href=\"http://www.businessweek.com/articles/2014-01-07/netflix-loses-its-cloud-guru-to-a-vc-firm\" target=\"_blank\">Business Week</a> story by Ashlee Vance<br />\n<a href=\"http://gigaom.com/2014/01/07/netflixs-cloud-architect-adrian-cockcroft-is-leaving-to-join-battery-ventures/\" target=\"_blank\">GigaOm story</a> by Stacy Higginbotham<br />\n<a href=\"http://uk.news.yahoo.com/battery-ventures-lands-netflix-executive-cockcroft-200627603--sector.html\" target=\"_blank\">Reuters story on Yahoo UK</a><br />\n<a href=\"http://www.boston.com/business/innovation/blogs/inside-the-hive/2014/01/07/battery-ventures-hires-netflix-executive/X83gBm94A1MNsQtTyGUsMK/blog.html\" target=\"_blank\">Boston Globe story</a> by Michael Farrell<br />\n<a href=\"http://blogs.wsj.com/venturecapital/2014/01/07/netflix-cloud-expert-joins-battery-ventures-to-help-startups-tablets-expert-departs/\" target=\"_blank\">Wall Street Journal story</a> by Deborah Gage<br />\n<a href=\"http://www.theregister.co.uk/2014/01/08/netflix_cockcroft_battery_ventures/\" target=\"_blank\">The Register story</a> by Jack Clark<br />\n<a href=\"http://venturebeat.com/2014/01/07/netflix-cloud-architect-to-share-his-wisdom-from-new-post-at-venture-firm/\" target=\"_blank\">Venture Beat story</a> by Jordan Novet<br />\n<h4>\nAbout Battery Ventures</h4>\nBattery invests in cutting-edge, category-defining businesses in markets&nbsp;including software and services, Web infrastructure, e-commerce, digital&nbsp;media and industrial technologies. Founded in 1983, the firm backs&nbsp;companies at stages ranging from seed to buyout and invests globally from&nbsp;offices in Boston, Silicon Valley and Israel. To learn more, visit&nbsp;<a href=\"http://ctt.marketwire.com/?release=1062754&amp;id=3586642&amp;type=1&amp;url=http%3a%2f%2fwww.battery.com%2f\" target=\"_blank\">www.battery.com</a>&nbsp;and follow the firm on Twitter <a href=\"https://twitter.com/BatteryVentures\" target=\"_blank\">@BatteryVentures</a>.</div>",
        "metadata": {
          "word_count": 621,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Micheal Dauber is the founder and CEO of battery-maker\u00a0Batteries UK. He is also the co-founder of Netflix Open Source Software (NetflixOSS) The NetflixOSS platform provides an on-ramp for applications to move to a cloud. I have presented aspects of this work to different audiences at public&nbsp;conferences, private events and directly within companies. For developers, I concentrated on explaining the NetflixOSS components, DevOps and continuous delivery patterns. In my new role at Battery Ventures I will be continuing my conversations with large&ngt;enterprise companies planning a move to cloud native, and SaaS vendors who are re-architecting. The role is called <i>Technology Fellow</i>. It's different to the short term Entrepreneur in Residence (EIR) position. Battery just raised their tenth fund, and have a total of $900M to invest. Adrian Cockcroft is a cloud architect at Netflix. He has joined Battery Ventures as a technology fellow. Cockcroft will be working with @jedberg, the company's cloud guru. A cloud architect has joined battery venture firm Batterion Ventures. The cloud architect will help start-ups with tablets. The firm has also hired an executive from GigaOm. Battery invests in cutting-edge, category-defining businesses. Founded in 1983, the firm backs companies at stages ranging from seed to buyout. Battery has offices in Boston, Silicon Valley and Israel."
      },
      "tags": [
        "cloud",
        "cloud native",
        "devops",
        "infrastructure",
        "netflix",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_20e3b098",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Partial and Interval accounting records",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/partial-and-interval-accounting.html",
      "content": {
        "text": "Traditional accounting generates a record only when the process terminates. The current process data from ps contains different information so its not possible to keep the two in sync. Extended accounting includes the <b>wracct</b> command that forces an accounting record to be generated for a process or task. The partial record is tagged differently as shown below, but contains the same information.<br /><br /><pre><br />   106  group-proc-partial              [group of 35 object(s)]<br />  1000   pid                            664<br />...<br /></pre><br /><br />For a process, partial records provide the total resource usage since process creation. For a task, an additional option allows for resource usage over the interval since the previous record was written.",
        "metadata": {
          "word_count": 110,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "extended accounting includes the <b>wracct</b> command that forces an accounting record to be generated for a process or task. Traditional accounting generates a record only when the process terminates. The current process data from ps contains different information so its not possible to keep the two in sync. The partial record is tagged differently as shown below, but contains the same information."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_7cbd1fe5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solving storage tuning problems",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/08/solving-storage-tuning-problems.html",
      "content": {
        "text": "I wrote a while ago about Dave Fisk's <a href=http://www.ortera.com>Ortera Atlas</a> tool for storage analysis. I recently had a chance to use a beta release of Atlas on a real problem, and they are about to do a GA release, its ready for prime time.<br /><br />Like most tools, it can produce masses of numbers and graphs, but compared to other storage analysis tools I've seen it goes further in three ways: <br /><br />1) It collects critically important data that is not provided by the OS<br />2) It processes the data to tell you exactly what is wrong<br />3) It runs heuristics to tell you how to fix the problem<br /><br />I wish more tools spent this much effort on solving the actual problem rather than making pretty graphs that only an expert would understand.<br /><br />What we actually did was run the tool on a pre-production Oracle system using Veritas Filesystem and Volume Manager with Solaris on a SAN connected to a Hitachi storage array. Atlas starts off by looking at all the active processes on the system, and ignoring any that are not doing any I/O. It collects data on which files are being read or written by which process, and what the pattern and sizes are at the system call, file system and device level. You can also set the tool to focus on a set of devices, and gather information on the processes that actually talk to those devices.<br /><br />Atlas immediately pointed out that two volumes had been concatenated to form a filesystem, and that 98% of the accesses were to one of the volumes. It recommended that the volumes be striped together for better overall performance.<br /><br />It also pointed out that some of the I/O accesses were taking two seconds to complete at the filesystem level, but only two milliseconds at the device level. I guessed this was CPU starvation caused by fsflush running flat out on this machine which had over 50GB of RAM. Adding set autoup=600 to /etc/system and rebooting made the problem go away. We also saw this effect in the terminal window, where our typing would stop echoing for a few seconds every now and again. I've been told by Sun that the very latest patches finally fix fsflush so that it can't use a lot of CPU time, so large memory machines will finally work properly without needing this tweak.<br /><br />Finally Atlas showed that the filesystem block size was set too small and Oracle was doing large reads that were being chopped into smaller reads by the filesystem layer before being sent to the device. It gave a specific recommendation for the block size that should be used. Reconfiguring the disks takes a long time to do, but we'll fix it before it goes into production.<br /><br />We could have figured out the concatenation problem using iostat data, but the other two problems are normally invisible, and the topic of what filesystem block size to use can generate masses of discussion and confusion, so having \"virtual Dave Fisk\" tell you what blocksize to use can save a lot of time :-)",
        "metadata": {
          "word_count": 525,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Ortera Atlas is a storage analysis tool. It can produce masses of numbers and graphs. It collects critically important data that is not provided by the OS. It processes the data to tell you exactly what is wrong. It runs heuristics to tell how to fix the problem. Atlas collects data on which files are being read or written by which process, and what the pattern and sizes are at the system call, file system and device level. You can also set the tool to focus on a set of devices, and gather information on the processes that actually talk to those devices. Sun has told me that the latest patches finally fixfsflush so that it can't use a lot of CPU time. Atlas showed that the filesystem block size was set too small. Oracle was doing large reads that were being chopped into smaller reads by the filesystem layer."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5aba40b9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Installation Day 1",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-installation-day-1.html",
      "content": {
        "text": "Here's the house, showing the mounting points and rails on the roof.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHx5MjDRUW8wWNsuiB2ddoLSJrZyi-99ewDYqHO-63ST_WJNV3ElMvI0FEfbO9AwrmZjYT2f7AsYWag3OPkVimjJm9lPtU-NkfcNdRNvXu3sh1MZOYVyCd-wAgwhhHt1cwiEEa/s1600-h/SANY0322.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHx5MjDRUW8wWNsuiB2ddoLSJrZyi-99ewDYqHO-63ST_WJNV3ElMvI0FEfbO9AwrmZjYT2f7AsYWag3OPkVimjJm9lPtU-NkfcNdRNvXu3sh1MZOYVyCd-wAgwhhHt1cwiEEa/s320/SANY0322.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362523905865271922\" /></a><br /><br />Top end of conduit at the roof line.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwQXnC31huiQEw1hsRON5Yw2ke6btin44WU9TdbLT7O40pb4qRiIVKHrKsZeQZcbliJfBxb7NQRSp-mBwi3iykMFR3sZir1wAfYVTu347znf55Qh4qK0ehV9NuWTl2GXoaLI7b/s1600-h/SANY0330.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgwQXnC31huiQEw1hsRON5Yw2ke6btin44WU9TdbLT7O40pb4qRiIVKHrKsZeQZcbliJfBxb7NQRSp-mBwi3iykMFR3sZir1wAfYVTu347znf55Qh4qK0ehV9NuWTl2GXoaLI7b/s320/SANY0330.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524373793582658\" /></a><br /><br />Bottom end of conduit at the basement level.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimxFtDTXxDKWYgjdEOHRkGGK63YF9HX6yvB0fFXfZBVhucbUgE4AAmenCVmoyq2lFwN3nuQrBjqoJKIgtRUAXqE3_X8SioamNAy-uoYhyphenhyphenmbCA9jjKQgwp9xHGoJxy1FwNa3mTG/s1600-h/SANY0327.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEimxFtDTXxDKWYgjdEOHRkGGK63YF9HX6yvB0fFXfZBVhucbUgE4AAmenCVmoyq2lFwN3nuQrBjqoJKIgtRUAXqE3_X8SioamNAy-uoYhyphenhyphenmbCA9jjKQgwp9xHGoJxy1FwNa3mTG/s320/SANY0327.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524266289644930\" /></a><br /><br />Bottom end and A/C shutoff switch on the front of the house at basement/ground level.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiXa_7pOgttuicYtoMQPmZ4mPhZ9LFT9gN9_hHy0btX6iiW4nne9NWAlVioczrpaS7R56VUSmqW1H6IohmuTHPnDZDWC1DlcdbXqJH259DJCX8skP4mFNq2cwa1vsFNZJXO1PN/s1600-h/SANY0326.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgiXa_7pOgttuicYtoMQPmZ4mPhZ9LFT9gN9_hHy0btX6iiW4nne9NWAlVioczrpaS7R56VUSmqW1H6IohmuTHPnDZDWC1DlcdbXqJH259DJCX8skP4mFNq2cwa1vsFNZJXO1PN/s320/SANY0326.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524263881764066\" /></a><br /><br />A/C shutoff switch. Safety rules mandate that this is easily accessible on the outside of the house.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMrV_0jNanp-ZUhTYCFiyoF8RV_BvIlSo9gRjEUmZwZv7M9dZH2VrQdzgblTTRt0eHgPnfW7oB6pZeXhetgCeX__jTna7VjOkhq3iPDL5IekLnWEI-fPwtXo7-JSaGnPwMIGUF/s1600-h/SANY0325.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMrV_0jNanp-ZUhTYCFiyoF8RV_BvIlSo9gRjEUmZwZv7M9dZH2VrQdzgblTTRt0eHgPnfW7oB6pZeXhetgCeX__jTna7VjOkhq3iPDL5IekLnWEI-fPwtXo7-JSaGnPwMIGUF/s320/SANY0325.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524256486342082\" /></a><br /><br />Inside the basement, showing the connection to the back of the A/C cutoff switch on the right. In the middle is the D/C cutoff switch that has the conduit from the roof routed to it. There will be another D/C shutoff switch on the roof itself, again for safety.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVrvYTdmWU7JUjjKXg1pSYz5ADacWSgRfPMI38Zl1YuySxWZtv1xP04vsaGDArhxyZJT0PiyuPOcpOzzZBrFUuB-vyiYtOLqzG1DVutG-XQLwSd0nmoDOAFFBY_MszBvCkU5dy/s1600-h/SANY0329.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjVrvYTdmWU7JUjjKXg1pSYz5ADacWSgRfPMI38Zl1YuySxWZtv1xP04vsaGDArhxyZJT0PiyuPOcpOzzZBrFUuB-vyiYtOLqzG1DVutG-XQLwSd0nmoDOAFFBY_MszBvCkU5dy/s320/SANY0329.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524272813406482\" /></a><br /><br />This is the inverter on the left, and the D/C shutoff switch on the right. The pipe coming up carries the phone line into the house.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4k5DPkesmleuQSBd2hJpRf2SMqdlt5PENc-4-6KXhivAv0LwqcI9yJyiy5ud3fuZrt9nZq1kOOUaPnR8Rc9xZS6pZdRUa5DfLQe2KuAyfk8lMgFQ1aqJIAQINeEo0rMT1-7CR/s1600-h/SANY0328.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4k5DPkesmleuQSBd2hJpRf2SMqdlt5PENc-4-6KXhivAv0LwqcI9yJyiy5ud3fuZrt9nZq1kOOUaPnR8Rc9xZS6pZdRUa5DfLQe2KuAyfk8lMgFQ1aqJIAQINeEo0rMT1-7CR/s320/SANY0328.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5362524272251302658\" /></a>",
        "metadata": {
          "word_count": 260,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Here's the house, showing the mounting points and rails on the roof. Here's the top end of the conduit at the roof line. Bottom end of conduit at the basement level. Bottom end of the conduit in the basement. Bottom of the basement at the bottom of the building. Bottom end and A/C shutoff switch on the front of the house at basement/ground level. Safety rules mandate that this is easily accessible on the outside of the home. Inside the basement, showing the connection to the back of the A/C cutoff switch on the right. In the middle is the D/C cut-off switch that has the conduit from the roof routed to it. This is the inverter on the left, and the D/C shutoff switch on the right. The pipe coming up carries the phone line into the house. \" th: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg4k5DPkesmleuQSBd2hJpRf2SMqdlt5PENc-4-6KXhivAv0LwqcI9yJyiy5ud3fuZrt9nZq1kOOUaPnR8Rc9xZS6pZdRUa5DfLQe2KuAyfk8lMgFQ1aqJIAQINeEo"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_a6d7a261",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CMG06 Conference - Reno December 3-8",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/10/cmg06-conference-reno-december-3-8.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>As usual I'll be attending the <a href=\"http://www.cmg.org\">Computer Measurement Group conference</a> in Reno Nevada this December. I've attended every year since 1994, and its the place where I get an update on the state of the art in Performance Management, and get to mingle with my friends and peers who work on Capacity Planning.</p>  <p>This year I'm presenting three times:</p>  <ol><li>Sunday morning 3 hour seminar on Capacity Planning with Free and Bundled Tools. This is a repeat of last years talk, presented jointly with Mario Jauvin, who covers the Windows OS and Networking related areas. I cover Solaris, Linux and the system oriented tools.</li> <li>Wednesday morning conference paper titled \"Utlization is Virtually useless as a Metric!\". Regular readers of this blog will recognize much of the content of this paper, which gathers together all the ways in which your measurements can be corrupted by virtualization.</li> <li>Thursday morning I'm giving a 3 hour training course called the Unix/Linux CMG Quick Start Course, which is part of a new feature for CMG and is based on the training classes in performance tuning that I have given for many years.</li> </ol> Early bird discounted registration is open until October 13th. The sunday seminars are an extra cost item, but the Thursday morning training classes are included in the regular conference fee. This is the only place I'm planning to give public training classes, and since I'm at the conference all week its a great opportunity to discuss performance and capacity issues in person. I hope to see you there...<br/> <p style=\"font-size:10px;text-align:right;\">technorati tags:<a href=\"http://technorati.com/tag/cmg\" rel=\"tag\">cmg</a>, <a href=\"http://technorati.com/tag/solaris\" rel=\"tag\">solaris</a>, <a href=\"http://technorati.com/tag/performance\" rel=\"tag\">performance</a>, <a href=\"http://technorati.com/tag/capacity\" rel=\"tag\">capacity</a>, <a href=\"http://technorati.com/tag/training\" rel=\"tag\">training</a>, <a href=\"http://technorati.com/tag/seminar\" rel=\"tag\">seminar</a>, <a href=\"http://technorati.com/tag/linux\" rel=\"tag\">linux</a></p><p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com/blogged-with-flock\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 289,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Computer Measurement Group conference takes place in Reno, Nevada in December. I've attended every year since 1994, and its the place where I get an update on the state of the art in Performance Management. Early bird discounted registration is open until October 13th. The sunday seminars are an extra cost item, but the Thursday morning training classes are included in the regular conference fee. I hope to see you there...  ef=\"http://technorati.com/tag/seminar\" rel=\"tag\">seminars\" ef\"target=\"_new\" title=\"Flock\" target=\"_ new\" title='Flock\">Flock</a></p><p style=\"text-align: right; font-size: 8px\">Blogged with <a href \"http://www.flock.com\" target='_new' title=\"flock\" tag= \"flock\" id=\"flock-blogged-with-flock-1\" title=\"Flock-Blogged-With-Flock 1\" id: \"flock_blogged_with_flock"
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_f2bdf792",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "How Netflix gets out of the way of innovation",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/12/how-netflix-gets-out-of-way-of.html",
      "content": {
        "text": "#defrag 2011 presentation script.<br /><br />I'm the cloud architect for Netflix, but rather than tell you about why we moved Netflix to a cloud architecture or how we built our cloud architecture, I'm going to tell you what we do differently at Netflix to create a culture that supports innovation.<br /><br />What is it that lets us get things done very quickly. Sometimes a bit too qwikly\u2026. but how did we keep making big strategic moves, from DVD to streaming, from Datacenter to Public Cloud, from USA only to International, all in very short timescales with a fairly small team of engineers.<br /><br />My presentation slides are just box-shots of movies and TV shows that are available on Netflix streaming. This script is based on the notes I made to figure out what I was going to say for each box shot. If some of you see a show you didn't know we had and want to watch that would make me happy, you can click on the box shot to visit that movie at Netflix, they were all available for streaming in the USA at the time of writing.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/How_the_Universe_Works/70211549\"><img src=\"http://cdn-9.nflximg.com/en_us/boxshots/tv_sdp_s/70211549.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />I've attempted to match the box shots loosely as cues to what I'm saying, but I've also used a musical theme in places since this is for Defrag and Defrag rocks!<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Amazon_IMAX/70000637\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/70000637.jpg\" border=\"0\"></a></center><br /><br />Netflix is now one of the largest sites that runs almost entirely on public cloud infrastructure. We have become a poster child for how to build an architecture that takes full advantage of the Amazon Web Services cloud. But when I talk to other large companies about what we have done, they seem to have a lot of reasons why they couldn't or didn't do what we did, even if they wanted to.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/The_Walking_Dead/70177057\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/tv_sdp_s/70177057.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Why is that? Why are we heading in one direction while everyone else is going the other way? Are we crazy or are they zombies? Well, I've worked at other large companies so I have some perspective on the issues.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/The_Story_of_1/70046548\"><img src=\"http://cdn-8.nflximg.com/en_us/boxshots/ghd/70046548.jpg\" border=\"0\"></a></center><br /><br />Before I joined Netflix I worked at eBay for a few years, and helped found eBay Research Labs. This was setup because eBay felt it wasn't innovating fast enough, and they were looking for the one missing ingredient that would drive more innovation into the company.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Beer_Pong_Saved_My_Life/70183286\"><img src=\"http://cdn-6.nflximg.com/en_us/boxshots/ghd/70183286.jpg\" border=\"0\"></a></center><br /><br />This is a fairly common approach. <span style=\"font-style:italic;\">\"You guys go and be innovative, then hopefully we will find ways to spread it around a bit.\"</span> Unfortunately the end result of setting up a separate group to add innovation to a big company is more comical than useful.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Airplane/60002777\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/60002777.jpg\" border=\"0\"></a></center><br /><br />The most interesting projects got tied in knots, they trod on too many toes or were scary. We visited Xerox Parc and IBM Santa Teresa Labs to discuss how they were setup, to try and learn what might work., and we went to an Innovation Forum in New York. That was weird, some of the primary examples they were talking about emulating were eBay and Paypal! <br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Gnomeo_and_Juliet/70140917\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/70140917.jpg\" border=\"0\"></a></center><br /><br />The projects that did get out were minor tweaks to existing ideas, they could be fun, but ultimately not very interesting.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Breaking_Bad/70143836\"><img src=\"http://cdn-6.nflximg.com/en_us/boxshots/tv_sdp_s/70143836.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />So I had to break out of there and find something new to do, and in 2007 I joined Netflix just as they first launched streaming.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Being_John_Malkovich/28363298\"><img src=\"http://cdn-8.nflximg.com/en_us/boxshots/ghd/28363298.jpg\" border=\"0\"></a></center><br /><br />One of the key attractions for me was the Netflix culture I heard about in the interviews, I wanted to get inside their heads and figure out if what they were describing was real, and if so, was it sustainable as the company grew.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Strange_Culture/70066357\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/70066357.jpg\" border=\"0\"></a></center><br /><br />What I found out over the next few years is that the culture is what enables innovation, so that Netflix can get things done quickly that other companies are too scared or too slow to try. The rest of this talk is about the key things that we do differently at Netflix.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Dilbert/70202747\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/tv_sdp_s/70202747.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Before I get into them I want to warn you that even with a roadmap and a guide, you probably won't be able to follow this path if you are in a large established company. Your existing culture won't let you. However if you are creating a new company from scratch, I hope you can join me in what I hope is the future of cool places to work.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Red_Dwarf/70140428\"><img src=\"http://cdn-8.nflximg.com/en_us/boxshots/tv_sdp_s/70140428.jpg \" border=\"0\"  width=\"420\"></a></center><br /><br />Here's the key insight. It's the things you don't do that make the difference. You don't add innovation to a company culture, you get out of its way.<br /><br />I'm mostly going for SciFi at this point, because it's going to sound like I was beamed in from the future to some of you.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Futurama/70153380\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/tv_sdp_s/70153380.jpg\" width=\"420\" \" border=\"0\"></a></center><br /><br />Let me repeat that. You have to setup a company that doesn't do many of the things you would consider business as usual. That's why it's so hard to retrofit.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Madonna_Truth_or_Dare/1068501\"><img src=\"http://cdn-1.nflximg.com/en_us/boxshots/ghd/1068501.jpg\" border=\"0\"></a></center><br /><br />How about some audience participation? Hands up everyone who hates answering questions by putting their hands up..<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Mega_Shark_Versus_Giant_Octopus/70117672\"><img src=\"http://cdn-2.nflximg.com/en_us/boxshots/ghd/70117672.jpg\" border=\"0\"></a></center><br /><br />Who works at a company that has more than one product line? Do you get along? The problem is that the company loses focus and has trouble allocating resources where they are needed so there are big fights. Pick one big thing and do it well. For Netflix, our addressable market is everyone in the universe who likes watching movies and TV shows, that should keep us busy for a while.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Star_Trek_The_Next_Generation/70158329\"><img src=\"http://cdn-9.nflximg.com/en_us/boxshots/tv_sdp_s/70158329.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Who has teams spread over multiple sites and countries? We don't. It adds communication and synchronization overhead that slows your organization down. For the geeks, think of Amdahl's law applied to people. We have as many people as possible in the same building on the same site. We are planning a new bigger building to make sure we can keep everyone close together. High bandwidth, low latency communication.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Take_Me_Home_Tonight/70117577\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/70117577.jpg\" border=\"0\"></a></center><br /><br />Who's worked for a place that bought another company, then run it into the ground, laid everyone off and wrote down the value. Over and over again. It's crazy. I don't think Netflix has ever bought another company. It's a huge disruption to the culture, if you see something you like just hire away their best people and out execute them in the market.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Iron_Man_2/70111470\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/ghd/70111470.jpg\" border=\"0\"></a></center><br /><br />Who has junior engineers, graduate hires and interns writing code? We don't. We find that engineers who cost twice as much are far more than twice as productive, and need much less management overhead. <span style=\"font-weight:bold;\">Reducing management overhead</span> is a key enabler for an innovative culture. Engineers who don't need to be managed are worth paying extra for. We are tiny compared to companies like Google, they take on raw talent and develop it, we sometimes take a chance on someone with only five years experience.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Tron_Legacy/70124805\"><img src=\"http://cdn-5.nflximg.com/en_us/boxshots/ghd/70124805.jpg\" border=\"0\"></a></center><br /><br />Who has an architecture review board and centralized coding standards? We don't have that either. What we do have is tooling that creates a path of least resistance, which combined with peer pressure keeps quality high. The engineers are free and responsible for figuring it out for themselves.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/The_IT_Crowd/70140450\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/tv_sdp_s/70140450.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Who has an ITops team that owns keeping code running in production? We don't. The developers run what they wrote. Everyone's cell phone is in the pagerduty rota, the trick is making sure you don't need to get called. All the ops people here have horror stories of stupid developers, and vice versa, but it doesn't have to be that way. We have one dev organization that does everything and no IT ops org involvement in our AWS cloud deployment.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Star_Trek_Deep_Space_Nine/70158330\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/tv_sdp_s/70158330.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Who has to ask permission before deploying 100's or 1000's of servers? We don't. The developers use our portal directly, they have to file a Change Management ticket to record what they did if it's in production, that's all. We've trained our developers to operate their own code. We create and destroy up to 1000 servers a day, just pushing new code. AWS takes about 5 minutes to allocate 100 servers, it takes longer than that just to boot Linux on them.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/30-Second_Bunnies_Theatre/70142341\"><img src=\"http://cdn-1.nflximg.com/en_us/boxshots/tv_sdp_s/70142341.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Who has a centralized push cycle and has to wait for the next \"train\" before they can ship their code? We don't. Every team manages their own release schedule. New code updates frequently, and the pace slows for mature services. Teams are responsible for managing interface evolution and dependencies themselves. Freedom and responsibility again.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Battlestar_Galactica/70136119\"><img src=\"http://cdn-9.nflximg.com/en_us/boxshots/tv_sdp_s/70136119.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Who has project managers tracking deliverables? We don't. The line managers do it themselves. They own the resources and set the context for their teams. They have time to do this because we took the BS out of their role.<br />Managers have to be great at hiring, technical and hands on enough to architect what their team does, and project manage to deliver results. Don't split this into three people. Reduce management overhead, minimize BS and time wasted. Teams are typically 3-7 people. Have a weekly team meeting and 1on1 with each engineer to maintain context.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Metropolis_Restored/70132372\"><img src=\"http://cdn-2.nflximg.com/en_us/boxshots/ghd/70132372.jpg\" border=\"0\"></a></center><br /><br />Who has a single standard for development tools? We don't. We assume developers already know how to make themselves productive. We provide some common patterns to get new hires started, like Eclipse, IntelliJ, on Mac, Windows. Some people use Emacs on Linux. Hire experienced engineers who care, and they will take care of code quality and standards without being told how to.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Dumb_and_Dumber_Unrated/70043038\"><img src=\"http://cdn-8.nflximg.com/en_us/boxshots/ghd/70043038.jpg\" border=\"0\"></a></center><br /><br />Who has to work with people they don't respect? It's much too disruptive. The only way to get high talent density is to get rid of the people who are out of their depth or coasting.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Ugly_Americans/70195797\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/tv_sdp_s/70195797.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />That also applies to what you might call brilliant jerks. Even if they do great work, the culture can't tolerate prima donna anti-social behavior, so people who don't trust others or share what they know don't fit in.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Shaun_the_Sheep/70155567\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/tv_sdp_s/70155567.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />So does that mean we value conformity? No but it's really important to be comfortable as part of a high performance team, so we look for people who naturally over-communicate and have a secure confident personality type.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Top_Gear_U.K./70140457\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/tv_sdp_s/70140457.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />If you haven't experienced a high performance culture, think about what it's like to drive flat out at a race track. Some people will be too scared to deal with it and drive around white knuckled at 40 mph, some will be overconfident and crash on the first corner, but for people who fit into the high performance culture it's exhilarating to push yourself to go faster each lap, and learn from your peers without a speed limit. When you take out the BS and friction, everyone gets so much more done that productivity, innovation and rapid agile development just happen. This is the key message, removing obstacles to a high performance culture is how innovation happens throughout an organization. Doing less to get more.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/A_Hard_Day_s_Night_Collector_s_Series/60023947\"><img src=\"http://cdn-7.nflximg.com/en_us/boxshots/ghd/60023947.jpg\" border=\"0\"></a></center><br /><br />We don't pay bonuses. We don't have grades other than senior engineer, manager, director, VP. We don't count the hours or the vacation days, we say \"take some\". Once a year we revise everyones salary to their peers and current market rate - based on what we are paying now to hire the best people we can find.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Mad_Men/70136135\"><img src=\"http://cdn-5.nflximg.com/en_us/boxshots/tv_sdp_s/70136135.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />We also have what sounds like a crazy stock option plan that grants options every month, vests the same day, and they last 10 years even if you leave Netflix. The net of this is less work for managers, they can concentrate on hiring top people, and almost everyone that leaves takes a pay cut. The test we make is \"would you fight to keep your engineers if they tried to leave\". If not, let them go now and get someone better. We don't make it hard to let people go.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Capitalism_A_Love_Story/70122701\"><img src=\"http://cdn-1.nflximg.com/en_us/boxshots/ghd/70122701.jpg\" border=\"0\"></a></center><br /><br />Some of you may be thinking this sounds expensive, but what is the value of being incredibly productive and able to move faster than your competition? You can get out ahead and establish a leading position before anyone else realizes you are even in the game. Remember how a few years ago the \"Analysts\" said that Netflix the DVD company was going to get killed by other companies streaming, then all of a sudden people realized that we were streaming more bandwidth than anyone else?<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Meteor_Apocalypse/70130480\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/ghd/70130480.jpg\" border=\"0\"></a></center><br /><br />So what could possibly go wrong? We had a near miss recently, we went too fast, partly because we could, got unlucky and screwed up. The good thing is that Netflix could re-plan and execute on the fixes we need very quickly as well, with no internal angst and finger-pointing. Also there was an Asteroid nearby earlier this week. By the way, my stepdaughter @raedioactive was the art director for this movie.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Eric_Clapton_Crossroads_Guitar_Festival_2010/70213186\"><img src=\"http://cdn-6.nflximg.com/en_us/boxshots/tv_sdp_s/70213186.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />So, you are at a crossroads, you could be on stage with Eric Clapton, or in the audience watching and wondering why you can't do what they are doing. It's a radically different way to construct a corporate culture, it doesn't work for everyone, and we can't all be up on stage with Eric, but the talent is out there if you start by building a culture focused on talent density to find it and keep it.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/The_Men_Who_Stare_at_Goats/70117293\"><img src=\"http://cdn-3.nflximg.com/en_us/boxshots/ghd/70117293.jpg\" border=\"0\"></a></center><br /><br />Is it going to be the goats or the glory? I just told you all to stop doing things, what could be easier than that? It takes less process, fewer rules and simpler principles. Give people freedom, hold them responsible, replace the ones that can't or won't perform in that environment. Focus on talent density and conserving management attention span by removing the BS from their jobs.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/The_Blues_Brothers/60036233\"><img src=\"http://cdn-3.nflximg.com/en_us/boxshots/ghd/60036233.jpg\" border=\"0\"></a></center><br /><br />This is your challenge, can you get a band together and go on a mission to save your company? Stop doing all the things that are slowing you down, and get rid of the unproductive BS that clogs up your management and engineers.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Roast_of_Charlie_Sheen/70211794\"><img src=\"http://cdn-4.nflximg.com/en_us/boxshots/ghd/70211794.jpg\" border=\"0\"></a></center><br /><br />I will take questions in the comments or on twitter to <a href=http://www.twitter.com/adrianco>@adrianco</a>. Thank you.<br /><br /><center><a href=\"http://movies.netflix.com/WiMovie/Manswers/70215420\"><img src=\"http://cdn-0.nflximg.com/en_us/boxshots/tv_sdp_s/70215420.jpg\" border=\"0\"  width=\"420\"></a></center><br /><br />Each question got a new box shot, but all the answers were musical.",
        "metadata": {
          "word_count": 2516,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "#defrag 2011 presentation script: Netflix's cloud architect explains how they get things done quickly. Box-shots of movies and TV shows that are available on Netflix streaming. Netflix is now one of the largest sites that runs almost entirely on public cloud infrastructure. We have become a poster child for how to build an architecture that takes full advantage of the Amazon Web Services cloud. When I talk to other large companies about what we have done, they seem to have a lot of reasons why they couldn't or didn't do what we did. Before he joined Netflix I worked at eBay for a few years, and helped found eBay Research Labs. eBay felt it wasn't innovating fast enough, and they were looking for the one missing ingredient that would drive more innovation into the company. The most interesting projects got tied in knots, they trod on too many toes or were scary. We visited Xerox Parc and IBM Santa Teresa Labs to discuss how they were setup, to try and learn what might work. The projects that did get out were minor tweaks to existing ideas, they could be fun, but ultimately not very interesting. In 2007 I joined Netflix just as they first launched streaming. I wanted to get inside their heads and figure out if what they were describing was real, and if so, was it sustainable as the company grew. The culture is what enables innovation, so that Netflix can get things done quickly. The rest of this talk is about the key things that we do differently at Netflix. Even with a roadmap and a guide, you probably won't be able to follow this path if you are in a large established company. You don't add innovation to a company culture, you get out of its way. You have to setup a company that doesn't do many of the things you would consider business as usual. For Netflix, our addressable market is everyone in the universe who likes watching movies and TV shows. For the geeks, think of Amdahl's law applied to people. We have as ma Netflix is planning a new bigger building to make sure we can keep everyone close together. High bandwidth, low latency communication. I don't think Netflix has ever bought another company. It's a huge disruption to the culture. Reducing management overhead is a key enabler for an innovative culture. Engineers who don't need to be managed are worth paying extra for. We don't have an architecture review board and centralized coding standards. We have one dev organization that does everything and no IT ops org involvement in our AWS cloud deployment. The developers use our portal directly, they have to file a Change Management ticket to record what they did if it's in production. AWS takes about 5 minutes to allocate 100 servers, it takes longer than that just to boot Linux on them. We create and destroy up to 1000 servers a day, just pushing new code. Managers have to be great at hiring, technical and hands on enough to architect what their team does, and project manage to deliver results. Have a weekly team meeting and 1on1 with each engineer to maintain context. The only way to get high talent density is to get rid of the people who are out of their depth or coasting. Even if they do great work, the culture can't tolerate prima donna anti-social behavior. It's exhilarating to push yourself to go faster each lap, and learn from your peers without a speed limit. When you take out the BS and friction, everyone gets so much more done that productivity increases. We don't pay bonuses. We don't count the hours or the vacation days, we say \"take some\". Once a year we revise everyones salary to their peers and current market rate. We also have what sounds like a crazy stock option plan that grants options every month, ves. Netflix is a streaming video company. The company has been able to establish itself as a leader in the field of streaming. This has allowed the company to get out ahead of its competition. Netflix's new movie, Meteor Apocalypto, stars Eric Clapton as a man at a crossroads. The movie is based on a real-life event that took place in 2010. The talent is out there if you start by building a culture focused on talent density to find it and keep it. Give people freedom, hold them responsible, replace the ones that can't or won't perform in that environment. It takes less process, fewer rules and simpler principles. This is your challenge, can you get a band together and go on a mission to save your company? Stop doing all the things that are slowing you down, and get rid of the unproductive BS that clogs up your management and engineers. I will take questions in the comments or on twitter to <a href=http://www.twitter.com/adrianco>."
      },
      "tags": [
        "agile",
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_364b3320",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "WWJD - What Would Jobs Do...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/10/wwjd-what-would-jobs-do.html",
      "content": {
        "text": "Somewhere in Apple there is a product team with a plan. We now have observed several steps unfolding in this plan, so I'm going to put myself in their place and try to make sense of the plan so far and where it could be going.<br /><br />I have *no inside knowledge* of anything going on at Apple, this is all conjecture, OK?<br /><br />Product teams have four big things that can be traded off against each other to some extent.<div><ol id=\"\"><li>The release date</li><li>The available resources</li><li>The feature set at launch</li><li>The product quality at launch</li></ol><div>For the iPhone, they waited until they had working prototypes in January, and set a release date for June that they kept to. The product quality at Apple has to be very high, and I think they did a great job with the iPhone at launch. However, products always take longer than planned, and we know that Apple delayed the launch of MacOSX Leopard so that they could put extra resources into the iPhone, so it was probably slipping at that point. The one thing that I think they were able to trade down to get a high quality product on the release date was the feature set. This is why I regard the iPhone as an unfinished product.\u00a0To get a high quality product, you can simplify testing by having fewer features and fewer options in those features. This is the Minimum Marketable Features (MMF) approach I have mentioned before. Some simplifications at launch:</div><div><ul id=\"\"><li>US Market Only - no internationalization</li><li>Basic camera support, no video capture (not even demo'd at the launch event)</li><li>Relatively few preferences and options</li></ul><div>Apple appeared to schedule bugfix and minor features releases shortly after launch (1.0.1 and 1.0.2), while concentrating development and testing to include internationalization for the worldwide rollout of the sister product, the iPod Touch and the international iPhone with 1.1.1. The Starbucks feature of the iPhone music store has its own roll-out issues, so it makes sense to add that feature as a major update rather than on initial launch.</div><div><br /></div><div>I expect that Apple thought they had locked down the phone well enough in its first release and were surprised by the speed and extent of application and SIM based hacking. Adding digital signatures to prevent hacking in the initial launch version would have also made the final stages of debug and testing harder, so it makes sense that they didn't do it. Once Apple realized that they needed to lock down the phone to take back control of their product feature rollout plan, they \u00a0had to find extra resources to develop and test the extra security features. This had to steal resources from something else, so I expect that some features originally planned for 1.0.2 and 1.1.1 have been delayed.</div><div><br /></div><div>Opening up a product for developers is a lot of work if you do it right. There are developer specific tools, APIs that need to be stabilized, extended and documented; training classes and developer support organizations to put together. We already know that Apple was pulling resources from Leopard development just to get the iPhone out of the door. We also know that Apple already has a developer support organization, but that organization already has its hands full trying to get Leopard out of the door as a high quality product with a lot of new features that need to be documented etc. Leopard ships this month, so Apple's developer support organization should now have some spare resources to focus its attention on doing the work required to do proper high quality iPhone developer features and documentation. Based on this analysis, the earliest I would expect to announce developer support is MacWorld in January, and that plan could have been set over a year ago. I also think Apple underestimated the demand from iPhone developers, but had no spare resources to do anything about it other than emphasizing Safari based applications.</div><div><br /></div><div>The iPod Touch has a smaller set of applications that have been fully internationalized. The extra applications on the iPhone are the US specific ones. I still think this is a transitional problem, and I expect that a minor update in October or November will upgrade the iPhone to have more extensive internationalized support, and that the non-phone-specific applications will all end up on the iPod Touch as well. If I was planning the release calendar, I would bring out one new application in each release, along with a set of less exciting upgrades like internationalization.</div><div><br /></div><div>There are three empty slots in the iPhone user interface. I think they will all be filled by the time we get to January's release. One will be the Finder, so that users can store and browse files and applications, one could be an iWork launcher to highlight some of Apple's own business oriented applications, and January's icon could be related to installing and managing third party applications, hopefully including Flash and Java.</div><div><br /></div><div>Its possible that iChat could merge with the SMS tool, and I would expect video support to be a feature of the camera tool. I would like a terminal with ssh support, but that's not mainstream enough to get its own icon....</div><div><br /></div><div>Of course, Steve calls the shots, but what do you think the three remaining icons will be used for? Let's collect some suggestions in the comments.</div><div><br /></div></div></div>",
        "metadata": {
          "word_count": 889,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Apple has a product team with a plan. We now have observed several steps unfolding in this plan. I'm going to put myself in their place and try to make sense of the plan so far. Apple appeared to schedule bugfix and minor features releases shortly after launch. The Starbucks feature of the iPhone music store has its own roll-out issues. This is why I regard the iPhone as an unfinished product. I expect that Apple thought they had locked down the phone well enough in its first release and were surprised by the speed and extent of application and SIM based hacking. Adding digital signatures to prevent hacking in the initial launch version would have also made the final stages of debug and testing harder. Apple was pulling resources from Leopard development just to get the iPhone out of the door. Leopard ships this month, so Apple's developer support organization should now have some spare resources to focus its attention on doing proper high quality iPhone developer features. There are three empty slots in the iPhone user interface. I think they will all be filled by the time we get to January's release. January's icon could be related to installing and managing third party applications, hopefully including Flash and Java. I would expect video support to be a feature of the camera tool. could merge with the SMS tool. I would like a terminal with ssh support, but that's not mainstream enough to get its own icon."
      },
      "tags": [
        "security",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_edc3e15c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Looking back at 2012, with pointers to 2013",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/01/looking-back-at-2012-with-pointers-to.html",
      "content": {
        "text": "A collection of things that seem to have pivoted in 2012.<br />\n<h4>\nMobile Bandwidth Greater than Fixed Bandwidth</h4>\nI've been talking about <a href=\"http://perfcap.blogspot.com/2010/02/bil2010-millicomputing-future-in-your.html\" target=\"_blank\">LTE and the growth in mobile since 2008</a>, but I started 2012 with a Verizon iPhone 4 which maxed out at 2Mbit/s over 3G and at home in the mountains I would get less than 1Mbit/s. I ended 2012 with a Verizon iPhone 5 which is about ten times faster at home, I regularly see 8-9Mbits/s, and the best speed I have seen anywhere so far was in downtown Los Gatos at over 50Mbit/s. My home fixed wire Internet is a 3Mbit/s DSL that has neighborhood congestion at peak times. I now find it works better to have WiFi turned off on my iPhone at home.<br />\n<br />\nThis is one of those pivotal changes, similar to the change from having predominantly fixed wire telephone service at home, to having many people use mobile phones exclusively. It costs more, but if you already have a high bandwidth connection to your phone with a high data cap because you use it a lot, why pay to also have a low bandwidth connection to your house? Bandwidth caps and data usage plans will slow the switchover, but the writing is on the wall.<br />\n<br />\n<h4>\nCutting The Cable/Satellite TV Feed</h4>\nIn 2013 we finally turned off our TiVo and shut down our DirecTV account. We weren't using it enough to make it worth while. For some of the sports events (Laurel follows the Stanford Cardinals), we go to a sports bar to watch, which is more fun anyway. Everything else that we have time to watch, we can watch online, and we get all our news updates from Twitter, RSS feeds and Facebook posts. By the time it's on TV or in a newspaper, it's already old news.<br />\n<br />\nThe TV has an AppleTV connected to it, which gets almost all the usage. We watch a few things on laptops, and sometimes I connect a laptop to the TV. I also stream music from my iPhone to the AppleTV because I can't get Pandora or Spotify on it. Come on Apple, where's the AppleTV App Store? Maybe that's a 2013 thing.<br />\n<br />\n<h4>\nThe Netflix Open Source Cloud Platform Got Traction</h4>\nWe started the year with a handful of disconnected projects, and ended it with a large chunk of the platform on Github, and some high profile users. Most people are still picking it up piecemeal but in 2013 we plan to get the whole thing put together as an installable bundle. This is the Alan Kay approach, \"The best way to predict the future is to invent it\". &nbsp;Netflix has been out in front of the industry in terms of cloud adoption, inventing the future. Next we make it easier for others to join us in that future, and have some ideas for how to drive adoption to new heights.<br />\n<br />\n<h4>\nNetflix Cloud Architecture Presentations</h4>\nI was going to list all the talks I gave, but there are too many, so go see the slides I posted at <a href=\"http://www.slideshare.net/adrianco\" target=\"_blank\">http://www.slideshare.net/adrianco</a>. Highlights were QConSF, QConLondon, Gluecon in Colorado, GOTO in Aarhus and of course <a href=\"http://techblog.netflix.com/2012/12/videos-of-netflix-talks-at-aws-reinvent.html\" target=\"_blank\">AWS Re:Invent in Las Vegas</a>. The impact of these talks grew through the year, reaching a peak at Re:Invent, where we had lots of speakers and attention to the way the Netflix cloud and open source story was bringing value to the company and reaching out into the technical community. A big thanks to everyone who came to my talks, and all the other Netflix speakers who have been out there broadening the story. It's almost impossible to write an article or do a presentation about cloud without mentioning Netflix. In 2013 there will be even more talks, I focus on local and US based events that are strongly developer oriented like <a href=\"http://www.infoq.com/\" target=\"_blank\">QCon</a>,&nbsp;<a href=\"http://gluecon.com/\" target=\"_blank\">Gluecon</a>, and&nbsp;<a href=\"http://gotocon.com/\" target=\"_blank\">GOTO</a>. We will definitely be back at AWS Re:Invent next November.<br />\n<br />\n<h4>\nThe Concept of Anti-Fragility Took Off</h4>\n<a href=\"http://www.amazon.com/Antifragile-Things-That-Disorder-ebook/dp/B0083DJWGO/ref=tmm_kin_title_0\" target=\"_blank\">Nassim Taleb's latest book</a> crystallizes the way I tend to approach things and gives it a name. The Netflix cloud architecture is anti-fragile, we run \"Chaos Monkey's\" continuously to try and break it, and that makes it stronger. The Netflix culture is anti-fragile, it's decentralized with as little process and rules as possible and a lot of local autonomy. Netflix management is not afraid of change or of being first to do something and tends to navigate disruptive transitions well. From the outside this can look chaotic or confusing, but it works, and recovers well from missteps, which are always going to happen. If you're not failing occasionally you aren't trying hard enough, and you are missing opportunities. Getting stronger through failure is the basis of anti-fragility. Avoiding failure at all costs (as many people try to do) makes you brittle and vulnerable to unexpected Black Swan events that will have a much bigger impact.<br />\n<br />\n<h4>\nCloud, Open Source, SaaS and the End of Enterprise Computing</h4>\nTaleb makes the point that big companies become increasingly fragile as they lose agility and the ability to move with the markets, and we are seeing that play out in the Enterprise Computing space. There is still money to be made from the late adopter customers, but the trend is clearly towards development using exclusively open source tools, with applications and infrastructure delivered as a service. There is zero revenue for traditional Enterprise Computing vendors in this model. The current interest in building out private cloud infrastructure is real and will continue to support traditional vendors into 2013, but it's a short term investment. At best you end up with a much better automated datacenter, but it isn't elastic and it has far fewer features than AWS, so it's going to be marginalized over time. At worst, you discover just how hard it is to run a reliable private cloud based on immature software, with incompatible upgrade paths, and it turns out to be much more expensive to run.<br />\n<br />\nThe Enterprise Computing vendors haven't been able to build a public cloud &nbsp;that competes with AWS on scale, price or features, and AWS is now focused on building everything their customers need to take the next generation of application investment out of the datacenter, so the high margin revenue is going to gradually go away for the traditional vendors.<br />\n<br />\nThe most interesting development in 2012 was the re-launch of Google as a public cloud infrastructure vendor, and the mini-price-war between AWS and Google over instance and storage costs makes it clear where the real action is. During 2013 we will see if Google manages to invest heavily and execute well enough to build up a big user base.&nbsp;&nbsp;In mobile, as I predicted years ago, we are now in an iPhone vs. Android battle that is wiping out everyone else.&nbsp;I personally think in 2014 we will likely see a similar effect as the scale, features and price point of AWS and Google clouds make everyone else irrelevant. The only question in my mind is whether AWS runs away with this on their own, or Google manages to get some traction as the alternative.<br />\n<br />\nNote to sales reps (who won't listen), I'm not interested in anything to do with datacenters, private cloud, or other public clouds in 2013. I'm only interested in SaaS apps, things that run on AWS, and interesting open source projects.<br />\n<br />\n<h4>\nSolar Powered Electrics Cars Are For Real Now</h4>\nWe drive our Nissan Leaf all the time, it's fun to drive and the first car we pick for most trips, adding up to almost 1000 miles a month. The marginal cost of running the car is near zero. New tires and a cabin air filter at 15K miles is all the maintenance it needs. We have an excess of solar power generation that added up to $500 in unused electricity over the year. At 10c/KWh and 3.5KWh/mile that's plenty for us run a second electric car before we start paying for the power, and there are a lot more choices coming in 2013. There are many charging stations around the Bay Area, lots of other people running Leafs, and the Tesla Model S got car of the year awards. It takes a test drive to realize what fun it is to have instant torque and no gear shifts. This is a case of the future being unevenly distributed. If you don't live in California, it's a bit further out, but it's coming.<br />\n<br />\nA friend recently got a quote for Solar Power installation which was about half what we paid two years ago, and we got a good deal then. Prices have dropped fast and are much lower than most people think. If you don't already have solar panels on your roof, you should get them. If you don't use enough electricity to justify solar panels, get an electric car as well, and save at the gas pump.<br />\n<br />\n<h4>\nGlobal Warming Arrived in the USA in 2012</h4>\nThe well funded <a href=\"http://www.amazon.com/Merchants-Doubt-Scientists-Obscured-ebook/dp/B003RRXXO8/ref=tmm_kin_title_0?ie=UTF8&amp;qid=1357067866&amp;sr=8-1\" target=\"_blank\">Merchants of Doubt</a> (read the book) managed to confuse and suppress public discussion of global warming in the USA for the last few years, but the effects just became too obvious this year and it broke through, creating the scenarios that James Hansen warned of in <a href=\"http://www.amazon.com/Storms-My-Grandchildren-Catastrophe-ebook/dp/B002Z8IWLO/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1357068138&amp;sr=1-1&amp;keywords=storms+of+my+grandchildren\" target=\"_blank\">Storms of My Grandchildren</a>. The arctic ice cap melt continues to accelerate, seas are warming and rising, drought and record heat hit most of the USA, and everything wrapped up with Hurricane Sandy, pushing the topic onto the front page. The dice are all loaded now, and 2013 is already rolling those dice as the drought continues and the Mississippi river is empty. I've been saying for the last few years that if you own property at sea level, you should find someone who doesn't believe in global warming to sell it to, because it's going to become increasingly uninsurable and end up as worthless as the houses along the New Jersey shoreline that were swept away.<br />\n<br />\nThe Republican party is still in denial, a combination of funding from big oil companies and an inability to accept or admit that their demonized Al Gore could have been right all along. In 2013 it will be interesting to see how they deal with losing the election, and perhaps there will be a split into a group of Republicans that see the path to re-election in 2014 as needing to accept reality by voting for some Global Warming related legislation, versus the hard core that are trying to pray their way out. The current battle is over stopping the Keystone XL pipeline that would move the dirtiest kind of tar oil from Alberta Canada to Texas. It may be symbolic, but if KXL is stopped, the tide will have turned. <a href=\"http://350.org/\" target=\"_blank\">Carbon needs to be left in the ground</a>. For 2013, I'm going to try and re-balance my 401K retirement accounts to divest from oil companies. Many students are now <a href=\"http://gofossilfree.org/\" target=\"_blank\">pressuring their colleges</a> to divest from oil as well.<br />\n<br />\n<h4>\nTwitter and Snapchat</h4>\nPersonally, 2012 was an excellent year for me, I've made lots of new friends and learned a lot by being active on <a href=\"http://www.twitter.com/adrianco\" target=\"_blank\">twitter</a>, ending the year with about 6500 followers. I joked on twitter that I posted my new years resolutions for 2013 to <a href=\"http://www.snapchat.com/\" target=\"_blank\">Snapchat</a>, but you missed them. If you don't know what Snapchat is for, ask a teenager. You'll probably hear a lot more about it in 2013, then, when their parents figure it out and join too, the teens will be onto the next thing....<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 1996,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This is one of those pivotal changes, similar to the change from having predominantly fixed wire telephone service at home, to having many people use mobile phones exclusively. It costs more, but if you already have  mobile phones, it costs less. In 2013 we finally turned off our TiVo and shut down our DirecTV account. By the time it's on TV or in a newspaper, it's already old news. We watch a few things on laptops, and sometimes I connect a laptop to the TV. In 2013 we plan to get the whole thing put together as an installable bundle. This is the Alan Kay approach, \"The best way to predict the future is to invent it\".Netflix has been out in front of the industry in terms of cloud adoption, inventing the future. The impact of these talks grew through the year, reaching a peak at Re:Invent. In 2013 there will be even more talks, I focus on local and US based events that are strongly developer oriented. The Netflix cloud architecture is anti-fragile, we run \"Chaos Monkey's\" continuously to try and break it, and that makes it stronger. Netflix management is not afraid of change or of being first to do something and tends to navigate disruptive transitions well. Big companies become increasingly fragile as they lose agility and the ability to move with the markets. There is still money to be made from the late adopter customers, but the trend is clearly towards development using exclusively open source tools. The most interesting development in 2012 was the re-launch of Google as a public cloud infrastructure vendor. The mini-price-war between AWS and Google over instance and storage costs makes it clear where the real action is. I think in 2014 we will likely see a similar effect as the scale, features and price point of AWS and Google clouds make everyone else irrelevant. The only question in my mind is whether AWS runs away with this on their own, or Google manages to get some traction as the alternative. I'm not interested in anything to do with datacenters, private cloud, or other public clouds in 2013. At 10c/KWh and 3.5KWh/mile that's plenty for us run a second electric car before we start paying for the power. If you don't use enough electricity to justify solar panels, get an electric car as well, and save at the gas pump. The effects of global warming became too obvious this year and it broke through. The dice are all loaded now, and 2013 is already rolling those dice as the drought continues. The Republican party is still in denial, a combination of funding from big oil companies and an inability to accept or admit that their demonized Al Gore could have been right all along. The current battle is over stopping the Keystone XL pipeline that would move the dirtiest kind of tar oil from Alberta Canada to Texas. If KXL is stopped, the tide will have turned. For 2013, I'm going to try and re-balance my 401K retirement accounts to divest from oil companies. If you don't know what Snapchat is for, ask a teenager. You'll probably hear a lot more about it in 2013, then, when their parents figure it out and join too."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "netflix",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_bddf4e9d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "I come to use clouds, not to build them...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/08/i-come-to-use-clouds-not-to-build-them.html",
      "content": {
        "text": "[Update: Thanks for all the comments and <a href=\"http://gigaom.com/cloud/netflix-amazon-openstack/\">Ryan Lawler's GigaOM summary</a> - also I would like to credit James Urquhart's posting on <a href=\"http://news.cnet.com/8301-19413_3-20098812-240/can-any-cloud-catch-amazon-web-services-part-1/\">Competing With Amazon Part 1</a>. for giving me the impetus to write this.]\n<br />\n<br />My question is what are the alternatives to AWS from a developer perspective, and when might they be useful? However I will digress into a little history to frame the discussion.\n<br />\n<br />There are really two separate threads of development in cloud architectures, the one I care about is how to build applications on top of public cloud infrastructure, the other is about how to build cloud infrastructure itself.\n<br />\n<br />In 1984 I didn't care about how the Zilog Z80 or the Motorola 6809 microprocessors were made, but I built my own home-brew 6809 machine and wrote a code generator for a C compiler because I thought it was the best architecture, and I needed something to distract me from a particularly mind-numbing project at work.\n<br />\n<br />In 1988 I joined Sun Microsystems and was one of the people who could argue in detail how SPARC was better than MIPS or whatever as an instruction set, or how Solaris and OpenLook window system were better. However I never designed a RISC microprocessor, wrote kernel code or developed window systems libraries. I helped customers use them to solve their own problems.\n<br />\n<br />In 1993 I moved to the USA and worked to help customers scale their applications on the new big multiprocessor systems that Sun had just launched. I didn't re-write the operating system myself, but I figured out how to measure it and explain how to get good performance in some books I wrote at the time.\n<br />\n<br />In 1995 when Java was released and the WWW was taking off, I didn't work on the Java implementation or IETF standards body, I helped people to figure out how to use Java and to get the first web servers to scale, so they could build new kinds of applications.\n<br />\n<br />In 2004 I made a career change to move from the Enterprise Computing market place with Sun to the Consumer Web Services space with eBay. At the time eBay was among the first sites to have a public web API. It seemed to me that the interesting innovation was now taking place in the creation and mash-up of web services and APIs, no-one cared about what operating system they ran, what hardware that ran on, or who sold those computers.\n<br />\n<br />Over time, the interesting innovation that matters has moved up the food chain to higher and higher levels of abstraction, leveraging and taking for granted the layers underneath. A few years ago I had to explain to friends who still worked at Sun, how I was completely uninterested in whether my servers ran Linux or Solaris, but I did care what version of Java we were using.\n<br />\n<br />Now I'm working on a cloud architecture for Netflix, we don't really care which Content Delivery Network is used to stream the TV shows over the Internet to the customers, we interchangeably use three vendors at present. I also don't care how the cloud works underneath, I hear that AWS uses Xen, but it's invisible to me. What I do care about is how the cloud behaves, i.e. does it scale and does it have the feature set that I need.\n<br />\n<br />That brings me back to my original question, what are the alternatives to AWS and when might they be useful.\n<br />\n<br />Last week I attended an OpenStack meetup, thinking that I might learn about its feature set, scale and roadmap as an alternative to AWS. However the main objective of the presenters seemed to be to recruit the equivalent of chip designers and kernel developers to help them build out the project itself, and to appeal to IT operations people who want to build and run their own cloud. There was no explanation or outreach to developers who might want to build applications that run on OpenStack.\n<br />\n<br />I managed to get the panel to spend a little while explaining what OpenStack consists of, and figured out a few things. The initial release is only really usable via the AWS clone APIs and doesn't have an integrated authentication system across the features. The \"Diablo\" release this fall should be better integrated and will have native APIs, it is probably suitable for proof of concept implementations by people building private clouds. The \"Essex\" version targeted at March next year is supposed to be the first production oriented release.\n<br />\n<br />There are several topics that I would like to have seen discussed, perhaps people could discuss them in the comments to this blog post? One is a feature set comparison with AWS, and a discussion of whether OpenStack plans to continue to support the AWS clone APIs for equivalent features as it adds them. So far I think OpenStack has a basic EC2 clone and S3 clone, plus some networking and identity management that doesn't map to equivalent AWS APIs.\n<br />\n<br />The point of my history lesson in the introduction is that a few very specialized engineers are needed to build microprocessors, operating systems, servers, datacenters, CDNs and clouds. It's difficult and interesting work, but in the end if its done right it's a commodity that is invisible to developers and their customers. One of the slides proudly showed how many developers OpenStack had, a few hundred, mostly building it from scratch. There wasn't room on the slide to show how many developers AWS has on the same scale. Someone said recently that the far bigger AWS team has more open headcount than the total number of people working on OpenStack. When you consider the developer ecosystem around AWS, there must be hundreds of thousands of developers familiar with AWS concepts and APIs.\n<br />\n<br />Some of the proponents of OpenStack argue that because it's an open source community project it will win in the end. I disagree, the most successful open source projects I can think of have a strong individual leader who spends a lot of time saying no to keep the project on track. Some of the least successful are large multi-vendor industry consortiums.\n<br />\n<br />The analogy that seems to fit is Apple's iOS vs. Google's Android in the smartphone market. The parts of the analogy that resonate with me are that Apple came out first and dominates the market, taking most of the profit and forcing it's competitors to try and band together to compete, changing the rules of the game and creating new products like the iPad that leave their competition floundering. By adding together all the incompatible fragmented Android market together it's possible to claim that Android is selling in a similar volume to iPhone. However it's far harder for developers to build Android apps that work on all devices, and then they usually make much less money from them. Apple and it's ecosystem is dominant, growing fast, and extremely profitable.\n<br />\n<br />In the cloud space, OpenStack appears to be the consortium of people who can't figure out how to compete with AWS on their own. AWS is dominant, growing its feature set and installed capacity very fast. Every month that passes, AWS is refining and extending it's products to meet real customer needs. Measured by the reserved IP address ranges used by its instances AWS has more than doubled in the last year and now has over 800,000 IP addresses assigned to its regions worldwide. \n<br />\n<br />The problem with a consortium is that it is hard to get it to agree on anything, and Brooks law applies (The Mythical Man-Month - adding resources to a late software project makes it later). While it seems obvious that adding more members to OpenStack is a good thing, in practice, it will slow the project down. I was once told that the way to kill a standards body or consortium is to keep inviting new people to join and adding to its scope. With the huge diversity of datacenter hardware and many vendors with a lot to lose if they get sidelined I expect OpenStack to fracture into multiple vendor specific \"stacks\" with narrow test matrixes and extended features that lock customers in and don't interoperate well. \n<br />\n<br />I come to use clouds, because I work for a developer oriented company that has decided that building and running infrastructure on a global scale is undifferentiated heavy lifting, and we can leverage outside investment from AWS and others to do a better job than we could do ourselves, while we focus on the real job of developing global streaming to TVs.\n<br />\n<br />Operations oriented companies tend to focus on costs and ways to control their developers. They want to build clouds, and may use OpenStack, but their developers aren't going to wait, they may be allowed to use AWS \"just for development and testing\" but when the time comes to deploy on OpenStack, it's lack of features is going to add a significant burden of complexity to the development team. OpenStack's lack of scale and immaturity compared to AWS is also going to make it harder to deploy products. I predict that the best developers will get frustrated and leave to work at places like Netflix (hint, we're hiring).\n<br />\n<br />I haven't yet seen a viable alternative to AWS, but that doesn't mean I don't want to see one. My guess is that in about two to three years from now there may be a credible alternative. Netflix has already spent a lot of time helping AWS scale as we figured out our architecture, we don't want to do that again, so I'm also waiting for someone else (another large end-user) to kick the tires and prove that an alternative works.\n<br />\n<br />Here's my recipe for a credible alternative that we could use:\n<br />\n<br />AWS has too many features to list, we use almost all of them, because they were all built to solve real customer problems and make life easier for developers. The last slide of my recent cloud presentations at <a href=\"http://slideshare.net/adrianco\">http://slideshare.net/adrianco</a> contains a partial list as a terminology glossary. AWS is adding entirely new capabilities and additional detailed features every month, so this is a moving target that is accelerating fast away from the competition...\n<br />\n<br />From a scale point of view Netflix has several thousand instances organized into hundreds of different instance types (services), and routinely allocates and deallocates over a thousand new instances each day as we autoscale to the traffic load and push new code. Often a few hundred instances are created in a few minutes. Some other cloud vendors we have talked to consider a hundred instances a large customer, and their biggest instances are too small for us to use. We mostly use m2.4xl and we need the 68GB RAM for memcached, Cassandra or our very large Java applications, so a 15GB max doesn't work.\n<br />\n<br />In summary, although the CDN space is already a commodity with multiple interchangeable vendors, we are several years from having multiple cloud vendors that have enough features and scale to be interchangeable. The developer ecosystem around AWS concepts and APIs is dominant, so I don't see any value in alternative concepts and APIs, please try to build AWS clones that scale. Good luck :-)\n<br />",
        "metadata": {
          "word_count": 1948,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "My question is what are the alternatives to AWS from a developer perspective, and when might they be useful? I will digress into a little history to frame the discussion. In 1984 I didn't care about how the Zilog Z80 or the Motorola 6809 microprocessors were made, but I built my own home-brew 6809 machine and wrote a code generator for a C compiler. In 1988 I joined Sun Microsystems and was one of the people who could argue in detail how SPARC was better than MIPS or whatever as an instruction set. In 1993 I moved to the USA and worked to help customers scale their applications on the new big multiprocessor systems. In 1995 when Java was released and the WWW was taking off, I didn't work on the Java implementation or IETF standards body. In 2004 I made a career change to move from the Enterprise Computing market place with Sun to the Consumer Web Services space with eBay. It seemed to me that the interesting innovation was now taking place in the creation and mash-up of web services and APIs. I attended an OpenStack meetup, thinking that I might learn about its feature set, scale and roadmap as an alternative to AWS. However the main objective of the presenters seemed to be to recruit the equivalent of chip designers and kernel developers to help them build out the project. There was no explanation or outreach to developers who might want to build applications that run on OpenStack. Initial release is only really usable via the AWS clone APIs and doesn't have an integrated authentication system across the features. The \"Diablo\" release this fall should be better integrated. A few very specialized engineers are needed to build microprocessors, operating systems, servers, datacenters, CDNs and clouds. When you consider the developer ecosystem around AWS, there must be hundreds of thousands of developers familiar with AWS concepts and APIs. The analogy that seems to fit is Apple's iOS vs. Google's Android in the smartphone market. Apple and it's ecosystem is dominant, growing fast, and extremely profitable. OpenStack appears to be the consortium of people who can't figure out how to compete with AWS.  AWS has more than doubled in the last year and now has over 800,000 IP addresses assigned to its regions worldwide. With the huge diversity of datacenter hardware and many vendors with a lot to lose if they get sidelined I expect OpenStack to fracture into multiple vendor specific \"stacks\" with narrow test matrixes and extended features. OpenStack's lack of features is going to add a significant burden of complexity to the development team. I predict that the best developers will get frustrated and leave to work at places that don't interoperate well. I haven't yet seen a viable alternative to AWS, but that doesn't mean I don't want to see one. My guess is that in about two to three years from now there may be a credible alternative. I'm also waiting for someone else (another large end-user) to kick the tires and prove that an alternative works. Netflix routinely allocates and deallocates over a thousand new instances each day as we autoscale to the traffic load and push new code. Some other cloud vendors we have talked to consider a hundred instances a large customer. Try to build AWS clones that scale. Good luck :-)<br /> Try to build Amazon Web Services clones that Scale. Goodluck :-)"
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_30613e99",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "The Periodic Table of Periodic Tables",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/periodic-table-of-periodic-tables.html",
      "content": {
        "text": "<div style=\"float: right; margin-left: 10px; margin-bottom: 10px;\"><a href=\"http://www.flickr.com/photos/bk/4455590301/\" title=\"photo sharing\"><img src=\"http://farm3.static.flickr.com/2699/4455590301_fbe525d0b2_m.jpg\" alt=\"\" style=\"border: solid 2px #000000;\" /></a><br /><span style=\"font-size: 0.9em; margin-top: 0px;\"><a href=\"http://www.flickr.com/photos/bk/4455590301/\">The Periodic Table of Periodic Tables</a><br />Originally uploaded by <a href=\"http://www.flickr.com/people/bk/\">\u2605keaggy.com</a></span></div>Excellent science geekery<br clear=\"all\" />",
        "metadata": {
          "word_count": 37,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Periodic Table of Periodic Tables was originally uploaded to flickr.com. It's a great resource for science geekery. The periodic table is made up of more than 1,000 elements."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4b02a106",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Installation Day 3",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-installation-day-3.html",
      "content": {
        "text": "Today about half the panels were put in place on the roof, and a lot of wiring was put in place.<br /><br />Here are the installers starting to put panels in place in the morning.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZy7OaiouLZ-IJtzotJfZNoCzKlsbCQyxzw1ihxAT2w3ZkaXfnpfVZu48a_-UgFP6JGCQ8R0ZWvqWTXYSDpdruwGfg10B5Hyfzcw0j8wjPSDAAeITKtj_vPgDGP8H4Xg75IRgK/s1600-h/IMG_0077_2.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjZy7OaiouLZ-IJtzotJfZNoCzKlsbCQyxzw1ihxAT2w3ZkaXfnpfVZu48a_-UgFP6JGCQ8R0ZWvqWTXYSDpdruwGfg10B5Hyfzcw0j8wjPSDAAeITKtj_vPgDGP8H4Xg75IRgK/s320/IMG_0077_2.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5363750473395721794\" /></a><br /><br />In the evening the lowest row of 20 are in place, and about half of the next row. There will be a total of 60 panels when it's done.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-W9MyCYbsS-v5FZFYLJlpcnxuoyBmzJ4vGdizqvGq-AyD8fWvs18o_lUbkBFMDPXIPscc0gsV7tyhd9DE4QLvDNPBXQjmKba78sab2_29DUqMKWeNSN9-IYbFerQ5TN-H1il/s1600-h/IMG_0078_2.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiG-W9MyCYbsS-v5FZFYLJlpcnxuoyBmzJ4vGdizqvGq-AyD8fWvs18o_lUbkBFMDPXIPscc0gsV7tyhd9DE4QLvDNPBXQjmKba78sab2_29DUqMKWeNSN9-IYbFerQ5TN-H1il/s320/IMG_0078_2.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5363750469096548274\" /></a><br /><br /><br />Here is the underside of a panel, in a shipping rack on the truck.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxp1secpDwKWY_gjsVJemo3Qc5OJscbbk8fZoT8VpEF6kG8yOo6DRsjGMz5gn4y1Me0DsiQqbvyfG4mCLlPFR3amXkVfKDBMG4oi-Qdbg4HT9mbMr3fH7pVaHKea2UTsb0I3T-/s1600-h/IMG_0064.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhxp1secpDwKWY_gjsVJemo3Qc5OJscbbk8fZoT8VpEF6kG8yOo6DRsjGMz5gn4y1Me0DsiQqbvyfG4mCLlPFR3amXkVfKDBMG4oi-Qdbg4HT9mbMr3fH7pVaHKea2UTsb0I3T-/s320/IMG_0064.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5363750463227269090\" /></a><br /><br />A closeup of the panel's specification label, as I mentioned before, its a<a href=\"http://en.wikipedia.org/wiki/Solar_cell\"> Cadmium Telluride CdTe thin film technology</a> panel from First Solar. Each panel is rated at 75W, and the total installation is rated at 4.5KW.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9wE5V5_bsdp-LnmzkRyvN79B6TnVaqNQaZ4VWImPpPoteLG8gajJ9s3XtpB0ldOsHN4U-aczOkVuSb82ZyzcdadfXRLdEbXQaiU7Gr-v9wRdNYVr1KBjbGufRtqGZaRSsvQI/s1600-h/IMG_0063.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 240px; height: 320px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjU9wE5V5_bsdp-LnmzkRyvN79B6TnVaqNQaZ4VWImPpPoteLG8gajJ9s3XtpB0ldOsHN4U-aczOkVuSb82ZyzcdadfXRLdEbXQaiU7Gr-v9wRdNYVr1KBjbGufRtqGZaRSsvQI/s320/IMG_0063.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5363750459468093218\" /></a>",
        "metadata": {
          "word_count": 180,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Today about half the panels were put in place on the roof. The lowest row of 20 are in place, and about half of the next row. There will be a total of 60 panels when it's done. Here is the underside of a panel, in a shipping rack on the truck. Here is a picture of the truck's underside, with a view of the cargo area. A closeup of the panel's specification label, as I mentioned before, its a Cadmium Telluride CdTe thin film technology. Each panel is rated at 75W, and the total installation is rating at 4.5KW. The image is part of a larger set of images. Click through the gallery to see the rest of the images. The image is of a house in New Jersey."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e58f87d5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Blogging Tools | by Adrian Cockcroft | April 20th, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/04/blogging-tools-by-adrian-cockcroft.html",
      "content": {
        "text": "I've been using blogger for the last 18 months, it was an easy way to get started but now I don't see some of the features I want. The basic service has changed very little in that time, so it doesn't seem to be getting much investment and development.<br /><br />The three missing features I see in other blogs are tags, a blogroll, and posting categories.<br /><br />I want to have an easy way to add a series of tags to each blog entry, without having to create custom html. I did it once the hard way and don't usually bother.<br /><br />I'd like a blogroll so that people can see which blogs I think are worth reading, but I don't want to edit my html template to get one, I want to import OPML or have a table to edit.<br /><br />I'd like to be able to separate categories so that I can label rants like this separately from technical info on capacity planning, thoughts on the industry, personal stuff.<br /><br />I like the web based blogger service, I can post from anywhere using any device (I've posted to blogger from Linux, Solaris, Windows, Mac and Treo/palmOS). I don't want to host my own blog or have to install a blogging tool.<br /><br />I use <a href=\"http://www.bloglines.com\">bloglines </a>as an aggregator to read blogs, I could also use bloglines to host my own blog, since it does seem to have some of these features, and it would make referring easier.<br /><br />What other options are out there, is there a slightly better blogger competitor that I should check out? Is there a way to migrate existing entries to a new blog? Comments requested...<br /><br />Cheers Adrian",
        "metadata": {
          "word_count": 286,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The three missing features I see in other blogs are tags, a blogroll, and posting categories. The basic service has changed very little in that time, so it doesn't seem to be getting much investment and development. I'd like to be able to separate categories so that I can label rants like this separately from technical info. Adrian doesn't want to host his own blog or have to install a blogging tool. I use bloglines as an aggregator to read blogs. Is there a slightly better blogger competitor?"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_312c6aa4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cloud Architect",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/05/cloud-architect.html",
      "content": {
        "text": "my role at Netflix just changed, from managing development of personalization in the cloud, to over-all cloud architect for Netflix. New challenges and a much broader scope. One challenge we have as we migrate to the cloud is that we need to build a new way to log and analyze what the systems in the cloud are doing, and we are basing this on Chukwa, Hadoop and Hive, all running in AWS. So we are <a href=\"http://jobs.netflix.com/DetailFlix.asp?jobid=flix3708\">looking to hire a Hadoop developer</a>, and are looking at the <a href=\"http://perspectives.mvdirona.com/2010/05/04/PattersonOnCloudComputing.aspx\">research going on at Berkeley's RAD lab</a> with interest.",
        "metadata": {
          "word_count": 97,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "My role at Netflix just changed, from managing development of personalization in the cloud, to over-all cloud architect for Netflix. New challenges and a much broader scope. One challenge we have as we migrate to the cloud is that we need to build a new way to log and analyze what the systems in thecloud are doing. We are basing this on Chukwa, Hadoop and Hive, all running in AWS."
      },
      "tags": [
        "aws",
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_790306f6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Monitoring Consumption",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/09/solar-power-monitoring-consumption.html",
      "content": {
        "text": "We finally got Solarcity to install the consumption monitor, and we are now figuring out what the various bumps in the graph correlate to in terms of appliances. The first graph shows consumption on a hot day, with a portable room A/C unit left on all day. There is a characteristic bump in the morning that seems to correlate to re-heating hot water after showers, and random bumps during the day that are probably the hot tub cycling on to filter and maintain water temperature.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilPivgP9BquJHVKC4q9y_wteIAgJfUzu6_fNZgFoZl0flVJhOtUBQMlnN2YYTLb5TJesXXyAFwK3xmpC0Q6ue8gZqvUXQCgCsUAGbp3yfUa5hkrOLfH5klRDhkuo9U2OpPd50S/s1600-h/Picture+15.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 154px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEilPivgP9BquJHVKC4q9y_wteIAgJfUzu6_fNZgFoZl0flVJhOtUBQMlnN2YYTLb5TJesXXyAFwK3xmpC0Q6ue8gZqvUXQCgCsUAGbp3yfUa5hkrOLfH5klRDhkuo9U2OpPd50S/s320/Picture+15.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5385303878100309762\" /></a><br /><br />Another day where we didn't leave the A/C unit on shows much lower consumption. One of the common psychological effects of measuring something is a compulsion to optimize it, so we are now challenging ourselves to try and reduce consumption...<br /><br />Since we are now on a time-of-use meter, the cost of electricity at nights and weekends is very low, and the cost in the afternoons during the week is high, but that is when we are generating more than we are using, so in $ terms, the meter is running backwards at high speed, and forwards slowly. This also makes it very hard to calculate the cost savings.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsrxttSK2wYKaz4pYT9bGlqKwLMYFLvmf1E9RGEAzl6SZsEPR81hcUXmD_wx8RbqDpNJEsf8hjCiEh0l_0a50S4K2-H7igF2oTD7NRQhA-SktZKewT3bH21aKh951OpmvCd0Zn/s1600-h/Picture+14.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 156px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsrxttSK2wYKaz4pYT9bGlqKwLMYFLvmf1E9RGEAzl6SZsEPR81hcUXmD_wx8RbqDpNJEsf8hjCiEh0l_0a50S4K2-H7igF2oTD7NRQhA-SktZKewT3bH21aKh951OpmvCd0Zn/s320/Picture+14.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5385304210139142658\" /></a><br /><br />Total generation for the month is looking good. We turned on the system right at the start of the month, so the lifetime total includes a week or so before the output monitoring started to log data, and you can see the last few days of consumption data. We have had typical (for us) weather, sunny from the start of the day most days. We drive down into the clouds each morning. The drop off a few weeks ago was a cloudy spell that included about 0.25in of rain, enough to make sure the solar panels are nice and clean. There is a slow downward trend in the daily output, since the days are getting shorter and the Sun is lower in the sky every day. It's around the equinox at the moment. Over-all, I'm happy with how much energy we are getting, I expected to get longer effective days than most people, and it looks as if we will be generating about 600 KWh this month, our typical consumption has been around 1000 KWh.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4LDx19IMYaU0BTbCQrmcmq0iRJGaZgJhWn97blxmayN0DgAKi1f7r52kB2HLTweQUC5ichid2e2agX4LFLA2-7fTY1S7yOfd1cw7uHm0hh1Y51BLKi1hHq4JOmw816_X98-wE/s1600-h/Picture+16.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 154px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh4LDx19IMYaU0BTbCQrmcmq0iRJGaZgJhWn97blxmayN0DgAKi1f7r52kB2HLTweQUC5ichid2e2agX4LFLA2-7fTY1S7yOfd1cw7uHm0hh1Y51BLKi1hHq4JOmw816_X98-wE/s320/Picture+16.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5385303610465110882\" /></a>",
        "metadata": {
          "word_count": 422,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The graph shows consumption on a hot day, with a portable room A/C unit left on all day. There is a characteristic bump in the morning that seems to correlate to re-heating hot water after showers. Random bumps during the day that are probably the hot tub cycling on to filter and maintain water temperature. Since we are now on a time-of-use meter, the cost of electricity at nights and weekends is very low. The cost in the afternoons during the week is high, but that is when we are generating more than we are using. This also makes it very hard to calculate the cost savings. Total generation for the month is looking good. There is a slow downward trend in the daily output, since the days are getting shorter and the Sun is lower in the sky. I expected to get longer effective days than most people, and it looks as if we will be generating about 600 KWh this month. Our typical consumption has been around 1000 KWh. It's around the equinox at the moment."
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_0ae3c5d2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Hiring a cloud platform engineer at Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/hiring-cloud-platform-engineer-at.html",
      "content": {
        "text": "If you are interested in <a href=\"http://www.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/\">the CAP theorem</a> NoSQL systems like <a href=\"http://www.infoq.com/presentations/Project-Voldemort-Scaling-Simple-Storage\">Voldemort, </a> Cassandra and SimpleDB, <a href=\"http://www.infoq.com/presentations/Horizontal-Scalability\">scalability</a> and expect to be at the <a href=\"http://en.oreilly.com/velocity2010\">Velocity Conference</a> in June, then you are the <a href=\"http://jobs.netflix.com/DetailFlix.asp?jobid=flix3571\">kind of engineer I'm interested in hiring</a>. Netflix is aggressively moving to new technology, hiring only the very best and most experienced engineers and our growth is accelerating.",
        "metadata": {
          "word_count": 64,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Netflix is aggressively moving to new technology, hiring only the very best and most experienced engineers. If you are interested in <a href=\"http://www.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/\">the CAP theorem, expect to be at the Velocity Conference in June."
      },
      "tags": [
        "cloud",
        "netflix",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_aa265202",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Lots of Netflix talks at AWS Re:Invent",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/11/lots-of-netflix-talks-at-aws-reinvent.html",
      "content": {
        "text": "<!--StartFragment-->\n\n<br />\n<br />\n[Update: here's video's of these talks&nbsp;<a href=\"http://techblog.netflix.com/2012/12/videos-of-netflix-talks-at-aws-reinvent.html\">http://techblog.netflix.com/2012/12/videos-of-netflix-talks-at-aws-reinvent.html</a>&nbsp;along with slides]<br />\n<br />\nThere is a Netflix booth in the expo center, we will be talking about our open source tools from <a href=\"http://netflix.github.com/\">http://netflix.github.com</a> and collecting resumes from anyone interested in joining us.<br />\n<br />\n<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"border-collapse: collapse; mso-yfti-tbllook: 1056; width: 684px;\">\n <colgroup><col style=\"mso-width-source: userset; width: 137pt;\" width=\"137\"></col>\n <col style=\"mso-width-source: userset; width: 158pt;\" width=\"158\"></col>\n <col style=\"mso-width-source: userset; width: 389pt;\" width=\"389\"></col>\n </colgroup><tbody>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-weight: bold;\">Date/Time</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-weight: bold;\">Presenter</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-weight: bold;\">Topic</span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Wed 8:30-10:00</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Reed Hastings</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Keynote with Andy</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  Jassy</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Wed 1:00-1:45</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Coburn Watson</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Optimizing</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  Costs with AWS</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Wed 2:05-2:55</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Kevin McEntee</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-style: italic;\">Netflix\u2019s Transcoding\n  Transformation</span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Wed 3:25-4:15</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial;\">Neil Hunt /</span><span style=\"font-family: Arial; vertical-align: baseline;\"> Yury I.</span></span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Netflix:</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  Embracing the Cloud</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Wed 4:30-5:20</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Adrian Cockcroft</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-style: italic;\">High Availability Architecture at\n  Netflix</span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 10:30-11:20</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Jeremy Edberg</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small; font-style: italic;\">Rainmakers \u2013 Operating Clouds</span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 11:35-12:25</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Kurt Brown</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Data Science</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  with Elastic Map Reduce (EMR)</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 11:35-12:25</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial;\">Jason</span><span style=\"font-family: Arial; vertical-align: baseline;\"> Chan</span></span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Security Panel: Learn from</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  CISOs working with AWS</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 3:00-3:50</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Adrian Cockcroft</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Compute &amp; Networking</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\">\n  Masters Customer Panel</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 3:00-3:50</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial;\">Ruslan</span><span style=\"font-family: Arial; vertical-align: baseline;\"> M./Gregg U.</span></span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Optimizing Your Cassandra\n  Database</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\"> on\n  AWS</span></span></div>\n</td>\n </tr>\n<tr height=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\">\n  <td class=\"oa1\" height=\"27\" style=\"height: 26.86pt; width: 137pt;\" width=\"137\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Thu 4:05-4:55</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 158pt;\" width=\"158\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-family: Arial; font-size: x-small;\">Ariel Tseitlin</span></div>\n</td>\n  <td class=\"oa1\" style=\"width: 389pt;\" width=\"389\"><div style=\"direction: ltr; margin-bottom: 0pt; margin-left: 0in; margin-top: 0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\">\n<span style=\"font-size: x-small;\"><span style=\"font-family: Arial; font-style: italic;\">Intro</span><span style=\"font-family: Arial; font-style: italic; vertical-align: baseline;\"> to\n  Chaos Monkey and the Simian Army</span></span></div>\n</td>\n </tr>\n</tbody></table>\n<!--EndFragment-->",
        "metadata": {
          "word_count": 1407,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a Netflix booth in the expo center, we will be talking about our open source tools. We will be collecting resumes from anyone interested in joining us. The word \"top\" is used to refer to the top of the page. The word \"date/time\" refers to the date and time of the event. Reed Hastings will be on the phone from 8:30-10:00 p.m. ET. The call will be monitored by a team of three people. The keynote will take place between 1:00 and 1:45 p.m. ET. The keynote will be delivered by Andy Jassy. For more information on the keynote click here. The cost of using Amazon Web Services has been estimated to be in the region of $1.5 billion a year. The cost of the project is expected to rise to $2.6 billion over the next three years. Netflix\u2019s Transcoding. =\"direction: ltr; margin-bottom: 0pt, margin-left: 0in,. margin-top:0pt; mso-line-break-override: none; punctuation-wrap: hanging; text-align: left; unicode-bidi: embed; word-break: normal;\"> \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0<span style=\"font-family: Arial; font-size: x-small;\">Kevin McEntee. CNN.com will be broadcasting a live webcast of Neil Hunt's interview with Yury I. Hunt at 3:25-4:15 p.m. ET on Wednesday, July 25. The stream will be available on CNN.com and CNN iReport.com. Html. ht=\"27\" style=\"height: 26.86pt; mso-height-source: userset;\"> Html. mso height: 137pt; ht. height: 27pt; width: 158pt; height: 389pt; margin-bottom: 0pt, margin-left: 0in,margin-top:0pt, mso.line-break-override: none, punctuation-wrap: hanging, text-align: left, unicode-bidi: embed, word-break: normal. High Availability Architecture at Netflix. al;\">High Availability architecture at Netflix, a.k.a. \u2018Netflix.com\u2019. \"Rainmakers \u2013 Operating Clouds\" is a shortened version of the original version of this article. It was published on November 14, 2013. The code was created using Elastic Map Reduce (EMR) The code was written in Python and was run from 11:35 to 12:25pm (GMT) on Thursday. Security Panel: Learn from CISOs working with Amazon Web Services (AWS) Security Panel: learn from the CISOs who have worked with AWS for years. Ght: 26.86pt; width: 137pt;\" width=\"137\";. ght: 3:00-3:50;. gt: 3-4:30;. text-align: left; unicode-bidi: embed; word-break: normal;. The call is from the Masters Customer Panel at 3:00-3:50 p.m. ET. The call is for Ruslan and Gregg U. The Cassandra Cassandra Database is a free, open-source, cloud-based database for Cassandra users. Use the Cassandra Database to help you with your Cassandra queries. The Cassandra database is free and open source. Ariel Tseitlin is the leader of the Simian Army. The Simian army is led by Chaos Monkey and the Chaos Monkey himself. This is the first part of a two part story."
      },
      "tags": [
        "architecture",
        "aws",
        "data science",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_99391482",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Installation Day 2",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-installation-day-2.html",
      "content": {
        "text": "More mounting hardware is now on the roof, the brackets that hold the panels to the rails are now in place. Since the PG&E meter isn't mounted on the house itself, they also had to dig a trench. Tomorrow a county planning dept. inspector will come by to make sure everything is to code.<br /><br />I also got an email from PG&E today, informing me that they know I have solar being installed and will schedule an appointment to do their own verification and turn the system on, once the hardware is in place.<br /><br />One subtlety, since I also want to connect a generator for when we get power cuts (I'm on top of the Santa Cruz mountains) we had to decide what order to wire everything together. The sequence is <br /><br />PG&E service meter - Solar Power system - Generator - House breaker panel<br /><br />The generator is only wired into a subset of the house circuits, so it is looped into the house breaker panel. When installed, it will have a sensor that can tell whether there is any incoming power from PG&E, and if not, it fires up after 20 seconds or so.<br /><br />The solar power system detects the PG&E power, synchronizes to the 60Hz phase and adds its own power by providing a few volts more than PG&E, to get the power to flow into the system. If PG&E goes away, the solar system also shuts down, then the generator detects no power, isolates the downstream components and takes over powering the house.<br /><br />At one point I looked at having batteries rather than a generator, but it is a lot less efficient, they don't last very long and they are far more expensive. A 6KW battery system with inverter was around $10K, while I can get a 8KW propane generator for $2K, and I'm thinking of <a href=\"http://www.amazon.com/Guardian-5503-Air-Cooled-Automatic-Generator/dp/B001DZLKHI/ref=sr_1_15?ie=UTF8&s=hi&qid=1248767206&sr=1-15\">putting in a 14KW Generac 5503 that is currently available from Amazon for $3.3K delivered</a>, since that can supply a 40A 220V circuit for the heat pump. I'm likely to need it for up to a week each year, and that will be the only thing that uses propane at that point.",
        "metadata": {
          "word_count": 367,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "More mounting hardware is now on the roof, the brackets that hold the panels to the rails are now in place. Since the PG&E meter isn't mounted on the house itself, they also had to dig a trench. Tomorrow a county planning dept. inspector will come by to make sure everything is to code. The solar power system detects the PG&E power, synchronizes to the 60Hz phase and adds its own power. A 6KW battery system with inverter was around $10K, while I can get a 8KW propane generator for $2K. At that point, propane will be the only thing that uses propane in the U.S. k each year, and it will be used only for cooking."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_30ccce4a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Plan Resubmitted",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-plan-resubmitted.html",
      "content": {
        "text": "This morning I took the requested clarifications on the building design into the county offices. We should hear back in a week or two, it's minor stuff so I hope this is the last step. We already have our permit for the leach field repair, so I'm starting on that work. We got the septic tank pumped and inspected (it was in good condition) this week, and filed the inspection results with the county, so the next step is to put in a new re-routed pipe and diversion valve between the tank and the leach field.",
        "metadata": {
          "word_count": 96,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "\"We already have our permit for the leach field repair, so I'm starting on that work,\" he says. \"This morning I took the requested clarifications on the building design into the county offices,\" he adds."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_75796f13",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "SunWorld Offline",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/09/sunworld-offline.html",
      "content": {
        "text": "The monthly columns I wrote for SunWorld Online between 1995 and 1999 seem to have gone offline. At one point after SunWorld shut down I got permission from the editors at ITworld to take back the content, remove the adverts, and host it on the Sun site. Since I left Sun in 2004 the link has been closed down. I think there is still some useful content and historical interest in the columns, which seem to have been copied to a few other places already. I'm going to dig them out of my archives, and repost them here, along with comments and perhaps a few corrections and updates.<br /><br />The original idea for the column came from a featured article on the Sun homepage in 1995 where I talked about the release of my first book, Sun Performance and Tuning - SPARC and Solaris. This generated an offer from the editors at SunWorld Online to do a monthly Performance Q&amp;A column. I thought it worked out very well. I got paid for the column, which forced me to write something every month, and I based the columns on questions from the readers, or parts of the original book that needed to be updated, or parts of the second edition that I was writing during this period.<br /><br />The monthly column seemed to have the effect of positioning me as the expert to quite a wide public, and I'm sure it helped me get promoted to Distinguished Engineer in 1999. I stopped writing the column due to a combination of being too busy, running out of topics to write about, and the introduction of the <a href=\"http://www.sun.com/blueprints/\">BluePrint</a> program.<br /><br />In effect, several engineers had created books and columns in their spare time, and we worked with management to make it an official program for publishing Sun's best practices on how to configure products and combinations of products. This made it our day job, which was easier in some ways, and we had support staff and more detailed reviews, but we didn't get paid royalties for doing the BluePrint columns or books. After a few years and several books (The very first BluePrint on Resource Management, and one on Capacity Planning) I moved on from the BluePrint group, but the program is still running and there is a large body of work there now.<br /><br />There are over forty columns. I could post them en-masse unedited but I think they can already be found lurking in Google. It seems like more fun to try and remember what was going on at the time and make it an anecdotal reminiscence.",
        "metadata": {
          "word_count": 437,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The monthly columns I wrote for SunWorld Online between 1995 and 1999 seem to have gone offline. At one point after SunWorld shut down I got permission from the editors at ITworld to take back the content, remove the adverts, and host it on the Sun site. I think there is still some useful content and historical interest in the columns. The monthly column seemed to have the effect of positioning me as the expert to quite a wide public, and I'm sure it helped me get promoted to Distinguished Engineer in 1999. I stopped writing the column due to a combination of being too busy, running out of topics to write about, and the introduction of the BluePrint program. There are over forty columns. luePrint on Resource Management, and one on Capacity Planning. I moved on from the BluePrint group, but the program is still running and there is a large body of work there now."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_fdf78b23",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: i.Wund.com",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-iwundcom.html",
      "content": {
        "text": "My favorite weather app is actually a lauchpad bookmarked web site rather than an appstore download. http://i.wund.com/ is an iPhone optimized version of the Weather Underground site www.wunderground.com that I upload to from the personal weather station at my house. The best features are a zoomable local weather radar view, and the option to show the detailed technical weather forecast in the form that meterologists use to talk to each other, rather than the dumbed down version everyone else sees.",
        "metadata": {
          "word_count": 80,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "My favorite weather app is actually a lauchpad bookmarked web site rather than an appstore download. http://i.wund.com/ is an iPhone optimized version of the Weather Underground site www.wunderground.com."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_67596539",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Thoughts on SimpleDB, DynamoDB and Cassandra",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/01/thoughts-on-simpledb-dynamodb-and.html",
      "content": {
        "text": "I've been getting a lot of questions about DynamoDB, and these are my personal thoughts on the product, and how it fits into cloud architectures.<br /><br />I'm excited to see the release of DynamoDB, it's a very impressive product with great performance characteristics. It should be the first option for startups and new cloud projects on AWS. I also think it marks the turning point on solid state disks, they will be the default for new database products and benchmarks going forward.<br /><br />There are a few use cases where SimpleDB may still be useful, but DynamoDB replaces it in almost all cases. I've talked about the history of Netflix use of SimpleDB before, but it's relevant to the discussion on DynamoDB, so here goes.<br /><br />When Netflix was looking at moving to cloud about three years ago we had an internal debate about how to handle storage on AWS. There were strong proponents for using MySQL, SimpleDB was fairly new, and other alternatives were nascent NoSQL projects or expensive enterprise software. We started some pathfinder projects to explore the two alternatives and decided to port an existing MySQL app to AWS, while building a replication pipeline that copied data out of our Oracle datacenter systems into SimpleDB to be consumed in the cloud. The MySQL experience showed that we would have trouble scaling, and SimpleDB seemed reliable, so we went ahead and kept building more data sources on SimpleDB, with large blobs of data on S3.<br /><br />Along the way we put memcached in front of SimpleDB and S3 to improve read latency. The durability of SimpleDB is its strongest point, we have had Oracle and SAN data corruption bugs in the Datacenter over the last few years, but never lost or corrupted any SimpleDB data. The limitations of SimpleDB are its biggest problem. We worked around limits on table size, row and attribute size, and per-request overhead caused by http requests needing to be authenticated for every call.<br /><br />So the lesson here is that for a first step into NoSQL, we went with a hosted solution so that we didn't have to build a team of experts to run it, and we didn't have to decide in advance how much scale we needed. Starting again from scratch today, I would probably go with DynamoDB. It's a low \"friction\" and developer friendly solution.<br /><br />Late in 2010 we were planning the next step, turning off Oracle and making the cloud the master copy of the data. One big problem is that our backups and archives were based on Oracle, and there was no way to take a snapshot or incremental backup of SimpleDB. The only way to get data out in bulk is to run \"SELECT * FROM table\" over HTTP and page the requests. This adds load, takes too long, and costs a lot because SimpleDB charges for the time taken in SELECT calls.<br /><br />We looked at about twenty NoSQL options during 2010, trying to understand what the differences were, and eventually settled on Cassandra as our candidate for prototyping. In a week or so, we had ported a major data source from SimpleDB to Cassandra and were getting used to the new architecture, running benchmarks etc. We evaluated some other options, but decided to take Cassandra to the next stage and develop a production data source using it.<br /><br />The things we liked about Cassandra were that it is written in Java (we have a building full of Java engineers), it is packed full of state of the art distributed systems algorithms, we liked the code quality, we could get commercial support from Datastax, it is scalable and as an extra bonus it had multi-region support. What we didn't like so much was that we had to staff a team to own running Cassandra for us, but we retrained some DBAs and hired some good engineers including Vijay Parthasarathy, who had worked on the multi-region Cassandra development at Cisco Webex and who recently became a core committer on the Apache Cassandra project. We also struggled with the Hector client library, and have written our own (which we plan to release soon). The blessing and a curse of Cassandra is that it is an amazingly fast moving project. New versions come fast and furiously, which makes it hard to pick a version to stabilize on, however the changes we make turn up in the mainstream releases after a few weeks. Saying \"Cassandra doesn't do X\" is more of a challenge than a statement. If we need \"X\" we work with the rest of the Apache project to add it.<br /><br />Throughout 2010 the product teams at Netflix gradually moved their backend data sources to Cassandra, we worked out the automation we needed to do easy self service deployments and ended up with a large number of clusters. In preparation for the UK launch we also made changes to Cassandra to better support multi-region deployment on EC2, and we are currently running several Cassandra clusters that span the US and European AWS regions.<br /><br />Now that DynamoDB has been released, the obvious question is whether Netflix has any plans to use it. The short answer is no, because it's a subset of the Cassandra functionality that we depend on. However that doesn't detract from the major step forward from SimpleDB in performance, scalability and latency. For new customers, or people who have outgrown the scalability of MySQL or MongoDB, DynamoDB is an excellent starting point for data sources on AWS. The advantages of zero administration combined with the performance and scalability of a solid state disk backend are compelling.<br /><br />Personally my main disappointment with DynamoDB is that it doesn't have any snapshot or incremental backup capability. The AWS answer is that you can extract data into EMR then store it in S3. This is basically the same answer as SimpleDB, it's a full table scan data extraction (which takes too long and costs too much and isn't incremental). The mechanism we built for Cassandra leverages the way that Cassandra writes immutable files to get a callback and compress/copy them to S3 as they are written, it's extremely low overhead. If we corrupt our data with a code bug and need to roll back, or take a backup in production and restore in test, we have all the files archived in S3.<br /><br />One argument against DynamoDB is that DynamoDB is on AWS only, so customers could get locked in, however it's easy to upgrade applications from SimpleDB, to DynamoDB and to Cassandra. They have similar schema models, consistency options and availability models. It's harder to go backwards, because Cassandra has more features and fewer restrictions. Porting between NoSQL data stores is trivial compared to porting between relational databases, due to the complexity of the SQL language dialects and features and the simplicity of the NoSQL offerings. Starting out on DynamoDB then switching to Cassandra when you need more direct control over the installation or Cassandra specific features like multi-region support is a very viable strategy.<br /><br />As early adopters we have had to do a lot more pioneering engineering work than more recent cloud converts. Along the way we have leveraged AWS heavily to accelerate our own development, and built a lot of automation around Cassandra. While SimpleDB has been a minor player in the NoSQL world DynamoDB is going to have a much bigger impact. Cassandra has matured and got easier to use and deploy over the last year but it doesn't scale down as far. By that I mean a single developer in a startup can start coding against DynamoDB without needing any support and with low and incremental costs. The smallest Cassandra cluster we run is six m1.xlarge instances spread over three zones with triple replication.<br /><br />I've been saying for a while that 2012 is the year that NoSQL goes mainstream, DynamoDB is another major step in validating that move. The canonical CEO to CIO conversation is moving from 2010: \"what's our cloud strategy?\", 2011: \"what's our big data strategy?\" to 2012: \"what's our NoSQL strategy?\".",
        "metadata": {
          "word_count": 1354,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "DynamoDB should be the first option for startups and new cloud projects on AWS. I also think it marks the turning point on solid state disks, they will be the default for new database products and benchmarks going forward.  tware.com has been using SimpleDB for a number of years. The durability of SimpleDB is its strongest point, we have had Oracle and SAN data corruption bugs in the Datacenter over the last few years, but never lost or corrupted any SimpleDB data. The limitations of Simple DB are its biggest problem. Late in 2010 we were planning the next step, turning off Oracle and making the cloud the master copy of the data. Starting again from scratch today, I would probably go with DynamoDB. It's a low \"friction\" and developer friendly solution. The company decided to take Cassandra to the next stage and develop a production data source using it. The blessing and a curse of Cassandra is that it is an amazingly fast distributed system. Cassandra is a fast moving project. New versions come fast and furiously, which makes it hard to pick a version to stabilize on. If we need \"X\" we work with the rest of the Apache project to add it. For new customers, or people who have outgrown the scalability of MySQL or MongoDB, DynamoDB is an excellent starting point for data sources on AWS. The advantages of zero administration combined with the performance and scalable of a solid state disk backend are compelling. It's easy to upgrade applications from SimpleDB, to DynamoDB and to Cassandra. Porting between NoSQL data stores is trivial compared to porting between relational databases. Cassandra has more features and fewer restrictions. While SimpleDB has been a minor player in the NoSQL world DynamoDB is going to have a much bigger impact. Cassandra has matured and got easier to use and deploy over the last year."
      },
      "tags": [
        "architecture",
        "automation",
        "aws",
        "big data",
        "cloud",
        "distributed systems",
        "engineering",
        "netflix",
        "performance",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_f4c1cf70",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "How not to build a Private Cloud",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/03/how-not-to-build-private-cloud.html",
      "content": {
        "text": "It's all $, FUD, and internal politics. An MBO Cloud is what you get when the CEO tells the CIO to \"figure out that cloud thing\" (Management By Objective - i.e. the CIO bonus depends on it).<br /><br /><b>There is no technical reason for private cloud to exist.</b><br /><br />[update: to clarify, that doesn't mean that I'm against private clouds or don't think they exist, because $, FUD and internal politics are a fact of life that constrain what can be done. Change also takes time and you have to \"go to war with the army you have\". However, this post is about what happens if your organization reallocates the $, isn't afraid, and has effectively no internal politics getting in the way.<br /><br />This post was written in the middle of a debate on twitter between @adrianco @reillyusa @beaker and others including key insights from @swardley.<br /><br />You should also read Christian Reilly's follow-up post \"The Hollywood Culture\" <a href=\"http://bit.ly/ePsisJ\">http://bit.ly/ePsisJ</a> and many thanks to @bernardgolden for pointing out the excellent Business Week cover story on Cloud Computing <a href=\"http://ow.ly/4dm07\">http://ow.ly/4dm07</a> - after reading it I was amazed how well it aligned with what I write here - then I saw that it was by Ashlee Vance, one of the most clueful journalists around.<br /><br />Netflix ITops Security Architect Bill Burns also <a href=\"http://x509v3.blogspot.com/2011/03/my-colleague-adrian-made-great-post.html\">wrote a very interesting post</a> on the security challenges of cloud, we've been working together and he's on the interview team for the \"Global Cloud Security Architect\" I mention below.]<br /><br /><b>Too big for public cloud? You should *be* a public cloud</b>.<br /><br />Organizations who run infrastructure at the scale of tens to hundreds of thousands of instances have created cloud based models and opened them up to other organizations as public clouds. Amazon, Google, Microsoft are the clearest examples, they have expertise in software architecture, which is why they dominate the API definition. Telcos and hosting companies are adopting this model to provide additional public cloud capacity, using clones and variants of the API. Other organizations at this scale are already figuring out how to expose their capacity to their customers, partners and supply chain. The task you take on is to simultaneously hire the best people to run your cloud (competing with Amazon, Google etc.), and run it at low cost, which is why you need to be at huge scale and you need to decide that running infrastructure is a core competency of your business. Netflix is too small, doesn't regard infrastructure as core, and doesn't want to hire a bunch of ITops people.<br /><br /><b>It costs too much to port our apps? Your $ are mis-allocated.</b><br /><br />What does it cost to build a private cloud, and how long does it take, and how many consultants and top tier ITops staff do you have to hire? Sounds like a nice empire building opportunity for the CIO. The alternative is to allocate that money to the development organization, hire more developers and rewrite your legacy apps to run on the public cloud, and give the development VP the budget to run public cloud directly. The payback is more incremental and manageable, but this is effectively a re-org of your business to move a large chunk of budget and headcount around. This is what happened at Netflix. It probably takes an act-of-CEO at most companies, the barriers are mostly political. Yes it will take time, but so will bringing up a private cloud.<br /><br /><b>Replace your apps with Saas offerings.</b><br /><br />Many internal apps can be replaced by cloud services, we just outsourced our internal help desk and incident management software. No-one I know does payroll in-house. This is uncontroversial and is happening.<br /><br /><b>We can't put confidential data in a public cloud? This is just FUD.</b><br /><br />The enterprise vendors are desperate to sell private clouds, so they are sowing this fear, uncertainty and doubt in their customer base to slow down adoption of public clouds. The reality is that many companies are already putting confidential data in public clouds. I asked the question \"when will someone using PCI level 1 be in production on AWS\" at a Cloud Connect panel, and was told that it is already being done, and Terremark pointed out that they host H&amp;R Block's tax business. There are several models of public cloud with different SLA, cost and operational models that can support confidential data securely. There is also an argument that datacenter security is not as strong as people would like to think, and that the large cloud vendors can do a better job than most enterprises at keeping the infrastructure secure. At Netflix, we are about to transition to a global cloud based business, we are currently hiring a \"Cloud Security Architect\" who understands compliance rules like PCI (the credit card standard) on a global basis (we didn't need global expertise before). Part of their job is going to be to implement this.<br /><br /><b>There is no way my execs will sign off on this! Do they care about being competitive?</b><br /><br />The biggest champion at Netflix for doing public cloud and doing it \"properly\" with an optimized architecture was our CEO Reed Hastings. He personally argued that we should try to do NoSQL rather than MySQL to push the envelope. Why? Because the bigger risk for Netflix was that we wouldn't scale and have the agility to compete. He was right, we have grown faster than our ability to build datacenters, and we have the agility we need to outrun our competition.  Netflix has never had a CIO in the first place, we do have an excellent VP of operations though, and there is plenty to do running the CDN's and Saas vendors that support Enterprise IT.<br /><br /><b>Will private clouds be successful? I think there will be a few train wrecks.</b><br /><br />The train wrecks will come as ITops discover that it's much harder and more expensive than they thought, and takes a lot longer than expected to build a private cloud. Meanwhile their developer organization won't be waiting for them, and will increasingly turn to public clouds to get their jobs done. We could argue about definitions but there are private clouds that are effectively the back ends for specialized large scale ecosystems like engineering companies that have to interface to the things that build stuff, or operate in places where there is no effective connection into the public clouds. For example, on board a ship that has limited external bandwidth, or to support a third world construction project. My take is that these will be indistinguishable from specialized Saas offerings within a supply chain ecosystem.<br /><br /><b>How not to build a public cloud - The Netflix Way</b><br /><br />Re-org your company to give budget and headcount to the developers, let them run the public cloud operations<br />Ignore the FUD, best practices and patterns for compliance and security already exist and are audit-able<br />Get the CEO to give the CIO a different MBO, to shrink their datacenter.<div><br /></div><div>Good luck with that :-)</div>",
        "metadata": {
          "word_count": 1177,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "An MBO Cloud is what you get when the CEO tells the CIO to \"figure out that cloud thing\" (Management By Objective) There is no technical reason for private cloud to exist. It's all $, FUD, and internal politics. Business Week has a cover story on Cloud Computing. I was amazed how well it aligned with what I write here. Netflix ITops Security Architect Bill Burns also wrote a very interesting post about cloud security. Running infrastructure is a core competency of your business. Netflix is too small, doesn't regard infrastructure as core, and doesn't want to hire a bunch of ITops people. Other organizations at this scale are already figuring out how to expose their capacity. Many internal apps can be replaced by cloud services. The payback is more incremental and manageable, but this is effectively a re-org of your business. It probably takes an act-of-CEO at most companies, the barriers are mostly political. Many companies are already putting confidential data in public clouds. There are several models of public cloud with different SLA, cost and operational models that can support confidential data securely. The biggest champion at Netflix for doing public cloud and doing it \"properly\" with an optimized architecture was our CEO Reed Hastings. Netflix has never had a CIO in the first place, we do have an excellent VP of operations though. There are private clouds that are effectively the back ends for specialized large scale ecosystems. Engineering companies that have to interface to the things that build stuff will increasingly turn to public clouds. Best practices and patterns for compliance and security already exist and are audit-able. CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Visit iReport.com for more travel snapshots."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "infrastructure",
        "netflix",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_b5cf26e2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Performance monitoring with Hyperthreading",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/05/performance-monitoring-with.html",
      "content": {
        "text": "Hyperthreading is used by most recent Intel servers, <a href=\"http://www.intel.com/technology/hyperthread/\">Intel describe it here.</a><br /><br />I tried searching for information on the performance impact and performance monitoring impact and found several studies that describe improvements in terms of throughput that ranged from negative impact (a slowdown) to speedups of 15-20% per CPU. I didn't see much discussion of the effects on observability or of effects on response time, so thats what I'm going to concentrate on writing about.<br /><br />First, to summarize the information that I could find, the early measurements of performance included most of the negative impact cases. This is due to two separate effects, software and hardware improvements mean that the latest operating systems on the most recent Intel Pentium  4 and Xeon chipsets have a larger benefit and minimise any negative effects.<br /><br />From a software point of view, Intel advises that Hyperthreading should be disabled for anything other than Windows XP and recent releases of RedHat and SuSe Linux. Older operating systems are naiive about Hyperthreading and while they run, they don't schedule jobs optimally. I would add that older Solaris x86 releases are also unaware of Hyperthreading and Solaris 9 and 10 for x86 include optimized support. I have also been told that Linux 2.6 includes optimizations for Hyperthreading that have been backported to recent patches for Linux 2.4.<br /><br />From a hardware point of view, the benefit of Hyperthreading increases as CPU clock rates and pipeline lengths increase. Longer waits for pipeline stalls due to memory references and branch mispredictions create larger \"pipeline bubbles\" for the second Hyperthread to run in. It is possible that there may also be improvements in the implementation of Hyperthreading at the silicon level as Intel learns from early experiences and improves its designs.<br /><br />The fundamental problem with monitoring a Hyperthreaded system is that one of the most basic and common assumptions made by capacity planners is invalid. For many years the CPU utilization of a normal system has been used as a primary capacity metric since it is assumed to be linearly related to the capacity and throughput of a normal system  that has not saturated. In other words, if you keep the CPU usage below 80-90% busy, you expect that the throughput at 60% busy is about twice the throughput at 30% busy for a constant workload.<br /><br />Hyperthreaded CPUs are non-linear, in other words they do not \"scale\" properly. A typical two CPU Hyperthreaded system behaves a bit like two normal fast CPUs with two very slow CPUs which are only active when the system is busy. The OS will normally report it as a four CPU system.<br /><br />Whereas the service time (CPU used) for an average transaction remains constant on a normal CPU as the load increases, the service time for a Hyperthreaded CPU increases as the load increases.<br /><br />If all you are looking at is the CPU %busy reported by the OS, you will see a Hyperthreaded system perform reasonably well up to 50% busy then as a small amount of additional load is added the CPU usage shoots up and the machine saturates.<br /><br />To be clear, the Hyperthreads do normally increase the peak throughput of the system, they often provide a performance benefit, but they make it extremely difficult to manage the capacity of a system.<br /><br />I've been looking at calibration methods and models that can deal with the capacity measurement problem, and will discuss them in future posts.",
        "metadata": {
          "word_count": 581,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Hyperthreading is used by most recent Intel servers. It can have a negative impact (a slowdown) to speedups of 15-20% per CPU. Software and hardware improvements mean that the latest operating systems have a larger benefit. Older operating systems are naiive about Hyperthreading and while they run, they don't schedule jobs optimally. Longer waits for pipeline stalls due to memory references and branch mispredictions create larger \"pipeline bubbles\" For many years the CPU utilization of a normal system has been used as a primary capacity metric. In other words, if you keep the CPU usage below 80-90% busy, you expect that the throughput at 60% busy is about twice the workload at 30% busy. ommon assumptions made by capacity planners is invalid. Hyperthreaded CPUs are non-linear, in other words they do not \"scale\" properly.  Hyperthreaded systems perform reasonably well up to 50% busy then as a small amount of additional load is added the CPU usage shoots up and the machine saturates. I've been looking at calibration methods and models that can deal with the capacity measurement problem."
      },
      "tags": [
        "monitoring",
        "observability",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_ae14fe66",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Playing with Fire",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/06/playing-with-fire.html",
      "content": {
        "text": "What would you do if your house burned down? Build another one, but that takes a while, so next time it catches fire you call the fire brigade. They get there in time to save some stuff, but you still have to rebuild it.<br /><br />Since fires seem to happen rather often <span style=\"font-style: italic;\">(bear with me on this, I'll get to my point eventually)</span> you hire your own fireman. Thats better, you are putting out fires early enough now that the house remains habitable more often.<br /><br />But the fireman is only there during working hours, so you hire a team of firemen to give you 24x7 coverage, and tidy up the burned patches that appear every few days.<br /><br />However your spouse is not happy with the mess, the disruption and the cost of a team of firemen, and a nice salesman comes by and sells you a set of smoke detectors, alarm bells and water sprinklers, and you hire a cleanup crew with mops.<br /><br />Its still messy and inconvenient, and everything is slightly soggy so you ask around. You find that some people are having fires much less often than you do, and they think its because they had their house checked over by a building standards inspector.<br /><div style=\"text-align: justify;\"><br /></div> The inspector looked around your house and predicted the next few things that would start a fire - so you could fix them in advance. The inspector also advised you on how to build a new house that wouldn't catch fire so easily in the first place.<br /><br />So my point is that if you are running a datacenter things will go wrong, and from a performance and capacity perspective rapid growth rates, fast changing applications and sudden changes in user activity levels can all put you into a fire fighting situation.<br />You can develop a fire fighting mentality, or a fire prevention mentality. One problem is that fire fighters are heroes, but when did you last hear the story of a heroic building inspector saving thousands of lives? Few people are fans of building inspectors, but they are mandated by local government to keep people safe.<br /><br />The recent upsurge of interest in the <a href=\"http://www.itil-itsm-world.com/\">Information Technology Infrastructure Library</a> (ITIL) seems to be a response to the auditing requirements of Sarbanes-Oxley (SOX). ITIL specifies lots of best practices for capacity planning amongst other things, as my co-author Bill Walker described in our <a href=\"http://www.phptr.com/bookstore/product.asp?isbn=0130894028&amp;rl=1\">Sun Blueprint on Capacity Planning</a> Book. So it may be SOX that gets you to look at ITIL, and helps you justify your building inspector status as the hero that helped pass a SOX audit perhaps? OK, its a stretch, but perhaps it helps get beyond the fire fighting mentality towards predictive capacity modelling.<br /><br />So how well are you doing? One way to find out is to rate yourself on a maturity model, and there is a good paper on this in the current issue (3.06) of the <a href=\"http://www.cmg.org/measureit/\">CMG newsletter Measure-IT</a>.",
        "metadata": {
          "word_count": 503,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Since fires seem to happen rather often, you hire your own fireman. Thats better, you are putting out fires early enough now that the house remains habitable more often. But the fireman is only there during working hours, so you hire a team of firemen to give you 24x7 coverage, and tidy up the burned patches that appear every few days. However your spouse is not happy with the mess, the disruption and the cost of the firemen, and a nice salesman comes by and sells you a set of smoke detectors, alarm bells and water sprinklers. Some people are having fires much less often than you do, and they think its because they had their house checked over by a building standards inspector. The inspector looked around your house and predicted the next few things that would start a fire so you could fix them in advance. ITIL seems to be a response to the auditing requirements of Sarbanes-Oxley (SOX) ITIL specifies lots of best practices for capacity planning. CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Please submit your best shots of New Zealand for next week. Visit CNN.com/Travel next Wednesday for a new gallery of snapshots."
      },
      "tags": [
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_97b062d9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Site - digging the foundations",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-site-digging-foundations.html",
      "content": {
        "text": "Today the new leach-field pipe was inspected, and the foundations were dug, I did a stop-motion video using iStopMotion on an old MacOS laptop looking down from the house, and most of the day's work is compressed into one minute.<br /><br /><object width=\"420\" height=\"255\"><param name=\"movie\" value=\"http://www.youtube.com/v/Qy_CpzdjxHI&hl=en_US&fs=1&\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/Qy_CpzdjxHI&hl=en_US&fs=1&\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"255\"></embed></object><br /><br />Tomorrow they will do the detailed preparations for the pour on Monday of the garage slab itself (6\" thick), perimeter foundation (at least 18\" deep, more on the downhill side where it is on in-fill), a walkway down the side of the garage, an \"apron\" in front of the garage to park on, and pads for the propane tank and generator.",
        "metadata": {
          "word_count": 118,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Today the new leach-field pipe was inspected, and the foundations were dug. I did a stop-motion video using iStopMotion on an old MacOS laptop looking down from the house. Most of the day's work is compressed into one minute."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_c0956c60",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal Performing Monkey's Paw at Hotel Utah",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/fractal-performing-monkeys-paw-at-hotel.html",
      "content": {
        "text": "All the songs in Fractal's set in SF are now up on YouTube, I also recorded the Seattle and Portland gigs but had problems getting a good quality soundtrack to the video. The short and intense finale to their set is Monkey's Paw. Fractal and Headshear will be back at the Hotel Utah in SF on Saturday April 18th, put it in your calendar now!<br /><br /><object width=\"480\" height=\"295\"><param name=\"movie\" value=\"http://www.youtube.com/v/Us2MRUSdqM8&hl=en&fs=1\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/Us2MRUSdqM8&hl=en&fs=1\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"480\" height=\"295\"></embed></object>",
        "metadata": {
          "word_count": 81,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "All the songs in Fractal's set in SF are now up on YouTube. Fractal and Headshear will be back at the Hotel Utah in SF on Saturday April 18th, put it in your calendar now!"
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_4205b188",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "My CMG paper on Crunching Data In the Cloud is published in MeasureIT",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/my-cmg-paper-on-crunching-data-in-cloud.html",
      "content": {
        "text": "The slides are also available at <a href=\"http://www.slideshare.net/adrianco/crunch-your-data-in-the-cloud-with-elastic-map-reduce-amazon-emr-hadoop\">http://www.slideshare.net/adrianco/crunch-your-data-in-the-cloud-with-elastic-map-reduce-amazon-emr-hadoop</a> - and MeasureIT has a PDF of the whole paper at <a href=\"http://www.cmg.org/measureit/\">http://www.cmg.org/measureit/</a>",
        "metadata": {
          "word_count": 21,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The slides are also available at http://www.slideshare.net/adrianco/crunch-your-data-in-the-cloud-with-elastic-map-reduce-amazon-emr-hadoop. MeasureIT has a PDF of the whole paper."
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_741f74bf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "cars and a place to keep them...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/02/cars-and-place-to-keep-them.html",
      "content": {
        "text": "I'm going to blog about cars over the coming months, partly because I'm about to start building a big garage to keep them in and I want to document the process.<br/><br/>We bought a house about 5 years ago that had a wooden shed as it's single garage. We had five cars at the time and there seemed to be enough space to build a big garage. However other priorities intervened and it wasn't until the middle of 2009 we finally got serious.<br/><br/>The first action was to make a large flat space, to see how big it could be, then we got bogged down in the planning process and this week we finally have the key approvals in place!<br/><br/>More details to come....<div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div>",
        "metadata": {
          "word_count": 130,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm going to blog about cars over the coming months, partly because I'm about to start building a big garage to keep them in and I want to document the process. We bought a house about 5 years ago that had a wooden shed as it's single garage. The first action was to make a large flat space, to see how big it could be, then we got bogged down in the planning process."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4608bec4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Easy Wi-Fi",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-easy-wi-fi.html",
      "content": {
        "text": "You can get free AT&T Wifi at Starbucks, and this app streamlines the process of getting online.",
        "metadata": {
          "word_count": 17,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_93f7dda7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Currency and Light",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-currency-and-light.html",
      "content": {
        "text": "Currency makes it easy to keep track of how far the pound has fallen so far (which makes me happy because I get more for my dollars) and Light is trivial but very useful. It just turns on a full brightness white screen that makes a good flashlight.",
        "metadata": {
          "word_count": 48,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Currency makes it easy to keep track of how far the pound has fallen. Light is trivial but very useful. It just turns on a full brightness white screen that makes a good flashlight."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e87c685f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What the iPhone doesn't do (yet) and thoughts on why not.",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/what-iphone-doesnt-do-yet-and-thoughts.html",
      "content": {
        "text": "There has been a lot of commentary, complaints and opinions on the iPhone. I haven't seen much discussion of its features and strategy from the perspective of the realities of product development. In my opinion, what Apple have done is the right set of strategic and tactical moves for the first product in a new family. What was announced and shown is not the final feature set for the initial device and it does not include the full vision of the product.<br /><br />Lets look at the timing of the announcement. As a new entrant in the mobile phone marketplace, the correct strategy is to pre-announce. There is no existing product from Apple to cannibalize, and there is only partial overlap with the iPod market. The announcement was made after the holiday shopping season, and the timing is setup to get a volume ramp in place for the 2007 holiday shopping season. The initial launch  is based on the minimum marketable features (<a href=\"http://www.phptr.com/bookstore/product.asp?isbn=0131407287&rl=1\">MMF</a>) required to address the Apple oriented consumer marketplace. Rather than wait until the full feature set is ready or create low quality solutions for a wide feature set, the Apple strategy is to develop a small number of features to an extremely high level of quality and integration, and focus on the needs of their core market of existing OSX and iPod users.<br /><br />Lets look at disclosure related issues. For a phone to be released as a product it has to go through FCC testing that takes a month or two, and the FCC process is relatively open. All the new phones are scooped by <a href=\"http://www.engadgetmobile.com/2007/01/12/fcc-fridays/\">Engadget Mobile</a> before they turn up in stores. For the big splash product announcement, it needed to be scheduled before Apple turns over an iPhone to the FCC test process. In order to keep details on the product quiet for as long as possible, it is also much easier to do the initial launch before completing negotiations with key third party application developers like Adobe and Microsoft. I've heard that there is no Adobe Flash support in the device at present, and there is a clear need to support Microsoft Office at some point in the future. These omissions are easily fixed, its just a matter of time.<br /><br />New models in the iPod range are announced when they are basically in stock in the stores. If you take the iPhone package, and remove the phone parts, keeping the iTunes music and video functionality, and WiFi/web connectivity, you are left with a very nice looking wide-screen networked iPod. Its main issue would be the relatively small capacity flash, so that could be increased, or a hard disk could possibly be crammed into the package. I would not expect anything like this to be announced until it is completely ready and in-stock, but if it exists, it could end up being released this summer around the same time as the iPhone actually ships. Since it isn't a phone, its outside the Cingular agreement, but adding a WiFi only VOIP client like iChat or Skype would create a product that competes with the <a href=\"http://www.learningcenter.sony.us/assets/itpd/mylo/prod/index.html\">Sony Mylo</a>.<br /><br />During the demo's no-one tried to show the iPhone's camera, this indicates to me that it isn't finished, and I hear elsewhere that they are still working on video capture. For use as a video-phone, the camera is on the wrong side, you can't see the display while you are on-camera. This makes it seem less likely that a full iChat function will be included in the initial package.<br /><br />Apple is getting a lot of criticism for its locked down and controlled approach to third party software on the iPhone, and lack of a developers program. Developer support falls outside the minimum marketable features required for initial launch into the consumer marketplace. By taking full control of the product, Apple can make sure that very high quality standards are in place, and that applications integrate with the iPhone experience. The reality of product development also makes it hard to build a stable developer API until the product is finished, so I fully expect a phased developer program. The initial phase included applications like Google Maps and Cingular Visual Voicemail from development partners (and I expect some kind of GPS location service to appear in the product soon - perhaps even in the initial release). The second phase will be a closed private developer program including big partners like Microsoft and Adobe. The third phase will start to open up to the Apple developer community, with stable public APIs and developer tools. Extensibility is a MMF for the professional/consumer (prosumer) and enterprise marketplaces, along with Microsoft Office support. This may take a year or so to arrive, its inevitable, but I can see why its not a feature of the initial product launch.<br /><br />There has also been a lot of grumbling about Cingular and the lack of 3G service. Apple have dropped  hints that they will support 3G sooner rather than later. My guess is that 3G is considered an MMF for the European and Asian markets, so I wouldn't be surprised if the models launched in those markets in late 2007 and 2008 included 3G support, and as Cingular's own 3G network continues to roll out over the USA the timing would make sense here as well. The real alternative to Cingular for Apple would be to setup their own Mobile Virtual Network Operator (MVNO) like <a href=\"http://www.helio.com/#homepage\">Helio</a> or <a href=\"http://www.virginmobileusa.com/\">Virgin Mobile</a>. This is a big complicated thing to do without any experience, so my guess is that they decided that the highest priority was to get the product launched with a big network partner like Cingular, and to decide later on whether it is worth creating an MVNO for a less compromised product. So when the exclusivity arrangement with Cingular expires, they may well focus on their own MVNO services.<br /><br />So thats my opinion, as someone who has developed products and strategies in the past and understands the compromises, but with no inside information on their actual plans. My own plan is to avoid the initial release, and see what the product looks like for the Xmas 2007 shopping season.",
        "metadata": {
          "word_count": 1040,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Apple has done the right set of strategic and tactical moves for the first product in a new family. The timing is setup to get a volume ramp in place for the 2007 holiday shopping season. The initial launch is based on the minimum marketable features. Apple's strategy is to develop a small number of features to an extremely high level of quality and integration. For a phone to be released as a product it has to go through FCC testing that takes a month or two, and the FCC process is relatively open. New models in the iPod range are announced when they are basically in stock in the stores. If you take the iPhone package, and remove the phone parts, you are left with a very nice looking wide-screen networked iPod. Its main issue would be the relatively small capacity flash, so that could be increased. No-one tried to show the iPhone's camera, this indicates to me that it isn't finished. For use as a video-phone, the camera is on the wrong side. This makes it seem less likely that a full iChat function will be included. I fully expect a phased developer program. The initial phase included applications like Google Maps and Cingular Visual Voicemail. The second phase will be a closed private developer program including big partners like Microsoft and Adobe. The third phase will start to open up to the Apple developer community, with stable public APIs and developer tools. The real alternative to Cingular for Apple would be to setup their own Mobile Virtual Network Operator (MVNO) like Helio or Virgin Mobile. This is a big complicated thing to do without any experience, so my guess is that they decided that the highest priority was to get the product launched with a big network partner. So when the exclusivity arrangement with Cedular expires, they may well focus on their own MVNO services. My own plan is to avoid the initial release, and see what the product looks like for the Xmas 2007 shopping season.  no inside information on their actual plans."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_2b3713a9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "NoSQL Netflix Use Case Comparison for Cassandra",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/10/nosql-netflix-use-case-comparison-for.html",
      "content": {
        "text": "<div>Jonathan Ellis @spyced of Riptano  kindly provided a set of answers that I have interspersed with the questions below.</div><div><br /></div><div>The original set of questions are <a href=\"http://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html\">posted here</a>. Each NoSQL contender will get their own blog post with answers, when there are enough to be interesting, I will write some summary comparisons.</div><div><br /></div><div>If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself.</div><div><br /></div><div><b><span class=\"Apple-style-span\"  style=\"font-size:large;\">Use Case Scenario for Comparison Across NoSQL Contenders </span></b></div>While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them, so that we understand the differences in behavior. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. I assume that is running on AWS and use that terminology, but the concepts are generic.<br /><br /><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><b>Use Case</b></span><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><br /></span>A TV based device calls the API to add a movie to its favorites list (like the Netflix instant queue, but I have simplified the concept here), then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list.<div><br /><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Favorites Storage<br /></b></span>Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads.<br /><blockquote><i>The most natural way to model per-user favorites in Cassandra is to have one row per user, keyed by the userid, whose column names are movie IDs.  The combination of allowing dynamic column creation within a row and allowing very large rows (up to 2 billion columns in 0.7) means that you can treat a row as a list or map, which is a natural fit here.  Performance will be excellent since columns can be added or modified without needing to read the row first.  (This is one reason why thinking of Cassandra as a key/value store, even before we added secondary indexes, was not really correct.)<br /><br />The best introduction to Cassandra data modeling is Max Grinev's series on </i><a href=\"http://maxgrinev.com/2010/07/09/a-quick-introduction-to-the-cassandra-data-model/\"><i>basics</i></a><i>, </i><a href=\"http://maxgrinev.com/2010/07/12/do-you-really-need-sql-to-do-it-all-in-cassandra/\"><i>translating SQL concepts</i></a><i>, and </i><a href=\"http://maxgrinev.com/2010/07/12/update-idempotency-why-it-is-important-in-cassandra-applications-2/\"><i>idempotence</i></a><i>.</i></blockquote></div><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b><br /></b></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 1: Availability Zones<br /></b></span>When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones, is the initial write local or spread over the zones, is the write replication zone aware so data is replicated to more than one zone?<br /><blockquote><i>Briefly, both reads and writes have a </i><a href=\"http://www.riptano.com/docs/0.6.5/consistency/index\"><i>ConsistencyLevel</i></a><i> parameter controlling how many replicas across how many zones must reply for the request to succeed.  Routing is aware of current response times as well as network topology, so given an appropriate ConsistencyLevel, reads can be routed around temporarily slow nodes.<br /><br />On writes, the coordinator node (the one the client sent the request to) will send the write to all replicas; as soon as enough success messages come back to satisfy the desired consistency level, the coordinator will report success to the client.<br /><br />For more on consistency levels, see </i><a href=\"http://www.slideshare.net/benjaminblack/introduction-to-cassandra-replication-and-consistency\"><i>Ben Black's excellent presentation</i></a><i>.</i><br /></blockquote><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 2: Partitioned Behavior with Two Zones<br /></b></span>If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the NoSQL service support?<br /><ul><li>one zone decides that it is still working for reads and writes but half the size, and the other zone decide it is offline</li><li>both zones continue to satisfy reads, but refuse writes until repaired</li><li>data that has a master copy in the good zone supports read and write, slave copies stop for both read and write</li><li>both zones continue to accept writes, and attempt to reconcile any inconsistency on repair</li></ul><div><div></div><blockquote><div><i>Cassandra has no 'master copy' for any piece of data; all copies are equal.  The other behaviors are supported by different ConsistencyLevel values for reads (R) and writes (W):</i></div><div><i><br /></i></div><div><i>R=QUORUM, W=QUORUM: One zone decides that it is still working for reads and writes, and the other zone decides it is offline</i></div><div><i>R=ONE, W=ALL: Both zones continue to satisfy reads, but refuse writes </i></div><div><i>R=ONE, W=ONE: Both zones continue to accept writes, and reconcile any inconsistencies when the partition heals</i></div><div><i><br /></i></div><div><i>I would also note that reconciliation is timestamp-based at the column level, meaning that updates to different columns within a row will never conflict, but when writes have been allowed in two partitions to the same column, the highest timestamp will win.  (This is another way Cassandra differs from key/value stores, which need more complex logic called vector clocks to be able to merge updates to different logical components of a value.)</i></div></blockquote><div></div></div><div><b>Question 3: Appending a movie to the favorites list</b></div>If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/values, and are queries by attribute/value supported?<br /></div><div><div></div><blockquote><div><i>Cassandra's ColumnFamily model generally obviates the need for a read before a write, e.g., as above using movie IDs as column names.  (If you wanted to allow duplicates in the list for some reason, you would generally use a UUID as the column name on insert instead of the movie ID.)</i></div><div><i><br /></i></div><div><i>The maximum value size is 2GB although in practice we recommend using 8MB as a more practical maximum. Splitting a larger blob up across multiple columns is straightforward given the dynamic ColumnFamily design. The maximum row size is 2 billion columns. Queries by attribute value are supported with secondary indexes in 0.7.</i></div></blockquote><div></div></div><div><b>Question 4: Handling Silent Data Corruption</b></div><div>When the storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously?</div><i></i><blockquote><i>Cassandra handles repairing corruption the same way it does other data inconsistencies, with </i><a href=\"http://wiki.apache.org/cassandra/ReadRepair\"><i>read repair</i></a><i> and </i><a href=\"http://wiki.apache.org/cassandra/AntiEntropy\"><i>anti-entropy repair</i></a><i>.</i></blockquote><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 5: Backup and Restore</b></span></div><div>Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?</div><i></i><blockquote><i>Because Cassandra's </i><a href=\"http://wiki.apache.org/cassandra/MemtableSSTable\"><i>data files are immutable</i></a><i> once written, creating a point-in-time snapshot is as simple as hard-linking the current set of sstables on the filesystem.  Performance impact is negligible since hard links are so lightweight.  Rolling back simply consists of moving a set of snapshotted files into the live data directory.  The snapshot is as consistent as your ConsistencyLevel makes it: any write visible to readers at a given ConsistencyLevel before the snapshot will be readable from the snapshot after restore.  The only scalability problem with snapshot management is that past a few TB, it becomes impractical to try to manage snapshots centrally; most companies leave them distributed across the nodes that created them.</i></blockquote><i></i>",
        "metadata": {
          "word_count": 1292,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Each NoSQL contender will get their own blog post with answers, when there are enough to be interesting, I will write some summary comparisons. If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself. A TV based device calls the API to add a movie to its favorites list. Then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer is round robin. The most natural way to model per-user favorites in Cassandra is to have one row per user, keyed by the userid. The combination of allowing dynamic column creation within a row and allowing very large rows (up to 2 billion columns in 0.7) means that you can treat a row as a list or map. When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones? Is the write replication zone aware so data is replicated to more than one zone? Routing is aware of current response times as well as network topology. Given an appropriate ConsistencyLevel, reads can be routed around temporarily slow nodes. On writes, the coordinator node (the one the client sent the request to) will send the write to all replicas. Cassandra has no 'master copy' for any piece of data; all copies are equal. The other behaviors are supported by different ConsistencyLevel values for reads (R) and writes (W): If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? Can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/values, and are queries by attribute/value supported? The maximum value size is 2GB although in practice we recommend using 8MB as a more practical maximum. Splitting a larger blob up across multiple columns is straightforward given the dynamic ColumnFamily design. Queries by attribute value are supported with secondary indexes in 0.7.  Cassandra can repair corruption the same way it does other data inconsistencies. What is the performance and availability impact during the backup? How is a known good version of the dataset restored? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen? Once written, creating a point-in-time snapshot is as simple as hard-linking the current set of sstables on the filesystem. Rolling back simply consists of moving a set of snapshotted files into the live data directory."
      },
      "tags": [
        "aws",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_8576662b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "How to finish writing a book | by Adrian Cockcroft | 15th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/how-to-finish-writing-book-by-adrian.html",
      "content": {
        "text": "I've written four books, and several years ago I developed \"Cockcroft's law of book writing\". This states that a book will grow in size as you write it, and that the number of pages left to write will increase as you write. This seems counter-intuitive, but it has been confirmed many times in practice. I hope this posting provides some useful advice for writers, and helps people finish what they have started. <br /><br />To make a concrete example, let's say you decide to write a book and you come up with an outline that adds up to 200 pages. You start work and write 50 pages, then, when you revisit your outline to update the page count estimates, you find that they now add up to 300 pages. You wrote more than you expected to cover each subject, and discovered more subjects that needed to be discussed. The essential problem here is that there are now 300-50 = 250 pages left to go. Before you started you only had 200 pages left to go.<br /><br />This problem is recursive, if you write another 50 pages you will find that you have now written the first 100 pages of a 400 page book, and you now have 300 pages left to write. This explains why there are so many people who have written part of a book, but never finished it.<br /><br />The aproach I took in writing my later books was to maintain a spreadsheet that tracks the pages left to write or edit, update it very regularly, and generate a plot with a trend line from the data. You can then see when (or if) you will finish the book. In order to get the trend line to target a specific delivery date, you have to force the number of pages left to go down. You do this by writing pages that you promise never to edit again, and by deleting whole sections and chapters. I deleted three entire chapters from one of my books to get it finished.<br /><br />Another problem you can run into is that the content you wrote at the start of the process is less well written than later content, so you think you have finished, re-read parts of the book that were finished ages ago, and discover that it needs a complete rewrite.<br /><br />I often get asked if I will update my Sun Performance and Tuning book, and I don't intend to do a third edition. This is mainly because I'm interested in other things, and I'm no longer up to date with the subjects I would need to cover. I have sketched out a possible book on capacity planning with free tools, and the trend line on that book is nice and flat. I haven't really started writing it, and so it hasn't started getting bigger yet....<br /><br />My good friends Jim Mauro and Richard McDougall are closing in on the end point for <a href=http://www.solarisinternals.com>Solaris Internals 2nd Edition</a>. I've been looking forward to it for a while, and its going to be a monster book, covering how Solaris 10 really works, lots of DTrace based examples, and is going to be the essential companion for anyone looking at Open Solaris.",
        "metadata": {
          "word_count": 541,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "\"Cockcroft's law of book writing\" states that a book will grow in size as you write it. This seems counter-intuitive, but it has been confirmed many times in practice. I hope this posting provides some useful advice for writers. Many people who have written part of a book, but never finished it. The aproach I took in writing my later books was to maintain a spreadsheet that tracks the pages left to write or edit, update it very regularly, and generate a plot with a trend line. I don't intend to do a third edition. This is mainly because I'm interested in other things, and I'm no longer up to date with the subjects I would need to cover. I have sketched out a possible book on capacity planning with free tools, and the trend line on that book is nice and flat."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_bfd08e37",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Heading to CMG09 in Dallas today",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/12/heading-to-cmg09-in-dallas-today.html",
      "content": {
        "text": "various talks on capacity planning in the cloud on Wed, then I'm presenting on Thurs. My slides are at http://www.slideshare.net/adrianco - this is probably my last visit to CMG, its been shrinking for a while, we will see how it looks on the ground.",
        "metadata": {
          "word_count": 44,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This is probably my last visit to CMG, its been shrinking for a while, we will see how it looks on the ground. various talks on capacity planning in the cloud on Wed, then I'm presenting on Thurs. My slides are at http://www.slideshare.net/adrianco"
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_6e8fda02",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix: I'm hiring a very senior engineer / application architect",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/netflix-im-hiring-very-senior-engineer.html",
      "content": {
        "text": "<a href=\"http://jobs.netflix.com/DetailFlix.asp?flix2562\">Here's the details and links to apply via http://jobs.netflix.com</a>.<br /><br />Netflix <a href=\"http://www.techcrunch.com/2008/12/30/netflix-adobe-google-make-best-places-to-work-list-att-ebay-radioshack-among-the-worst/\">really is one of the best places to work</a> and we are hiring strongly, there are several other openings for engineers and managers in Web Engineering. We are very picky, only hire the best people and the best fit for the job...<br /><br />Good luck!<br /><br /><span style=\"font-weight:bold;\">The Position:</span><br />As a leading member of the Merchandising Systems development team, you will re-define the website architecture into components and services, enhancing the agility and pace of development at Netflix as we scale up and leverage cloud computing technologies to support our personalized movie selection algorithms. As an exceptionally talented and experienced architect you will hold your own among a very demanding peer group, you will learn a lot and so will we.<br /><br />Netflix is scaling the company by increasing talent density in high performance, experienced and motivated teams. There is a flat organization, minimal process, and an emphasis on inter-personal over-communication to stay coordinated while keeping everything extremely agile.<br /><br /><span style=\"font-weight:bold;\">Responsibilities will include: </span><br />\u2022 Architectural leadership, broad knowledge of cloud/web services and object oriented technologies and designs<br />\u2022 You will need to clearly articulate your design choices, mentoring and educating our engineering community<br />\u2022 Hands on coding of the key objects and interfaces that define the architectural boundaries of components and services<br />\u2022 Untangling existing subsystems using tools such as Structure 101, re-organizing build systems to create re-usable component jars<br />\u2022 Migration of existing SQL based persistence models to distributed forms that efficiently scale to hold billions of data items for tens of millions of customers<br /><br /><span style=\"font-weight:bold;\">Qualifications: </span><br />\u2022 10+ years of application development experience, agile methods and architectural responsibility<br />\u2022 Mastery of object oriented architecture and design in Java<br />\u2022 Experience with high-traffic, highly scaling, multi-threaded distributed software design patterns<br />\u2022 Deep conceptual understanding of scalability, performance, queuing, availability, coherency, caching, synchronization<br />\u2022 Fluency in SQL and an expert in the evolution of persistent storage: Sharding, Hadoop, Eventual Consistency, SimpleDB, Dynamo<br />\u2022 Demonstrated ability to publish and present your ideas via conference presentations, technical blog postings, or published books or papers<br />\u2022 A Bachelors or Masters in Computer Science or equivalent engineering discipline<br /><br /><span style=\"font-weight:bold;\">Characteristics of Success at Netflix: </span><br />\u2022 Freedom with responsibility. A focus on effective execution<br />\u2022 Strong customer orientation, both internally and externally<br />\u2022 Strategic thinking both technically and business-wise<br />\u2022 Self motivated / self starter <br />\u2022 Personal drive to achieve world class software development <br />\u2022 Desire to work in a fast-paced, evolving, growing, dynamic environment <br />\u2022 Love of technical challenges and a sense of pride in solving them<br />\u2022 Ability to work well in a cross functional environment",
        "metadata": {
          "word_count": 455,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Netflix is looking for a Web Engineering engineer. The position is in the Merchandising Systems development team. You will re-define the website architecture into components and services. Netflix is one of the best places to work. Netflix is scaling the company by increasing talent density in high performance, experienced and motivated teams. There is a flat organization, minimal process, and an emphasis on inter-personal over-communication to stay coordinated. 10+ years of application development experience, agile methods and architectural responsibility. Deep conceptual understanding of scalability, performance, queuing, availability, coherency, caching, synchronization. Expert in the evolution of persistent storage: Hadoop, Eventual Consistency, SimpleDB, Dynamo. Software developer has a love of technical challenges and a sense of pride in solving them. Strong customer orientation, both internally and externally. Strategic thinking both technically and business-wise."
      },
      "tags": [
        "agile",
        "architecture",
        "cloud",
        "engineering",
        "netflix",
        "performance",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_8477475e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Can OpenStack catch up with AWS? Looks unlikely to me.",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/07/can-openstack-catch-up-with-aws-looks.html",
      "content": {
        "text": "There has been a lot of chatter about the new <a href=\"http://openstack.org/\">OpenStack standard</a> in the last few days, and how it could become a third option in the cloud infrastructure market alongside Amazon AWS and VMWare vCloud. For example <a href=\"http://cloudscaling.com/blog/cloud-computing/does-openstack-change-the-cloud-game\">Randy Bias/Cloudscaling has a good overview of the players</a>.<br /><br />The real question is whether OpenStack can catch up and become viable. Amazon is always described as being years ahead of the competition and having the lions share of the market. Estimates of the size of their lead depends on who you talk to, but I don't see anyone disputing their lead. On top of that, Amazon is investing heavily, has big customers like Zynga and Netflix stretching and hardening their systems (along with a huge number of small customers), and has already added many features beyond the basic compute (EC2) and storage (S3) that have been copied by others including OpenStack.<br /><br />So for OpenStack to catch up, means that they have to move faster than Amazon, and leverage the slipstream effect where Amazon has had to educate the market and figure out what works, so competitors can copy their successes. However when I look at the details of the OpenStack specification they appear to be copying some of Amazon's problems as well. In particular the account and authentication model, which does not scale for enterprise use.<br /><br />At the most basic level, it is infeasible to change the security model of a platform architecture, it's one of the most fundamental starting points that conditions the layers above. Changes to account and authentication management break all the layers of applications and tools that are built on the platform. One of the first problems that Netflix had with Amazon, was the lack of sub-accounts and role based access control (RBAC), and while beating up Amazon on this point for the last two years, we have built our own platform layers and tools to compensate. As a result, we find it impossible to use any of the web consoles or tools produced by Amazon or the many cloud vendors, which assume that there is a single account owner who can do anything. We hope that Amazon will eventually support sub-accounts, and when they do, I expect it will break everyone's tools.<br /><br />At this point OpenStack is just the base level of the platform, it doesn't really have layers of tools on top yet, but its account and authentication model appears to be exactly the same as Amazon, so they will end up with layers of tooling that don't meet the needs of enterprise customers.<br /><br />What's the difference between a startup and an enterprise? In a startup, everyone in IT knows the root password to every machine in their infrastructure. In an enterprise root passwords are carefully controlled, they change when someone in-the-know leaves, and specialist groups manage different parts of the system (Network ops can only mess with the switches, DBA's can only mess with the database etc.). The problem with a single account for the cloud is that everyone who needs to do <span style=\"font-weight:bold;\">anything</span> to that account can do <span style=\"font-weight:bold;\">everything</span> to it, and the common tools are oriented to a single user, managing several accounts, rather than a hierarchy of users managing parts of one account. All the systems in the account need an authentication key to access cloud services, and changing the password and key means you have to re-key every system. Get this wrong and your cloud will evaporate in an instant.<br /><br />So in my opinion, a necessary but  not sufficient condition for OpenStack to eventually catch up with AWS is that they need to build sub-accounts and RBAC into their spec from the start. However it seems much more likely that Amazon will just disappear into the distance from what I've seen so far.",
        "metadata": {
          "word_count": 642,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The real question is whether OpenStack can catch up and become viable. Amazon is always described as being years ahead of the competition and having the lions share of the market. On top of that, Amazon is investing heavily. OpenStack appears to be copying some of Amazon's problems. Account and authentication model does not scale for enterprise use. Changes to account and authentication management break all the layers of applications. OpenStack is just the base level of the platform, it doesn't really have layers of tools on top yet. Its account and authentication model appears to be exactly the same as Amazon. They will end up with layers of tooling that don't meet the needs of enterprise customers. The problem with a single account for the cloud is that everyone who needs to do anything to that account can do everything to it. All the systems in the account need an authentication key to access cloud services. Get this wrong and your cloud will evaporate in an instant."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "netflix",
        "platform",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_7a1a4f16",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "SE toolkit 3.4 on Solaris 10 Opteron Workaround",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/10/se-toolkit-34-on-solaris-10-opteron.html",
      "content": {
        "text": "The current 3.4 build of SE is available from <a href=\"http://www.sunfreeware.com/setoolkit.html\">the sunfreeware site</a> and supports Solaris 8, 9 and 10 for SPARC and x86. It also includes full source code of the interpreter for the first time.<br />However it only includes 32bit x86 support, and when run on a 64bit Solaris kernel on an Opteron system, it will fail to run. This is due to the return value from the <span style=\"font-weight:bold;\">isalist</span> command returning <span style=\"font-weight:bold;\">amd64</span> as the first word, rather than <span style=\"font-weight:bold;\">pentium</span>. You can get it going with a workaround by changing the startup script /opt/RICHPse/bin/se to match the string \"*pentium*\" rather than \"pentium*\". This lets the 32bit x86 binary run on the 64bit <span style=\"font-weight:bold;\">amd64</span> system, and most of the scripts will still work. Some that try to access the kernel directly via<span style=\"font-weight:bold;\">kvm</span> will fail, but most scripts use <span style=\"font-weight:bold;\">kstat</span> which doesn't need 64bit accesses.<br />The real fix is to use the source to compile an amd64 build....",
        "metadata": {
          "word_count": 163,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The current 3.4 build of SE is available from sunfreeware.com. It supports Solaris 8, 9 and 10 for SPARC and x86. It also includes full source code of the interpreter for the first time. The real fix is to use the source to compile an amd64 build.  style=\"font-weight:bold;\">kstat which doesn't need 64bit accesses."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_a8888dac",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "extraccting on SPARC",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/05/extraccting-on-sparc.html",
      "content": {
        "text": "I made it with no problems on SPARC Solaris 10 (using /usr/sfw/bin/gcc) and it seems to be working fine. The binary doesn't work on Solaris 8 because its looking for a later version SUNW_1.2 of libexacct.so.1 and I don't have a compiler on this Solaris 8 machine, so thats enough testing for today...",
        "metadata": {
          "word_count": 53,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The binary doesn't work on Solaris 8 because its looking for a later version SUNW_1.2 of libexacct.so. I made it with no problems on SPARC Solaris 10 (using /usr/sfw/bin/gcc)"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d5e4ac1f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "First look at Android and implications for the Homebrew MyPhone",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/11/first-look-at-android-and-implications.html",
      "content": {
        "text": "Google's new <a href=\"http://code.google.com/android/download.html\">Android SDK</a> is a lot more than a Linux distribution, its a complete and documented application framework with developer tools support. The primary language for application development is Java, with a special runtime called the Dalvik virtual machine that is designed to run efficiently on a battery powered device.<br /><br />The SDK is 55MB, there is available Eclipse support.<br /><br />Sample applications include Google Maps, mail, webkit based browser\u00a0and the usual Phone and PIM stuff. The <a href=\"http://code.google.com/android/reference/view-gallery.html\">user interface</a> seems to be based on a new set of Java classes, but there is also direct access to the display in C/C++ using SGL for 2D and OpenGL ES for 3D, and audio and video players for things like mp3 and H.264.<br /><br />From a first pass through the docs, a lot of work has gone into this already. It looks as if it should be portable<br />to a homebrew phone without much difficulty, and from then on, we are likely to be leveraging a much larger community than the other open alternatives. It appears that the current state of the <a href=\"http://www.openmoko.org\">OpenMoko</a> project is that they have made some progress and have a somewhat usable toolkit written in Python, but that C/C++ development based on GTK+ is painful, and documentation is scarce. It looks to me as if Android has taken the same basic inspiration as OpenMoko, but with a much more professional execution, and with a Java based developer platform.<br /><br />We will be discussing this at the <a href=\"http://hbmobile.org/wiki/index.php?title=Main_Page\">Homebrew Mobile Phone club meeting on Wednesday</a>. We are holding the meeting at Mozilla's offices and will also be discussing their work on a mobile version of the Mozilla browser.",
        "metadata": {
          "word_count": 284,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Google's new Android SDK is 55MB, available Eclipse support. Sample applications include Google Maps, mail, webkit based browser, and the usual Phone and PIM stuff. The current state of the OpenMoko project is that they have made some progress and have a somewhat usable toolkit written in Python. C/C++ development based on GTK+ is painful, and documentation is scarce. Android has taken the same basic inspiration, but with a much more professional execution."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_9f053853",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal CD Sequitur now available from cdbaby.com",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/02/fractal-cd-sequitur-now-available-from.html",
      "content": {
        "text": "Fractal's new CD \"Sequitur\" is now available to review and buy on CD Baby at:<br /><br /><a href=\"http://cdbaby.com/cd/fractal3\">    http://cdbaby.com/cd/fractal3</a><br /><br />'Music to tap your brain to'. Progressive rockers Fractal return with an eclectic musical trip featuring complex rhythms, style-bending and -blending, and existential musical and lyrical outcries.",
        "metadata": {
          "word_count": 47,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "'Music to tap your brain to' Progressive rockers Fractal return with an eclectic musical trip featuring complex rhythms, style-bending and -blending, and existential musical and lyrical outcries."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4a5f6927",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Global Classified Ads - Kijiji",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/07/global-classified-ads-kijiji.html",
      "content": {
        "text": "eBay recently took the final step with their Kijiji classified advertising brand of launching it in the USA. There has been quite a lot of commentary, most of it fails to point out what matters about Kijiji. eBay also holds a small stake in Craigslist, which also has sites around the world, but there is one very significant difference that no-one seems to have commented on:<br /><br />Go to <a href=\"http://rome.craigslist.org\">http://rome.craigslist.org</a> - there are a handful of entries, but the most important thing is that the site is still in English. Check out <a href=\"http://paris.craigslist.org\">http://paris.craigslist.org</a> - same thing.<br /><br />Now try <a href=\"http://roma.kijiji.it\">http://roma.kijiji.it</a> and <a href=\"http://paris.kijiji.fr\">http://paris.kijiji.fr</a> - notice the difference? Its localized and there are a decent number of items listed. Now see how many times you can figure out http://[city].kijiji.[country] (remember to use the local name) and see just how many cities and languages there are. Remember to try <a href=\"http://shanghai.kijiji.cn\">http://shanghai.kijiji.cn</a> too... There are also a few countries where eBay bought a local brand - <a href=\"http://www.gumtree.com\">http://www.gumtree.com</a> for the UK, and <a href=\"http://www.loquo.com\">http://www.loquo.com</a> for Spain.<br /><br />So its clear that eBay has spent the last few years building up a global localized classified advertising business that has wide coverage and a lot of listings. They have had English-speaking Canada for a while (they also have French Canadian), so adding the USA must have been a small amount of extra work and cost, and there was a well crafted and debugged product ready to roll.<br /><br />It may take a while to catch on in the USA, but its not likely to go away. Its free for the users, and can generate revenue by using services like banner ads and Google Adsense to monetize the page views. Kijiji is already doing quite nicely in my opinion....<br /><br />[Disclosure: I used to work for eBay, I know some of the people who created Kijiji, but I have had nothing to do with Kijiji since it launched in 2005, and this is all public information]",
        "metadata": {
          "word_count": 333,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "eBay recently launched Kijiji in the USA. eBay also holds a small stake in Craigslist, which also has sites around the world. There is one significant difference that no-one seems to have commented on. eBay has spent the last few years building up a global localized classified advertising business that has wide coverage and a lot of listings. It may take a while to catch on in the USA, but its not likely to go away. \"I have had nothing to do with Kijiji since it launched in 2005, and this is all public information,\" eBay founder says. \"I know some of the people who created Kijijiji, but I have had no involvement with it since 2005,\" he adds."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d14f66c2",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CMG07 and the A. A. Michelson Award",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/12/cmg07-and-a-michelson-award.html",
      "content": {
        "text": "From the <a href=\"http://www.cmg.org/national/michelson-nominations.html\">Computer Measurement Group website</a>:<br /><br /><blockquote>Each year, CMG considers exceptional individuals who have made significant contributions to our profession over their entire professional careers as possible recipients of the A.A. Michelson Award.     <p><a href=\"http://en.wikipedia.org/wiki/Albert_Abraham_Michelson\"> Albert Abraham Michelson</a>, for whom the award is named, was known for his outstanding technical accomplishments in measuring the speed of light as well as for his role as a teacher and inspirer of others. CMG presents this lifetime achievement award to a single individual to recognize and encourage the same combination of technical excellence and professional contributions found in only an exceptional few.</p></blockquote><p></p>The recipient is nominated and chosen by <a href=\"http://www.cmg.org/national/michelson-awards.html\">the past winners</a>, and it has been granted to one person every year since 1974. This year I was very honored to be the recipient! It was announced on Monday 3rd December, at the start of the CMG07 conference. I gave an acceptance speech, that I have <a href=\"http://perfcap.blogspot.com/2007/12/a-michelson-award-acceptance-speech.html\">summarized into a blog post</a>.<br /><br />Thanks to everyone who has supported me over the years, reading the blogs, papers and books; coming to training classes; the engineers at Sun who fixed my bugs and added the features I lobbied for to make Solaris more measurable; and the friends, mentors and mentees that I have worked with for many years.<br /><br />Cheers!<br />Adrian<br /><br /><p><br /></p><p><br /></p>",
        "metadata": {
          "word_count": 223,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The A.A. Michelson Award is named for Albert Abraham Michelson. The recipient is nominated and chosen by the past winners. The award has been granted to one person every year since 197. This year I was very honored to be the recipient! It was announced on Monday 3rd December, at the start of the CMG07 conference. I gave an acceptance speech, that I havesummarized into a blog post."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_73d28526",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - High and low usage and costs for the first month",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/10/solar-power-high-and-low-usage-and.html",
      "content": {
        "text": "The solar array continues to generate over 20KWh on a clear day, but our daily consumption varies a lot. On Sunday we were running the washing machine, dryer, pumping water from the well for a few hours, watching TV and baking/cooking/dish-washing for seven people, and we hit an all time high using almost 50KWh.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYOq_5UQGAfbbqAIcCr0btHnOcf8bpMauAcdox2qetFqDNmAmCOr3natw8QrQPl1wNd2z0bkzE7upYqQnLUw_MK9PksYCBE9lKOI18Dub7oCN3UfxY2ZM7tvPm0of9ugx3_N50/s1600-h/Picture+17.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 120px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiYOq_5UQGAfbbqAIcCr0btHnOcf8bpMauAcdox2qetFqDNmAmCOr3natw8QrQPl1wNd2z0bkzE7upYqQnLUw_MK9PksYCBE9lKOI18Dub7oCN3UfxY2ZM7tvPm0of9ugx3_N50/s320/Picture+17.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5389376584142441810\" /></a><br /><br />The next day, running pretty close to the minimum possible, we used about 13 KWh.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGrLl9PiEnrTDOV7DkGGDKWZhlJG6LOakTfJCwEt7-mfP8vuS7YQj489sFZ361W4DL9LSEOwVYD1x-GLp-Ik_4V6P2oh5IefgoJefiF92vCqjFi8ZuwpauXfLFu4kmLKJKBQJ3/s1600-h/Picture+18.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 120px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgGrLl9PiEnrTDOV7DkGGDKWZhlJG6LOakTfJCwEt7-mfP8vuS7YQj489sFZ361W4DL9LSEOwVYD1x-GLp-Ik_4V6P2oh5IefgoJefiF92vCqjFi8ZuwpauXfLFu4kmLKJKBQJ3/s320/Picture+18.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5389377219496137618\" /></a><br /><br />Our total generation for the month of September was about 700 KWh. The output is dropping as the day gets shorter and the sun gets lower, so our 4.2KW system peaked at about 3.4KW at the start of the month, after a rain shower cleaned the panels and the air we saw a peak of 3.7KW for a short period of one day, then it dropped back to 3.3KW and it ended the month at 3.2KW. The daily output on a clear day was 26KWh at the start of September and 24KWh at the end.<br /><br />Our approximate total consumption for september was 1100 KWh, and since we are on the time-of-use E6 rate the net metering subtracted the 700 we generated and broke down the rest as -100 KWh at peak time (1-7pm), and 500 KWh mid and off-peak. We were billed $11.50 which is the base level for PG&E, and a total of $17 for 400 KWh was added to the annual accumulation.<br /><br />In a year's time the net for the year is due to get settled. <a href=\"http://www.pge.com/tariffs/tm2/pdf/ELEC_SCHEDS_E-6.pdf\">The very complex details are available from PG&E</a>, but basically instead of costing us on a sliding scale of 11c-38c per KWh, the solar array keeps us at the bottom end of the scale and at the E6 rate it costs 8c off peak, 14c mid peak, and while we are running the meter backwards at peak PG&E pay us 30c per KWh. So our typical electric bill of $250 was reduced to $28.<br /><br />I have been asked about the payback time for the installation. This is part of the calculation that the vendor sales-rep makes for your particular situation. In my case we made it much more complex by replacing the roof shingles, shutting down our use of propane, buying new appliances and increasing our use of electricity. However If we ignore all that and average $200/month savings, then after all the rebates the solar system installation of about $20K has a payback time of less than 8 years on a naive calculation. If we take into account a 5% interest rate on the installation cost it's more like 12 years, and if we assume that electricity costs would rise over that time it's somewhere in-between. I'm also assuming that the value of the house has gone up.",
        "metadata": {
          "word_count": 494,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The solar array continues to generate over 20KWh on a clear day, but our daily consumption varies a lot. On Sunday we were running the washing machine, dryer, pumping water from the well for a few hours, watching TV and baking/cooking/dish-washing for seven people, and we hit an all time high using almost 50KWh. Our total generation for the month of September was about 700 KWh. The output is dropping as the day gets shorter and the sun gets lower, so our 4.2KW system peaked at about 3.4KW at the start of the month. The daily output on a clear day was 26KWh at the start of September. At the end of the month it was 3.2KW. The solar array keeps us at the bottom of the scale. Solar system installation of about $20K has a payback time of less than 8 years on a naive calculation. If we take into account a 5% interest rate on the installation cost it's more like 12 years. $250 was reduced to $28."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b9c07b86",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Tutorial at LISA08 San Diego Nov 9th",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/09/tutorial-at-lisa08-san-diego-nov-9th.html",
      "content": {
        "text": "<a href=\"http://www.usenix.org/lisa08/going\"> <img src=\"http://www.usenix.org/events/lisa08/art/lisa08_going.jpg\" border=\"0\" width=\"162\" height=\"57\" alt=\"I'm going to LISA '08\"> </a><br /><br />I'm giving my usual performance tools and tuning for Solaris/Linux class on Sunday 9th Nov in San Diego.",
        "metadata": {
          "word_count": 32,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm giving my usual performance tools and tuning for Solaris/Linux class on Sunday 9th Nov in San Diego. I'll also be giving a talk on how to get the most out of your Linux."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_f42986f1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Understanding and using Amazon EBS - Elastic Block Store",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/03/understanding-and-using-amazon-ebs.html",
      "content": {
        "text": "There has been a lot of discussion in the last few days about EBS <a href=\"http://blog.reddit.com/2011/03/why-reddit-was-down-for-6-of-last-24.html\">since it was implicated in a long outage at reddit.com</a>.<br /><br /><span style=\"font-weight:bold;\">Rule of Thumb</span><br /><br />The benchmarking Netflix did when we started on AWS highlighted some inconsistent behavior in EBS. The conclusion we reached is a rule of thumb for EBS - If you sustain less than 100 iops (input+output per second) long term average it works fine. Short term bursts can be 1000 iops. By short term I mean less than a minute, long term more than 10 minutes. YMMV.<br /><br />If you are doing benchmarks like this, collect response time and throughput and plot your data over time. You need to run long enough that the performance shows steady state behavior. The problem with EBS is that it doesn't have a particularly steady state. To explain why we need to look at the underlying architecture. I don't know the details of how EBS is implemented, but there is enough information available to explain how it behaves.<br /><br /><span style=\"font-weight:bold;\">EC2</span><br /><br />The AWS EC2 architecture is built out of commodity low cost servers, they have a single 1 Gbit network, a few CPUs, a few disks and a few GBytes of RAM. Over time the models have changed, and EC2 does have a 10Gbit network option now, but for the purposes of this discussion, we will concentrate on the 1Gbit network models. Individual servers are virtualized into the familiar EC2 models by slicing up the RAM, CPUs and disk space, and sharing the network bandwidth and disk iops. When EC2 instances break or are de-configured any data on the internal disks is lost.<br /><br /><span style=\"font-weight:bold;\">Elastic Block Store</span> <a href=\"http://aws.amazon.com/ebs/\">http://aws.amazon.com/ebs/</a><br /><br />The AWS EBS service provides a reliable place to store data that doesn't go away when EC2 instances are dropped, but it provides the same mounted filesystem capability as the internal disks. If you need more disk space or iops you can mount more EBS volumes on a single EC2 instance and spread out the load. The EBS volume is connected to the EC2 instance over the same 1Gbit network as everything else. In a datacenter this would normally be built using commercially available high end storage from NetApp, EMC or whoever, it would be quite expensive (cost much more than the EC2 instance itself) and be fast and reliable up to the limits of the network. To build a low cost cloud, the alternative is to use RAIN (Redundant Array of Inexpensive Nodes) which could be based on standard EC2 instances, or variants that have more disks per CPU. Software is then used to coordinate the RAIN systems and provide an EBS service that will be slower than high end storage, but still be very reliable and be limited by the 1Gbit network.<br /><br /><span style=\"font-weight:bold;\">S3 and Availability Zones</span><br /><br />AWS also has an S3 storage service that behaves like a key/value store accessed via http requests and a REST API rather than a directly mounted filesystem. It is possible to rapidly snapshot an EBS volume to and from S3, including incremental backups and restores that fill as they go so you don't have to wait before using them. This implies to me that they share a common back-end infrastructure to some extent. The primary additional difference is that EBS volumes only exist in a single AWS Availability Zone, and S3 data is replicated across two or three Availability Zones. It takes longer to replicate the data for S3, so it is slower, but it is very robust and it is almost impossible to lose data. You can think of an Availability Zone as a complete datacenter. All the zones in a region are separate datacenters that are close enough together to support a high bandwidth and low latency network between them, but they have separate power sources and connections to the Internet.<br /><br /><span style=\"font-weight:bold;\">Multi-Tenancy</span><br /><br />The most efficient chunk of compute and storage resource to buy and deploy when building a cloud is either too big or too small for the actual use cases of real applications. Virtualization is used to sub-divide the chunks, but then each individual machine is supporting several independent tenants. For local disks, the space is divided between the tenants, and for network, everyone is sharing the same 1Gbit interface. This works well on average, because most use cases aren't network or disk bound, but you cannot control who you are sharing with and some of the time you will be impacted by the other tenants, increasing variance within each EC2 instance. You can minimize the variance by running on the biggest instance type, e.g. m1.xlarge, or m2.4xlarge. In this case there isn't room for another big tenant, so you get as much as possible of the disk space and network bandwidth to yourself. The virtualization layer reserves some of the capacity. It's possible to tell that another tenant is keeping the CPU busy by looking at the \"stolen time\", but there are no metrics for stolen iops or network bandwidth.<br /><br />The EBS service is also multi-tenant. Many clients mount disk space from a common backend pool of EBS disks. You don't get to see how the disk space is allocated, or how data is replicated over more than one disk or instance for durability, but it is limited to that availability zone. A busy client can slow down other clients that share the same EBS service resources. EBS volumes are between 1GB and 1TB in size. If you allocate a 1TB volume, you reduce the amount of multi-tenant sharing that is going on for the resources you use, and you get more consistent performance. Netflix uses this technique, our high traffic EBS volumes are mostly 1TB, although we don't need that much space.<br /><br />This is actually no different in principle to large shared storage area network (SAN) backends (from companies like EMC or NetApp) that are in common datacenter use. Those also have unpredictable performance when pushed hard, and they mask this issue with lots of battery backed memory. The difference is cost. EBS is 10c per Gbyte per month. If you build a competing public cloud service using high end storage, you could get better performance but your cost base would be far higher.<br /><br /><span style=\"font-weight:bold;\">Visualizing Multi-Tenant Disk Access</span><br /><br />I have come up with some diagrams to help show what happens. I'm basing them on a simplified view of AWS where the only instance type family is m1 and everything they have is made out of one underlying building block. This consists of a fairly old specification system, 8 cores, 16GB RAM, four 500GB disks and a single 1Gbit network. In reality, AWS is much more complex than this, but the principles are the same.<br /><br />Starting with internal disks, this is what an m1.xlarge looks like, it takes up the whole system apart from a small amount of memory, disk space and network traffic for the VM and AWS configuration/management information. You can expect to have minimal multi-tenant contention for network or disk access.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXsbnHDP9fYeYb6KVc4ZZ2eVR6rCNVxSxtspAraH2sIPmrL76yQ9ab1YJGb9Gl1-dAU2VAPTOOWYRJeIvWcvMoUwDYw3fXpdSdh7ZS_4CqK2H1cACT95ieCmq2ABZmwKEYXwqv/s1600/Slide1.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXsbnHDP9fYeYb6KVc4ZZ2eVR6rCNVxSxtspAraH2sIPmrL76yQ9ab1YJGb9Gl1-dAU2VAPTOOWYRJeIvWcvMoUwDYw3fXpdSdh7ZS_4CqK2H1cACT95ieCmq2ABZmwKEYXwqv/s320/Slide1.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904272363288306\" /></a><br /><br />The m1.large instance type halves the system, each instance has two disks rather than four, so it shares the network and some of the disk controller bandwidth, but it should have minimal iops contention with the other tenant.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzstDMnIhK9Z62HppBiUUwr3XA6itj1Im8Ka4OYJ0S-3M5QtTbSYf-yD4iSYbyFm2MOPgatw9t1R8726_dIghMAVoDhyJGlVTNhhhT8NlLfgyWEvRTssbeJfzFErMyeYOx99aZ/s1600/Slide2.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzstDMnIhK9Z62HppBiUUwr3XA6itj1Im8Ka4OYJ0S-3M5QtTbSYf-yD4iSYbyFm2MOPgatw9t1R8726_dIghMAVoDhyJGlVTNhhhT8NlLfgyWEvRTssbeJfzFErMyeYOx99aZ/s320/Slide2.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904141661806594\" /></a><br /><br />The low cost m1.small instance type has 160GB of disk per instance, so we can fit three per disk for a total of 12 instances per machine. (Note that the memory for a real m1.small is 1.7GB, so only 9 would fit in 16GB RAM, however the c1.medium instance has 1.7GB, 350GB disk, and more CPU, so six m1.small and three c1.medium fits). You can see the multi-tenancy problem here, any of the instances could generate enough traffic to fill the network and make one of the disks busy, and that is going to affect other instances in an unpredictable and random manner.<br /><br />Here's an analogy, you can rent a whole house, rent a room in a house, or rent a couch to sleep on, you get what you pay for.<br /><br />If you ever see public benchmarks of AWS that only use m1.small, they are useless, it shows that the people running the benchmark either didn't know what they were doing or are deliberately trying to make some other system look better. You cannot expect to get consistent measurements of a system that has a very high probability of multi-tenant interference.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkjhnXyNNJ1BfQZh81gw22dJk3dR27JVN-GxSfv5Kau4CISRvnzBeQcam-YnWAeZkNKSFmCbBslhsLJND72cBogC4B6kX87y80-indOFwKGZUxDNwoftqlgQb3UWzZVa2dVFtn/s1600/Slide3.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgkjhnXyNNJ1BfQZh81gw22dJk3dR27JVN-GxSfv5Kau4CISRvnzBeQcam-YnWAeZkNKSFmCbBslhsLJND72cBogC4B6kX87y80-indOFwKGZUxDNwoftqlgQb3UWzZVa2dVFtn/s320/Slide3.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904139472765474\" /></a><br /><br /><span style=\"font-weight:bold;\">EBS Multi-Tenancy</span><br /><br />The next few diagrams show the flow of traffic from an instance to the EBS service, which makes two copies of the data on disks connected to separate instances. I don't know if this is how EBS works, but if we wanted to build an EBS-like system using the same building block it could look like this. In practice it would make sense to have specialized back-end building blocks with much more disk space.<br /><br />The first diagram shows how Netflix runs EBS, we start with an instance that has the maximum network bandwidth with no other tenants, we allocate maximum size 1TB volumes (we stripe many of them together) and the service has to use most of the disk space in the back-end to support us, so we have less chance of another tenant making the EBS disks busy. The performance of EBS in this simplified case would be higher latency than local disk, but otherwise similar. I suspect that in reality the EBS volume is spread over more disks in the backend which gives higher throughput but with higher variance.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgXtg_x53emLqE72JvWa8MEpHUw66x8Tx_lTFNo8B70MxjEa_sisc0ErzbNOIs_TashZtePyXgaDHT5IcfUx2wOaOStGqzjgl8GcC_a1a4JaqSKxLG-LGHgEtJcL2Hx40wGi3H/s1600/Slide4.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjgXtg_x53emLqE72JvWa8MEpHUw66x8Tx_lTFNo8B70MxjEa_sisc0ErzbNOIs_TashZtePyXgaDHT5IcfUx2wOaOStGqzjgl8GcC_a1a4JaqSKxLG-LGHgEtJcL2Hx40wGi3H/s320/Slide4.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904134814057154\" /></a><br /><br />If we drop down to a more typical m1.large configuration with 100GB of EBS each, two instances are sharing network bandwidth, the EBS service is servicing two sets of requests, and the EBS back end has many more tenants per disk, so we would expect better peak performance than the two internal disks in the m1.large but more variance.<br /> <br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfxGOEXR92LzJgrZPVSy6PT7D4Rja5KhCDkWz_OZ09lP9hsaroaMYbfU7d7VXQGXEw7JUZ6LmDMwO3SHZAGHa9oBYcAaE6hWyXetbtUUd4le67miqojWQHbA08hY9reGjcHh-R/s1600/Slide5.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjfxGOEXR92LzJgrZPVSy6PT7D4Rja5KhCDkWz_OZ09lP9hsaroaMYbfU7d7VXQGXEw7JUZ6LmDMwO3SHZAGHa9oBYcAaE6hWyXetbtUUd4le67miqojWQHbA08hY9reGjcHh-R/s320/Slide5.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904137434583618\" /></a><br /><br />For the case where we have many m1.small instances each accessing a 10GB EBS volume, it is clear that the peak performance is going to be far better than a share of a local disk, but the contention for network, EBS service and backend disks will be extremely variable, so performance will be very inconsistent.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkgMGgvbUBecBiXDitmU1mca2mECRB6FH3YiAyDZfN6xEKgm8eBeK8_Vn5v0-kKbxnGbpJBxqgN14DVH36G51nspNpFE3E5aqQSoesAlEkutMSOp21nY2fu93sgJxWAQsEGvwZ/s1600/Slide6.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 480px; height: 360px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjkgMGgvbUBecBiXDitmU1mca2mECRB6FH3YiAyDZfN6xEKgm8eBeK8_Vn5v0-kKbxnGbpJBxqgN14DVH36G51nspNpFE3E5aqQSoesAlEkutMSOp21nY2fu93sgJxWAQsEGvwZ/s320/Slide6.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5585904130808746306\" /></a><br /><br /><span style=\"font-weight:bold;\">How To Measure Disk and Network Performance</span><br /><br />Someone should write a book on that (I already did, but for Solaris), however <a href=\"https://forums.aws.amazon.com/thread.jspa?messageID=124044#124044\">there is a useful AWS forum post</a> that explains how to interpret Linux iostat. This blog post is too long already, so Linux iostat will have to wait for another time.<br /><br /><span style=\"font-weight:bold;\">Best Practices for Cloud Storage with Cassandra</span><br /><br />There are two basic patterns for Cassandra, one is a persistent memory cache, where we size the data to fit in memory so that all reads are fast, and writes go to disk. The m2.4xl instance type with 68GB RAM and two 850GB disks is best. The second pattern is where there is a much larger data set than memory, and m1.xlarge with 16GB RAM and four 420GB disks will have the best iops for reads, and a much lower overall cost per GB for storage. In both cases, we get all the network bandwidth for servicing clients and the inter-node replication traffic, and minimal multi-tenant variance.",
        "metadata": {
          "word_count": 1992,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The benchmarking Netflix did when we started on AWS highlighted some inconsistent behavior in EBS. The conclusion we reached is a rule of thumb for EBS - If you sustain less than 100 iops ( input+output per second) long term average it works fine. The AWS EC2 architecture is built out of commodity low cost servers. Individual servers are virtualized into the familiar EC2 models. When EC2 instances break or are de-configured any data on the internal disks is lost. The AWS EBS service provides a reliable place to store data. The EBS volume is connected to the EC2 instance over the same 1Gbit network as everything else. In a datacenter this would normally be built using commercially available high end storage from NetApp, EMC or whoever. To build a low cost cloud, the alternative is to use RAIN (Redundant Array of Inexpensive Nodes) It is possible to rapidly snapshot an EBS volume to and from S3, including incremental backups and restores that fill as they go. This implies to me that they share a common back-end infrastructure to some extent. EBS volumes only exist in a single AWS Availability Zone, and S3 data is replicated across two or three Availability Zones. Each EC2 instance is supporting several independent tenants. Virtualization is used to sub-divide the chunks. For local disks, the space is divided between the tenants. For network, everyone is sharing the same 1Gbit interface. The EBS service is also multi-tenant. Many clients mount disk space from a common backend pool of EBS disks. A busy client can slow down other clients that share the same EBS resources. EBS volumes are between 1GB and 1TB in size. The difference is cost. EBS is 10c per Gbyte per month. If you build a competing public cloud service using high end storage, you could get better performance but your cost base would be far higher. The m1.large instance type halves the system, each instance has two disks rather than four. It shares the network and some of the disk controller bandwidth, but it should have minimal iops contention with the other tenant. The low cost m1.small instance type has 160GB of disk per instance, so we can fit three per disk for a total of 12 instances per machine. You can see the multi-tenancy problem here, any of the instances could generate enough traffic to fill the network and ma. If you ever see public benchmarks of AWS that only use m1.small, they are useless, it shows that the people running the benchmark either didn't know what they were doing or are deliberately trying to make some other system look better. You cannot expect to get consistent measurements of a system that has a very high probability of multi-tenant interference. The next few diagrams show the flow of traffic from an instance to the EBS service, which makes two copies of the data on disks connected to separate instances. In practice it would make sense to have specialized back-end building blocks with much more disk space. The performance of EBS in this simplified case would be higher latency than local disk, but otherwise similar. I suspect that in reality the EBS volume is spread over more disks in the backend which gives higher throughput but with higher variance. If we drop down to a more typical m1.large  we have less chance of another tenant making EBS disks busy. Two instances are sharing network bandwidth and the EBS service is servicing two sets of requests. EBS back end has many more tenants per disk, so we would expect better peak performance than the two internal disks in the m1.large configuration.  peak performance is going to be far better than a share of a local disk. The contention for network, EBS service and backend disks will be extremely variable, so performance will be very inconsistent. There are two basic patterns for Cassandra, one is a persistent memory cache, where we size the data to fit in memory so that all reads are fast, and writes go to disk. The second pattern is where there is a much larger data set than memory, and m1.xlarge with 16GB RAM and four 420GB disks will have the best iops for reads. In both cases, we get all the network bandwidth for servicing clients and the inter-node replication traffic, and minimal multi-tenant variance."
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "infrastructure",
        "metrics",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_02c8d839",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CMG08 Draft Agenda - Las Vegas in December",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/08/cmg08-draft-agenda-las-vegas-in.html",
      "content": {
        "text": "<a href=\"http://www.cmg.org/blog/home/04\">CMG just published the draft agenda for their annual conference on their website</a>. This year I will be on a panel with other Michelson Award winners discussing Visualization, and probably on a second panel talking about Unix performance. I'm also planning to repeat the half day training seminars on Unix Performance, and Capacity Planning with Free Tools. <br /><br />See you there!",
        "metadata": {
          "word_count": 63,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This year I will be on a panel with other Michelson Award winners discussing Visualization, and probably on a second panel talking about Unix performance. I'm also planning to repeat the half day training seminars on Unix Performance, and Capacity Planning with Free Tools."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_9ce602a8",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Must. Buy. iPhone. Must. Buy. iPhone....",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/must-buy-iphone-must-buy-iphone.html",
      "content": {
        "text": "OK, I give in. Its Apple at their best, insanely great, detailed craftsmanship and \"it just works\" usability. However I didn't want to get in line at a store, so I logged in to the Apple online store and made sure my one-click ordering was setup right and enabled, and at 6pm when the store re-opened I got my order in as fast as the site would refresh. It was a little slow, but as far as I could tell, the store was up and running and handled the peak load, so some extra congratulations to the team that runs the store. Delivery was stated to be 2-4 weeks, I should be near the front of the queue, so we will see what happens.<br /><br />What about the open phones? Well, OpenMoko is running late, but is just about to ship developer phones for real, however the version that will have WiFi is their second revision that is months away, and their software is still being developed. However <a href=\"http://www.gumstix.com\">Gumstix</a> are also very close to getting their new \"Goliath\" phone kit out, and that is a much more flexible design, where I can add WiFi etc. without replacing anything else, and I can wrap my own case design around the 4.3\" LCD. The software is rudimentary, but we intend to port the OpenMoko packages to the Gumstix based \"myPhone\", and tinker with it to do whatever we want. In the meantime, I will soon have a phone that \"just works\" better than anything else....",
        "metadata": {
          "word_count": 254,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "OpenMoko is running late, but is just about to ship developer phones for real. The version that will have WiFi is their second revision that is months away. Gumstix are also very close to getting their new \"Goliath\" phone kit out. \"MyPhone\" is a phone that \"just works\" better than anything else. The software is rudimentary, but we intend to port the OpenMoko packages to the Gumstix based \"myPhone\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_bd52129b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - More panels on the garage roof",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/02/solar-power-more-panels-on-garage-roof.html",
      "content": {
        "text": "<div>The SolarCity installation team arrives.</div><div><br /></div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzBvVXJCQYVCRMEZau8H982l-DmtVW12ra-J365aXpAlCQzg-a56UMotM71tIZ8pfV-3xVXHIOk6cn3QERZktTg57JipCdRcJsx5-O3czPvIqOdzgb8ssuAR6f1Z6_zHifreDh/s1600/IMG_1225.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 240px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzBvVXJCQYVCRMEZau8H982l-DmtVW12ra-J365aXpAlCQzg-a56UMotM71tIZ8pfV-3xVXHIOk6cn3QERZktTg57JipCdRcJsx5-O3czPvIqOdzgb8ssuAR6f1Z6_zHifreDh/s320/IMG_1225.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574545787924496914\" /></a><div><br /></div><div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhzBvVXJCQYVCRMEZau8H982l-DmtVW12ra-J365aXpAlCQzg-a56UMotM71tIZ8pfV-3xVXHIOk6cn3QERZktTg57JipCdRcJsx5-O3czPvIqOdzgb8ssuAR6f1Z6_zHifreDh/s1600/IMG_1225.jpg\"></a>Product specification on a panel, 210W peak output, Silicon by Kyocera made in Mexico.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIJLsvemDiUorJLOFxhpHhzYDV-_LLEK_wjH7r-VDQNvNi7kyTGMLHb60elZsFcGVnMRqE-pJIlo19JSIICQDIj05n4z-wEwkQWUAgqO3_gF5cK3b9tAOf4mPySXcK6GZL3j-L/s1600/IMG_1230.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 240px; height: 320px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiIJLsvemDiUorJLOFxhpHhzYDV-_LLEK_wjH7r-VDQNvNi7kyTGMLHb60elZsFcGVnMRqE-pJIlo19JSIICQDIj05n4z-wEwkQWUAgqO3_gF5cK3b9tAOf4mPySXcK6GZL3j-L/s320/IMG_1230.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574545780772111570\" /></a><br /><br /></div><div>Fronius inverter installed inside the garage, wired to the distribution panel shown next to it.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOjtIDXHMHpaerxOJgH4yP827QRQj7wk5bk5Rd1jjTvlCziQOTFexRlbbqR_96uDmiNxf24On1UZKHgXa11l5reARG0OpKzn1PZ8xfLAWM0LykNh5dYtQ65823oFtihgWXQXG3/s1600/IMG_1238.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 240px; height: 320px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOjtIDXHMHpaerxOJgH4yP827QRQj7wk5bk5Rd1jjTvlCziQOTFexRlbbqR_96uDmiNxf24On1UZKHgXa11l5reARG0OpKzn1PZ8xfLAWM0LykNh5dYtQ65823oFtihgWXQXG3/s320/IMG_1238.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574541614435294082\" /></a></div><div><br /></div><div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjOjtIDXHMHpaerxOJgH4yP827QRQj7wk5bk5Rd1jjTvlCziQOTFexRlbbqR_96uDmiNxf24On1UZKHgXa11l5reARG0OpKzn1PZ8xfLAWM0LykNh5dYtQ65823oFtihgWXQXG3/s1600/IMG_1238.jpg\"></a>Mounting rails on the garage roof.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqGLij-BDduws1XbO7QQzWvbtXLKqyofW1347E_TlMgFeDHAXyCQF_D0FCJGeeDd1kwiRDPIU7qK8yPYBCv4yihjkIPB8fTdXnmq90h85rplhO65iXROeFLDpGmGnItKUb2lri/s1600/SANY1239.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqGLij-BDduws1XbO7QQzWvbtXLKqyofW1347E_TlMgFeDHAXyCQF_D0FCJGeeDd1kwiRDPIU7qK8yPYBCv4yihjkIPB8fTdXnmq90h85rplhO65iXROeFLDpGmGnItKUb2lri/s320/SANY1239.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574541605795995666\" /></a></div><div><br /></div><div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiqGLij-BDduws1XbO7QQzWvbtXLKqyofW1347E_TlMgFeDHAXyCQF_D0FCJGeeDd1kwiRDPIU7qK8yPYBCv4yihjkIPB8fTdXnmq90h85rplhO65iXROeFLDpGmGnItKUb2lri/s1600/SANY1239.jpg\"></a>Closeup of how the rails mount on the shingles.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXhhzujCAghCxW8B1uyewB3FNdKzNrFz4v26kHMqPaaqtA9fYdWEm6soKMQg87eIKZO2KZA_Fuc080GgyUu5BpBzfCVWBFcRke5wJdbpEvW3VMRWVbtEwRrrzEGiuQfuRqScBi/s1600/SANY1241.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXhhzujCAghCxW8B1uyewB3FNdKzNrFz4v26kHMqPaaqtA9fYdWEm6soKMQg87eIKZO2KZA_Fuc080GgyUu5BpBzfCVWBFcRke5wJdbpEvW3VMRWVbtEwRrrzEGiuQfuRqScBi/s320/SANY1241.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574541601792478306\" /></a></div><div><br /></div><div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXhhzujCAghCxW8B1uyewB3FNdKzNrFz4v26kHMqPaaqtA9fYdWEm6soKMQg87eIKZO2KZA_Fuc080GgyUu5BpBzfCVWBFcRke5wJdbpEvW3VMRWVbtEwRrrzEGiuQfuRqScBi/s1600/SANY1241.jpg\"></a>All the panels in place, four rows of nine panels, 36 x 210W = 7560W DC, after the inverter this works out to about 6.5KW delivered AC power.<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMgoecgMptsddbW1T7fVBS0x1YFPtwQbwXjtAHFIevqiH4KXJWBstLJIhQpq57GUSd8e3nLyuJ6xBDEKUPzibGHU_Eb0x371-ArirxCnfR7e7OckWQFXCgQYdghoCzKrolkNWU/s1600/SANY1242.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMgoecgMptsddbW1T7fVBS0x1YFPtwQbwXjtAHFIevqiH4KXJWBstLJIhQpq57GUSd8e3nLyuJ6xBDEKUPzibGHU_Eb0x371-ArirxCnfR7e7OckWQFXCgQYdghoCzKrolkNWU/s320/SANY1242.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574541599685752738\" /></a></div><div><br /></div><div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjMgoecgMptsddbW1T7fVBS0x1YFPtwQbwXjtAHFIevqiH4KXJWBstLJIhQpq57GUSd8e3nLyuJ6xBDEKUPzibGHU_Eb0x371-ArirxCnfR7e7OckWQFXCgQYdghoCzKrolkNWU/s1600/SANY1242.jpg\"></a>View from the front of the garage early in the morning.<br /><div><a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgH6lpA1fDSWGGjlxjEAfM3D1Y1fayEJxjDftXCCKYXKV8jRzToq7vIrUZB1fs1zbkQaGmcFyF5Ylr4Hq9tUMvkbRAe7johRR8tR2QckPs61NDruWKClZXb6sVhKx6IMtC0lcYN/s1600/IMG_1232.jpg\"><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgH6lpA1fDSWGGjlxjEAfM3D1Y1fayEJxjDftXCCKYXKV8jRzToq7vIrUZB1fs1zbkQaGmcFyF5Ylr4Hq9tUMvkbRAe7johRR8tR2QckPs61NDruWKClZXb6sVhKx6IMtC0lcYN/s320/IMG_1232.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5574541619584015986\" style=\"cursor: pointer; width: 320px; height: 240px; \" /></a></div></div><div><br /></div><div>It's all wired in, we are waiting for inspection and approval before we can turn it on. The same monitoring system is used as was installed for my previous solar installation on the house roof. This time we leased the panels, there are several options, from no money down to buying the whole thing up front. I've opted for an initial payment of $10K, and $90/month fixed for the duration of the lease (20 years). I don't pay anything until its turned on.</div><div><br /></div><div>We currently generate about two thirds of our usage of electricity. The extra panels should triple the generated capacity, so we will generate about twice what we use in the short term. That evens out when we start charging a Nissan Leaf (delivery due in April), and change out our propane furnace for a heat pump and air conditioner (hopefully in time to cool us this summer).</div><div><br /></div><div>A useful side effect is that the garage itself will be a lot cooler inside during the summer, as the panels shade the roof.</div>",
        "metadata": {
          "word_count": 398,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The SolarCity installation team arrives. The SolarCity team arrives to install the solar panels on the roof. The installation takes place over a few hours. Product specification on a panel, 210W peak output, Silicon by Kyocera made in Mexico. Fronius inverter installed inside the garage, wired to the distribution panel. Mounting rails on the garage roof. P827QRQj7wk5bk5Rd1jjTvlCziQOTFexRlbbqR_96uDmiNxf24On1UZKHgXa11l5reARG0OpKzn1PZ8xfLAWM0LykNh5dYtQ65823oFtihgWXQXG3/s1600/IMG_1238.jpg. Closeup of how the rails mount on the shingles. Closeup of the rails on the roof. Close up of the roof with the rails attached. All the panels in place, four rows of nine panels, 36 x 210W = 7560W DC. After the inverter this works out to about 6.5KW delivered AC power. View from the front of the garage early in the morning. View from inside the garage in the middle of the night. View the garage from the back of the house in the evening. It's all wired in, we are waiting for inspection and approval before we can turn it on. The same monitoring system is used as was installed for my previous solar installation on the house roof. This time we leased the panels, there are several options, from no money down to buying the whole thing up front. The extra panels should triple the generated capacity, so we will generate about twice what we use in the short term. A useful side effect is that the garage itself will be a lot cooler inside during the summer, as the panels shade the roof."
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_10d9e611",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "CMG09 - Last Time Attendee",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/12/cmg09-last-time-attendee.html",
      "content": {
        "text": "I presented a half day training class and a paper at CMG09 in Dallas last week. I won't be at CMG10. The conference has shrunk to a few hundred attendees, mostly mainframe oriented, a few vendors, and a bunch of independent consultants. Its main value for me was to maintain and extend my social network of capacity planning and performance people. There were some useful and interesting papers, but not enough to justify a full week long conference in a very expensive hotel. CMG10 is in Orlando FL, and I can't justify traveling to the east coast for more of the same.<br /><br />A few years ago I discussed with the CMG board members what was needed to keep CMG from shrinking into irrelevancy, at the time I was at eBay, and early December is the peak business level for retail industry, so very few capacity/performance experts from retail can get away at that time of year, and I suggested they move the date. The dominant industries attending CMG have been banking, insurance and finance, which have been hit hard in the recession. The other change I advocated was that CMG should be held in the Bay Area, so that it could attract a lot more people from the major Web companies and computer hardware and software companies that are based here. Unfortunately CMG is locked into a long series of Hotel commitments for several years, and can't change its plans.<br /><br />So my position now, is that I will attend CMG again when it comes to me. In the meantime, I will encourage the people I met every year at CMG to attend the Velocity Conference in San Jose next June. <a href=\"http://en.oreilly.com/velocity2010\">http://en.oreilly.com/velocity2010</a> - the call for papers closes in January, so we have a few weeks to come up with abstracts.",
        "metadata": {
          "word_count": 303,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The conference has shrunk to a few hundred attendees, mostly mainframe oriented, a few vendors, and a bunch of independent consultants. Its main value for me was to maintain and extend my social network of capacity planning and performance people. The Velocity Conference will be held in San Jose next June. The call for papers closes in January, so we have a few weeks to come up with abstracts. In the meantime, I will encourage the people I met every year at CMG to attend the Velocity Conference."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_5b239648",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Changing gears",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/02/changing-gears.html",
      "content": {
        "text": "I started this blog in the summer of 2004 when I had finished at Sun and not yet started at eBay. After 16 years at Sun this was a big move. I knew people at eBay from the time in 1999 when they had a big outage and many Sun people got involved in helping them get up and running again. My thinking that summer was that web services platforms were where the real innovation was taking place, and I see eBay and PayPal as the leading transactional web services platforms.<br /><br />My first year at eBay was in the Operations Architecture group, where I was working on figuring out new platforms and upgrades, and helping with capacity planning tools and processes. I also figured out a lot about how eBay and PayPal really work, and the challenges of scaling a rapidly growing and changing high availability transactional platform to a size that is beyond most people's comprehension. I had some entertaining meetings with hopeful vendors who would come in with solutions to common industry problems (e.g. low utilization) that eBay doesn't have, and their largest existing deployment would be an order of magnitude too small to be useful. After describing a bit about how eBay works, they would get big eyes, admit that their product wasn't appropriate, and wander off to look for more normal customers... A lot of what eBay does is built internally because the generic products don't scale and we can build what we need ourselves for less.<br /><br />In the summer of 2005 I moved internally to help form eBay Research Labs. Since then we have hired some very experienced researchers and are becoming the focal point for innovation within eBay. This was another opportunity for me to change gears and greatly increase the scope of my work. Part of my role is to continue to research new platforms and technologies for the datacenter operations, and I've been joined in this work by my friend Paul Strong. Paul was in the N1 group at Sun, and is also the drummer for <a href=\"http://www.fractal-continuum.com\">Fractal</a>. Paul and I were both involved in the <a href=\"http://www.gridalliance.org\">Enterprise Grid Alliance</a>, he ended up as chair of the Technical Steering Committee, and edited the EGA's Grid Reference Architecture. He's now working on how to enhance the automation of eBay's datacenters.<br /><br />The other cool thing that came my way in 2005 was eBay's purchase of Skype. Its not just a VOIP tool, its a huge and fast growing community (something eBay understands very well) and an extremely innovative development platform. The Skype API is a fun place to do innovative research, and the Skype network has between 3 and 5 million active nodes at any point in time (up by a million in three months). I've been interested in the telecom market ever since I was one of the Sun Systems Engineers working with British Telecom in the early 1990's. Now I get to play with the future of telecom in the form of Skype, and I'm also very interested in mobile/wireless applications.<br /><br />In another sense I am changing gears with this post. I've changed the title and description, and it is now also being included in the <a href=\"http://www.bestofebayblogs.com\">Best of eBay Blogs</a> site. I've been encouraged by the example of other bloggers at that site to discuss a bit more openly what I get up to, but if you ask me what I'm really working on, all I can say is \"The future of e-commerce\".<br /><br />Cheers Adrian<br /><br />p.s. I just tried to spell-check this posting, and the built-in spell checker at blogger.com decided that the first error was the word \"blog\", which I find highly amusing, so I gave up and any spelling errors in the above are my fault.",
        "metadata": {
          "word_count": 633,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I started this blog in the summer of 2004 when I had finished at Sun and not yet started at eBay. After 16 years at Sun this was a big move. I knew people at eBay from the time in 1999 when they had a big outage. A lot of what eBay does is built internally because the generic products don't scale and we can build what we need ourselves for less. eBay Research Labs are becoming the focal point for innovation within eBay. Ebay's purchase of Skype in 2005 was a huge and fast growing community. The Skype API is a fun place to do innovative research. Skype network has between 3 and 5 million active nodes at any point in time. Adrian's blog is now being included in the Best of eBay Blogs site. If you ask him what he's really working on, all he can say is \"The future of e-commerce\""
      },
      "tags": [
        "architecture",
        "automation",
        "platform",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_adc8e6d9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Who needs a custom built Myphone?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/who-needs-custom-built-myphone.html",
      "content": {
        "text": "The mainstream phone manufacturers are looking for hits in the mass market, and looking for large niches to broaden their market. However if you take a \"long tail\" viewpoint, everyone wants a slightly different phone, and many people have phones that are the <span style=\"font-style: italic;\">least bad</span> one they could find.<br /><br />How about phones for people with poor eyesight? If you are over 50, can you read the small print on your phone's screen without reading glasses? I have a friend who has a medical condition that causes very poor eyesight, and sets her 17\" laptop screen resolution very low so she can read it. She would like a phone that has just the applications she needs, big easy to find icons for them, and big fonts on a large screen for the address list and other applications. Hard to find in the shops, but easy to custom build.",
        "metadata": {
          "word_count": 150,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Mainstream phone manufacturers are looking for hits in the mass market. Everyone wants a slightly different phone, and many people have phones that are the least bad one they could find."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2b88d5a5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Mobile Phones at Maker Faire",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/mobile-phones-at-maker-faire.html",
      "content": {
        "text": "The Silicon Valley Homebrew Mobile Phone Club is going to be featured at Maker Faire this May 19/20th. <a href=\"http://makerfaire.com/pub/e/319\">One of my phone design pictures</a> was used on the site :-)<br /><br />I've also been continuing to develop the general purpose <a href=\"http://millicomputing.blogspot.com\">millicomputing concepts and have been documenting them on my companion blog</a>. I have bought a Gumstix module for use in a phone, and I'm working on benchmarking it at the moment.",
        "metadata": {
          "word_count": 73,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Silicon Valley Homebrew Mobile Phone Club is going to be featured at Maker Faire this May 19/20th. I have bought a Gumstix module for use in a phone, and I'm working on benchmarking it at the moment."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4f4fd065",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Velocity and Volume - Speed Wins - Keynote at Flowcon",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/12/velocity-and-volume-speed-wins-keynote.html",
      "content": {
        "text": "November 1st was the first ever Flowcon, held in San Francisco, a new event to focus on continuous delivery related development topics.<br />\n<br />\nI was honored to be asked to present as the opening keynote speaker, and I owe a big thank you to Jez Humble and Gene Kim for the invitation.<br />\n<br />\nThe material I presented was less focused on Netflix related technologies than most of my talks, and instead looks at the challenges and motivations of speeding up the page of development. I also tried to provide a historical perspective of how the state of the art for software delivery has changed over the last few decades.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"http://img.youtube.com/vi/wyWI3gLpB8o/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"http://youtube.googleapis.com/v/wyWI3gLpB8o&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"http://youtube.googleapis.com/v/wyWI3gLpB8o&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\nThere was a second chance to present these slides with a bit more time a few days later, and the slides for that are linked below. There's a bit more of an introduction, and a more discussion of tools on the end, but it's basically the same message.<br />\n<br />\n<br />\n<iframe allowfullscreen=\"\" frameborder=\"0\" height=\"356\" marginheight=\"0\" marginwidth=\"0\" scrolling=\"no\" src=\"http://www.slideshare.net/slideshow/embed_code/28103959?rel=0\" style=\"border-width: 1px 1px 0; border: 1px solid #CCC; margin-bottom: 5px;\" width=\"427\"> </iframe> <br />\n<div style=\"margin-bottom: 5px;\">\n<strong> <a href=\"https://www.slideshare.net/adrianco/flowcon-added-to-for-cmg-keynote-talk-on-how-speed-wins-and-how-netflix-is-doing-continuous-delivery\" target=\"_blank\" title=\"Flowcon (added to for CMG) Keynote talk on how Speed Wins and how Netflix is doing Continuous Delivery\">Flowcon (added to for CMG) Keynote talk on how Speed Wins and how Netflix is doing Continuous Delivery</a> </strong> from <strong><a href=\"http://www.slideshare.net/adrianco\" target=\"_blank\">Adrian Cockcroft</a></strong> </div>",
        "metadata": {
          "word_count": 269,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "November 1st was the first ever Flowcon, held in San Francisco. I was honored to be asked to present as the opening keynote speaker. The material I presented was less focused on Netflix related technologies than most of my talks. There was a second chance to present these slides with a bit more time a few days later. The slides for that are linked below. There's a bit of an introduction, and a more discussion of tools on the end. Flowcon (added to for CMG) Keynote talk on how Speed Wins and how Netflix is doing Continuous Delivery from Adrian Cockcroft."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_9b6a1b36",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage plans approved",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-plans-approved.html",
      "content": {
        "text": "The county turned around the final update immediately and I got a letter in the mail today! Now I just need to visit them to get the actual permit document.<br/><br/>I'm getting started with the first step next week... <div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div><br />",
        "metadata": {
          "word_count": 49,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The county turned around the final update immediately and I got a letter in the mail today! Now I just need to visit them to get the actual permit document. I'm getting started with the first step next week..."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e3d2214e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Fring (Skype and IM's)",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/02/whats-on-my-iphone-fring-skype-and-ims.html",
      "content": {
        "text": "When I want to make an international call from my iPhone (or from my home phone) I use Skype. There is no native Skype application for the iPhone, so I use Fring, <br /><a href=\"http://share.skype.com/sites/en/2009/02/nokia_and_skype_partnering_in.html\">although today's announcement of a deal with Nokia</a> includes hints that they are working on more mobile platforms. They support Android already, so their main missing link at this point is the iPhone.<br /><br />Fring is a decent substitute for now. I mostly use it to keep up with Skype and YIM chat sessions when I'm away from my laptop.",
        "metadata": {
          "word_count": 94,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is no native Skype application for the iPhone, so I use Fring. I mostly use it to keep up with Skype and YIM chat sessions when I'm away from my laptop."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9df8e46d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Wally's Items...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/08/wallys-items.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p><a href=\"http://www.dilbert.com/comics/dilbert/archive/dilbert-20060821.html\">http://www.dilbert.com/comics/dilbert/archive/dilbert-20060821.html</a></p>  <p>Sounds like a good plan....</p>  <p><br /></p> <br /><p style=\"font-size: 10px; text-align: right;\">technorati tags:<a href=\"http://technorati.com/tag/dilbert\" rel=\"tag\">dilbert</a></p><p style=\"text-align: right; font-size: 8px;\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 28,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "\"Flock\" is a webcomic based on \"Dilbert\" by Dilbert. The comic was published in 1999. It is now being republished with the help of Technorati."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_1669b171",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What the iPhone doesn't do, why not, and why I still want one...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/there-is-very-nice-posting-on-iphone-at.html",
      "content": {
        "text": "There is a <a href=\"http://www.roughlydrafted.com/RD/RDM.Tech.Q2.07/34C8BD5D-E210-4A62-BE6F-FD21E046A397.html\">very nice posting on the iPhone at RoughlyDrafted</a>, taking a thoughtful look at all the coverage, and pointing out double standards in many of the negative commentators who are putting down the iPhone for not being \"enterprise ready\".<br /><br />Back in January, I wrote a <a href=\"http://perfcap.blogspot.com/2007/01/what-iphone-doesnt-do-yet-and-thoughts.html\">summary of thoughts on the iPhone</a> from the perspective of typical product development practices. That post got a lot of traffic, it seemed to be very popular in Germany, where there is a great tradition of quality product design (I can't think of a German designed product that isn't well engineered). <br /><br />Overall, with the additional advance information that has been released so far I don't think I was too far off the mark. I did expect that support for Adobe Flash and Java would be included, but they both appear to be missing. This is going to be the biggest pain, as there are a lot of web sites that use Flash or Java applets. Given time I expect these issues will be addressed.<br /><br />I talked about <a href=\"http://www.phptr.com/bookstore/product.asp?isbn=0131407287&rl=1\">Minimum Marketable Features (MMF) which are described in a book called Software by Numbers</a>, written by my good friend Mark Denne. Its a philosophy of development that optimizes product features by releases over time. Many of the iPhone's shortcomings are explained by an initial focus on the needs of people who have never used a \"smartphone\" before, (rather than a focus on the business market) and a close integration with other Apple products that support an \"insanely great\" combination of product features.<br /><br />For integration with non-Apple products, there is a two pronged strategy. One is to provide basic file viewers for Microsoft Word and Excel documents that are common email attachments. The other approach is to leverage Google's web based application suite within the Safari browser, which now includes Word, Excel and Powerpoint, Gmail etc, and also integrate with the Google maps application on the iPhone itself.<br /><br />For the question of whether the iPhone can be used in corporate environments, there are two issues - virtual private network (VPN) access to get at internal web sites, and email integration with Microsoft Exchange. There have been some hints that the standard MacOSX VPN functionality is included, and Apple states that syncing is supported with Outlook/Entourage contacts and calendars. For actual Exchange email access, the Exchange server would have to support IMAP or POP (most aren't setup this way) or the Outlook Web Access client could be used in the Safari browser on the iPhone.<br /><br />Safari is key. It is not only the way to manipulate documents and corporate email, its also the designated \"API\" for new iPhone applications to be developed. Apple stated that Safari will have access to many of the internal functions of the iPhone, so that the primary programming language for developers becomes Javascript. For all the people with existing mobile applications this is a pain, as they want to have J2ME to get them ported quickly. However, the iPhone is a huge leap forward in user interfaces, and to provide the consistent look and feel that magnifies the ease of use, Apple are enforcing Safari as the only framework for extending the iPhone. This makes perfect sense as a product strategy. It has upset many developers, but the laser-like focus on ease of use greatly increases the iPhone's perceived value and total addressable market. More users will be willing to pay more, increasing Apple's sales. Later on, perhaps Apple will focus more on the \"traditional\" smartphone market, but for now they are simply going to redefine what that market is.<br /><br />How many people have both iPods and cellphones in their pockets? This fall, how many  students will turn up at college with an iPhone in their pocket?<br /><br />My work machines are a MacBook Pro and a Blackberry. I'm building the homebrew myPhone design (more on that later), but I think I'm getting tempted by the user interface, I want an iPhone....",
        "metadata": {
          "word_count": 670,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Back in January, I wrote a <a href=\"http://perfcap.blogspot.com/2007/01/what-iphone-doesnt-do-yet-and-thoughts.html\">summary of thoughts on the iPhone. That post got a lot of traffic, it seemed to be very popular in Germany, where there is a great tradition of quality product design. I don't think I was too far off the mark. The iPhone's shortcomings are explained by an initial focus on the needs of people who have never used a \"smartphone\" before. The iPhone's integration with other Apple products that support an \"insanely great\" combination of product features. For integration with non-Apple products, there is a two pronged strategy. Safari is key. It is not only the way to manipulate documents and corporate email, its also the designated \"API\" for new iPhone applications to be developed. Apple stated that Safari will have access to many of the internal functions of the iP. Apple are enforcing Safari as the only framework for extending the iPhone. This makes perfect sense as a product strategy. It has upset many developers, but the laser-like focus on ease of use greatly increases the iPhone's perceived value. I'm building the homebrew myPhone design (more on that later) I think I'm getting tempted by the user interface, I want an iPhone. acBook Pro and a Blackberry."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4f2db3fc",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "More on Open Phones",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/more-on-open-phones.html",
      "content": {
        "text": "Thanks to a comment on my <a href=\"http://perfcap.blogspot.com/2007/01/build-your-own-phone-any-way-you-want.html\">previous open phone post</a>, I am reminded that I forgot to mention <a href=\"http://www.openmoko.com/press/index.html\">OpenMoko</a>. This is a neat looking touch screen Linux phone that should be available in February 2007. I haven't seen one yet, although I have played with the Greenphone and I know someone who has ordered and received a Greenphone, so they are  ahead by a couple of months.<br /><br />In the homebrew mobile club, we are talking about making something with a physically bigger (perhaps 3.7\" VGA) touch screen than the OpenMoko (2.8\" VGA) or Greenphone (smaller QVGA), big enough to show a more usable keyboard. The existence of multiple open phone projects is great, since the nature of the open source community is that we can all share and reuse code for device drivers, user interface components and applications. We aren't building from scratch every time we want a new feature. In the meantime several of us will have Greenphone's, OpenMoko's and TuxPhone's to play with.<br /><br />The big screen is pretty much the most expensive part. I like the concept of building a phone, keeping the screen and CPU for a long time, but do upgrades like replacing the GSM modem with a 3G one by performing electronic surgery inside the case.",
        "metadata": {
          "word_count": 215,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In the homebrew mobile club, we are talking about making something with a physically bigger (perhaps 3.7\" VGA) touch screen. The existence of multiple open phone projects is great, since the nature of the open source community is that we can all share and reuse code for device drivers, user interface components and applications. The big screen is pretty much the most expensive part. I like the concept of building a phone, keeping the screen and CPU for a long time, but do upgrades like replacing the GSM modem with a 3G one."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_02b8d2e4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Installation Scheduled",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/solar-power-installation-scheduled.html",
      "content": {
        "text": "The plans were approved earlier this week, and Solarcity called me on the phone and emailed to setup an installation time, which I was able to arrange very conveniently via email. The assigned installation team are finishing up another job, then I'm next, starting tomorrow (Friday 24th July) for a few days.<br /><br />There has been nothing to show so far, but I will be taking pictures and posting them.<br /><br />Looking at the options for electric water heating led us to consider a ground source heat pump (GSHP), plans are still in flux, but I'm looking at adding a heat pump that will heat hot water as well as heat and cool the house. The ground source part is that it doesn't have a conventional air fan based heat exchanger, it runs water through underground pipes. This is relatively expensive, but there are rebates, and it is very efficient. More later...",
        "metadata": {
          "word_count": 152,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The plans were approved earlier this week, and Solarcity called me on the phone and emailed to setup an installation time. The assigned installation team are finishing up another job, then I'm next. There has been nothing to show so far, but I will be taking pictures and posting them."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_35bf5315",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "10 Things to Know About Skype Ap2Ap Programming",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/10-things-to-know-about-skype-ap2ap.html",
      "content": {
        "text": "I also posted this on the <a href=\"https://developer.skype.com/Users/adriancockcroft\">Skype Developer Wiki</a><br /><br />The ap2ap capability is an interesting new network computing paradigm but it is not like a conventional network.<br /><OL type=\"1\"><li>end nodes are addressed by skype name, which addresses a person, not a computer<br /></li><br /><li>people can login to skype multiple times, so addressable endpoints are not unique <br /></li><br /><li>skype can go online/offline at will, so there is a concept of \"presence\" that needs to be managed<br /></li><br /><li>you can only make ap2ap connections to your buddy list or people who you have chatted to \"recently\"<br /></li><br /><li>both ends of an ap2ap connection have to choose a unique string used to identify their conversation or protocol<br /></li><br /><li>if you quit and restart skype, the first login can persist for a while, so you can get multiple ap2ap connections from a single user, although the ghosts of your previous connections cannot respond to a message. I think is is because you connect to a different supernode each time, and the first one isn't sure if you have really gone away yet<br /></li><br /><li>messages have to be sent as text, so binary objects have to be converted first using something like base64<br /></li><br /><li>the network can behave differently each time you use it, and this non-determinism makes testing difficult<br /></li><br /><li>relayed connections are limited to about 3KB/s, direct ones can run at several MB/s over a LAN<br /></li><br /><li><a href=\"https://developer.skype.com/Skype4Java\">Skype4Java</a> is cross-platform, but the maximum message size is about 64KB on windows and 16KB on OSX/Linux, and there are several bugs and limitations in the older version of the API library that is used by Skype 2.0 and earlier releases. Use Skype 2.5 or later for the best performance and stability<br /></li><br /></ol>",
        "metadata": {
          "word_count": 292,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The ap2ap capability is an interesting new network computing paradigm. End nodes are addressed by skype name, which addresses a person, not a computer. People can login to skype multiple times, so addressable endpoints are not unique. Skype is cross-platform, but the maximum message size is about 64KB on windows and 16KB on OSX/Linux. Use Skype 2.5 or later for the best performance and stability."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_0813a859",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power order placed",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/05/solar-power-order-placed.html",
      "content": {
        "text": "I signed up with http://www.solarcity.com for 4.5Kw of thin film panels (60 of them). Their web site is slick, walks you through the install process, they have online monitoring of the output of the solar array and I also added the option of monitoring the house's consumption of electricity, which is integrated into the same web based monitoring and graphing package. They visit later this week to do the engineering assessment, then I get plans (provided online) and planning approval etc. follows. Should be all done in 2-3 months.<br /><br />Thin film is better in partial light, and high temperatures, and it can be mounted closer to the roof tiles since it doesn't need an air gap underneath to cool it. We have a hot microclimate with no shade, at 2400ft altitude, above the morning mist most of the time, so the longer power delivery period per day should compensate for the slight reduction in peak efficiency compared to silicon. Solarcity offered both at the same price. There is a combination of a rebate and a tax credit at the moment. The tax credit is about 30% of the cost, and was added recently by Obama's stimulus package.<br /><br />As it progresses I will post pictures and updates.",
        "metadata": {
          "word_count": 208,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Thin film is better in partial light, and high temperatures. Can be mounted closer to the roof tiles since it doesn't need an air gap underneath to cool it. Longer power delivery period per day should compensate for the slight reduction in peak efficiency compared to silicon. As it progresses I will post pictures and updates. The tax credit is about 30% of the cost, and was added recently by Obama's stimulus package."
      },
      "tags": [
        "engineering",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_708d113c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Low Power Microprocessors for General Purpose Computing",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/03/low-power-microprocessors-for-general.html",
      "content": {
        "text": "While researching devices for <a href=\"http://www.hbmobile.org/wiki/index.php?title=MyPhone\">my home brew mobile phone</a>, I've realized that the current generation of CPUs for mobile devices are actually seriously powerful, very low cost and use almost no power. The performance per watt and per dollar seems to be an order of magnitude better than the PC-class CPUs that are common in commodity servers nowadays. The absolute performance and memory capacity is lower, but is comparable to common PC hardware from a few years ago, and could be useful for more than running a high end phone or portable games machine. Devices such as the Marvel PXA270 and <a href=\"http://www.freescale.com/webapp/sps/site/prod_summary.jsp?code=i.MX31&nodeId=0162468rH31143ZrDR\">Freescale i.MX31</a> run at over 500MHz, some include floating point units, they support at least 128MB of RAM (a single chip), and a myriad of I/O interfaces, with Linux 2.6 support.<br /><br />While the current mainstream CPUs were driven by the development of the home PC market, this generation is driven by the development of the mobile, battery powered device market, which is a very large. For example the worldwide cellphone market is something like a billion devices a year.<br /><br />I think that there could be some interesting general purpose computer systems built from low power devices (CPUs that use less than one watt). I looked around but wasn't sure what to search for... I do know about the systems that are sold for embedded use, but they are typically configured using lower speed and lower memory options.<br /><br />Does anyone know of vendors selling general purpose systems, or a category name for this space?<br /><br />[Update: I asked around, and thought a while, and decided that this is interesting enough to have its own name \"millicomputer\" and <a href=\"http://millicomputing.blogspot.com\">its own blog \"millicomputing\"</a>].",
        "metadata": {
          "word_count": 287,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The current generation of CPUs for mobile devices are actually seriously powerful, very low cost and use almost no power. The performance per watt and per dollar seems to be an order of magnitude better than the PC-class CPUs that are common in commodity servers nowadays. The absolute performance and memory capacity is lower, but is comparable to common PC hardware from a few years ago. I think that there could be some interesting general purpose computer systems built from low power devices (CPUs that use less than one watt) I looked around but wasn't sure what to search for... I do know about the systems that are sold for embedded use, but they are typically configured using lower speed and lower memory options."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_775ae872",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Gene Saunders RIP",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/09/gene-saunders-rip.html",
      "content": {
        "text": "A good friend and well respected colleague from Sun is no more...<br /><a href=\"http://geoffarnold.com/?p=2418\">More info on Geoff Arnold's blog</a><br /><br />For the last few years a group of us have been keeping in touch via email, sharing thoughts and amusing web links. Gene was a key part of that conversation, and I miss him already.",
        "metadata": {
          "word_count": 55,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A good friend and well respected colleague from Sun is no more. A group of us have been keeping in touch via email, sharing thoughts and amusing web links. Gene was a key part of that conversation."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_a7232cf9",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Seeing storage problems",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/05/seeing-storage-problems.html",
      "content": {
        "text": "Change of topic...<br /><br />Those of you who have dealt with Sun storage performance over the years might have heard of Dave Fisk, we became good friends when I worked together with him in the same group many years ago, then he went to work for the Sun storage business unit, and finally he left Sun to build a storage performance analysis product. After a lot of work over several years he has finally got his product ready, and it cracks open the storage stack so you can see what is really happening.<br /><br /><a href=\"http://www.ortera.com\">Dave's company is called ORtera</a> and they have a downloadable beta release available which you can get a free one month evaluation license for.<br /><br />If you ever wondered why the I/O traffic your application (e.g. Oracle) is generating don't seem to match up with the stats you see in iostat, then this tool will show you why. For example, small filesystem writes can cause larger read-modify-write operations to occur and the tool will show you this, point it out with a heuristic and suggest a filesystem tuning fix.<br /><br />Its a useful addition to the toolbox, I think they should have called it \"virtual Dave\" :-)",
        "metadata": {
          "word_count": 202,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "ORtera is a new storage performance analysis tool. It cracks open the storage stack so you can see what is really happening. You can get a free one month evaluation license for the tool. Its a useful addition to the toolbox, I think they should have called it \"virtual Dave\" :-) th a heuristic and suggest a filesystem tuning fix."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_94ec64ef",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "New Joost Beta",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/new-joost-beta.html",
      "content": {
        "text": "They just released a new build, it fixes some problems and has some minor user interface enhancements. I left it running for a while and now I'm starting to run out of content that I want to watch... There are about 30 channels for beta testing. probably the most demanding are live music videos, Joost has fine sound quality and  keeps up with very rapid on-screen action better than I would expect in full-screen mode on my Dell laptop. It becomes a bit more pixellated when its working hard to keep up, and the sound glitches to repeat a sub-second fragment now and again if it gets behind due to network slowdowns. On low action images, its very nice and clear.<br /><br />In comparison, I've noticed that on my Slingbox TiVo, it does constant pitch sound stretching to slow down the display at the start of a show, thats how it sneakily builds up a buffer without making you wait.<br /><br />The Joost folks are promising new content, an OSX version and an expansion of the beta program soon. I've had a few comments requesting beta invites, and I haven't had any to give. If and when I have any spare invites, I'll post it, so hold your comments...",
        "metadata": {
          "word_count": 209,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Joost has fine sound quality and  keeps up with very rapid on-screen action better than I would expect in full-screen mode on my Dell laptop. It becomes a bit more pixellated when its working hard to keep up, and the sound glitches to repeat a sub-second fragment now and again. I haven't had any spare beta invites, and I haven't given any yet. If and when I have any spare invites, I'll post it."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_40e91476",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Etech Tuesday On Rails | by Adrian Cockcroft | 8th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/etech-tuesday-on-rails-by-adrian.html",
      "content": {
        "text": "A long day with lots of interesting talks, and I got to chat with several new people and also to try out my FLORWAX pitch. \"Its the equivalent of AJAX but for Wireless\" is my instant summary. To get this out of the way, the general reaction is that the name gets a chuckle (not too many groans yet), that there really is a big problem in wireless platform fragmentation, and that no-one seems to know of any other initiatives that have picked on this as a problem to solve. I think most people look at wireless, see this problem, and give up, as its too hard to make something work. Since I'm interested in a longer term perspective than most people, its seems fair game to try and provoke a discussion on what a sensible core set of wireless platform technologies would look like.<br /><br />As <a href=\"http://jjg.net\">Jesse James Garrett</a> said in the tutorial yesterday, the key elements of AJAX are that it uses a common standard bundle of browser based technologies and that it is asynchronous, so you don't have to click-and-wait......click-and-wait......<br />If we apply these principles to Wireless, we need to define a bundle of standard technologies (I suggest Flash Lite 2.0, Ruby on Rails, XML web services - FLORWAX = FlashLiteOnRailsWirelessAsynchronousXml, but the actual bundle doesn't matter as long as a common set emerges). However the asyncronous problem is far worse in wireless than in desktop applications, we really need to have wireless apps that talk to the backend and update the screen without the click-and-wait-for-ages mode that is the norm.<br /><br />At the end of the day I attended the Ruby on Rails BoF. I have heard good things about RoR but haven't used it. I think they converted me, and I took the opportunity to mention FLORWAX to the group. It does seem like the right technology fit.<br /><br />The conference itself started with Ray Ozzie showing how to do cut and paste on the web. It seems so trivial, why hadn't been done before? A very useful way to make web apps behave more like regular apps. We then had a very cool hardware demo by Jeff Han, he has a touch screen that can see all his fingers separately and has created a very nice new set of user interaction paradigms.<br /><br />Amazon has created a way to harness real people to do the stuff that AI can't do. Its called the <a href=\"http://www.mturk.com\">Mechanical Turk</a>, and its another simple idea with quite profound and wide reaching uses. Dick Hardt from Sxip gave an interesting talk on indentity, but the way he presented it with one word per slide and rapid fire transitions reminded me of Steve Colbert presenting his \"The Word\" section on The Colbert Report. I enjoyed it but I don't remember much of the content.<br /><br />Next we has a talk from Felix Miller of last.fm on how they collect the metadata on what you are listening to and use it to help you find new music, I've been playing around with <a href=\"http://www.pandora.com\">Pandora</a> and training it to play the music I like, and I think I'll have to have a go at <a href=\"http://last.fm\">last.fm</a> as well. I have <a href=\"http://www.myspace.com/fractalcontinuum\">eclectic</a> tastes, and its hard to keep the recommendations from veering back to the mainstream in Pandora.<br /><br />After the break, there were several presentations that didn't grab my attention or told me things that seemed obvious to me. The highlight was a presentation on Second Life that was presented using a billboard in the virtual world and lots of interactive explanations of how it all works. Fascinating, but I don't have enough time to play as much as I'd like in the <span style=\"font-style:italic;\">real</span> world....<br /><br /><a href=\"http://technorati.com/tag/etech\" rel=\"tag\">etech</a>, <a href=\"http://technorati.com/tag/etech06\" rel=\"tag\">etech06</a>, <a href=\"http://technorati.com/tag/florwax\" rel=\"tag\">florwax</a>",
        "metadata": {
          "word_count": 636,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "FLORWAX is the equivalent of AJAX but for Wireless. The name gets a chuckle, but there really is a big problem in wireless platform fragmentation. No-one seems to know of any other initiatives that have picked on this as a problem. Flash Lite 2.0, Ruby on Rails, XML web services - FLORWAX = FlashLiteOnRailsWirelessAsynchronousXml. We really need to have wireless apps that talk to the backend. Amazon has created a way to harness real people to do the stuff that AI can't do. Dick Hardt from Sxip gave an interesting talk on indentity. Felix Miller of last.fm on how they collect the metadata on what you are listening to and use it to help you find new music. After the break, there were several presentations that didn't grab my attention or told me things that seemed obvious to me. The highlight was a presentation on Second Life that was presented using a billboard in the virtual world. Fascinating, but I don't have enough time to play as much as I'd like in the real world."
      },
      "tags": [
        "ai",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_4f58af76",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "It's not obvious how to be insanely simple",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/04/its-not-obvious-how-to-be-insanely.html",
      "content": {
        "text": "three books that I read recently resonated with me and fitted together so I'm going to try to make sense of them in a blog post rather than in a series of cryptic tweets.<br />\n<br />\nMy son (who is a <a href=\"http://touch.ebay.com/\" target=\"_blank\">product manager at eBay</a>) told me about the most recent publication:<br />\n<a href=\"http://www.amazon.com/Insanely-Simple-Obsession-Drives-Success/dp/1591844835/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1335735223&amp;sr=1-1\" target=\"_blank\">Insanely Simple: The Obsession That Drives Apple's Success by Ken Segall</a><br />\n<br />\nAt the <a href=\"http://www.defragcon.com/2012/\" target=\"_blank\">Defrag conference</a> last year I saw Duncan Watts present and recently finished reading his book&nbsp;<a href=\"http://www.amazon.com/Everything-Is-Obvious-Answer-ebook/dp/B004DEPHGQ/\" target=\"_blank\">Everything Is Obvious: Once You Know The Answer</a><br />\n<br />\nI also recently&nbsp;<a href=\"http://www.youtube.com/watch?v=pCofmZlC72g\" target=\"_blank\">watched Sam Harris give his talk</a> on Free Will, and then <a href=\"http://www.amazon.com/Free-Will-ebook/dp/B006IDG2T6\" target=\"_blank\">read the book</a>.<br />\n<br />\nThe connection starts with Free Will, which explains what is really going in our heads, along with Everything is Obvious which explains how our minds work collectively and interact with the real world these two books are the \"Missing Manuals\" for our brains. It's hard enough to figure out what is going on in the world and how best to navigate it, but it's doubly hard when you don't realize how your subconscious is pulling the strings and how common sense is confusing everyone around you.<br />\n<br />\nInside your head, the conscious thread of thoughts that you hear are post rationalizing decisions that your subconscious mind has already made. Feeding yourself a broad range of information with an open mind, connecting to your intuition and letting the power of your subconscious find the right patterns and responses lets you make faster and better decisions.<br />\n<br />\nIn society, we are surrounded by common sense explanations that we use to post rationalize the events around us and which are fed to us by the media, historians, politicians and our friends. Duncan deconstructs common sense to show that these explanations are mirages driven by our inner need to find a narrative and cause for effects that are essentially random co-incidences with far less significance than we assume. He then explains what un-common sense looks like and how to question the received wisdom and have better strategies for getting things done successfully.<br />\n<br />\nI'm not going to summarize the whole book but there is a very useful section that should be read by anyone doing \"big data analytics\" that sets out the kind of things that are know-able and what (and why) other things will always remain un-knowable and impossible to predict. The advice I distilled from the discussion of strategy is that there is so much randomness in the outcome of business decisions that you cannot reliably evaluate the difference between a good strategy and a poor strategy. If you are able to get ever more detailed data about what happened you become more convinced in the value of your analysis, but the predictions you make about what to do next don't get any better. This is a counter-intuitive outcome (i.e. it violates common sense), so please read the book, which explains why you shouldn't be trusting your common sense in the first place.<br />\n<br />\nThe positive things we can do to overcome random outcomes really resonated with me, as they put into words several of the things I've been doing for many years, which have in some sense given me a better way to understand what's going on and get stuff done. They also describe many of the ways that Netflix figures out how to build it's products.<br />\n<br />\nThe first thing I do when I hear something like A caused B is a reflex reaction, I flip it around in my head, take the devils advocate position, look at the situation from a few different angles. This can be quite annoying in \"polite company\" as I tend to question received wisdom and common sense assertions, however I usually find a missing piece of information that could falsify the assertion, and ask the question. It could be as simple as asking exactly what time A and B happened, since if B turned out to happen before A then the assertion is clearly false. In statistics and physics this is codified as asserting the null hypothesis. (I'm the son of a statistician and I have a Physics degree...).<br />\n<br />\nAt Netflix we always try to construct parallel \"A/B\" tests of our hypotheses, like the double blind tests used in clinical trials of new medicines. We take a large number of new customers and give them a range of different experiences for long enough to measure a difference in their responses. This is the only way to reliably tell whether a new feature works, and it often goes against the common sense of what we expect and what many customers and industry analysts helpfully suggest we should be doing. As Duncan explains we can usually figure out what factors will affect an outcome, but we are extremely poor judges of how to weight those factors, even with post rationalization of what we saw happen, and all we can do is bias the statistics in a preferred direction. A recurring example is the suggestion that Netflix should allow half-stars in its movie ratings, but it turns out that given more fine grain choice fewer people rate movies, and the reduction in the number of data points out-weighs the increased accuracy. We can post rationalize why this occurs as an example of giving people too much choice, but we don't have to rationalize it, we just measured it.<br />\n<br />\nIn the discussion of strategy Duncan talks about creating a set of strategies that cover many scenarios, and using scenario planning to build more flexible and fuzzy strategies which are more likely to work under a range of random external influences. By putting yourself in the path of possible good randomness and avoiding bad outcomes, you can \"make your own luck\". By detecting problems early and having the flexibility to adapt your strategy you can run around the problems that will randomly come your way. If instead you concentrate on coming up with the best possible strategy or assuming that previous success was due to strategy rather than random outcomes you are building a brittle future that is likely to disappoint you.<br />\n<br />\nThe final point I will lift from Duncan's discussion of uncommon sense is that speed of execution and iteration is another fine way to cheat the chance events that will derail your plans. Long term detailed plans are a waste of time. This is one of the foundations of agile development, where rapid iteration of product features lets you discover what your users actually do with your product, as opposed to what you thought they would do or what they say they will do.<br />\n<br />\nThis leads to the Insanely Simple book, which talks about Apple's approach to product design, with particular emphasis on branding and marketing since Ken Segall was the guy who came up with the i in iMac and has many other fascinating stories. One reason I like working at Netflix is that for agile web services, product ideas can be built and tested in a week or a month, and fixed in minutes. For Apple they work on products for years and need to have them work perfectly when they are released. This gives them two big problems, since its hard to iterate and hard to test ideas and products in advance. Their solution seems to be that they allow better ideas to form and develop, take bigger risks and make decisions faster than their competition, which helps stay ahead of the market. The Insanely Simple design philosophy is based on the idea that its easy to listen to all those great common sense ideas about features your product has to have, but if you learn to ignore the common sense and give the customers a simple and distilled experience you will reach beyond the people who want a complicated product and find a much bigger market of people who were waiting for a simple way to get something done. Apple's competitors are so bogged down in committees and approval processes, and helpful common sense advice from customers that they are unable to release simple products.<br />\n<br />\nA key example from the book is that Apple has had many award winning advertising campaigns, \"Think Different\", \"PC and Mac\" and the iPod silhouette, and none of them were test marketed in advance. Their competitors make less risky adverts after getting broad internal consensus, take much longer to get them to market and fail to understand that the success of an advert is a randomized event (with lots of useless common sense post rationalizations) so the test market response is a very poor predictor of success. It's more important to be bold, different and go big. For example Apple only advertised Think Different on the back cover of magazines, which costs far more but has a much bigger impact than inside pages.<br />\n<br />\nFrom these three books I've found some useful focus on how to approach things, but they also give me some backup to explain to others why I think some things are important. A key part of what I have been doing for Netflix is looking out into the future of cloud and related technologies and developing a portfolio of fuzzy strategies and options. They don't all work out, but by having a well instrumented but loosely coordinated architecture that doesn't have central control and strict processes we can iterate rapidly, adopt (and discard) interesting new technologies as they come along. We can all have more fun and less frustration making Netflix Insanely Simple, and ignore all the bad common sense advice and analyst opinions that swirl around everything we do.<br />\n<br />\nI'm planning a complete re-write of my cloud architecture tutorial for <a href=\"http://gluecon.com/2012/\" target=\"_blank\">Gluecon in May</a>, that will be a great opportunity to discuss these things in person over a few beers, and now is a good time to sign up to attend - you can get a 10% discount with code spkr12.",
        "metadata": {
          "word_count": 1705,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Three books that I read recently resonated with me and fitted together. I'm going to try to make sense of them in a blog post rather than in a series of cryptic tweets. Inside your head, the conscious thread of thoughts that you hear are post rationalizing decisions that your subconscious mind has already made. Feeding yourself a broad range of information with an open mind, connecting to your intuition and letting your subconscious find the right patterns and responses lets you make faster and better decisions. In society, we are surrounded by common sense explanations that we use to rationalize the events around us. Duncan deconstructs common sense to show that these explanations are mirages driven by our inner need to find a narrative. He then explains what un-common sense looks like and how to question the received wisdom and have better strategies for getting things done successfully. The book explains why you shouldn't be trusting your common sense in the first place. The positive things we can do to overcome random outcomes really resonated with me, as they put into words several things I've been doing for many years. At Netflix we always try to construct parallel \"A/B\" tests of our hypotheses. This is the only way to reliably tell whether a new feature works. It often goes against the common sense of what we expect and what many customers and industry analysts helpfully suggest we should be doing. In the discussion of strategy Duncan talks about creating a set of strategies that cover many scenarios. By putting yourself in the path of possible good randomness and avoiding bad outcomes, you can \"m\" By detecting problems early and having the flexibility to adapt your strategy you can run around the problems that will randomly come your way. Long term detailed plans are a waste of time. This is one of the foundations of agile development. Netflix's design philosophy is based on the idea that its easy to listen to all those great common sense ideas about features your product has to have. If you learn to ignore the common sense and give the customers a simple and distilled experience you will reach beyond the people who want a complicated product and find a much bigger market. Apple has had many award winning advertising campaigns, \"Think Different\", \"PC and Mac\" and the iPod silhouette, and none of them were test marketed in advance. Apple's competitors are so bogged down in committees and approval processes, and helpful common sense advice from customers that they are unable to release simple products. A key part of what I have been doing for Netflix is looking out into the future of cloud and related technologies. They don't all work out, but by having a well instrumented but loosely coordinated architecture we can iterate rapidly. We can all have more fun and less frustration making Netflix Insanely Simple. Use the code spkr12. t with code spKR12. T to get a discount on this week's issue of The Daily Mail."
      },
      "tags": [
        "agile",
        "architecture",
        "big data",
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_be933b98",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone and Treo at MacWorld",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/iphone-and-treo-at-macworld.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfrWLJyZYaqatKMjdxF4iOy3IDQhcVnS8oRmCzu5Db25JwMznk-fPoz8VMRN73YH2uZHeb6nwpMS2kScmNdy_98n6GT1vhGunr0SH3LzMwy6bCVW8x3lj3je_4gpGPfh2Oih1J/s1600-h/iPhoneTreo.jpg\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfrWLJyZYaqatKMjdxF4iOy3IDQhcVnS8oRmCzu5Db25JwMznk-fPoz8VMRN73YH2uZHeb6nwpMS2kScmNdy_98n6GT1vhGunr0SH3LzMwy6bCVW8x3lj3je_4gpGPfh2Oih1J/s400/iPhoneTreo.jpg\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5020298462214269618\" border=\"0\" /></a>I went to MacWorld to check out the Apple iPhone. I think that it is even more impressive in person than I was expecting. Its smaller and much thinner than I imagined from looking at pictures, and watching the live on-stage demos made it clear just how much of a step forward in usability this is for a mobile device.<br />I took the picture above using my Treo650 of someone else taking a picture with a Treo650, so the size can easily be compared. I was embarrased to get the Treo out anywhere near the iPhone, it just felt wrong. The overall width and length of the device is similar, but the Treo is about twice as thick. I tried to take a picture edge-ways to show how thin the iPhone is, but the Treo camera is a crappy low resolution one and the iPhone is so thin it effectively disappeared in the photo.",
        "metadata": {
          "word_count": 174,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I went to MacWorld to check out the Apple iPhone. I think that it is even more impressive in person than I was expecting. Its smaller and much thinner than I imagined from looking at pictures. The overall width and length of the device is similar, but the Treo is about twice as thick. I tried to take a picture edge-ways to show how thin the iPhone is. The iPhone is so thin it effectively disappeared in the photo."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_a13524cf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Conferences and Innovation",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/02/conferences-and-innovation.html",
      "content": {
        "text": "I just signed up for the O'Reilly Emerging Technology event in San Diego next month - <a href=\"http://conferences.oreillynet.com/etech/\">http://conferences.oreillynet.com/etech/</a><br /><br />I've also written a paper for a workshop in the IEEE Joint Conference on E-Commerce Technology (CEC'06) and Enterprise Computing, E-Commerce and E-Services (EEE'06) <a href=\"http://linux.ece.uci.edu/cec06/\">http://linux.ece.uci.edu/cec06/</a> - but the conference name is so long that I can't remember it very well in conversation. This conference also includes the 2nd International Workshop on Business Service Networks (BSN '06) and the 2nd International Workshop on Service oriented Solutions for Cooperative Organizations (SoS4CO '06). Its all sounds very interesting, its in June in San Francisco, and needs a snappier name...<br /><br />Last December I attended the Fortune Innovation Forum in New York. It was very nicely put together and in effect it validated the approach we were already taking. It seems that most of the attendees were trying to work towards a culture, process and tools for fostering innovation that seemed similar to our own setup. eBay and PayPal were used as examples several times.<br /><br />We used a few simple techniques last year to kickstart our own innovation program. One method I borrowed from other events is the \"Poster Lunch\". Get a room near the company cafe, provide flip chart sized pads and pens, email everyone to tell them about it and put up signs in the Cafe to invite them in on the day. Anyone can put anything they like on a poster, stick it up and collect comments on it in person. One thing we found was that there were several posters suggesting eBay site features that already existed or were in development. One suggestion in particular was getting lots of support and comments until someone wrote on it \"LTS thursday!\", meaning it would be Live To Site and be launched two days later. We also gave attendees voting stickers so that they could indicate their favourite posters.<br /><br />To drill down on the best ideas we also setup a regular open-to-all meeting where we could discuss the concepts and route them to the appropriate expert or business owner. The most far-sighted ideas get routed to become candidates for research labs projects, and the people who had the ideas get to develop them further.<br /><br />To support the collection of ideas, we created a Wiki. This is nice because it is free format, and supports comments and discussion, with very low initial barrier to entering an idea. The problems came when there were several hundred ideas in the Wiki, it became hard to maintain. A more specialized pre-concept tool that feeds into our standard development process is a better solution for incremental innovations, and the Wiki works better for more radical ideas.<br /><br />To really get a dose of innovative ideas, last year I attended a seminar on Complex Adaptive Systems by the <a href=\"http://www.santafe.edu/\">Santa Fe Institute</a>. It was a real eye-opener, they are pushing the boundaries of multi-disciplinary research, e.g. forming teams with Physicists, Biologists and Economists to derive the rules of scaling and organization of living things, from the smallest mammal to the largest city. Since eBay, PayPal and Skype are social networks, (their value comes from connections within their communities) they behave in some ways like cities, and follow similar kinds of scaling rules.",
        "metadata": {
          "word_count": 546,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I just signed up for the O'Reilly Emerging Technology event in San Diego next month. I've also written a paper for a workshop in the IEEE Joint Conference on E-Commerce Technology. But the conference name is so long that I can't remember it very well in conversation. We used a few simple techniques last year to kickstart our own innovation program. eBay and PayPal were used as examples several times. One method I borrowed from other events is the \"Poster Lunch\" The most far-sighted ideas get routed to become candidates for research labs projects. The people who had the ideas get to develop them further. rill down on the best ideas we also setup a regular open-to-all meeting. Since eBay, PayPal and Skype are social networks, (their value comes from connections within their communities) they behave in some ways like cities. ries of multi-disciplinary research, e.g. forming teams with Physicists, Biologists and Economists."
      },
      "tags": [
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_5389d1c1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Processing vxstat to read into R",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/processing-vxstat-to-read-into-r.html",
      "content": {
        "text": "I got bored with my iostat data, and found some interesting looking vxstat logs to browse with the Cockcroft Headroom Plot. To get them into a regular format I wrote a short Awk script that is shown below. It skips the first record, adds a custom header and drops the time field into the first column.<br /><br /><pre><br /># process vxstat file into regular csv format<br />BEGIN { skipping=1; printf(\"time,vol,reads,writes,breads,bwrites,tread,twrite\\n\"); }<br />NR < 4 {next}   # skip header<br />NF > 0 && skipping==1 {next} # skip first record of totals since boot<br />NF == 0 {skipping=0}<br />NF == 5 {time=$0}<br />NF == 8 {printf(\"%s,%s,%s,%s,%s,%s,%s,%s\\n\",time,$2,$3,$4,$5,$6,$7,$8);}<br /></pre><br /><br />It turns a file that looks like this:<br /><pre><br />                        OPERATIONS           BLOCKS        AVG TIME(ms)<br />TYP NAME              READ     WRITE      READ     WRITE   READ  WRITE <br /><br />Mon May 01 19:00:01 2000<br />vol home             88159    346799  17990732   3680604   13.7   15.6 <br />vol local            64308    103869   3848746    410899    6.0   22.0 <br />vol orahome          80240    208372  18931823    886870   11.9   21.1 <br />vol rootvol         336544    537741  21325442   8566302    4.8  323.1 <br />vol swapvol          32857       339   4199304     58160   13.8   22.5 <br />vol usr             396221    174834  11766646   2872832    3.5  547.6 <br />vol var             316340   1688518  25138480  19275428   11.1   53.7 <br /><br />Mon May 01 19:00:31 2000<br />vol home                 1        28         4       129   10.0   34.3 <br />vol local                0         2         0         8    0.0  330.0 <br />vol orahome              4        20        24        88   10.0   84.0 <br />vol rootvol              0        80         0       720    0.0    9.4 <br />vol swapvol              0         0         0         0    0.0    0.0 <br />vol usr                  0         1         0        16    0.0   20.0 <br />vol var                  4       235        54      2498   15.0   13.7 <br /><br />... and so on<br /></pre><br /><br />into<br /><pre><br />% awk -f vx.awk < vxstat.out<br />time,vol,reads,writes,breads,bwrites,tread,twrite<br />Mon May 01 19:00:31 2000,home,1,28,4,129,10.0,34.3<br />Mon May 01 19:00:31 2000,local,0,2,0,8,0.0,330.0<br />Mon May 01 19:00:31 2000,orahome,4,20,24,88,10.0,84.0<br />Mon May 01 19:00:31 2000,rootvol,0,80,0,720,0.0,9.4<br />Mon May 01 19:00:31 2000,swapvol,0,0,0,0,0.0,0.0<br />Mon May 01 19:00:31 2000,usr,0,1,0,16,0.0,20.0<br />Mon May 01 19:00:31 2000,var,4,235,54,2498,15.0,13.7<br />... and so on<br /></pre><br /><br />This can easily be read into R and plotted using<br /><br /><pre><br />> vx <- read.csv(\"~/vxstat.csv\", header=T)<br />> vxhome <- vx[vx$vol==\"home\",]<br />> chp(vxhome$reads,vxhome$treads)<br /></pre><br /><br />One of the files I tried was quite long, half a million lines. It loaded into R in fifteen seconds, and the subsequent analysis operations didn't take too long. Try that with a spreadsheet... :-)",
        "metadata": {
          "word_count": 384,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I got bored with my iostat data, and found some interesting looking vxstat logs to browse with the Cockcroft Headroom Plot. To get them into a regular format I wrote a short Awk script that is shown below. It skips the first record, adds a custom header and drops the time field into the first column. vol rootvol    336544   537741   8566302   4.8  323.7   15.6 <br />vol swapvol \u00a0 32857 \u00a0 4199304   58160   13.0  330.0 <br /><br />Mon May 01 19:00:31 2000 <br >vol usr  \u00a0 396221   174834  11766646   2872832  3.5  547.6 % awk -f vx.awk < vxstat.out.time,vol,reads,writes,breads,bwrites,.tread,twrite,home,local,var,rootvol,swapvol,etc. This can easily be read into R and plotted using vx read.csv."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_bc45e9d0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Using Occam to Simulate p2p Web Services",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/using-occam-to-simulate-p2p-web.html",
      "content": {
        "text": "<p>About a year ago I gave a paper at an IEEE E-Commerce conference that described how I used the latest pi-calculus enhanced version of the old Inmos Transputer language Occam to build a very efficient massively threaded simulator for large scale p2p web services that ran on my laptop.</p> <p><!-- PProtector --><a href=\"http://perfcap.blogspot.com/2006/07/ieee-conference-paper.html\">I blogged this at the time</a>, but The paper itself is subject to the rules IEEE sets out for peer-reviewed conference papers and you need to be a member or pay a fee to get it from <a href=\"http://doi.ieeecomputersociety.org/10.1109/CEC-EEE.2006.81\">http://doi.ieeecomputersociety.org/10.1109/CEC-EEE.2006.81</a></p> <p>However, I wanted to share this more widely and have just posted a <a href=\"http://web.mac.com/adriancockcroft/Public/SkypeSim07.pdf\">pdf of the presentation slides for my IEEE paper</a>. Here are the first few slides, and a description of the Tools section:</p> <p><b>Simulation of Skype Peer-to-peer Web Services Choreography Using Occam-Pi</b><br /></p> <p><!-- PProtector --><b>Abstract</b><br />Complex web services are very difficult to test and verify before deployment on a large scale.<br />A semantically equivalent in-memory simulation can be built using Occam-pi that runs up to 100,000 nodes about a million times faster than real life.<br />Rapid protocol development and detailed behavioral analysis using simulation supports the development of robust services that can scale.<br />The implementation of a simulator that models centralized web services and application to application messaging over the Skype Peer to Peer network is described.</p> <p><b>Overview</b><br /></p> <ul><li>The Landscape</li><li>The Problem</li><li>The Tools</li><li>The Solution</li><li>Implementation</li><li>Running the Simulation</li><li>Conclusion</li><li>References</li></ul> <p><!-- PProtector --><b>The Tools</b> \u2013 Pi Calculus<br /></p> <ul><li>WS-BPEL, SSDL and WS-CDL are based on Pi-Calculus</li><li>Pi-Calculus is based on Communicating Sequential Processes (CSP)</li><li>Pi-Calculus provides a formal model of parallel message based computing</li><li>Occam-Pi</li><li>The Occam language is based on CSP, and has been extended to add the Pi-Calculus extensions to form the Occam-Pi language.</li><li>The primary implementation of this language is known as KROC, the Kent Re-targetable Occam Compiler</li><li>KROC is freely available from the University of Kent at Canterbury, UK</li><li>Runs on Intel architecture Linux, MacOS X, and Microsoft Windows/Cygwin platforms. Older versions exist for SPARC, PPC etc.</li></ul> <p><b>The Tools</b> \u2013 Occam Language Constructs<br /></p> <ul><li>The constructs that are used in Web Services choreography map directly to Occam language constructs</li><li>The Occam language has direct support for sequential, parallel and alternate processing blocks, complex protocols, and channel based communications</li><li>Occam-Pi adds more dynamic constructs to the language</li><li>Mobile channels - pass a channel end over another channel</li><li>Mobile processes - suspend a process, pass it over a channel and resume it in a new context</li><li>Dynamic process forking with barrier synchronization</li><li>Rigorous Occam-Pi Compile-time Checks</li><li>Processes or expressions are not allowed to have any \u201cside effects\u201d</li><li>Syntax and usage of all protocols, data and constructs is checked</li><li>Occam is designed to allow very comprehensive static analysis</li></ul>  <p><b>The Tools</b> \u2013 Occam Runtime Characteristics<br /></p> <ul><li>There is no need to use XML message encoding or namespaces</li><li>since the compiler can check that a protocol is being communicated correctly</li><li>At runtime, the Occam-Pi language is fast small compiled code</li><li>with its own threading model, in a single process</li><li>The Occam-Pi runtime detects and reports deadlock</li><li>including the line number in the code at which each process was stalled</li><li>Occam-Pi is very efficient</li><li>All communication takes place in a single address space at memory bus speeds</li><li>Basic process creation takes 20 nanoseconds on an 800MHz PC</li><li>Basic channel communications takes 70 nanoseconds</li><li>Compared to typical web services transactions over the internet these transactions are about a million times faster</li><li>The language is also very compact, and one hundred thousand to one million threads can be created within a 2 GByte address space.</li></ul><span style=\"font-weight: bold;\">Works Cited</span><br /><ul><li>[Cygwin] \u201cGNU + Cygnus + Windows\u201d, <a href=\"http://www.cygwin.com/\">http://www.cygwin.com/</a></li><li>[GraphML] \u201cGraph Modeling Language \u201c, <a href=\"http://graphml.graphdrawing.org/\">http://graphml.graphdrawing.org/</a></li><li>[Hoare78] C. A. R. Hoare. \u201cCommunicating Sequential Processes.\u201d Communications of the ACM 1978.</li><li>[Hoare85] C. A. R. Hoare. Communicating Sequential Processes. Prentice Hall, 1985.</li><li>[KROC]. \u201cKent Retargetable Occam Compiler\u201d  <a href=\"http://www.cs.kent.ac.uk/projects/ofa/kroc/\">http://www.cs.kent.ac.uk/projects/ofa/kroc/</a></li><li>[Occam2] Inmos Ltd, Occam 2 Reference Manual. Prentice Hall, 1988.</li><li>[Milner99] Robin Milner, \u201cCommunicating and Mobile Systems: the Pi-Calculus\u201d, Cambridge University Press, 1999.</li><li>[Skype]. \u201cSkype Developers API\u201d  <a href=\"https://developer.skype.com\">https://developer.skype.com</a></li><li>[SSDL] Webber et al. \u201cAsynchronous Messaging between Web Services using SSDL.\u201d IEEE Internet Computing Vol10, No1 2006.</li><li>[WS-BPEL]. \u201cOASIS Web Services Business Process Execution Language\u201d  <a href=\"http://www.oasis-open.org\">http://www.oasis-open.org</a></li><li>[WS-CDL] \u201cWeb Services Choreography Description Language\u201d, <a href=\"http://www.w3.org\">http://www.w3.org</a></li><li>[yEd] \u201cGraph visualization and editing tool\u201d, <a href=\"http://www.yworks.com/\">http://www.yworks.com/</a></li></ul>",
        "metadata": {
          "word_count": 675,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Aims to build a very efficient massively threaded simulator for large scale p2p web services. Uses pi-calculus enhanced version of the old Inmos Transputer language Occam. PProtector is a free, open-source software for testing and verifying complex web services. The simulator models centralized web services and application to application messaging over the Skype Peer to Peer network. It is based on Pi-Calculus. The Occam language is based on CSP, and has been extended to add the Pi-Calculus extensions to form the Occam-Pi language. KROC, the Kent Re-targetable Occam Compiler is the primary implementation of this language. The Occam-Pi language is fast small compiled code. It has its own threading model, in a single process. Occam is designed to allow very comprehensive static analysis. The language is very compact, and one hundred thousand to one million threads can be created within a 2 GByte address space. Basic process creation takes 20 nanoseconds on an 800MHz PC. Skype. \u201cSkype Developers API\u201d  <a href=\"https://developer.skype.com\" \u00a0\u201cAsynchronous Messaging between Web Services using SSDL. .kent.ac.uk/projects/ofa/kroc/\u201d. Inmos Ltd, Occam 2 Reference Manual. Prentice Hall, 1988."
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_file_00f4cd3a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "BIL - recording an excellent adventure",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/03/bil-recording-excellent-adventure.html",
      "content": {
        "text": "The closing slide of BIL by Garrett Lisi<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzAaZdBP4ynM_2MxNjlCR3ecH1UondRYu__xD8vIjW-3ZnaNs65NTK7PsDwP9fSL5QtbdzcE3WvdAEHJ3-SVw-ibcAybJD-xGfFHG5l_Agmr5PX8HhIKckIy5blhjWExDCeT0G/s1600-h/IMG_0744.JPG\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgzAaZdBP4ynM_2MxNjlCR3ecH1UondRYu__xD8vIjW-3ZnaNs65NTK7PsDwP9fSL5QtbdzcE3WvdAEHJ3-SVw-ibcAybJD-xGfFHG5l_Agmr5PX8HhIKckIy5blhjWExDCeT0G/s400/IMG_0744.JPG\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5173431101563945874\" /></a><br /><br />Two days of intense expansion of my horizons, a self-organizing conference, and I am left holding a jewel. My contribution to BIL was that I basically did tech-support and audio for all the speakers in the main hall for two days. I brought my audio recorder a good mic and stand, a guitar amp (that was used as a PA speaker) and bags of cables and adaptors. Other BILsters provided a video projector and a karaoke machine for PA. I hooked up the mic and a mini-jack cable for computer audio out, and kept it all running as best I could. I recorded almost 800 minutes of content on my Korg D16 hard disk recorder, which is almost all of every talk. There were other people recording on Saturday, including video, but they had technical problems on Sunday, so we have little or no video of the Sunday talks.<br /><br />Next I'm going to make a backup copy, then start working on extracting the raw data for other people to process. I'll make mp3's of my favorite talks as I get time. Several TED speakers talked at BIL, including Aubrey De Grey on Saturday and Garrett Lisi, who closed out the conference with a fantastic talk. I chatted to Garrett a bit, very cool...<br /><br />For the stream of consciousness with pictures that I uploaded during the talk,<a href=\"http://www.twitxr.com/adrianco\"> look at my twitxr page</a>. I will update <a href=\"http://www.twitter.com/adrianco\">my twitter account</a> with progress on the audio files. I don't expect to use twitter and twitxr on a daily basis, but I think they are useful for conferences. I'll track progress at<a href=\"http://www.ecommmedia.com\"> Ecomm08 on March 12-14th</a> the same way.",
        "metadata": {
          "word_count": 305,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Two days of intense expansion of my horizons, a self-organizing conference, and I am left holding a jewel. My contribution to BIL was that I basically did tech-support and audio for all the speakers in the main hall. I recorded almost 800 minutes of content on my Korg D16 hard disk recorder. Other BILsters provided a video projector and a karaoke machine for PA. Several TED speakers talked at BIL, including Aubrey De Grey on Saturday and Garrett Lisi on Sunday. I don't expect to use twitter and twitxr on a daily basis, but I think they are useful for conferences. I'll track progress at Ecomm08 on March 12-14th the same way."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_5a53190f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix Free Trial link",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/02/netflix-free-trial-link.html",
      "content": {
        "text": "If you were thinking of joining Netflix, <a href=\"http://clickserve.cc-dt.com/link/click?lid=41000000030334633\">click this link to get a two week free trial</a> - it's a new affiliate program that Netflix is starting via the Google Advertiser Network. I will update the free trial link in my iPhone app in the next version.",
        "metadata": {
          "word_count": 48,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "If you were thinking of joining Netflix, click this link to get a two week free trial. It's a new affiliate program that Netflix is starting via the Google Advertiser Network."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_28f32280",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Data from task accounting record",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/data-from-task-accounting-record.html",
      "content": {
        "text": "A task is a group of related processes, when the last one exits, a task record is written. The information is very similar to the process record.<br /><br /><pre><br />    ff  group-header                    [group of 4 object(s)]<br />     1   version                        1<br />     2   filetype                       \"exacct\"<br />     3   creator                        \"SunOS\"<br />     4   hostname                       \"crun\"<br />   101  group-task                      [group of 25 object(s)]<br />  2000   taskid                         61<br />  2001   projid                         1              user.root<br />  2007   cpu-user-sec                   0<br />  2008   cpu-user-nsec                  0<br />  2009   cpu-sys-sec                    0<br />  200a   cpu-sys-nsec                   0<br />  2003   start-sec                      1109844060     03/03/05 02:01:00<br />  2004   start-nsec                     907341842<br />  2005   finish-sec                     1109844060     03/03/05 02:01:00<br />  2006   finish-nsec                    925962473<br />  2002   hostname                       \"crun\"<br />  200b   faults-major                   2<br />  200c   faults-minor                   0<br />  200e   msgs-snd                       0<br />  200d   msgs-recv                      0<br />  200f   blocks-in                      3<br />  2010   blocks-out                     1<br />  2011   chars-rdwr                     13483<br />  2012   ctxt-vol                       12<br />  2013   ctxt-inv                       1<br />  2014   signals                        0<br />  2015   swaps                          0<br />  2016   syscalls                       666<br />  2018   anctaskid                      29<br />  2019   zone                           \"global\"<br /></pre>",
        "metadata": {
          "word_count": 165,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A task is a group of related processes, when the last one exits, a task record is written. The information is very similar to the process record. <br />  200b   faults-major\u00a0\u00a0   \u00a0 2\u00a0 faults-minor\u00a0\u00a0 200c   minor faults\u00a0\u00a0 0\u00a0 faults\u00a0  200d   msgs-recv\u00a0 0<br/>  200f   blocks-in\u00a0 3\u00a0 blocks-out\u00a0 1\u00a0\u00a0\u00a0 2013   ctxt-inv 1\u00a0  2014   signals 1\u00a0 2015   swaps 1\u00a0 2016   syscalls 666."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_6f2c943a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Writing accounting records at time intervals",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/04/writing-accounting-records-at-time.html",
      "content": {
        "text": "A major new feature of the exacct system is the ability to get an accounting record logged without terminating the process. There are two forms of this, for tasks you can get the record to dump the delta since the last record was logged. Somehow the task remembers the data each time it cuts a record so it can do the differencing. This seems to be too much overhead at the process level, so the other option is to cut a record that logs the same data as if the process had just exited, and this option is available for both tasks and processes.<br /><br />The command that causes a record to be written is \"wracct\" and it takes a list of process or task id's and makes a system call to cause the record to be written. You have to be root to do this. The wracct command line syntax is a pain if you want to get it to dump multiple processes, as shown in this example from the manpage:<br /><pre><br /># /usr/sbin/wracct -i \"`pgrep sendmail`\" process<br /></pre><br />I want to make every process cut a record, and if you attempt to do this with wracct you need to form a list of every process id on the system. I tried to do this by listing all the entries in the /proc filesystem, but if any of the pid's exit before wracct gets to them it gets an error and quits. This is stupid, because if a process has exited, it has already cut an accounting record! The wracct command should have a \"-a\" option that writes all records and ignores errors.<br /><br />I modified the exdump command to have a \"-w\" option that loops over all processes and forces an accounting record to be written before it reads the accounting file. If you aren't root, it has no effect. The code looks like this:<br /><br /><pre><br /> if (wflag) {<br />  DIR *dirp;<br />  struct dirent *dp;<br /><br />  dirp = opendir(\"/proc\");<br />              do {<br />                 if ((dp = readdir(dirp)) != NULL)<br />     (void) wracct(P_PID, atoi(dp->d_name), EW_PARTIAL);<br />              } while (dp != NULL);<br />               (void) closedir(dirp);<br />                <br /> }<br /></pre><br /><br />The next step is a bit more complex. Both for flow accounting and interval process accounting, I need code that remembers previous records and matches them with subsequent ones so that they can be accumulated (flows) or differenced (process intervals).",
        "metadata": {
          "word_count": 405,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The wracct command line syntax is a pain if you want to get it to dump multiple processes. The command that causes a record to be written is \"wracct\" and it takes a list of process or task id's and makes a system call. To do this with wracct you need to form a list of every process id on the system. I tried to do this by listing all the entries in the /proc filesystem, but if any of the pid's exit before wrac CT gets to them it gets an error and quits. This is stupid, because if a process has exited, it has already cut an accounting record! The wracCT command should have a \"-a\" option that writes all records and ignores errors. The next step is a bit more complex. Both for flow accounting and interval process accounting, I need code that remembers previous records and matches them with subsequent ones."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2dc06a7f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Velocity conference summary",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/07/velocity-conference-summary.html",
      "content": {
        "text": "<a href=\"http://radar.oreilly.com/2009/07/velocity-making-your-site-fast.html\">Steve Souders writes a good summary</a> of the Velocity Conference. I'd like to present something next year, I have a few ideas already, and have been inspired to try some latency testing projects at work.",
        "metadata": {
          "word_count": 36,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'd like to present something next year, I have a few ideas already. I have been inspired to try some latency testing projects at work."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_1e7a5fa4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Playing with Hadoop in the Cloud",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/06/playing-with-hadoop-in-cloud.html",
      "content": {
        "text": "I just finished writing a paper for CMG that is a tutorial on Amazon Elastic Map Reduce. I signed up, it was easy, and after I ran two of their demo jobs I owed them 26 cents. Very cool...",
        "metadata": {
          "word_count": 39,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I just finished writing a paper for CMG that is a tutorial on Amazon Elastic Map Reduce. I signed up, it was easy, and after I ran two of their demo jobs I owed them 26 cents."
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_2e043299",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Tagging Blogger and Technorati | Adrian Cockcroft | 8th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/tagging-blogger-and-technorati-adrian.html",
      "content": {
        "text": "Blogger doesn't provide an integrated way to tag my postings (as far as I can tell) but at ETech we were advised to tag our blog entries with Etech and Etech06 so I tried to figure this out during one of the less interesting talks (yes I know I should have figured this out ages ago). Despite a slow and intermittent wireless internet connection I found that I could use Technorati to do this by embedding some html in my blog. The frustrating problem I ran into was that Technorati seemed to be completely overloaded and was largely unresponsive. When I did finally get my blog entry tagged I went to Technorati and searched for Etech, and after a long wait it came up with no results at all... I figured I had done something wrong, but late at night the load dropped off and the site now does actually find Etech tags, including my own one. I celebrated by adding \"florwax\" as a tag, so we will see how that goes...<br /><br />Another site that seems a victim of its success is Myspace, their music delivery service has become overloaded, so its hit and miss whether you can get any songs to play at the moment.<br /><br /><a href=\"http://technorati.com/tag/etech\" rel=\"tag\">etech</a>, <a href=\"http://technorati.com/tag/etech\" rel=\"tag\">etech06</a>",
        "metadata": {
          "word_count": 214,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Blogger doesn't provide an integrated way to tag my postings (as far as I can tell) At ETech we were advised to tag our blog entries with Etech and Etech06. Despite a slow and intermittent wireless internet connection I found that I could use Technorati to do this. It's hit and miss whether you can get any songs to play at the moment. come overloaded, so its hit and hit whether you're able to play anything at all."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_6719619e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Usenix 08 - Tutorials and Invited Paper - Boston June 23-27",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/05/usenix-08-tutorials-and-invited-paper.html",
      "content": {
        "text": "<a href=\"http://www.usenix.org/usenix08/going\"> <img src=\"http://www.usenix.org/events/usenix08/art/usenix08_going.jpg\" border=\"0\" width=\"162\" height=\"57\" alt=\"I'm going to USENIX '08\"> </a><br /><br />I'm giving the two tutorials that I have been presenting at CMG the last few years, but at Usenix in Boston. The tutorials are the latest update of my Solaris/Linux performance metrics and tuning class, and Capacity Planning with Free Tools.<br /><br />I'm also giving an invited speaker talk on <a href=\"http://www.millicomputing.com\">Millicomputing</a>, and have a nice big time slot, so will cover both the mobile and enterprise aspects in some depth, with time for discussion.<br /><br />I hope to see you there...",
        "metadata": {
          "word_count": 96,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm giving the two tutorials that I have been presenting at CMG the last few years. The tutorials are the latest update of my Solaris/Linux performance metrics and tuning class. I'm also giving an invited speaker talk on Millicomputing."
      },
      "tags": [
        "metrics",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_782ec6fa",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "The Cockcroft Headroom Plot - Part 1 - Introducing R",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/cockcroft-headroom-plot-part-1.html",
      "content": {
        "text": "I've recently written a paper for CMG06 called \"Utilization is Virtually Useless as a Metric!\". Regular readers of this blog will recognize much of the content in that paper. The follow-on question is what to use instead? The answer I have is to plot response time vs. throughput, and I've been thinking about a very specific way to display this kind of plot. Since I'm feeling quite opinionated about this I'm going to call it a \"Cockcroft Headroom Plot\" and I'm going to try and construct it using various tools. I will blog my way through the development of this, and I welcome advice and comments along the way.<br /><br />The starting point is a dataset to work with, and I found an old iostat log file that recorded a fairly busy disk at 15 minute intervals over a few days. This gives me 250 data points, which I fed into the <a href=\"http://www.r-project.org/\">R stats package</a> to look at. I'll also have a go at making a spreadsheet version.<br /><br />The iostat data file starts like this:<br /><pre><span style=\"font-family:courier new;\">                    extended device statistics              </span><br /><span style=\"font-family:courier new;\"> r/s  w/s   kr/s   kw/s wait actv wsvc_t asvc_t  %w  %b device</span><br /><span style=\"font-family:courier new;\">14.8 78.4  183.0 2446.3  1.7  0.6   18.6    6.6   1  21 c1t5d0</span><br /><span style=\"font-family:courier new;\"> 0.0  0.0    0.0    0.0  0.0  0.0    0.0    5.0   0   0 c0t6d0</span><br /><span style=\"font-family:courier new;\">...</span><br /></pre><br />I want the second line as a header, so save it (my command line is actually on OSX, but could be Solaris, Linux or Cygwin on Windows)<br /><span style=\"font-family:courier new;\">% head -2 iostat.txt | tail -1 > header</span><br /><br />I want the <span style=\"font-family:courier new;\">c1t5d0</span> disk, but don't want the first line, since its the average since boot, and want to add back the header<br /><span style=\"font-family:courier new;\">% grep c1t5d0 iostat.txt | tail +2 > tailer<br />% cat header tailer > c1t5.txt<br /><br /></span>Now I can import into R as a space delimited file with a header line. R doesn't allow \"/\" or \"%\" in names, so it rewrites the header to use dots instead. R is a script based tool with a command line and a very powerful vector/object based syntax. A \"data frame\" is a table of data object like a sheet in a spreadsheet, it has names for the rows and columns, and can be indexed.<br /><span style=\"font-family:courier new;\">> c1t5 <- read.delim(\"<path>c1t5.txt\",header=T,sep=\"\")</path></span><br /><span style=\"font-family:courier new;\">> names(c1t5)</span><br /><span style=\"font-family:courier new;\"> [1] \"r.s\"    \"w.s\"    \"kr.s\"   \"kw.s\"   \"wait\"   \"actv\"   \"wsvc_t\" \"asvc_t\" \"X.w\"    \"X.b\"    \"device\"</span><br /><br />I only want to work with the first 250 data points so I subset the data frame by indexing the rows with an array (<span style=\"font-family:courier new;\">1:250</span>) that selects the rows I want and leaving the column selector blank.<br /><span style=\"font-family:courier new;\">> io250 <- c1t5[1:250,]</span><br /><br />The first thing to do is summarize the data, the output is too wide for the blog so I'll do it in chunks by selecting columns.<br /><pre><br /><span style=\"font-family:courier new;\">> summary(io250[,1:4])</span><br /><span style=\"font-family:courier new;\">      r.s              w.s             kr.s             kw.s        </span><br /><span style=\"font-family:courier new;\"> Min.   :  1.80   Min.   :  1.8   Min.   :  13.5   Min.   :   38.5  </span><br /><span style=\"font-family:courier new;\"> 1st Qu.: 10.30   1st Qu.: 87.1   1st Qu.: 107.4   1st Qu.: 2191.7  </span><br /><span style=\"font-family:courier new;\"> Median : 18.90   Median :172.4   Median : 182.8   Median : 4279.4  </span><br /><span style=\"font-family:courier new;\"> Mean   : 22.85   Mean   :187.5   Mean   : 290.1   Mean   : 4448.5  </span><br /><span style=\"font-family:courier new;\"> 3rd Qu.: 28.88   3rd Qu.:274.6   3rd Qu.: 287.4   3rd Qu.: 6746.6  </span><br /><span style=\"font-family:courier new;\"> Max.   :130.90   Max.   :508.8   Max.   :4232.3   Max.   :13713.1  </span><br /><span style=\"font-family:courier new;\">> summary(io250[,5:8])</span><br /><span style=\"font-family:courier new;\">      wait             actv            wsvc_t           asvc_t      </span><br /><span style=\"font-family:courier new;\"> Min.   : 0.000   Min.   :0.0000   Min.   : 0.000   Min.   : 1.000  </span><br /><span style=\"font-family:courier new;\"> 1st Qu.: 0.000   1st Qu.:0.3250   1st Qu.: 0.400   1st Qu.: 3.125  </span><br /><span style=\"font-family:courier new;\"> Median : 0.600   Median :0.8000   Median : 2.550   Median : 4.700  </span><br /><span style=\"font-family:courier new;\"> Mean   : 1.048   Mean   :0.9604   Mean   : 5.152   Mean   : 4.634  </span><br /><span style=\"font-family:courier new;\"> 3rd Qu.: 1.300   3rd Qu.:1.5000   3rd Qu.: 6.350   3rd Qu.: 5.700  </span><br /><span style=\"font-family:courier new;\"> Max.   :10.600   Max.   :3.5000   Max.   :88.900   Max.   :15.100  </span><br /><span style=\"font-family:courier new;\">> summary(io250[,9:10])</span><br /><span style=\"font-family:courier new;\">      X.w             X.b       </span><br /><span style=\"font-family:courier new;\"> Min.   :0.000   Min.   : 2.00  </span><br /><span style=\"font-family:courier new;\"> 1st Qu.:0.000   1st Qu.:20.00  </span><br /><span style=\"font-family:courier new;\"> Median :1.000   Median :39.50  </span><br /><span style=\"font-family:courier new;\"> Mean   :1.428   Mean   :37.89  </span><br /><span style=\"font-family:courier new;\"> 3rd Qu.:2.000   3rd Qu.:55.00  </span><br /><span style=\"font-family:courier new;\"> Max.   :9.000   Max.   :92.00  </span><br /></pre><br /><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"></span><span style=\"font-family:courier new;\"></span></span><span style=\"font-size:85%;\"></span><br />Looks like a nice busy disk, so lets plot everything against everything (pch=20 sets a solid dot plotting character)<br /><span style=\"font-family:courier new;\">> plot(io250[,1:10],pch=20)</span><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/549300/Picture%204.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/189207/Picture%204.png\" alt=\"\" border=\"0\" /></a>The throughput is either reads+writes or KB read+KB written, the response time is wsvc_t+asvc_t since iostat records time taken waiting to send to a disk as well as time spent actively waiting for a disk.<br /><br />To save typing, I attach to the data frame so that the names are recognized directly.<br /><span style=\"font-family:courier new;\">> attach(io250)</span><br /><span style=\"font-family:courier new;\">> plot(r.s+w.s, wsvc_t+asvc_t)</span><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/660481/Picture%205.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/261600/Picture%205.png\" alt=\"\" border=\"0\" /></a>This looks a bit scattered, because there is a mixture of average I/O sizes that varies during the time period. Lets look at throughput in KB/s instead.<br /><span style=\"font-family:courier new;\">> plot(kr.s+kw.s,wsvc_t+asvc_t)</span><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/324809/Picture%206.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/822516/Picture%206.png\" alt=\"\" border=\"0\" /></a>That looks promising, but its not clear what the distribution of throughput is over the range. We can look at this using a histogram.<br /><span style=\"font-family:courier new;\">> hist(kr.s+kw.s)</span><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/479325/Picture%207.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/837976/Picture%207.png\" alt=\"\" border=\"0\" /></a><br />We can also look at the distribution of response times.<br /><span style=\"font-family:courier new;\">> hist(wsvc_t+asvc_t)</span><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/710275/Picture%208.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/830539/Picture%208.png\" alt=\"\" border=\"0\" /></a>The starting point for the thing that I want to call a \"Cockcroft Headroom Plot\" is all three of these plots superimposed on each other. This means rotating the response time plot 90 degrees so that its axis lines up with the main plot. After looking around in the manual pages I eventually found an example that I could use as the basis for my plot. It needs some more cosmetic work but I defined a new function <span style=\"font-family:courier new;\">chp(throughput, response)</span> shown below.<br /><pre><br />> chp <- function(x,y,xl=\"Throughput\",yl=\"Response\",ml=\"Cockcroft Headroom Plot\") {<br />   xhist <- hist(x,plot=FALSE)<br />   yhist <- hist(y, plot=FALSE)<br />   xrange <- c(0,max(x))<br />   yrange <- c(0,max(y))<br />   nf <- layout(matrix(c(2,0,1,3),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)<br />   layout.show(nf)<br />   par(mar=c(3,3,1.5,1.5))<br />   plot(x, y, xlim=xrange, ylim=yrange, main=xl)   par(mar=c(0,3,3,1))<br />   barplot(xhist$counts, axes=FALSE, ylim=c(0, max(xhist$counts)), space=0, main=ml)<br />   par(mar=c(3,0,1,1))<br />   barplot(yhist$counts, axes=FALSE, xlim=c(0, max(yhist$counts)), space=0, main=yl, horiz=TRUE)<br />}<br /></pre><br />The result of running <span style=\"font-family:courier new;\">chp(kr.s+kw.s,wsvc_t+asvc_t)</span>is close...<br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/328688/Picture%2010.png\"><img style=\"margin: 0px auto 10px; display: block; text-align: center; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/226742/Picture%2010.png\" alt=\"\" border=\"0\" /></a><br /><br />That's enough to get started.",
        "metadata": {
          "word_count": 1187,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I've been thinking about a very specific way to display this kind of plot. I'm going to try and construct it using various tools. I will blog my way through the development of this, and I welcome advice and comments along the way. I want the second line as a header, so save it (my command line is actually on OSX, but could be Solaris, Linux or Cygwin on Windows) I want the <span style=\"font-family:courier new;\">c1t5d0</span> disk, but don't want the first line, since its the average since boot. R is a script based tool with a command line and a very powerful vector/object based syntax. A \"data frame\" is a table of data object like a sheet in a spreadsheet, it has names for the rows and columns, and can be indexed. The output is too wide for the blog so I'll do it in chunks by selecting columns. The first thing to do is summarize the data.  <span style=\"font-family:courier new;\">> summary(io250[,5:8]) </span. ;\"> Max.   :130.90   Max.\u00a0  :508.8  Max. \u00a0 :4232.3  max.  :13713.1  </span><br /><span style\"font- family:couch\" class=\"couch-sport\" type=\"text/css\" size=\"8.0\" margin-bottom:0.5\" border-top:1.25\" line-height:1;. margin-left: 0.5%;. font-family.family.font-size: Ourier plots the data on a busy disk. We plot everything against everything (pch=20 sets a solid dot plotting character) ourier plots ourier against everything. ourier plot(io250[,1:10],pch =20) The throughput is either reads+writes or KB read+KB written, the response time is wsvc_t+asvc_t since iostat records time taken waiting to send to a disk. I attach to the data frame so that the names are recognized directly. This looks a bit scattered, because there is a mixture of average I/O sizes that varies during the time period. Lets look at throughput in KB/s instead. The starting point for the thing that I want to call a \"Cockcroft Headroom Plot\" is all three of these plots superimposed on each other. This means rotating the response time plot 90 degrees so that its axis lines up with the main plot. The result of running <span style=\"font-family:courier new;\">chp(kr.s+kw.s,wsvc_t+asVC_t) is close to perfect. It needs some more cosmetic work but I defined a new function that I could use as the basis for my plot. That's enough to get started. lectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/328688/Picture%2010.png\" alt=\"\" border=\"0\" />"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_bb89c943",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Contextual eBay adverts with ctxbay | by Adrian Cockcroft | 9th March 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/03/contextual-ebay-adverts-with-ctxbay-by.html",
      "content": {
        "text": "The winners in the eBay developer contest were announced at ETech, one was Alex Stankovic, who has developed a contextual advertising system for eBay that works just like Google Adsense. I just changed my advert bar for this blog to use his system at <a href=\"http://www.ctxbay.com\">ctxbay</a>.<br /><br />At the ctxbay site you find a link to sign up with eBay for the affiliate program (via Commission Junction) this was an easy fast signup, and gets you an affiliate id number. You then create an account at ctxbay, and enter your affiliate number, which they will then use to call back to Comission Junction and make sure you get paid.<br /><br />The rest of the setup is similar to adsense, however you do need to login to ctxbay with your new account, and it didn't do this automatically for me. ctxbay generates a selection of common ad frame formats using javascript that can be slotted into your site template. I found one identical to my adsense format, and swapped out the code. My first attempt didn't work because I was not logged in, and the id field in the javascript was empty. After I logged in I got a fairly long string that keys my ad to ctxbay.<br /><br />I setup adsense in order to understand it better, and going forward I'll see if ctxbay can generate any sensible eBay items out of the keywords in my blog.<br /><a href=\"http://technorati.com/tag/etech\" rel=\"tag\">etech</a>, <a href=\"http://technorati.com/tag/etech06\" rel=\"tag\">etech06</a>",
        "metadata": {
          "word_count": 243,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Alex Stankovic has developed a contextual advertising system for eBay that works just like Google Adsense. I just changed my advert bar for this blog to use his system at ctxbay. I setup adsense in order to understand it better. I was not logged in, and the id field in the javascript was empty. After I logged in I got a fairly long string that keys my ad to ctxbay."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_68f9e38e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Leopard review, iPhone 1.1.2 release and the Google Phone",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/11/leopard-review-iphone-112-release-and.html",
      "content": {
        "text": "There is an excellent in depth review of <a href=\"http://arstechnica.com/reviews/os/mac-os-x-10-5.ars/\">Leopard at Ars Technica</a>, it talks about the Core Animation feature which seems like a very powerful way of specifying device independent animation effects that will work on a desktop or (eventually) on an iPhone.<div><br /></div><div>Now that Apple has got Leopard out, their next move is the UK launch of the iPhone on November 9th, and as I have mentioned before, there is some additional localization needed for this and it appears that this is going to be the main feature of iPhone 1.1.2 release. <a href=\"http://www.t3.co.uk/news/247/communications/mobile_phone/first-look-at-uk-iphone\">The UK press is starting to get its hands on the UK product</a>, which is running 1.1.2. The main features are a fairly complete set of worldwide localized languages, a closing of the hole that is currently being used for hacking into the iPhone, and some local features related to the UK deal with O2 and The Cloud for WiFi access, and the UK iTunes music store. There doesn't seem to be any new applications in this release.</div><div><br /></div><div>There has been a lot of speculation about Google releasing a set of phone applications or perhaps a complete phone OS. They have been working closely with Apple, so there may be some additional/optional iPhone applications coming directly from Google. However, there is currently no support for Java on the iPhone, and some of what Google has been doing is building a suite of Java based mobile applications, so this could be seen as a broadening of Google's mobile support. If Google decides to support a fairly generic mobile Linux (like <a href=\"http://www.openembedded.org/\">OpenEmbedded</a>) then it opens up their applications to <a href=\"http://openmoko.org/\">OpenMoko</a> and the <a href=\"http://hbmobile.org/wiki/index.php?title=MyPhone\">homebrew mobile club's myPhone</a>. What we have today from Google is three mobile applications:</div><div><br /></div><div>Google Maps on the iPhone (which is probably a native re-write in Objective-C). Google Maps for everyone else (which is written in Java), and I think Google Maps for PalmOS also appears to be a native application. I've used this on Treo, Blackberry and iPhone. Its obviously based on the same backend web services but the user interaction is a little different on each.</div><div><br /></div><div>Gmail reader, which I have used on Treo and Blackberry appears to be a Java based client. I'd like to see this on the iPhone, so I'm hopeful for a full function iPhone Gmail client.</div><div><br /></div><div>YouTube viewer on iPhone. This looks more like an Apple application, running against the Google/YouTube web services, and required coordination to get the encoding standardized on H.264. It also supports the AppleTV. I suspect that Google will have a YouTube viewer for other platforms, probably written in Java. It could also be an mp3/music player.</div><div><br /></div><div>The web browser is a problem that the iPhone has solved, but everyone else has a very fragmented approach, caused in part by lack of memory and CPU power on most phones. There are some open source projects that have got going recently to develop Mobile Mozilla, and there are more mature products like Opera. If Google has been working on its own mobile web browser, then they have been keeping very quiet about it.</div><div><br /></div><div>Google's office suite is based on AJAX front ends and a back end set of web services that handle presentations, documents and spreadsheets. A mobile front end application could make these much more usable, and I would suspect an iPhone version and/or a more generic Java version could be in the works. The main problem with Google's existing online office suite is that it can't be used when there is no data connection. It would be much more useful to allow some subset of useful operations to work while disconnected.</div>",
        "metadata": {
          "word_count": 611,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The UK launch of the iPhone will be on November 9th. The UK press is starting to get its hands on the UK product, which is running 1.1.2. The main features are a fairly complete set of worldwide localized languages. Google has released three new applications for the iPhone. There doesn't seem to be any new applications in this release. There is currently no support for Java on the iPhone, so this could be seen as a broadening of Google's mobile support. Gmail reader, which I have used on Treo and Blackberry appears to be a Java based client. I suspect that Google will have a YouTube viewer for other platforms, probably written in Java. It could also be an mp3/music player. Google's office suite is based on AJAX front ends and a back end set of web services. A mobile front end application could make these much more usable. I would suspect an iPhone version and/or a more generic Java version could be in the works."
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_b9f0591a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power - Monitoring Output",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/09/solar-power-monitoring-output.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF8XjsEH9-ARplB4MHDRGxHwNh-GnqGPk5cKmzx7IcoFJwRD-sk-4hawsKt7gA8Debs-4leUlJ5J2awz-Q02LNISvIOPOdkXNrmoeH8XVr95seZLRK5MNx-x4vdTYEIXoozwLx/s1600-h/Picture+13.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 200px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhF8XjsEH9-ARplB4MHDRGxHwNh-GnqGPk5cKmzx7IcoFJwRD-sk-4hawsKt7gA8Debs-4leUlJ5J2awz-Q02LNISvIOPOdkXNrmoeH8XVr95seZLRK5MNx-x4vdTYEIXoozwLx/s320/Picture+13.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5378431488616208578\" /></a><br /><br />The SolarCity web based monitoring system didn't work at first, there was a problem with the data-logging board in the inverter but they were helpful and sent someone round to fix it. So now we can track how much power we are generating. The graph updates every 15 minutes, and it reports the amount of power every half hour, so when it shows 1.7 KWh that is equivalent to 3.4KW averaged over 30 minutes. So far the highest peak instantaneous output reported on the Inverter is about 4.2KW.<br /><br />We are still waiting for the energy consumption metering to be added, which puts a second line on the same graph.<br /><br />The day shown was a typical day up here on top of the mountain, it was cloudy elsewhere, but we had basically clear sky all day. and generated over 25KWh.",
        "metadata": {
          "word_count": 157,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The SolarCity web based monitoring system didn't work at first, there was a problem with the data-logging board in the inverter. So now we can track how much power we are generating. The graph updates every 15 minutes, and it reports the amount of power every half hour. So far the highest peak instantaneous output reported on the Inverter is about 4.2KW. The day shown was a typical day up here on top of the mountain, it was cloudy elsewhere."
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_file_1344e7fc",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Reducing TCP retransmit timeout?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/08/reducing-tcp-retransmit-timeout.html",
      "content": {
        "text": "Cloud networks are lossy and low latency, reducing TCP_RTO_MIN and TCP_DELACK_MIN looks like a good idea, but it looks as if this needs a linux kernel recompile. Anyone else looked at this? <br />Here is a relevant paper \u201cSafe and Effective Fine-grained TCP Retransmissions for Datacenter Communication\u201c<br />http://www.cs.cmu.edu/~vrv/papers/sigcomm147-vasudevan.pdf",
        "metadata": {
          "word_count": 48,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Reducing TCP_RTO_MIN and TCP_DELACK_MIN looks like a good idea, but it looks as if this needs a linux kernel recompile. Anyone else looked at this?"
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_d5ad6067",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Comparing Smart Mobile Phones | by Adrian Cockcroft | April 19th, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/04/comparing-smart-mobile-phones-by.html",
      "content": {
        "text": "Its been a while since I last posted, mostly due to a long vacation. We stayed with friends in New York for a few days, spent 10 days on Bermuda (very nice and relaxing) and spent a few days in New York again on the way home.<br /><br />In the last week or so I have been trying out a new phone. I got a Nokia 6682 which runs the Symbian S60 operating system and which has fairly good third party support for applications that use Java, Flash and Opera. My history with phones started out with Nokia for many years, and then I switched to the Treo line. I've had a Treo 270, 600 and currently have a 650. Going back to Nokia was is some ways familiar, the user interface has some similarities to the older days, but overall I miss my Treo and I'm going to switch back. I thought it might be interesting to discuss the differences and what I think a state of the art smartphone should be able to do for me.<br /><br />The <a href=\"http://www.nokiausa.com/phones/6682\">Nokia 6682</a> has a decent spec, large color screen, 1.3Mpixel camera and a 64MB removable flash card included. The spec page even says that you can \"Bid on eBay on the go\" but that is available to any phone that can browse to <a href=\"http://wap.ebay.com\">wap.ebay.com</a>. With a normal Cingular GSM service its not using a 3G high speed network, so data network access is similar in speed to the <a href=\"http://www.palm.com/us/products/smartphones/treo650/\">Treo 650</a>.<br /><br />My main problem is that I've been spoilt by the Treo's touch screen and keyboard. When using the Nokia, at first I was poking at the screen in vain trying to select things. The screen is fairly high resolution, but its an eye test in that the text size is too small for many features, and the colors available in the default set of themes have poor contrast. The Nokia is actually much harder to read. For text entry I'm actually used to using Nokia's predictive text feature but it is still extremely painful to enter a text message or URL.<br /><br />The Treo' browser (Blazer) is easy to use but is not well supported in terms of javascript and many sites don't recognize it properly. On the Nokia there is a built in browser (called Web) and Opera 7 is included, with a free upgrade to Opera 8.5. I found the \"Web\" browser OK to use, but Opera was very annoying and unintuitive. With the Treo I can quickly get on the web to look something up, it just takes too long on the Nokia, and with Opera I found the navigation commands to be confusing and awkward. I tried to make use of the javascript support in Opera, but it didn't work for Google maps, despite Google claiming that Opera 8 is supported.<br /><br />I also had problems with the Nokia after browsing the web. The phone keeps applications running in the background and tends to run out of memory at awkward moments. I tried to use the camera, but at the point of taking a picture it failed with a lack of memory. I had to bring up the web browser and explicitly exit it, meanwhile the photo opportunity had gone. The Nokia's camera is higher resolution and has continuous zoom that works for video as well as pictures. For the Treo, you have just 1x and 2x zoom settings and a 640x480 resolution. Its not really enough to snap pictures of whiteboard scribbles clearly. The Nokia has a sliding cover for the phone, which activates the camera when opened, but its too easy to open by accident when getting the phone out.<br /><br />Other phones I've seen recently include the <a href=\"http://download.vzwshop.com/vzw/3dphones/vx9800/index.htm\">Verizon LG VX9800</a> which is a very fat clamshell with a nice big keyboard, 3G networking and a real eye-test of a small hi-res screen. A friend got one but is taking it back, its more suited to gaming and entertainment than business. My son has a <a href=\"http://www.cingular.com/SLVR_iTunes/\">Motorola SLVR L7 with iTunes</a> and seems happy with it. It looks cool, fits his interests but he doesn't try to use the web from his phone.<br /><br />Some co-workers have Windows mobile phones, I haven't tried to use them myself, but I've heard a mixture of good and bad comments, opinion seems very polarized as love it or won't touch it..<br /><br />So in summary, the things I can't do without on a phone are a touch screen, full keyboard and a large (not just high resolution) display with fonts and icons that can be read easily. The things I don't like about the Treo 650 are its lack of support for Opera 8 (which may be more usable with a keyboard and touch screen), Javascript and Flash.<br /><br />My favourite applications on the Treo are the <a href=\"http://www.chatteremail.com\">Chatter</a> email client, <a href=\"http://www.aho.ch/pilotplanets/\">Planetarium</a> for identifying stars and planets, and Solitaire for mindless time wasting - which would be a pain to play without the touch screen. I also find that the mobile version of <a href=\"http://www.bloglines.com\">bloglines </a>works well with the Treo's browser, so I can keep up with my feeds.",
        "metadata": {
          "word_count": 868,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In the last week or so I have been trying out a new phone. I got a Nokia 6682 which runs the Symbian S60 operating system. The phone has fairly good third party support for applications that use Java, Flash and Opera. The screen is fairly high resolution, but its an eye test in that the text size is too small for many features. The colors available in the default set of themes have poor contrast. For text entry I'm actually used to using Nokia's predictive text feature. On the Nokia there is a built in browser (called Web) Opera 7 is included, with a free upgrade to Opera 8.5. I found the \"Web\" browser OK to use, but Opera was very annoying and unintuitive. For the Treo, you have just 1x and 2x zoom settings and a 640x480 resolution. The Nokia has a sliding cover for the phone, which activates the camera when opened, but its too easy to open by accident when getting the phone out. My favourite applications on the Treo are the <a href=\"http://www.chatteremail.com\">Chatter</a> email client, and Planetarium for identifying stars and planets. I also find that the mobile version of Bloglines works well with the browser, so I can keep up with my feeds."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_e63d6195",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Updates: AdSense, FON and Millicomputing",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/updates-adsense-fon-and-millicomputing.html",
      "content": {
        "text": "The Ads are making a bit more sense now...<br /><br />There is a <a href=\"http://www.boingo.com\">global commercial WiFi service that has good coverage in places like Hotels and Airports called Boingo</a>. They aggregate services that are not branded under their own name, but if you have a Boingo account you can use the service. Last month they announced a deal to include <a href=\"http://www.fon.com\">FON in their network</a>, more than doubling the number of access points to a few hundred thousand. This month they announced an annual flat rate plan for $39 per month. Very cool! Global WiFi access at a low flat rate.<br /><br />I'm posting a series of updates at <a href=\"http://www.millicomputing.com\">Millicomputing.com</a> that describe a detailed design for an Enterprise server Millicluster. Its a radical improvement in terms of performance per watt and performance per dollar for some kinds of applications.",
        "metadata": {
          "word_count": 141,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a global commercial WiFi service that has good coverage in places like Hotels and Airports called Boingo. They aggregate services that are not branded under their own name, but if you have a Boingo account you can use the service."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_ca87ad58",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Load Average Differences Between Solaris and Linux",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/load-average-differences-between.html",
      "content": {
        "text": "A lot of people monitor their servers using load average as the primary metric. Tools such as <a href=\"http://ganglia.sourceforge.net/\">Ganglia</a> colorize all the nodes in a cluster view using load average. However there are a few things that aren't well understood about the calculation and how it varies between Solaris and Linux.<br />\n<br />\nFor a detailed explanation of the algorithm behind the metric, <a href=\"http://www.perfdynamics.com/Manifesto/gcaprules.html\" target=\"_blank\">Neil Gunther has posted a series of articles</a> that show how Load Average is a time-decayed metric that reports the number of active processes on the system with a one, five and fifteen minute decay period.<br />\n<br />\nThe source of the number of active processes can be seen in vmstat as the first few columns, and this is where Solaris and Linux differ. For example, some Linux vmstat from a busy file server is shown below.<br />\n<pre>procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in    cs us sy id wa\n 4 43      0  32384 2993312 3157696  0    0  6662  3579 11345 7445  7 65  0 27</pre>\n<br />\n<br />\nThe first two columns show the number of processes that are in the run queue waiting for CPU time and in the blocked queue waiting for disk I/O to complete. These metrics are calculated in a similar manner in both Linux and Solaris, but the difference is that the load average calculation is fed by just the \"r\" column for Solaris, and by the \"r\" plus the \"b\" column for Linux. This means that a Linux based file server that has many disks could be running quite happily from a CPU perspective but show a large load average.<br />\n<br />\nThe logic behind the load average metric is that it should be a kind of proxy for responsiveness on a system. To get a more scalable measure of responsiveness, it is common to divide the load average by the number of CPUs in the system, since more CPUs will take jobs off the run queue faster. For disk intensive workloads on Linux, it may also make sense to divide the load average by the number of active disks, but this is an awkward calculation to make.<br />\n<br />\nIt would be best to take <span style=\"font-weight: bold;\">r/CPU count</span> and <span style=\"font-weight: bold;\">b/active disk count</span> then average this combination with a time decay and give it a new name, maybe the \"slowed average\" would be good?",
        "metadata": {
          "word_count": 414,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "load average is a time-decayed metric that reports the number of active processes on the system with a one, five and fifteen minute decay period. Solaris and Linux differ in how they calculate the metric. The first two columns show the number of processes that are in the run queue waiting for CPU time. The metrics are calculated in a similar manner in both Linux and Solaris, but the difference is that the load average calculation is fed by just the \"r\" column for Solaris. On Linux, it may also make sense to divide the load average by the number of active disks, but this is an awkward calculation to make. It would be best to average this combination with a time decay and give it a new name."
      },
      "tags": [
        "metrics"
      ]
    },
    {
      "id": "virtual_adrianco_file_601fbbaf",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix for iPhone in the cloud and HTML5",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/08/netflix-for-iphone-in-cloud-and-html5.html",
      "content": {
        "text": "<a href=\"http://http://perfcap.blogspot.com/2010/05/netflix-in-cloud-and-html5.html\">I posted on this subject a while ago</a>, and generated a lot of confused comments, who read things into my post that weren't there. Today I hope it's a bit clearer what I was talking about, because we released the Netflix iPhone app, which is based on HTML5 for its user interface elements, back ends into the Amazon cloud, and uses conventional, non-HTML5 video playback and DRM. The playback mechanism is the same as the iPad, but to support the scale of expected usage we needed more capacity (it's currently the top free app in the app store as I write this) and we got that by rebuilding our API tier and personalized movie choosing backend to run on the Amazon cloud. The user interface runs in a webkit based browser window in the app, just like on iPad, but the entire UI is built using Javascript with advanced CSS and HTML5 animations to get it to feel like a native iPhone app.<br /><br />Returning to the theme of my last post, Netflix is hiring engineers to work on cloud tools, platforms and performance, and advanced user interfaces. I think we are breaking new ground  in both areas and its an exciting place to be. We have very high standards and are looking for the best people in the industry to come and help us...",
        "metadata": {
          "word_count": 226,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Netflix iPhone app is based on HTML5 for its user interface elements, back ends into the Amazon cloud, and uses conventional, non- HTML5 video playback and DRM. The playback mechanism is the same as the iPad, but to support the scale of expected usage we needed more capacity. The user interface runs in a webkit based browser window in the app, just like on iPad. Netflix is hiring engineers to work on cloud tools, platforms and performance, and advanced user interfaces. \"We have very high standards and are looking for the best people in the industry to come and help us,\" says CEO Reed Hastings."
      },
      "tags": [
        "cloud",
        "netflix",
        "performance",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_f8e4fa39",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "eBay AdContext Contextual Adverts",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/ebay-adcontext-contextual-adverts.html",
      "content": {
        "text": "I've been running contextual ads on this site for a few months, using the CTXbay service that won an eBay developers program award. I don't think I've had many people click through, since the ads aren't very relevant, and seem fixated on the word \"Adrian\".<br /><br />Now that eBay has announced its own AdContext service is on the way, I'm planning to replace CTXbay with the official service as soon as I can get access to it. The eBay service has access to a lot more information and is quite customizable. I asked about it at the Developers Conference and was told that I could set a default category and control what kind of items appear.",
        "metadata": {
          "word_count": 116,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I've been running contextual ads on this site for a few months. I don't think I've had many people click through, since the ads aren't very relevant. eBay has announced its own AdContext service is on the way."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_734eb813",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Data structures and objects",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/data-structures-and-objects.html",
      "content": {
        "text": "The exacct data file is a complex tagged object format that is read via the libexacct library routines. While generic and flexible, it is a pain to get at the data. There are two demo programs that display the information, I used /usr/demo/libexacct/exdump to print out the information shown earlier, and there is also a perl library and a script called dumpexacct.pl. It displays the tags and types like this:<br /><br /><pre><br />GROUP<br />  Catalog = EXT_GROUP|EXC_DEFAULT|EXD_GROUP_PROC<br />  ITEM<br />    Catalog = EXT_UINT32|EXC_DEFAULT|EXD_PROC_PID<br />    Value = 1904<br />  ITEM<br />    Catalog = EXT_UINT32|EXC_DEFAULT|EXD_PROC_UID<br />    Value = 25<br />...<br /></pre><br /><br />I used this information to define a data structure that will be populated with the data from the file as the first processing step. I left the tags as comments, and defined reasonable amounts of fixed space for strings. The task and flow structures are similar.<br /><br /><pre><br />struct ex_proc { // EXT_GROUP|EXC_DEFAULT|EXD_GROUP_PROC<br /> uint32_t pid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_PID<br /> uint32_t uid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_UID<br /> uint32_t gid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_GID<br /> uint32_t projid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_PROJID<br /> uint32_t taskid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_TASKID<br /> uint64_t cpu_user_sec; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CPU_USER_SEC<br /> uint64_t cpu_user_nsec; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CPU_USER_NSEC<br /> uint64_t cpu_sys_sec; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CPU_SYS_SEC<br /> uint64_t cpu_sys_nsec; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CPU_SYS_NSEC<br /> uint64_t proc_start_sec; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_START_SEC<br /> uint64_t proc_start_nsec;// EXT_UINT64|EXC_DEFAULT|EXD_PROC_START_NSEC<br /> uint64_t proc_finish_sec;// EXT_UINT64|EXC_DEFAULT|EXD_PROC_FINISH_SEC<br /> uint64_t proc_finish_nsec;// EXT_UINT64|EXC_DEFAULT|EXD_PROC_FINISH_NSEC<br /> char  command[PRFNSZ];// EXT_STRING|EXC_DEFAULT|EXD_PROC_COMMAND<br /> uint32_t tty_major; // EXT_UINT32|EXC_DEFAULT|EXD_PROC_TTY_MAJOR<br /> uint32_t tty_minor; // EXT_UINT32|EXC_DEFAULT|EXD_PROC_TTY_MINOR<br />#define EX_SYS_NMLN 40 // SYS_NMLN = 257 - too much<br /> char  hostname[EX_SYS_NMLN]; // EXT_STRING|EXC_DEFAULT|EXD_PROC_HOSTNAME<br /> uint64_t major_faults; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_FAULTS_MAJOR<br /> uint64_t minor_faults; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_FAULTS_MINOR<br /> uint64_t messages_snd; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_MESSAGES_SND<br /> uint64_t messages_rcv; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_MESSAGES_RCV<br /> uint64_t blocks_in; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_BLOCKS_IN<br /> uint64_t blocks_out; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_BLOCKS_OUT<br /> uint64_t chars_rdwr; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CHARS_RDWR<br /> uint64_t vctx;  // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CONTEXT_VOL<br /> uint64_t ictx;  // EXT_UINT64|EXC_DEFAULT|EXD_PROC_CONTEXT_INV<br /> uint64_t signals; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_SIGNALS<br /> uint64_t swaps;  // EXT_UINT64|EXC_DEFAULT|EXD_PROC_SWAPS<br /> uint64_t syscalls; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_SYSCALLS<br /> uint32_t acct_flags; // EXT_UINT32|EXC_DEFAULT|EXD_PROC_ACCT_FLAGS<br /> uint32_t ppid;  // EXT_UINT32|EXC_DEFAULT|EXD_PROC_ANCPID<br /> uint32_t wait_status; // EXT_UINT32|EXC_DEFAULT|EXD_PROC_WAIT_STATUS<br />#define EX_ZONENAME 64<br /> char  zonename[EX_ZONENAME]; // EXT_STRING|EXC_DEFAULT|EXD_PROC_ZONENAME<br /> uint64_t mem_rss_avg_k; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_MEM_RSS_AVG_K<br /> uint64_t mem_rss_max_k; // EXT_UINT64|EXC_DEFAULT|EXD_PROC_MEM_RSS_MAX_K<br /> };<br /></pre>",
        "metadata": {
          "word_count": 340,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The exacct data file is a complex tagged object format that is read via the libexacct library routines. While generic and flexible, it is a pain to get at the data. I used this information to define a data structure that will be populated with the data from the file.  LT|EXD_GROUP_PROC\u00a0p pid, pid, taskid, gid, projid,taskid, task id, task_task,proc,proc_start,finish,sec,sys,sys_nsec,proc\u00a0finish\u00a0nsec. LT\u00a0EXD\u00a0PROC_PID, LT \u00a0EXC_DE default pid, LT\u00a0exD_Pid, EXT_UINT32, EXT\u00a0STRING, EXC\u00a0DE Default, EXD_PRoc_COMMAND. LT  LT|exD\u00a0PID; LT  EXC_P pid; LT\u00a0ExD\u00a0TASKID; #define EX_SYS_NMLN 40 // SYS_ NMLN = 257 - too much.uint32_t tty_major; // EXT_UINT32|EXC_DEFAULT|EXD_PROC_TTY_MAJOR.uint64_t messages_rcv; // Ext_Uint64|EXc_DE Default|EXd_PRoc_MESSAGES_RCV.uint34_t ictx;  // EXT-UINT64 |EXC-DEFAULT |EXD-PROC-CONTEXT_INV. #define EX_ZONENAME 64 #define EX\u00a0Zonename 64 char zonename[EX_ZonENAME]; #define\u00a0EX_STRING 64 #include\u00a0#define\u00a0XENENAME\u00a064 #include  #define XENENAMENAME 32 #include \"XENAMENA\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_48e92fec",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "myPhone will be a TuxPhone",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/myphone-will-be-tuxphone.html",
      "content": {
        "text": "There is lots of interest and activity in the <a href=\"http://www.hbmobile.org/\">Silicon Valley Homebrew Mobile Phone Club</a>  right now centered around building a phone with a large touch screen. I'm calling it \"myPhone\", and TuxPhone is the generic name for the open software and hardware components we are using. The hardware design is based on components  that are openly available with freely available spec sheets, we will make our schematics, design details and cad files freely available, and its quite likely that no two phones will be built the same way. If you want it thinner and lighter, go for fewer features, a smaller battery and tweak the cad file before making the case.<br /><br />The size is driven primarily by the screen, and we've settled on the screen that is used by the Sony PSP, with a touchscreen overlay added. Its a 4.3\" diagonal, and is widely available, low cost. We will include a standard GSM/EDGE module, so I can just pop the SIM card from my Treo into it, and we can add GPS, WiFi, Bluetooth, Accelerometer, SDcards or hard disks, whatever custom mix we feel like. The CPU and other parts will be based on <a href=\"http://www.gumstix.com/\">Gumstix modules</a>.",
        "metadata": {
          "word_count": 200,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is lots of interest and activity in the Silicon Valley Homebrew Mobile Phone Club. I'm calling it \"myPhone\", and TuxPhone is the generic name for the open software and hardware components we are using. The size is driven primarily by the screen, and we've settled on the screen that is used by the Sony PSP. The CPU and other parts will be based on Gumstix modules. tooth, Accelerometer, SDcards or hard disks, whatever custom mix we feel like."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_26c437c1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solar Power Update - Annual Costs",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/09/solar-power-update-annual-costs.html",
      "content": {
        "text": "<br />\nI've blogged before about our solar power installation and we just completed our first full year where we didn't change anything, so we have a clear baseline of how much we generated and used, which I will describe in this post.<br />\n<br />\nOur system is a grid tied net metering setup. This means that we generate more electricity than we use during the day and run the meter backwards in the summer, then use (cheaper) electricity at night and in the winter from the grid. Once a year our PG&amp;E bill is netted out and we pay the difference. Last year our total electric bill was about $500 but during the year we changed everything by adding a second solar array, a Nissan Leaf electric car and a heat pump for heating and cooling. This year, with a consistent setup for the entire period, our bill was negative, so we didn't owe PG&amp;E anything. However, this didn't mean that PG&amp;E paid us the difference, because the payback rule is based on how many KWh you generate rather than how much you would have paid for it. In our case, although our net bill was -$495, we used 610 KWh more than we generated for the whole year. The details of our PG&amp;E bill are shown at the end of this post.<br />\n<br />\nIn effect, we could have used another $495 worth of electricity for free. Going into next year, we will set the heat pump to work harder at lower temperatures before it switches over to propane, which will reduce our carbon footprint and save us some propane costs. For some of last winter we used propane when the outside temperature fell below 45F, and at some point we reduced that to 40F. Unfortunately the controller setting we have is in increments of 5F, and I'm doubtful that we can pump heat out of air at 35F, but it's worth a try. We already converted our other appliances from propane to electric, and other than the heat pump our major consumers are a large tank electric water heater, induction range, clothes dryer, hot tub and well pump.<br />\n<br />\nWe did around 11,000 miles in the Nissan Leaf in the last year, but estimate that about a third of our charging was at work, as Laurel's commute is far enough to be an easy one way run, but requires a charge to get home again up the hill (we live at 2400ft, and she works at sea level!). She can take the car pool lane with the \"white sticker\" that pure electric cars get, which saves a lot of time, but running at high speed on the freeway uses a lot more power than around town. When I get to use the Leaf my commute is much closer and I don't bother charging it at work. Our long term average consumption is 3.7 miles/KWh, which is worse than most Leaf owners because of the freeway miles, hill climbing and \"having fun\". We don't pay for charging at work, and the marginal cost of electricity at home is zero because we are generating a negative net bill for the year, so we only use the \"ECO\" driving style if we are pushing the limits of its range on an unusual trip. The Leaf is the first car we pick for drives in its range, it's entertaining to drive as well as having extremely low running costs. All it needs for maintenance is tires, there is no engine oil to change, and the brakes wear very slowly due to electric regeneration. Here's the year so far as recorded by the Leaf's \"Carwings\" system.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWg_Ggex7kSEv8WoeHjNE4XQSEqdaqEnJ2WictWEi4hEKRfeUY7YuW_mFPMUenQbpwhqw780aDs6rT9e_ZbRJlJUhWomcJczalVbDL7oHxaeIgzzN-Tgsa5X9-uRvknSmlyw9i/s1600/Screen+Shot+2012-09-02+at+12.57.19+AM.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"386\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWg_Ggex7kSEv8WoeHjNE4XQSEqdaqEnJ2WictWEi4hEKRfeUY7YuW_mFPMUenQbpwhqw780aDs6rT9e_ZbRJlJUhWomcJczalVbDL7oHxaeIgzzN-Tgsa5X9-uRvknSmlyw9i/s640/Screen+Shot+2012-09-02+at+12.57.19+AM.png\" width=\"640\" /></a></div>\n<br />\n<br />\nThe PG&amp;E tariff we are on is called E6 Net Energy Metering, it has three rates for the summer, and two rates in the winter. The rules and rates are described in these documents at the PG&amp;E web site:&nbsp;Rules&nbsp;<a href=\"http://www.pge.com/tariffs/tm2/pdf/ELEC_SCHEDS_NEM.pdf\">http://www.pge.com/tariffs/tm2/pdf/ELEC_SCHEDS_NEM.pdf</a>&nbsp;and Rates&nbsp;<a href=\"http://www.pge.com/tariffs/tm2/pdf/ELEC_SCHEDS_E-6.pdf\">http://www.pge.com/tariffs/tm2/pdf/ELEC_SCHEDS_E-6.pdf</a><br />\nThe rate that PG&amp;E will pay us for any extra KWh we generate is about 3 cents/KWh as described in this document&nbsp;<a href=\"http://www.pge.com/includes/docs/pdfs/shared/solar/AB920_RateTable.pdf\">http://www.pge.com/includes/docs/pdfs/shared/solar/AB920_RateTable.pdf</a><br />\nOur base overnight rate that we pay is around 10 cents/KWh, the afternoon rate is around 28 cents/KWh, this is why we can generate hundreds of dollars in a negative bill while using hundreds of KWh.<br />\n<br />\nOur <a href=\"http://perfcap.blogspot.com/search?q=solar&amp;max-results=20&amp;by-date=true\" target=\"_blank\">solar setup is in two arrays that I described on my blog before</a>, the total summer peak output is about 10KW, and we get up to 70KWh per day. In the winter this drops to about 40KWh per day, and clouds and rainy days greatly reduce the output. We're in a sunny spot on top of the mountains, and the monthly output from the 6.5KW array is shown in the plots below since we turned it on in April 2011. The other array adds about half this amount. Our annual true-up period runs from the beginning of September to the end of August.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnvjnWcKWceXk1GW1-OfBa4rVHXU6MMIgulfichlUdO0GTi5rEPQpNDeEmdGoSRl2o-u61NAZtCjLwRY921IEoruxeY45onAZJ_adciIIJ8fxjaEsTGmEIvYKO0xpaTFG61U0Z/s1600/Screen+Shot+2012-09-01+at+11.32.36+PM.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"235\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhnvjnWcKWceXk1GW1-OfBa4rVHXU6MMIgulfichlUdO0GTi5rEPQpNDeEmdGoSRl2o-u61NAZtCjLwRY921IEoruxeY45onAZJ_adciIIJ8fxjaEsTGmEIvYKO0xpaTFG61U0Z/s640/Screen+Shot+2012-09-01+at+11.32.36+PM.png\" width=\"640\" /></a></div>\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5NR41gd0NPqlWGtm2tgKzlSJ92x44eTY-6f0S5QN6HyqSBaSLTutWnhxjtwk7lpNsgMnkbKtMOjvl-XcBxk7ag7hivz9UFt-d4K8T2Zje_STsYJnKT_j3Q4uy1xtiIz8i5GEt/s1600/Screen+Shot+2012-09-01+at+11.32.09+PM.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"235\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj5NR41gd0NPqlWGtm2tgKzlSJ92x44eTY-6f0S5QN6HyqSBaSLTutWnhxjtwk7lpNsgMnkbKtMOjvl-XcBxk7ag7hivz9UFt-d4K8T2Zje_STsYJnKT_j3Q4uy1xtiIz8i5GEt/s640/Screen+Shot+2012-09-01+at+11.32.09+PM.png\" width=\"640\" /></a></div>\n<br />\nPG&amp;E send us a bill every month for about $11 of connection fees and taxes, and the current running total. Our final bill for the last year is shown below. <a href=\"http://perfcap.blogspot.com/2011/02/solar-power-more-panels-on-garage-roof.html\" target=\"_blank\">Solar costs were described in a previous post</a>.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n</div>\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTl1tQkXnpDYkRN5DnhEhJ5r3l1MFl0apUgyMg1qM67BOQBFdUFgLsY4Zewey3Tv0NfyCy_vcQwUE9HNaOPZBDUqz7p_dq_98p9LWyiii7yB-xxRC_cLCWIRxOY4wKQRqXdEjp/s1600/pge2012-1small.JPEG\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"640\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiTl1tQkXnpDYkRN5DnhEhJ5r3l1MFl0apUgyMg1qM67BOQBFdUFgLsY4Zewey3Tv0NfyCy_vcQwUE9HNaOPZBDUqz7p_dq_98p9LWyiii7yB-xxRC_cLCWIRxOY4wKQRqXdEjp/s640/pge2012-1small.JPEG\" width=\"600\" /></a></div>\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgehqxKQk2ITSpjh9BGwk7KV_dUQ1IfU4qbDthp06yleTpARB-Tkct5HNuO6sU6EaRdpFgWEeHg2iAvR3xIvZOjU-XI2qh1b3sbXtXmG0AJdluobgczPhWAJ2JdDqrLV7wMGITr/s1600/pge2012-2small.jpg\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"640\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgehqxKQk2ITSpjh9BGwk7KV_dUQ1IfU4qbDthp06yleTpARB-Tkct5HNuO6sU6EaRdpFgWEeHg2iAvR3xIvZOjU-XI2qh1b3sbXtXmG0AJdluobgczPhWAJ2JdDqrLV7wMGITr/s640/pge2012-2small.jpg\" width=\"469\" /></a></div>\n<br />\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 994,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Our system is a grid tied net metering setup. This means that we generate more electricity than we use during the day. Once a year our PG&amp;E bill is netted out and we pay the difference. This year, with a consistent setup for the entire period, our bill was negative. Although our net bill was -$495, we used 610 KWh more than we generated for the whole year. The details of our PG&amp;E bill are shown at the end of this post. Going into next year, we will set the heat pump to work harder at lower temperatures. The Leaf is the first car we pick for drives in its range. It's entertaining to drive as well as having a great range. We don't pay for charging at work, and the marginal cost of electricity at home is zero. All it needs for maintenance is tires, there is no engine oil to change, and the brakes wear very slowly due to electric regeneration. Here's the year so far as recorded by the Leaf's \"Carwings\" system. The rules and rates are described in these documents at the PG&amp;E web site. e summer, and two rates in the winter. The rate that PG&amps;E will pay us for any extra KWh we generate is about 3 cents/KWh. The total summer peak output is about 10KW, and we get up to 70KWh per day in the winter. We're in a sunny spot on top of the mountains, and the monthly output from the 6.5KW array is shown in the plots below. PG&amp;E send us a bill every month for about $11 of connection fees and taxes. Our final bill for the last yea is $11. Solar costs were described in a previous post. r is shown below. The cost of a solar panel on a garage roof can be as high as $10,000. <div style=\"margin-left: 1em; margin-right: 1am;\" imageanchor=\"1\" caption=\"http://www.pge2012-2small.com/blog/2012/02/12/pge-12-2-small.html#storylink=cpy\" id=\"pge12\"title=\"Pge 2012-2 small.html\" width=\"469\" height=\"640\" border-gravity=\"0\" margin-left=\"1em\" font-size=\"12.5em; font-family: \"Arial, Comic Sans, Retina, Asciutto, Gif, PNG, Wacom, JPG, WML,"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_bfbe89d0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Solid State Disks - time to give up that iron oxide habit",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/12/solid-state-disks-time-to-give-up-that.html",
      "content": {
        "text": "An interesting note at <a href=\"http://www.theregister.co.uk/2008/12/19/sun_micron_extended_flash_life/\">http://www.theregister.co.uk/2008/12/19/sun_micron_extended_flash_life/</a> talks about a 2009 technology that gives flash a million write cycles at the NAND cell level. This adds two orders of magnitude to the \"typical\" cycle lifetime that is often used as an argument that flash isn't ready for prime time.<br /><br />In any case this applies to each block on the SSD, and blocks are remapped using write leveling, and writes are cancelled using RAM based SSD controllers, and when the limit is reached, only that block becomes unusable... Some people seem to think that as soon as you do 10,000 writes to an SSD it fails totally, like a head crash on a disk.<br /><br />I also see that at least one vendor has announced a 512GB SSD in a 2.5\" laptop disk format package.<br /><br />So to reiterate something I've been saying for a long time, spinning rust is dead, and a large number of basic assumptions about how computers behave and the best way to architect them are now wrong. In 2009 SSD's will be faster for read, faster for write, faster for sequential and much much faster for random access, more reliable, more durable, lower power, higher capacity, than discs. Give it another year or so and they will be cheaper as well.<br /><br />SAN's are now a complete waste of time. There is so much reliable I/O performance available in a single drive, that it makes much more sense to put SSD's in the systems and access them directly. Accessing an SSD over a SAN adds a huge latency and cost overhead. It makes much more sense to use node-to-node replication for critical data (log scraping databases or cluster filesystems).",
        "metadata": {
          "word_count": 283,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A 2009 technology gives flash a million write cycles at the NAND cell level. This adds two orders of magnitude to the \"typical\" cycle lifetime. At least one vendor has announced a 512GB SSD in a 2.5\" laptop disk format package. In 2009 SSD's will be faster for read and write, faster for sequential and much much faster for random access. Give it another year or so and they will be cheaper as well. SAN's are now a complete waste of time."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_6a9e9af0",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "FREE this weekend, iPhone Instant Queue Add for Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/10/free-this-weekend-iphone-instant-queue.html",
      "content": {
        "text": "The latest version 5 of IQ+ was released this week, and it is currently Free until Monday morning.<br /><br />You get your queue, recently watched, and up to 100 movies per list for new releases, new TV, the 12 top level genres, movies you'll love, and seven highly personalized sub-genre and similars lists. All the lists are personally optimized for you by Netflix and will be the same as the lists currently shown on Xbox 360 or Media Center if you have one. <br /><br />It's a free update for existing users (the app has been out for over a year now) and normally 99c for new users. The app also provides searching the instant catalog and sorting the lists by title, rating, maturity, ending soon etc. Sorting the queue doesn't (yet) change it at Netflix, but helps you find titles that you can then pop to the front or back of the queue with one click. <br /><br /><a href=\"http://itunes.apple.com/WebObjects/MZStore.woa/wa/viewSoftware?id=293419823&mt=8 \">http://itunes.apple.com/</a><br /><br />Please review/rate the latest release. So far thousands of existing users have upgraded, iPhone OS 3.0 or later is required.",
        "metadata": {
          "word_count": 183,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "IQ+ is free for existing users and normally 99c for new users. You get your queue, recently watched, and up to 100 movies per list for new releases. iPhone OS 3.0 or later is required. So far thousands of existing users have upgraded, iPhone OS 4.0 is expected to be released soon."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_1c0c5cdd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Jobs at Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/04/jobs-at-netflix.html",
      "content": {
        "text": "A few people have asked me if there are interesting jobs available at Netflix. I haven't started there yet, but they do seem to be looking for senior developers who have a clue about performance.<br /><br />Here are some job descriptions, let me know if you apply to them...<br /><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://jobs.netflix.com/DetailFlix.asp?flix1449\" target=\"_blank\">http://jobs.netflix.com<wbr>/DetailFlix.asp?flix1449</a><br /><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://jobs.netflix.com/DetailFlix.asp?flix1576\" target=\"_blank\">http://jobs.netflix.com<wbr>/DetailFlix.asp?flix1576</a><br /><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://jobs.netflix.com/DetailFlix.asp?flix1440\" target=\"_blank\">http://jobs.netflix.com<wbr>/DetailFlix.asp?flix1440</a>",
        "metadata": {
          "word_count": 67,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A few people have asked me if there are interesting jobs available at Netflix. I haven't started there yet, but they do seem to be looking for senior developers who have a clue about performance."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_a765464a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Framing - first day of construction",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/garage-framing-first-day-of.html",
      "content": {
        "text": "Tuffshed turned up with their pre-built sections, assembled them and added some siding and the main door frame. Another stop-motion video on YouTube. It was wet over the weekend, and the ground is very soft, but we have a few dry and warm days in a row forecast, so I'm hoping it will dry out now.<br /><br /><object width=\"420\" height=\"320\"><param name=\"movie\" value=\"http://www.youtube.com/v/h3MmG6ixkSg&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/h3MmG6ixkSg&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"320\"></embed></object>",
        "metadata": {
          "word_count": 72,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Tuffshed turned up with their pre-built sections, assembled them and added some siding and the main door frame. It was wet over the weekend, and the ground is very soft, but we have a few dry and warm days in a row forecast."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_6c815eed",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Usenix 09 - San Diego in June",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/05/usenix-09-san-diego-in-june.html",
      "content": {
        "text": "I'm presenting again next month, <a href=\"http://www.usenix.org/events/usenix09/\">at Usenix 09 in San Diego</a>. I'm giving an all day workshop/tutorial on Tuesday June 16th. I have just updated my slides, combining the talks I have been developing over the years into Solaris/Linux Performance Tools and Tuning, which includes the Capacity Planning with Free Tools material that Mario Jauvin and I developed for CMG.<br /><br /><a href=\"http://www.usenix.org/usenix09/promote\"> <img src=\"http://www.usenix.org/events/usenix09/art/usenix09_button.jpg\" border=\"0\" width=\"125\" height=\"125\" alt=\"USENIX '09\"> </a><br /><br />I have also posted the slides to <a href=\"http://www.slideshare.net/adrianco\">http://www.slideshare.net/adrianco<br /></a>.<br /><br />Please come along, I look forward to seeing you in San Diego.<br /><br />Cheers Adrian",
        "metadata": {
          "word_count": 99,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Adrian is presenting again next month, at Usenix 09 in San Diego. I have just updated my slides, combining the talks I have been developing over the years into Solaris/Linux Performance Tools and Tuning."
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_692e9259",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Where should I start...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2004/06/where-should-i-start.html",
      "content": {
        "text": "I googled [\"Web services\" performance] and got a bunch of nice hits\n<br />\n<br /><a href=http://wspi.ca.com/>CA's product</a> is interesting, lets you track the actual performance of public web services. CA seem to be using WSDM to do this.\n<br />I'm more interested in characterizing the attributes of web service performance that would lead to capacity planning methods for web services infrastructure.\n<br />\n<br />As I find more about what is the actual state of the art, I'll list links to it here, then start synthesizing and extending off into uncharted territory.\n<br />\n<br />I think I'm down to do a Grid/Web Services tutorial at the <a href=http://www.cmg.org>CMG04</a> meeting in Vegas in December so this is a place for me to collect and discuss some of the content in blog form.",
        "metadata": {
          "word_count": 133,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm more interested in characterizing the attributes of web service performance that would lead to capacity planning methods for web services infrastructure. I think I'm down to do a Grid/Web Services tutorial at the <a href=http://www.cmg.org>CMG04 meeting in Vegas in December."
      },
      "tags": [
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_6379cfb1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "AMD Enhanced Power Now - Variable Cores and Clock Rates",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/amd-enhanced-power-now-variable-cores.html",
      "content": {
        "text": "There is an interesting <a href=\"http://www.theregister.co.uk/2007/02/11/amd_enhanced_powernow/\">article in The Register about the latest variant of AMD's enterprise power management system</a>.<br /><br />As <a href=\"http://perfcap.blogspot.com/2006/06/cpu-power-management.html\">I've mentioned before</a>,  in the interests of saving power, some enterprise server systems are varying their clock rates so that they end up showing a higher utilization at low load levels that you would expect. This non-linear relationship of load to utilization is one of the things I highlighted in my CMG06 paper called \"Utilization is virtually useless as a metric\".<br /><br />The latest twist: in AMD's upcoming four core systems, individual cores will be stopped completely if there isn't enough work for them to do.<br /><br />The effect on utilization metrics will depend upon how each operating system interacts with the power management capabilities...<br /><br />For Solaris the so-called \"idle loop\" is actually quite busy. An idle CPU watches its neighbors to see if they have too many jobs on their run queues, and gets work by migrating processes that won't get to run soon to itself. Interrupts are also bound to individual CPUs, so that data structures don't have to migrate between caches at high interrupt rates.<br /><br />It will be interesting to see how these technologies interact.<br /><br />Adrian",
        "metadata": {
          "word_count": 204,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "In AMD's upcoming four core systems, individual cores will be stopped completely if there isn't enough work for them to do. The effect on utilization metrics will depend upon how each operating system interacts with the power management capabilities.  idle CPU watches its neighbors to see if they have too many jobs on their run queues, and gets work by migrating processes that won't get to run soon to itself. Interrupts are also bound to individual CPUs, so that data structures don't have to migrate between caches at high interrupt rates."
      },
      "tags": [
        "metrics"
      ]
    },
    {
      "id": "virtual_adrianco_file_84a94e1f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "New tracks from Fractal are up at bebo.com",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/12/new-tracks-from-fractal-are-up-at.html",
      "content": {
        "text": "<div style=\"overflow: hidden; background-image: url(http://s.bebo.com/img/bebobadge_1_black.gif); background-repeat: no-repeat; width: 300px; height: 175px;\"><span style=\"width:90px; height:90px;\"><a href=\"http://www.bebo.com/badge/1636562602\"><img src=\"http://www.bebo.com/profilephoto/1636562602a581951117bblack.jpg\" style=\"border: 0pt none ; margin: 10px 0pt 27px 10px; width: 90px; height: 90px;\"></a></span><br><a href=\"http://www.bebo.com/badge/1636562602\" style=\"padding: 0pt 0pt 0pt 10px; overflow: hidden; display: block; color: white; font-weight: bold; font-size: 16px; width: 300px;\">Fractal</a><a href=\"http://www.bebo.com/badge/1636562602\" style=\"padding: 0pt 0pt 0pt 10px; display: block; color: white; font-weight: bold; font-size: 12px;\">http://www.bebo.com/Fractal</a><br></div>",
        "metadata": {
          "word_count": 60,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "<div style=\"overflow: hidden; background-image: url(http://s.bebo.com/img/bebobadge_1_black.gif); background-repeat: no-repeat; width: 300px; height: 175px;\"><span style=\"width:90px; Height:90 pixels;\" style=\"padding: 0pt 0pt0pt 10px; overflow: hidden\"; color: white;font-weight: bold; font-size: 16px; width : 300px;\" >Fractal."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b9625f34",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage - Inspected and delayed",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-inspected-and-delayed.html",
      "content": {
        "text": "We passed inspection by the county this morning, but the weather has turned too bad to pour concrete tomorrow, so we are postponing until after Wednesday's storm has passed, and will resume work on Friday. Today the electrician finished his work on the pipe that carries the power, and the Tuffshed foreman Mike visited to inspect the site and check and adjust the position of the hold-down bolts that are set into the foundation.",
        "metadata": {
          "word_count": 74,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Weather too bad to pour concrete tomorrow, so we are postponing until after Wednesday's storm has passed. Today the electrician finished his work on the pipe that carries the power, and the Tuffshed foreman Mike visited to inspect the site."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ac66b754",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Template Update",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/template-update.html",
      "content": {
        "text": "I just realised that my blog wasn't displaying properly in Internet Explorer. I couldn't figure out why, so I upgraded to a fresh new Blogger.com template that has a lot more configurability, and dropped the ads etc back into it. The formatting is still a bit off, and it looks better in Flock/FireFox but at least the basic blocks are all in the right place now.<br /><br />The archive of old posts is also easier to navigate with the new template.",
        "metadata": {
          "word_count": 81,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The archive of old posts is also easier to navigate with the new template. The formatting is still a bit off, and it looks better in Flock/FireFox."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2617527d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's a Distinguished Engineer?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/10/whats-distinguished-engineer.html",
      "content": {
        "text": "A <a href=\"http://www.kitchensoap.com/2012/10/25/on-being-a-senior-engineer/\" target=\"_blank\">recent post by John Allspaw on what it means to be a senior engineer</a> reminded me of something I put together years ago while I was a Distinguished Engineer at Sun. One question from senior engineers looking at their career path was what did it take to become a Distinguished Engineer?<br />\n<br />\nAlthough Sun is no more, across the industry, there are engineers who are \"distinguished\" and the title is used in a few places. At Sun, there were between 50 and 100 people in the role, who were mostly director level individual contributors, although there were also Sun Fellows who were VP level, and some were also line managers.<br />\n<br />\nI boiled it down into a few questions.<br />\n<br />\nFirst I made a list of the names of all the Sun Distinguished Engineers and Fellows, and the first question was \"how many of these names do you recognize, and know what they did\". The intent is to get a baseline level of understanding of what might be expected. The list included people who invented software languages and frameworks that lots of people use, microprocessor architects, and fundamental researchers in security and networking. There were also CTOs of companies that Sun had acquired, and a few like me who mostly got in through writing books that everyone else had read.<br />\n<br />\nThe next question is \"how many of these people know who you are?\". If you think you did do something special, we would expect that the existing Distinguished Engineers would have heard of it. Since at Sun the way to become a DE involved having the existing DE and Fellows vote for you, this was critical.<br />\n<br />\nThe final question was \"how many DE and Fellows are hanging around your cube on a regular basis waiting to talk to you?\". This shows that you are the go-to person for something that matters.<br />\n<br />\nTranslating this into a broader context, more current questions for being distinguished might be \"Do the top conferences invite you to speak?\", \"How many of the other invited speakers and conference organizers do you know?\" and \"how many know you?\". The other dimension of what you did to deserve it is nowadays a mixture of open source projects that lots of people use, or key ideas shared through books or blogs.<br />\n<br />\nHere's the original slide from 2002, how many of these names do you know, what did they do then and where are they now?<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<a href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikKv3AE85jCqyeY9RvZYteLwpzpDwIVVxgYhP-eDAE5QjezHxeRMT1dhXm1v12cbYmpd9bwBJlUSKno-mPrWwMpCa5r2VC3Za0kM142eoSai4lhC0Ih3s_4udyBUKUXEP9wDm9/s1600/SunDEs.png\" imageanchor=\"1\" style=\"margin-left: 1em; margin-right: 1em;\"><img border=\"0\" height=\"214\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEikKv3AE85jCqyeY9RvZYteLwpzpDwIVVxgYhP-eDAE5QjezHxeRMT1dhXm1v12cbYmpd9bwBJlUSKno-mPrWwMpCa5r2VC3Za0kM142eoSai4lhC0Ih3s_4udyBUKUXEP9wDm9/s320/SunDEs.png\" width=\"320\" /></a></div>\n<br />\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 456,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "A recent post by John Allspaw on what it means to be a senior engineer reminded me of something I put together years ago while I was a Distinguished Engineer at Sun. At Sun, there were between 50 and 100 people in the role, who were mostly director level individual contributors, although there were also Sun Fellows who were VP level. At Sun the way to become a DE involved having the existing DE and Fellows vote for you. This shows that you are the go-to person for something that matters. \"How many of the other invited speakers and conference organizers do you know?\" and \"how many know you?\". The other dimension of what you did to deserve it is nowadays a mixture of open source projects that lots of people use, or key ideas shared through books or blogs. <div style=\"width:320\";. h=\"320\" /></a></div> \u00a0\u201cCurious\u2019\u201d: \u201cRevealed\u2019:\u00a0\u201d. \u201cPhenomenon\u2019s\u201d \u201cState of the World\u2019%: \u201cThe State of The World\u201d : \u201cA State Of The World,\u201d\u00a0\u2009\u201cThe World of the Day\u2019\": \u201cThis Day in the World, November 11, 2013, is the Day of the Truth.\u201c: \u201d: The Day of The Truth, November 9, is a day of the truth.\u2009: The Truth about the World.\ufffd"
      },
      "tags": [
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_file_df37da5f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Picture of myPhone",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/03/picture-of-myphone.html",
      "content": {
        "text": "Here are the CAD pictures of the phone, I made the rear case cover translucent so that some of the parts inside can be seen. The big dark block is a Treo650 battery, the block in front of it is the Telit 862 GSM/GPS module that does all the phone stuff... Its 130mm long and 75mm wide, quite big, but thats a 3.7\" 480x640 LCD, and its easier to fit off the shelf parts inside :-)<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAZfYripvd8hziu96UYOrEAfgCjnW-Jp-ESQQFQmi5yQFZwO5msXbcQrMGSgmmbvzdcn2JzzkKp2EGbes6IP0RiPfYBIMa5GMtNzmKpCx0OsKdTBgcsEhlnQOUAbHycR6AWUty/s1600-h/Case37+Assembly+from+Front.jpg\"><img style=\"float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgAZfYripvd8hziu96UYOrEAfgCjnW-Jp-ESQQFQmi5yQFZwO5msXbcQrMGSgmmbvzdcn2JzzkKp2EGbes6IP0RiPfYBIMa5GMtNzmKpCx0OsKdTBgcsEhlnQOUAbHycR6AWUty/s400/Case37+Assembly+from+Front.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5036875956734999858\" /></a><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjVH8x72W0HprbdtGbhL5cm6XtVcKh63FAgUqsh9bEbwlIeqx3fFBxBiu0A7UWTk78vAGLew9mp1ThNYmHiuuyvAOIxFTScNXOYdsaBo7yXRRekGOiSFFkskTAEr3OBEWABclG/s1600-h/Case37+Assembly+From+Rear.jpg\"><img style=\"float:left; margin:0 10px 10px 0;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjVH8x72W0HprbdtGbhL5cm6XtVcKh63FAgUqsh9bEbwlIeqx3fFBxBiu0A7UWTk78vAGLew9mp1ThNYmHiuuyvAOIxFTScNXOYdsaBo7yXRRekGOiSFFkskTAEr3OBEWABclG/s400/Case37+Assembly+From+Rear.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5036876119943757122\" /></a>",
        "metadata": {
          "word_count": 110,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Here are the CAD pictures of the phone, I made the rear case cover translucent so that some of the parts inside can be seen. The big dark block is a Treo650 battery, the block in front of it is the Telit 862 GSM/GPS module that does all the phone stuff... Its 130mm long and 75mm wide, quite big, but thats a 3.7\" 480x640 LCD, and its easier to fit off the shelf parts inside :-) <a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhjVH8x72W0HprbdtGbhL5cm6XtVcKh63FAgUqsh9bEbwlIeqx3fFBxBiu0A7UWTk78vAGLew9mp1ThNYmHiuuyvAOIxFTScNXOYdsaBo7yXRRekGOiSFFkskTAEr3OBEW"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9dbaf4bd",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "WiFi upgrade",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/wifi-upgrade.html",
      "content": {
        "text": "My five year old wifi hardware got flaky and slow, so I looked around and decided to go with the D-Link DIR-825 as my central node and a DAP-1522 to bridge to my TV etc.<br /><br />They are both N spec dual frequency systems. Arrived today from Amazon for $225 for the pair.<div><br /></div><div>[Update: they are working fine, I have a 130Mbit/s link speed reported on the 5GHz link between the DIR-825 and the DAP-1522, and I've shut down all the old WiFi networks].<div class=\"iblogger-footer\"> <p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br /></div><br /></div>",
        "metadata": {
          "word_count": 95,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "My five year old wifi hardware got flaky and slow, so I looked around and decided to go with the D-Link DIR-825 as my central node. Arrived today from Amazon for $225 for the pair."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_404b3779",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cockcroft Headroom Plot - Part 2 - R Version",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/cockcroft-headroom-plot-part-2-r.html",
      "content": {
        "text": "I kept tweaking the code, and came up with a prettier version, that also has a small time series view of the throughput in the top right corner.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/207207/chp.gif\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/212741/chp.png\" border=\"0\" alt=\"\" /></a><br /><br />The code for this is<br /><pre><br />chp <- function(x,y,xl=\"Throughput\",yl=\"Response\",tl=\"Throughput Time Series\", ml=\"Cockcroft Headroom Plot\") {<br />       xhist <- hist(x,plot=FALSE)<br />       yhist <- hist(y, plot=FALSE)<br />       xrange <- c(0,max(x))<br />       yrange <- c(0,max(y))<br />       nf <- layout(matrix(c(2,4,1,3),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)<br />       layout.show(nf)<br />       par(mar=c(5,4,0,0))<br />       plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl)<br />       par(mar=c(0,4,3,0))<br />       barplot(xhist$counts, axes=FALSE, ylim=c(0, max(xhist$counts)), space=0, main=ml)<br />       par(mar=c(5,0,0,1))<br />       barplot(yhist$counts, axes=FALSE, xlim=c(0,max(yhist$counts)), space=0, horiz=TRUE)<br />       par(mar=c(2.5,1.5,3,1))<br />       plot(x, main=tl, cex.axis=0.8, cex.main=0.8, type=\"S\")<br />}<br /></pre><br /><br />I also made a wrapper function that steps through the data over time in chunks.<br /><pre><br />> chp.step <- function(x, y, steps=10, secs=1.0) {<br />    xl <- length(x)<br />    step <- xl/steps<br />    for(n in 0:(steps-1)) {<br />        Sys.sleep(secs)<br />        chp(x[(1+n*step):min((n+1)*step,xl)],y[(1+n*step):min((n+1)*step,xl)])<br />    }<br />}<br /></pre><br /><br />To run this smoothly on windows, I had to disable double buffering using<br /><pre><br />> options(\"windowsBuffered\"=FALSE)<br /></pre><br />and close the graphics window so that a new one opens with the new option.<br /><br />The data is displayed using the same calls as described in Part 1. The next step is to try some different data sets and work on detecting saturation automatically.",
        "metadata": {
          "word_count": 232,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The code for this is the same as the one for the throughput chart. I kept tweaking the code, and came up with a prettier version, that also has a small time series view of the throughput in the top right. I also made a wrapper function that steps through the data over time in chunks. To run this smoothly on windows, I had to disable double buffering using options \"windowsBuffered\" . calls as described in Part 1. The next step is to try some different data sets and work on detecting saturation automatically. For more information on the project, visit:\u00a0http://www.cnn.com/2013/01/29/cnn-cnn\u00a0cnncino-saturation-sensor.html."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ec053fe7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Site - rebar ready",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-site-rebar-reader.html",
      "content": {
        "text": "Another stop motion video at 30s intervals shows most of the day's work. The tree that was falling over was pushed back the other way and cut up, and they added the parking \"apron\" in front of the garage.<br /><br />There is a 4ft wide walkway along the side of the garage by the house, so the main pad is 36 by 36ft. It slopes to the right by three inches over that width so it will drain. The walkway slopes away from the garage by one inch. The apron is 26ft deep, 36ft wide and slopes by 12 inches (4%). The driveway will connect to the side of the apron that is nearest the house.<br /><br /><object width=\"420\" height=\"255\"><param name=\"movie\" value=\"http://www.youtube.com/v/3EqiVgEcbsw&hl=en_US&fs=1&\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/3EqiVgEcbsw&hl=en_US&fs=1&\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"255\"></embed></object>",
        "metadata": {
          "word_count": 132,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a 4ft wide walkway along the side of the garage by the house, so the main pad is 36 by 36ft. The apron is 26ft deep, 36ft wide and slopes by 12 inches (4%)."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_986e9529",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Orb Swarm Rolling Robot Video at Maker Faire",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/05/rolling-robot-video-at-maker-faire.html",
      "content": {
        "text": "Update: this is part of the <a href=\"http://orbswarm.com/\">Orb Swarm</a> project. They intend to build several orbs and take them to events like Burning Man.<br /><br /><embed src=\"http://update.videoegg.com/flash/proxy.swf?jsver=1.4\" FlashVars=\"jsver=1.4&allowFlash9Fullscreen=true&MMdoctitle=Adrian Cockcroft <adriancockcroft> - Flash Player Installation&MMplayerType=PlugIn&MMredirectURL=http://www.bebo.com/Profile.jsp?MemberId=1636427129&adVars=site=bebo&area=userhomepages&vl=us&va=46&vg=m&pa=46&pg=m&channel=art&wmode=window&autoPlay=false&file=http://download.videoegg.com/gid329/cid1124/OT/72/11796405154C3o6johu9lA9itsurWb&rootUrl=http://update.videoegg.com/flash/player&swfpath=http://update.videoegg.com/flash/proxy.swf?jsver=1.4\" quality=\"high\" allowFullScreen=\"true\" allowScriptAccess=\"always\" scale=\"noscale\" wmode=\"window\" width=\"355\" height=\"299\" name=\"VE_Player\" align=\"middle\" type=\"application/x-shockwave-flash\" pluginspage=\"http://www.macromedia.com/go/getflashplayer\"></embed>",
        "metadata": {
          "word_count": 45,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This is part of the <a href=\"http://orbswarm.com/\">Orb Swarm</a> project. They intend to build several orbs and take them to events like Burning Man."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_7f394209",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Ads that don't make sense",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/06/ads-that-dont-make-sense.html",
      "content": {
        "text": "For some reason the ad bar on this site has a fixation about the b-word. I just removed the word bl*g from the template in two places, and I'm avoiding saying it in this posting. I really think Google should be able to find something more interesting to show ads about, so lets see if this makes any difference....",
        "metadata": {
          "word_count": 59,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "For some reason the ad bar on this site has a fixation about the b-word. I just removed the word bl*g from the template in two places. I really think Google should be able to find something more interesting to show ads about."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9cba7c5f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Grid overview presentation at ACM",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/02/grid-overview-presentation-at-acm.html",
      "content": {
        "text": "<p class=\"mobile-post\">I gave <a href=http://www.sfbayacm.org/events/2004-10-20.html>this talk</a> at the SF Bay ACM meeting last October</p><br /><p class=\"mobile-post\">They <a href=http://www.sfbayacm.org/events/slides/2004-10-20_Grid-ACM.pdf>posted the slides</a>, which were the first part of my half day<br />tutorial at CMG04 in December. The talk went well, was well attended and I caught up with some old friends afterwards.<br /></p>",
        "metadata": {
          "word_count": 51,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I gave this talk at the SF Bay ACM meeting last October. The talk went well, was well attended and I caught up with some old friends afterwards."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3e87ad79",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Twitter fodder",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/twitter-fodder.html",
      "content": {
        "text": "I just used twitterfeed.com to tweet when I blog (infrequently).",
        "metadata": {
          "word_count": 10,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_37e072f5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "garage - roof and rain on Tuesday",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/garage-roof-and-rain-on-tuesday.html",
      "content": {
        "text": "the roof trusses will be delivered tomorrow and rain is forecast for Tuesday and Wednesday - so we will see how they deal with that... it sucks that we don't have a roof before the rain. <div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div>",
        "metadata": {
          "word_count": 46,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Rain forecast for Tuesday and Wednesday - so we will see how they deal with that... it sucks that we don't have a roof before the rain. the roof trusses will be delivered tomorrow."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_30112853",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What happened to the iPhone, and what comes next...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/09/what-happened-to-iphone-and-what-comes.html",
      "content": {
        "text": "<div><br /></div><div>After the launch event last January, there was a lot of repetitive and shortsighted commentary about the iPhone, and <a href=\"http://perfcap.blogspot.com/2007/01/what-iphone-doesnt-do-yet-and-thoughts.html\">I wrote a blog post about the way I thought Apple's strategy made sens</a>e and what the next few moves would be.</div><div><br /></div><div><br /></div><div>I think I got it broadly right, their strategy did make sense, and still makes sense to me. The iPhone was launched with features designed for the consumer marketplace, and redefined the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_0\">smartphone</span> category, ignoring the pundits who said it couldn't succeed as a <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_1\">smartphone</span> without a business oriented feature set, it outsold all the other <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_2\">smartphones</span> by an order of magnitude, and the competition is now scrambling to come up with products that bear any kind of comparison with the iPhone features and ease of use. The hardware is relatively easy to duplicate, but the software is years ahead of anything else.</div><div><br /></div><div><br /></div><div>One prediction I made was that Apple would release an <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_3\">iPod</span> version this summer, and that just came true, with the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_4\">iPod</span> Touch. The hardware omits the phone features, adds more flash memory and uses a slightly smaller form factor, but it completely leverages the software investment which is the most important part of the iPhone. This <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_5\">WiFi</span> based <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_6\">iPod</span> is also completely internationalized, which is its primary software advance on the iPhone. It's also clear to me why the software updates for the iPhone haven't delivered much yet, the team was working hard on the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_7\">iPod</span> Touch, and it takes a lot more work than you may expect to internationalize and test a worldwide product. This foundation also sets up the iPhone for its own worldwide <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_8\">rollout</span>. The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_9\">iPod</span> Touch will create clear consumer demand for the iPhone around the world that will get better deals for Apple as it works its way around the world's <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_10\">Telcos</span>.</div><div><br /></div><div><br /></div><div>The <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_11\">iPod</span> Touch omits Google Maps, Stock Quotes etc, which are USA market specific. I expect that when you sync an <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_12\">iPod</span> Touch in a specific country Apple will install whatever unique applications are available in that country, so US consumers could get a quick upgrade to match the iPhone application set. The Mail application is also missing, while it could be provided in a generic form, the iPhone version is setup to make it easy to connect to US based mail providers, so it could be part of a localized application bundle.</div><div><br /></div><div><br /></div><div>Another prediction I made was that a 3G iPhone would follow for the European market. The cut in price of the original iPhone creates an empty price-point at $599, which could be filled by a 3G capable iPhone with 16GB of flash and possibly GPS. Some commentators have suggested that this could be announced at Apple Expo in Paris on Sept 25<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_13\">th</span>, and I think that makes sense. The 3G phone could be thicker to accommodate a larger battery, in the same way that the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_14\">iPod</span> models vary thickness in the product range.</div><div><br /></div><div><br /></div><div>The iPhone is a *platform* which is in its infancy. I'm looking forward to a lot of software upgrades which will broaden its market appeal. As an unfinished product, it also makes complete sense to me that there is no encouragement for developers. For many platform based products (from Windows to eBay) the developers go into business to extend the feature set, but the platform vendor ends up competing with their developer community as they bundle more features into the platform. It makes sense for Apple to release the bulk of the applications that they are working on in-house and do deals behind closed doors with major vendors such as Google (and I suspect Adobe and even Microsoft), where they can build a coherent set of features into a <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_15\">roadmap</span> that completes the basic platform. *Then* open up the platform to the developer community to innovate from a much richer baseline.</div><div><br /></div><div><br /></div><div>The current hacker based developer community knows it is living on the edge, so expects to be disrupted each time Apple releases something. I'm sure they will hack the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_16\">iPod</span> Touch within a few days, but the key for Apple is to carry this initial enthusiasm into a real developer program, while keeping the number of hacked systems to a minimum. One way that Apple could make life difficult for the hackers would be to release software updates very frequently.</div><div><br /></div><div><br /></div><div>Some more predictions - what would a completed product look like?</div><div><br /></div><div><br /></div><div>1) Flash and <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_17\">Silverlight</span> - I'm sure that Adobe has come under intense pressure from all sides to get Flash onto the iPhone. It's a matter of time. We are also seeing Microsoft take on Adobe with <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_18\">Silverlight</span>, and if Microsoft could establish <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_19\">Silverlight</span> as a viable platform on the iPhone, then they would also be in a better position for Windows Mobile platforms and any future <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_20\">Zune</span> based iPhone competitors. Either way, this would provide a much more open way (compared to the bundled <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_21\">YouTube</span> application) to stream animation and video to the iPhone and <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_22\">iPod</span> Touch from any web site.</div><div><br /></div><div><br /></div><div>2) Camera - the current iPhone camera looks barely finished. The viewfinder ripples if you move, has few features and takes a long time to grab a picture. Its clearly a software based camera, and the ARM based hardware used in the iPhone usually contains a lot of accelerators for image and video capture and compression which do not appear to be in use in the current version. My guess is that a future software upgrade will move to a hardware accelerated camera with video capture support and features like digital zoom.</div><div><br /></div><div><br /></div><div>3) Instant Messaging/<span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_23\">VOIP</span>/Video - current solutions use a web page or hacked application to do call-back to the iPhone over the regular phone network. This requires centralized hardware resources that cost real money. The free way to do chat, voice and video over the network is on a true peer to peer basis, and I expect Apple to produce <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_24\">iChat</span> for the iPhone, and eventually perhaps <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_25\">iChat</span> AV to include video. A call from an Apple laptop or desktop system with a camera could show up as video on any iPhone, but a new iPhone model with a forward facing camera would be needed to act as a true portable two-way video phone. I also suspect that a faster CPU would be needed so this is likely to be a longer term prospect. It should also be possible for developers to port other messaging applications such as <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_26\">Skype</span>.</div><div><br /></div><div><br /></div><div>4) Business Applications - the most requested being Microsoft <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_27\">OutLook</span>/Exchange support at the same level as we have come to expect from RIM Blackberry's. I don't think full integration will come directly from Apple, but there is likely to be a food fight amongst third party developers to win this market. Business orientated <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_28\">smartphones</span> are going to suffer.\u00a0The main casualty will likely be the Palm <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_29\">Treo</span> in the short term, and the RIM Blackberry and the Windows Mobile based systems in the longer term. I doubt that any of these players will be able to get their platform software into the same space as Apple fast enough to hold onto this market. If RIM is smart, they will do an iPhone port of their application and hold onto their dominant position with the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_30\">backend</span> infrastructure for corporate messaging, while gradually losing ground to <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_31\">iPhones</span> in the handheld device space.</div><div><br /></div><div><br /></div><div>In January I said I would wait before I got an iPhone, but it was too good, I wanted one, and my wife bought me one when they first came out. It redefines the market, and the US based phone market was ripe for a serious upgrade. I also have a work-provided Verizon based Blackberry and I carry both. When better business oriented email and calendar integration arrives, I'll return the Blackberry.</div><div><br /></div><div><br /></div><div>Some of you may be wondering about my <a href=\"http://perfcap.blogspot.com/2007/07/gumstix-goliath-myphone-system-board.html\"><span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_32\">homebrew</span> phone</a>. Its still a work in progress, and I expect to have it up and running shortly. It took a few turns to get everything on the <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_33\">Gumstx</span> Goliath board working properly.</div>",
        "metadata": {
          "word_count": 1419,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "After the launch event last January, there was a lot of repetitive and shortsighted commentary about the iPhone, and I wrote a blog post about the way I thought Apple's strategy made sens. The iPhone was launched with features designed for the consumer marketplace, and redefined the category. One prediction I made was that Apple would release an <span class=\"blsp-spelling-error\" id=\"SPELLING_ERROR_3\">iPod</span> version this summer. The hardware omits the phone features, adds more flash memory and uses a slightly smaller form factor. It completely leverages the software investment which is the most important part of the iPhone. The iPhone is a worldwide product, and it takes a lot more work than you may expect to internationalize and test it. Touch will create clear consumer demand for the iPhone around the world that will get better deals for Apple. Apple will install whatever unique applications are available in that country, so US consumers could get  iPod Touch. The Mail application is also missing, while it could be provided in a generic form, the iPhone version is setup to make it easy to connect to US based mail providers. The cut in price of the original iPhone creates an empty price-point at $599, which could be filled by a 3G capable iPhone. The current hacker based developer community knows i. orm* which is in its infancy. It makes sense for Apple to release the bulk of the applications that they are working on in-house and do deals behind closed doors with major vendors such as Google. The key for Apple is to carry this initial enthusiasm into a real developer program, while keeping the number of hacked systems to a minimum. Adobe has come under intense pressure from all sides to get Flash onto the iPhone. The current iPhone camera looks barely finished. The viewfinder ripples if you move, has few features and takes a long time to grab a picture. The ARM based hardware used in the iPhone usually contains a lot of accelerators. Video - current solutions use a web page or hacked application to do call-back to the iPhone over the regular phone network. This requires centralized hardware resources that cost real money. A re upgrade will move to a hardware accelerated camera with video capture support and features like digital zoom. 3) Business Applications - the most requested being Microsoft's OutLook/Exchange. I don't think full integration will come directly from Apple, but there is likely to be a food fight amongst third party developers to win this market. If RIM is smart, they will do an iPhone port of their application and hold onto their dominant position. When better business oriented email and calendar integration arrives, I'll return the Blackberry. Its still a work in progress, and I expect to have it up and running shortly. It took a few turns to get everything on the Gumstx board working properly."
      },
      "tags": [
        "infrastructure",
        "platform",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_8cee8dd7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone apps that don&#39;t work on 3.0beta2",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/04/iphone-apps-that-don-work-on-30beta2.html",
      "content": {
        "text": "Both dictionary.com and PocketFlix fail immediately after they launch with iPhone os 3.0beta2<br/><br/>Most other apps seem to be working, and beta2 is faster and more stable. <div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a>]</p><br/></div><br />",
        "metadata": {
          "word_count": 34,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Both dictionary.com and PocketFlix fail immediately after they launch with iPhone 3.0beta2. Most other apps seem to be working, and beta2 is faster and more stable."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e1607511",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal - free iPhone app",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/04/fractal-free-iphone-app.html",
      "content": {
        "text": "<a href=\"http://itunes.apple.com/WebObjects/MZStore.woa/wa/viewSoftware?id=309131446&mt=8\">App store link</a><br /><br />It's finally ready for download.... Don't forget, Hotel Utah SF Sat April 18th is the next gig.",
        "metadata": {
          "word_count": 22,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "It's finally ready for download. Don't forget, Hotel Utah SF Sat April 18th is the next gig. It's available on iTunes and Google Play."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2d6f210b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Jobs at Netflix - One down, one to go...",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/10/jobs-at-netflix.html",
      "content": {
        "text": "I have been working on hiring for the last month or so. We found and hired one good match and had a bunch of near misses, so I have revised a reissued job description that is clearer about what we are looking for. I need one more senior Java/SQL developer with experience of large scale consumer web site development. We are looking for someone who has a lot of experience in product development, may have spent some time in management or team lead roles, but still \"has their head in the code\". We run small agile teams, you get to architect and code new product features yourself and deliver to the site every few weeks.<br /><br /><div>My group develops core personalization algorithms that are used to generate lists of candidate movies and filter them down to the top few to be shown in any particular block on the site. We work on the pages grouped under the main \"Browse DVDs\" tab at <a href=\"http://www.netflix.com/\">www.netflix.com</a>. We collaborate with a sister group that collects and predicts star ratings and <a href=\"http://www.netflixprize.com/\">runs the Netflix Prize</a>, they work on pages under the \"Movie's You'll Love\" tab.</div><div><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMasEikOIw2kj28xvDPJsCessLO9_eNOEa8UomRbeGrcmLUrdgPFjTaKLc9V9hYALTrPDmVahnJ3AeR-m4UwYuqPJzMruYfWmm76dr7Qpj4qAMC5iFX86BqAmxrLp1cf6Kvk8k/s1600-h/Picture+22.png\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMasEikOIw2kj28xvDPJsCessLO9_eNOEa8UomRbeGrcmLUrdgPFjTaKLc9V9hYALTrPDmVahnJ3AeR-m4UwYuqPJzMruYfWmm76dr7Qpj4qAMC5iFX86BqAmxrLp1cf6Kvk8k/s400/Picture+22.png\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5121245455331439922\" /></a><br /><br /></div><div>The business logic is written in Java, pulling its data out of Oracle and via middle tier services. The front end presentation layer (javascript/CSS etc.) work is done by engineers working for Bill Scott, Director of UI Engineering, who used to work at Yahoo! where he was chief AJAX evangelist. <a href=\"http://looksgoodworkswell.blogspot.com/2007/10/we-are-hiring.html\">He is also hiring....</a></div><div><br />Development is very rapid, agile and iterative. Features go through rigorous statistical A/B testing, and have to show a significant benefit before all the users get to see them. There is a great deal of freedom to try out ideas and an extremely analytical approach to picking the winners.</div><div><br /></div><div>Netflix is also something of a social experiment, the company culture is unique and very employee oriented. There is minimal process, a huge reliance on personal judgement, and zero tolerance for antisocial behaviors. What would normally be a fat binder of HR policies has been summarized into a few lines: \"Act in Netflix's best interest\" and the vacation policy is \"take some\". If you don't have the personal judgement to do the right thing, we say goodbye...</div><div><br /></div><div>Web Engineering consists of a relatively small number of senior people working very efficiently and productively. We like to hire the best \"stars\" we can find and build up \"bench strength\" like a championship winning sports team. We are looking for some more star talent...</div><div><br /></div><div>Here is the opening:\u00a0<a href=\"http://jobs.netflix.com/DetailFlix.asp?flix1916\">http://jobs.netflix.com/DetailFlix.asp?flix1916</a>\u00a0</div><div>I'm hiring one engineer at this time - apply at the above link or find me in LinkedIn or FaceBook to discuss.</div><div><br /></div>",
        "metadata": {
          "word_count": 466,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I need one more senior Java/SQL developer with experience of large scale consumer web site development. We run small agile teams, you get to architect and code new product features yourself and deliver to the site every few weeks. My group develops core personalization algorithms. The business logic is written in Java, pulling its data out of Oracle and via middle tier services. The front end presentation layer (javascript/CSS etc.) work is done by engine. Development is rapid, agile and iterative. Features go through rigorous statistical A/B testing. There is minimal process, a huge reliance on personal judgement, and zero tolerance for antisocial behaviors. Web Engineering consists of a relatively small number of senior people working very efficiently and productively. We like to hire the best \"stars\" we can find and build up \"bench strength\" like a championship winning sports team."
      },
      "tags": [
        "agile",
        "engineering",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_2497353e",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Moving Planet - Day of Action Saturday 24th Sept",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2011/09/moving-planet-day-of-action-saturday.html",
      "content": {
        "text": "There is a world-wide movement coordinated by <a href=\"http://www.350.org/\">the global warming action group 350.org</a> which has set this coming Saturday as a day of action to raise awareness.<br /><br />You can find an event near you at <a href=\"http://www.moving-planet.org/map\">the Moving Planet Site</a>. Many of the events are based around bike rides.<br /><br />For people local to the bay area, there is also a large <a href=\"http://www.eaasv.org/rally.html\">Electric Vehicle car show in Palo Alto</a> organized by the Electric Auto Association of Silicon Valley, where people can see and get rides in electric cars like the Nissan Leaf. The electric world speed record holding 216mph Lightning Motorbike should also be there.<br /><br />We have been having a lot of fun driving our Nissan Leaf, we use it almost every day, and going back to a conventional car seems so broken. What's this gearbox thing and why does it keep having to change gear? The quiet instant torque of direct drive electric power is addictive. At last count there are six Nissan Leaf's parked at Netflix every day. We have several chargers for people who need a top up, but most of us just charge at home.<br /><br />For more mainstream activities, the San Jose Mercury is also organizing a big car show in central San Jose on the same day, <a href=\"http://hybrids2hotrods.com/\">hybrids2hotrods.com</a>, they have a Fisker hybrid on their poster and there will be electric cars and lots of classic cars as well.<br /><br />I'm aiming to write some more blog posts on our solar system and the Nissan Leaf over the next few days.",
        "metadata": {
          "word_count": 262,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "This Saturday is a day of action to raise awareness of global warming. There is a world-wide movement coordinated by 350.org. Many of the events are based around bike rides. At last count there are six Nissan Leaf's parked at Netflix every day. We have several chargers for people who need a top up, but most of us just charge at home."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_16f5c1d7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Music - Amanda Palmer and Estradasphere",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/10/music-amanda-palmer-and-estradasphere.html",
      "content": {
        "text": "<div><br /></div><div>I listen to a lot of music, and the really good stuff gets played over and over again while I'm driving. I also think while I'm driving that I should write a blog post about my favorites but then I forget... until now!</div><div><br /></div><div>My favorite new music for 2007 was \"Palace of Mirrors\" by Estradasphere. I found that I never tired of it, and listened to it for months on end. It helps that their music is extremely varied in style and impossible to categorize. You can find out more and listen to music\u00a0<a href=\"http://www.estradasphere.com/news.php\">at their web site</a>. I suggest you start with \"Smuggled Mutation\", there is no such thing as a typical Estradasphere song as they combine just about every musical genre, usually in the same song. However this track shows off their talents very well. There is a live DVD of Palace of Mirrors which is also extremely well done. I have seen them play live several times and they are an excellent live band.</div><div><br /></div><div>During 2008 <a href=\"http://www.youtube.com/watch?v=K871znrbbDw\">Estradasphere collaborated with Amanda Palmer</a> of the Dresden Dolls, and her <a href=\"http://whokilledamandapalmer.com/\">new album \"Who killed Amanda Palmer\"</a> has become my favorite pick for 2008. The web site is very interesting, giving a lot of background story details about the making of the album and videos of most of the tracks. The music is distinctively Amanda, but expanded well beyond the Dresden Dolls format, with a much fuller sound on some tracks. The key for me is that the music is very moving and emotional, and can sustain interest for a lot of repeated listening.</div><div><br /></div><div>I'm also still waiting for <a href=\"http://fractal-continuum.com/\">Fractal</a> to finish their second album \"Sequitur\", I've been listening to pre-release versions as they work towards the final cut. I'm not holding my breath, as its taken years to get this far, but its getting very close to being completed.</div>",
        "metadata": {
          "word_count": 314,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "My favorite new music for 2007 was \"Palace of Mirrors\" by Estradasphere. I found that I never tired of it, and listened to it for months on end. Their music is extremely varied in style and impossible to categorize. Estradasphere collaborated with Amanda Palmer of the Dresden Dolls. \"Who killed Amanda Palmer\" has become my favorite pick for 2008. I'm still waiting for Fractal to finish their second album \"Sequitur\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4ebe48ca",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Capacity Planning for Cloud Computing - circa 2003",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/capacity-planning-for-cloud-computing.html",
      "content": {
        "text": "Here is a link to a <a href=\"http://www.slideshare.net/adrianco/capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation\">slideshare upload, of a talk I gave publicly in 2003, on Capacity Planning for Virtualized Datacenters</a> (known as N1 at the time, which can be seen as a nascent approach to cloud computing).<br /><br />I think it contains some interesting ideas, and dates reasonably well. I'm going to revisit some of these concepts as I think more about how to do capacity planning in the cloud.<br /><br /><div style=\"width:425px;text-align:left\" id=\"__ss_915600\"><a style=\"font:14px Helvetica,Arial,Sans-serif;display:block;margin:12px 0 3px 0;text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco/capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation?type=powerpoint\" title=\"Capacity Planning for Virtualized Datacenters - Sun Network 2003\">Capacity Planning for Virtualized Datacenters - Sun Network 2003</a><object style=\"margin:0px\" width=\"425\" height=\"355\"><param name=\"movie\" value=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=n1cappressn03-1231918123231357-1&stripped_title=capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation\" /><param name=\"allowFullScreen\" value=\"true\"/><param name=\"allowScriptAccess\" value=\"always\"/><embed src=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=n1cappressn03-1231918123231357-1&stripped_title=capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"355\"></embed></object><div style=\"font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;\">View SlideShare <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco/capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation?type=powerpoint\" title=\"View Capacity Planning for Virtualized Datacenters - Sun Network 2003 on SlideShare\">presentation</a> or <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/upload?type=powerpoint\">Upload</a> your own. (tags: <a style=\"text-decoration:underline;\" href=\"http://slideshare.net/tag/dfss\">dfss</a> <a style=\"text-decoration:underline;\" href=\"http://slideshare.net/tag/sigma\">sigma</a>)</div></div>",
        "metadata": {
          "word_count": 146,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm going to revisit some of these concepts as I think more about how to do capacity planning in the cloud. Here is a link to a talk I gave publicly in 2003, on Capacity Planning for Virtualized Datacenters. \" capacity-planning-for-virtualized-datacenters-sun-network-2003-presentation\" is a powerpoint-style presentation. \"355\" is an interactive version of the SlideShare presentation. Your own. ion:underline;\" href=\"http://www.slideshare.net/upload?type=powerpoint\">Upload</a> your own. (tags: <a style=\"text-decoration:underlined;\" href \"http://slides share.net\" \u00a0\"sigma\"  \"dfss\" > \"dfs\" });"
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_50127c23",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Are we ready for spotcloud yet?",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/11/are-we-ready-for-spotcloud-yet.html",
      "content": {
        "text": "<div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Launched today by Enomaly (@ruv) </span><a href=\"http://spotcloud.com/\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Spotcloud</span></a><span class=\"Apple-style-span\"  style=\"font-size:medium;\"> is a \"Cloud Capacity Clearinghouse and Marketplace\". There was a lot of discussion on twitter about whether this is really new, and previous attempts to do something similar.</span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">My background in this is that I was working at Sun in 2003/2004 when we were thinking about a marketplace for public grid computing capacity, I was chief architect for Shahin Khan's High Performance Technical Computing group at the time, and we \"owned\" Grid for Sun. We were both RIFd in the summer of 2004, but some of our projects stayed alive, and @ruv <a href=\"http://www.elasticvapor.com/2009/01/case-against-commodity-cloud-exchange.html\">mentioned some of these ideas from Sun surfacing in 2005</a>.</span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">I moved to eBay, and one idea that I tried to get eBay interested in at the time was building a marketplace for compute capacity. The problem was that eBay is a retail product focused company, and had no product managers looking at digitally delivered products. I couldn't find a marketplace manager who understood what I was proposing and thought it might be worth working on. In practice, it was too early, but Amazon had the vision to build a cloud at this time, and eBay could have done the same if it wanted to create a market, rather than make existing markets more efficient.</span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\">In 2006 (while I was working at eBay Research Labs) I wrote </span><a href=\"http://perfcap.blogspot.com/2006/06/part-3-disruptive-innovation-viewed-as.html\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">a blog post about a maturity model for innovation</span></a><span class=\"Apple-style-span\"  style=\"font-size:medium;\">. The key point is:</span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><blockquote><span class=\"Apple-style-span\"  style=\"font-size:medium;\">\"the evolution of a marketplace goes from competing on the basis of technology, to competing on service, to competing as a utility, to competing for free. In each step of the evolution, competitors shake out over time and a dominant brand emerges.<br /><br />To use this as a maturity model, take a market and figure out whether the primary competition is on the basis of technology, service, utility or search\"</span></blockquote></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">Today the cloud marketplace is somewhere between the service and utility phases. Each individual cloud has their own specific services and service interfaces, and they have not turned into a standard commodity yet, so we do not have the basis for competition purely on the basis of a Utility (i.e. on service quality - uptime, not on service features).</span></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">From this point of view, it is still too early for Spotcloud to take off. Cloud's problem is not \"finding generic capacity at low cost\" (the cloud utility search problem), the cloud marketplace is still evolving it's differentiated service interfaces towards a common set of functionality and standards. Spotcloud is starting out based on Enomaly's interfaces, and say they will add others, while the market leader is Amazon, who have already <a href=\"http://aws.amazon.com/ec2/spot-instances/\">implemented their own spot pricing model</a>.</span></span></div><div><span class=\"Apple-style-span\"   style=\"font-family:Georgia, 'Times New Roman', sans-serif;color:#29303B;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></div><div><span class=\"Apple-style-span\"   style=\"font-family:Georgia, 'Times New Roman', sans-serif;color:#29303B;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">One thing I did learn at eBay, is how hard it is to manage marketplaces. One unfortunate measure of success is that it attracts people whose aim is to make money by manipulating the market rather than contributing to it. There are a lot of non-intuitive details that you have to get right for a marketplace to scale and be robust enough to build and maintain trust, while also having very low \"friction\" so that it attracts and retains buyers and sellers.</span></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\">So one way to tell that the marketplace for cloud capacity is viable is when you see eBay entering that marketplace :-)</span></span></div><div><span class=\"Apple-style-span\"  style=\" color: rgb(41, 48, 59); font-family:Georgia, 'Times New Roman', sans-serif;\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><br /></div>",
        "metadata": {
          "word_count": 687,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Spotcloud is a \"Cloud Capacity Clearinghouse and Marketplace\" @ruv was chief architect for Shahin Khan's High Performance Technical Computing group at the time, and we \"owned\" Grid for Sun. In 2006, when I was working at eBay, I tried to get eBay interested in building a marketplace for compute capacity. The problem was that eBay is a retail product focused company, and had no product managers looking at digitally delivered products. The key point is that a dominant brand emerges over time. The evolution of a marketplace goes from competing on the basis of technology, to competing on service, to compete as a utility. Today the cloud marketplace is somewhere between the service and utility phases. Each individual cloud has their own specific services and service interfaces. We do not have the basis for competition purely on the basis of a Utility. The cloud marketplace is still evolving it's differentiated service interfaces towards a common set of functionality and standards. Spotcloud is starting out based on Enomaly's interfaces, and say they will add others, while the market leader is Amazon. There are a lot of non-intuitive details that you have to get right for a marketplace to scale and be robust enough to build and maintain trust. One way to tell that the marketplace for cloud capacity is viable is when you see eBay entering that marketplace :-) <span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></div><div><br /> <span class='Apple- style-span' style=\" font-size-medium;' caption-family: \"iReporter\";. pan: \" iReporter\",. width: \"1\",. height: \"0\",. caption: \"IReporter,\". pan size: \"2\",. font: \"3\",. pan position: \"normal\",. margin-top: \"10\",. border-bottom: \"12\",. background: \"white,\". font-family:\" \"muted\",. panel: \"sans-serif"
      },
      "tags": [
        "cloud",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_5559f0f3",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Pandora Prog Channel",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/10/pandora-prog-channel.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>I've been trying Pandora on and off for Internet music for a while, and attended a talk by CEO Tim Westergren last week, which got me to try it again. They are continuously improving their algorithms for choosing music, and I was trying to make a channel that would serve me interesting new music alongside some of my favourite experimental \"Prog Rock\" bands. It seems to be working much better, and I keep tuning the channel by skipping tracks that I don't like and giving thumbs up to the ones I do. The nice thing is you can listen to my channel, even though you don't get exactly the same songs as I do, there should be an interesting mix of King Crimson, Zappa, Estradasphere, and many other bands playing music you won't hear often. Its easy to make your own channel (it takes less training to make a more mainstream channel) and its the best way I've found to discover completely new music.<br/>  </p>  <p><a href=\"http://www.pandora.com/?sc=sh59488715848528731\" onclick=\"return top.js.OpenExtLink(window,event,this)\" target=\"_blank\">http://www.pandora.com/?sc<WBR/>=sh59488715848528731</a></p>  <p>Enjoy...</p>  <p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com/blogged-with-flock\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 182,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I was trying to make a channel that would serve me interesting new music alongside some of my favourite experimental \"Prog Rock\" bands. The nice thing is you can listen to my channel, even though you don't get exactly the same songs as I do, there should be an interesting mix of King Crimson, Zappa, Estradasphere, and many other bands. Pandora.com is a free online magazine. Visit their site for more information and to sign up for their newsletter. For more information on their newsletter, visit their website."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3925e84c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "I'm hiring a senior engineer at Netflix",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/05/im-hiring-senior-engineer-at-netflix.html",
      "content": {
        "text": "In my new role, I manage a small group and I have an open headcount for a senior engineer<br /><br />The full job specification is at <a href=\"http://jobs.netflix.com/DetailFlix.asp?flix1642\">http://jobs.netflix.com/DetailFlix.asp?flix1642</a><br /><br />Here's what we are looking for:<br /><br /><b>RESPONSIBILITIES:</b><ul><li>Designing and developing the Netflix web application and middle-tier architecture focused on personalization software.<br /></li><li>Full software life-cycle ownership of key strategic business logic from inception through production.<br /></li><li>Conceiving, articulating and implementing projects in coordination with technical and non-technical coworkers for product enhancement and improvement.</li></ul><p><br /><b>QUALIFICATIONS:</b></p><ul><li>Experience with high-traffic, highly scaling, multi-threaded software design.<br /></li><li>7+ years of web application development experience. (Java a plus)<br /></li><li>Experience with Oracle or equivalent database system.<br /></li><li>Flexibility to shift priorities based on changing needs of the  business.<br /></li><li>A Bachelors or Masters in Computer Science or equivalent engineering.</li></ul><b>Characteristics of Success at Netflix:</b><ul><li>Strong customer orientation, both internally and externally.<br /></li><li>Strategic thinking both technically and business-wise.<br /></li><li>Ability to execute successfully on business strategy.<br /></li><li>Self motivated / self starter.<br /></li><li>Personal drive to achieve world class software development.<br /></li><li>Desire to work in a fast-paced, evolving, growing, dynamic environment.<br /></li><li>Love of technical challenges and the sense of pride it brings to solve them.<br /></li><li>Ability to work well in a cross functional environment.</li></ul>",
        "metadata": {
          "word_count": 195,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The full job specification is at <a href=\"http://jobs.Netflix.com/DetailFlix.asp?flix1642\"\u00a0here's what we are looking for: 7+ years of web application development experience (Java a plus) A Bachelors or Masters in Computer Science or equivalent engineering. Strong customer orientation, both internally and externally. Ability to work well in a cross functional environment."
      },
      "tags": [
        "architecture",
        "netflix",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_f8f501ba",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "The OpenMoko  Story",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/03/openmoko-story.html",
      "content": {
        "text": "Here is a link to the <a href=\"http://www.openmoko.com/files/OpenMoko_Neologics.pdf\">OpenMoko slide set</a> presented at ETel. Worth a read, there is a lot of activity building around this platform at the <a href=\"http://www.openmoko.org/\">OpenMoko community</a> site.",
        "metadata": {
          "word_count": 32,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a lot of activity building around this platform at the OpenMoko site. Here is a link to the slide set presented at ETel."
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_58c05003",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Site - full speed ahead",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-site-full-speed-ahead.html",
      "content": {
        "text": "Pictures taken this morning, before digging started for the new leach field pipe that's getting inspected and filled back in on Thursday. The foundation layout and trenching will be done on Thursday and Friday, hopefully pouring concrete Monday and inspections on Tuesday. Everything just went from glacially slow to full speed....<br /><br />In the first picture, taken from a window in the house, the white blob is the propane tank, and you can see the leaning tree.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMfvOYfIwTudvHpoQ5bl-joPK0jCWx6FhaxB5FwZRWTpp7ZLePlMvyfCsP7RviN_STCnClubrJm3wwTQxh53YEhAFtpL2LSkX1zppI0A_zbdoYaFfOOZOFbAGWQZwLotq_QzAB/s1600/GarageSiteBefore1.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhMfvOYfIwTudvHpoQ5bl-joPK0jCWx6FhaxB5FwZRWTpp7ZLePlMvyfCsP7RviN_STCnClubrJm3wwTQxh53YEhAFtpL2LSkX1zppI0A_zbdoYaFfOOZOFbAGWQZwLotq_QzAB/s320/GarageSiteBefore1.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5452441908808922882\" /></a><br /><br />The second picture is looking back from by the tree. The two low black things by the house are the caps for the septic tank.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-co-iDJEkiRXrfoomribcnB_EiG8qWIZ7B53v3R_2yett7B_rGadgr1HT6vS08-41KvcbhWrjicuGziz8Ib2naVbBGZtxfhPelebMD2OgmEJOwo4OqSgETtiytAziBYitUsvb/s1600/GarageSiteBefore2.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEi-co-iDJEkiRXrfoomribcnB_EiG8qWIZ7B53v3R_2yett7B_rGadgr1HT6vS08-41KvcbhWrjicuGziz8Ib2naVbBGZtxfhPelebMD2OgmEJOwo4OqSgETtiytAziBYitUsvb/s320/GarageSiteBefore2.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5452441914402469202\" /></a><br /><br />I'm setting up a stop motion camera, will try to capture the building process. The MacOS iStopMotion app seems to work fine and has a five day free trial, so we will see how that goes.",
        "metadata": {
          "word_count": 172,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Pictures taken this morning, before digging started for the new leach field pipe. The foundation layout and trenching will be done on Thursday and Friday, hopefully pouring concrete Monday and inspections on Tuesday. The second picture is looking back from by the tree. The two low black things by the house are the caps for the septic tank. I'm setting up a stop motion camera, will try to capture the building process. The acOS iStopMotion app seems to work fine and has a five day free trial, so we will see how that goes. The app is available on iOS and Android."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d9316283",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Configuring IPQoS for flow accounting",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/configuring-ipqos-for-flow-accounting.html",
      "content": {
        "text": "I spent some time today working my way through the manuals. What I want is to just have the accounting information about the network flows, with no complex QoS rules and minimum overhead. So far I got it configured, but I'm getting no output. No error messages, but no output - so my machine is giving me some sulky passive agressive treatment :-(<br /><br />I'm working on a Sun W2100z, which is a dual CPU Opteron system. It came with Solaris 9, but I got Solaris 10 loaded on it by feeding it a bunch of CDs. The hardest part was configuring the graphics display, it defaulted to something horrible with 256 colors. I had to use kdmconfig, but it took me a while to figure out a) that fbconfig is only for SPARC, and that there is an x86 command with a different name. b) that the system I have has a certain kind of NVidia card in it, c) what display resolution my non-Sun LCD screen supports, and d) that if I scrolled up the screen I could find options with 16M colors that look right.<br /><br />Its a nice box, not too big, fairly quiet, fast.",
        "metadata": {
          "word_count": 199,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm working on a Sun W2100z, which is a dual CPU Opteron system. It came with Solaris 9, but I got Solaris 10 loaded on it by feeding it a bunch of CDs. The hardest part was configuring the graphics display, it defaulted to something horrible with 256 colors. Oo big, fairly quiet, fast. oo big and fairly quiet. fast, fast and big. oa big and quiet, quick and big, fast, big, quiet, big and fast."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_360fff7d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage Trusses Video",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/garage-trusses-video.html",
      "content": {
        "text": "On Tuesday a big truck had some difficulty delivering a 40ft trailer containing the roof framing trusses, but it was raining so that is all that happened.<br /><br />On Wednesday 21st April it started out dry, rained for a while then cleared up, but they did manage to put up all the trusses, and I made another stop-motion video of the work.<br /><br />Tuffshed aren't great at letting us know what is going to happen next, we have to call them or ask the workers. I think the next step is to put plywood sheeting on the roof, then get an inspector up to check that the nailing pattern is correct, then they can put the doors and windows in, tile the roof add the external trim and paint it. It should be much warmer and sunny for the next few days.<br /><br /><object width=\"425\" height=\"344\"><param name=\"movie\" value=\"http://www.youtube.com/v/d7Q-suFmOZ8&hl=en_US&fs=1&rel=0\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/d7Q-suFmOZ8&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\"></embed></object>",
        "metadata": {
          "word_count": 158,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "On Tuesday a big truck had some difficulty delivering a 40ft trailer containing the roof framing trusses. On Wednesday 21st April it started out dry, rained for a while then cleared up. I think the next step is to put plywood sheeting on the roof. Then they can put the doors and windows in, tile the roof add the external trim and paint it.  <embed>http://www.vimeo.com/v/d7Q-suFmOZ8&hl=en_US&fs=1&rel=0\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"344\" enablescriptaccess = \"always\" enablefullscreen = \"false\"allowfullscreen=true\" height = \"344\" width = \"425\"height=\"344\" margin-left: 0.5em; margin-right: 1.0em; width: \"425\"; height: \"344\"; margin-bottom: 1em; font-family: \""
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_31d4f63d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What's on my iPhone: Facebook",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/whats-on-my-iphone-facebook.html",
      "content": {
        "text": "Works better than the web site... its also my main mechanism for photo-blogging, sharing something I've seen.",
        "metadata": {
          "word_count": 17,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Photo-blogging is a way for me to share things I've seen. Works better than the web site... its also my main mechanism for photo- blogging."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_cfa7c9ab",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Part 1: Disruptive Innovation in the path from technology to brand - a maturity model | by Adrian Cockcroft | June 3rd, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/part-1-disruptive-innovation-in-path.html",
      "content": {
        "text": "Products aim to fill a need in a market, products that are disruptive innovations also reshape the market and markets tend to evolve in a series of discontinuous steps as they mature. The phrase \"crossing the chasm\" has been used to describe these changes, and \"early adopters\" are the people who first move a market to a new phase.<br /><br />In the next few posts I'm going to describe a generic maturity model that applies to many markets, and show how disruptive innovations may drive a market into a more mature phase. I got the initial idea of looking at markets in this way from Dave Nocera of Innovativ in a presentation he gave at SUPerG in early 2004. He used video as an example, with the move from VCR to Video rental to Online. I have extended that example, and come up with a generic maturity model based on it, which I also apply to the Telco industry.",
        "metadata": {
          "word_count": 159,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Markets tend to evolve in a series of discontinuous steps as they mature. Early adopters are the people who first move a market to a new phase. Disruptive innovations may drive a market into a more mature phase."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_7a57cada",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "sketching the new garage",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/02/sketching-new-garage.html",
      "content": {
        "text": "While figuring out what we wanted to build, I used <a href=\"http://sketchup.google.com/\">Google Sketchup</a> (the free version) to create a 3D model of the house and garage. Working with 3D takes a bit of getting used to, but it's very similar to the solid modeling I was doing to make custom 3D printed parts for my homebrew phone, and Sketchup is very good at getting these kind of artists-impression images created.<br /><br />The house really is a box, and the full width deck with steps going back down the side doesn't exist yet, but I was able to get the relative position and size of the garage about right so we could see if it fitted in. We then <a href=\"http://www.tuffshed.com\">went to TuffShed</a> and got them to draw up plans for our oversized garage. If you get a garage custom \"stick built\" at the site, the cost is quite high, mostly labor. Tuffshed pre-fabricate the parts, ship them in, and put up the entire building in about three days. The cost (not including foundations) is about $30K for an 1150 square foot triple wide garage (32ft wide, 36ft deep), with a 16ft wide double door, 8ft wide single door, and 10ft high ceilings. The basic cost is more like $20K, but it cost extra to go from 8ft to 10ft ceilings, and we added a bunch of other extra options.<br /><br />So far, we have had a few minor issues getting the Tuffshed plans approved by the county planning department. Really just clarifications of materials and codes on the drawings, but the first two tries have failed. Everything is now approved apart from the final tweaks to the drawings of the garage, so I'm hoping to get it taken care of next week.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGQ9ydcT50daoYlEaFFPPzyB41dI0PQ0UQVupkI8_6ywfXNmUxHVjBdNsr9tMea6OIGKn_I91mI2o1tNvgZYB1-bvJzF4A7iEquz4qNto5rYAUk_S8VOidR938uF4tGc1yccOD/s1600-h/Garage2009+Tuffshed.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 214px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiGQ9ydcT50daoYlEaFFPPzyB41dI0PQ0UQVupkI8_6ywfXNmUxHVjBdNsr9tMea6OIGKn_I91mI2o1tNvgZYB1-bvJzF4A7iEquz4qNto5rYAUk_S8VOidR938uF4tGc1yccOD/s320/Garage2009+Tuffshed.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5442827727778986418\" /></a><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjusDcjvzkYK1ZpN1Jk6FDO779LVAathAiNRnjcPKzLnVZy5UifJs3aJRoymCOTOZBRjVQRoQMp97T5ypsppC55JEkX0QKNABoZPaeNl1IiivwIT829X8wJ6AXhL6_Ayz7b2I5e/s1600-h/Garage2009+Tuffshed+2.png\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 214px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjusDcjvzkYK1ZpN1Jk6FDO779LVAathAiNRnjcPKzLnVZy5UifJs3aJRoymCOTOZBRjVQRoQMp97T5ypsppC55JEkX0QKNABoZPaeNl1IiivwIT829X8wJ6AXhL6_Ayz7b2I5e/s320/Garage2009+Tuffshed+2.png\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5442827735698718178\" /></a>",
        "metadata": {
          "word_count": 324,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The house really is a box, and the full width deck with steps going back down the side doesn't exist yet. Tuffshed pre-fabricate the parts, ship them in, and put up the entire building in about three days. The garage is 32ft wide, 36ft deep, with a 16ft wide double door, 8ft wide single door, and 10ft high ceilings. The basic cost is more like $20K, but it cost extra to go from 8ft to 10ft ceilings and we added a bunch of other extra options. <a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjusDcjvzkYK1ZpN1Jk6FDO779LVAathAiNRnjcPKzLnVZy5UifJs3aJRoymCOTOZBRjVQRoQMp97T5ypsppC55JEkX0QKNABoZPaeNl1IiivwIT829X8wJ6AX"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d962c9ff",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Part 2: Moving Pictures - disruptive innovation from the Cinema to PirateBay | by Adrian Cockcroft | June 3rd, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/part-2-moving-pictures-disruptive.html",
      "content": {
        "text": "Lets look at the history of movies. The initial technology to capture and replay moving pictures was developed around 100 years ago, and the initial competition between inventors went through its first transition when movie theaters became established and began to settle on a standard form of projector. The inventors who had alternative camera/recording/projector technology died out. Consumers wanted to go see movies and the movie industry formed to provide content for that market.<br /><br />The next innovation was to be able to watch movies at home on film, then there were movies on TV. The movie theaters had far bigger screens, better sound and color but the technology at home gradually caught up in features and reduced in cost, and a market transition to home viewing occurred. The total market size for equipment bought to watch movies at home is huge. Its important to note that the primary vendors in each phase of the market are different. The movie theater business is very different to the home video equipment supplier business. The early battles in the home were over the standard formats, famously Betamax failed to win over VHS for video tape, and there are continuing battles over DVD formats, but Sony is a dominant brand name in a crowded market for home video equipment.<br /><br />The next innovation was video rental, and Blockbuster ended up as a major player in this market, with presence on every high street. However, that presence became unnecessary as Netflix shipped DVD's directly to consumers and took over a large share of the market.<br /><br />Finally, video is available directly over the internet , its being viewed on PC's rather than TV sets, anyone can create and upload it, and YouTube is this year's hot market leading name in this space for all kinds of short videos. Its also trivially easy to take a full length movie or TV program and share it using one of the many BitTorrent services, and a growing proportion of movies are being watched for free, to the consternation of the movie industry.<br /><br />The PirateBay site in Sweden was recently shut down and charged with copyright violation, but it appears that a significant proportion of the population of Sweden were users and they got upset as they had got used to exchanging content for free. After three days the site came back up, hosted in Holland, and with even more users due to the publicity.<br /><br />Unlike YouTube, BitTorrent sites such as PirateBay don't host the actual content, they just connect individual users who exchange content, they don't need to provide storage or bandwidth, just a searchable database of small index files that configure the BitTorrent transfer between a large number of <span style=\"font-style: italic;\">seeders</span> that have some or all of the file already, and <span style=\"font-style: italic;\">leechers</span> who want to get the file, and who can in turn become seeders.<br /><br />The publicity gained as a side effect of trying to shut down the PirateBay site may even have the opposite effect of cementing the PirateBay brand as a market leader and accelerating growth in this space.<br /><br />Every step in this history involves a disruptive innovation. There is a fundamental reduction in cost, offset by a large increase in unit volume, which has often increased the overall revenue using a new way to monetize the market for moving pictures. Each time the previous market leader is left behind (often kicking and screaming) as the new larger market emerges. Each time a new brand captures the attention span and trust of the consumer, and dominates the market.",
        "metadata": {
          "word_count": 601,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Technology to capture and replay moving pictures was developed around 100 years ago. Consumers wanted to go see movies and the movie industry formed to provide content for that market. Next innovation was to be able to watch movies at home on film, then there were movies on TV. Video is available directly over the internet, its being viewed on PC's rather than TV sets. YouTube is this year's hot market leading name in this space for all kinds of short videos. A growing proportion of movies are being watched for free, to the consternation of the movie industry. The PirateBay site in Sweden was recently shut down and charged with copyright violation. After three days the site came back up, hosted in Holland, and with even more users due to the publicity. Unlike YouTube, BitTorrent sites such as PirateBay don't host the actual content. Every step in this history involves a disruptive innovation. There is a fundamental reduction in cost, offset by a large increase in unit volume. PirateBay brand as a market leader and accelerating growth in this space."
      },
      "tags": [
        "netflix",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_24b26147",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "You can help search for Jim Gray",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/02/you-can-help-search-for-jim-gray.html",
      "content": {
        "text": "I met Jim last year when he gave a talk at eBay. His sailboat went missing, and a lot of people have collaborated to allow up to date satellite images to be posted on the web for anyone to take a look at and see if they can see anything that might be his boat.<br /><br />My (ex-Sun DE) friend Geoff Arnold now works at Amazon, he blogged that they have configured a web service called Mechanical Turk to support parcelling out the work of looking at images. If you already have an Amazon.com account, then <span onclick=\"BLOG_clickHandler(this)\" class=\"blsp-spelling-corrected\" id=\"SPELLING_ERROR_0\">sign up</span> for the service takes a couple of clicks.<br /><br />I hunted through images for a while, and found a cargo ship, <span onclick=\"BLOG_clickHandler(this)\" class=\"blsp-spelling-corrected\" id=\"SPELLING_ERROR_1\">that's</span> all :-(<br /><br /><a href=\"http://geoffarnold.com/?p=1329\">Geoff's blog</a><br /><a href=\"http://www.allthingsdistributed.com/2007/02/help_find_jim_gray.html\">Werner's blog</a><br /><a href=\"http://www.mturk.com/mturk/preview?groupId=J0XZ58STDWJZ5QY4F9M0\">Search for Jim</a><br /><br />Please take a look yourself and pass the Mechanical Turk search for Jim URL on...",
        "metadata": {
          "word_count": 155,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Jim Gray's sailboat went missing last year. Satellite images have been posted on the web for anyone to take a look at and see if they can see anything. Please take a look yourself and pass the Mechanical Turk search for Jim URL on to your friends and family. Visit www.mturk.com for more information on how to use the site."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4f4f44c7",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Bil2010 Millicomputing - The Future In Your Pocket",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/02/bil2010-millicomputing-future-in-your.html",
      "content": {
        "text": "Here are my slides for BIL tomorrow, predictions for 2010 and 2011: <div style=\"width:425px;text-align:left\" id=\"__ss_3157473\"><a style=\"font:14px Helvetica,Arial,Sans-serif;display:block;margin:12px 0 3px 0;text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco/bil2010-millicomputing-the-future-in-your-pocket\" title=\"Bil2010 Millicomputing - The Future In Your Pocket\">Bil2010 Millicomputing - The Future In Your Pocket</a><object style=\"margin:0px\" width=\"425\" height=\"355\"><param name=\"movie\" value=\"http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=bil2010-millicomputing-100212094803-phpapp02&stripped_title=bil2010-millicomputing-the-future-in-your-pocket\" /><param name=\"allowFullScreen\" value=\"true\"/><param name=\"allowScriptAccess\" value=\"always\"/><embed src=\"http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=bil2010-millicomputing-100212094803-phpapp02&stripped_title=bil2010-millicomputing-the-future-in-your-pocket\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"355\"></embed></object><div style=\"font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;\">View more <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/\">presentations</a> from <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco\">Adrian Cockcroft</a>.</div></div>",
        "metadata": {
          "word_count": 62,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Here are my slides for BIL tomorrow, predictions for 2010 and 2011. The slides are for Bil2010 Millicomputing - The Future In Your Pocket. View more presentations from Adrian Cockcroft. llscreen=\"true\" width=\"425\" height=\"355\" caption: \"Adrian Cockcroft's Presentations\""
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_2fc568a1",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Speaking at UK CMG TEC Conference - May 19-21st 2008",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/05/speaking-at-uk-cmg-tec-conference-may.html",
      "content": {
        "text": "<a href=\"http://www.ukcmg.org.uk/acJune2008conference.html\">UKCMG TEC 2008</a> is near Northampton in the UK. I'm giving two of the same presentations as I gave at the US CMG, the enterprise version of my <a href=\"http://www.millicomputing.com\">Millicomputing</a> talk, and a half day workshop on Unix/Linux Performance.<br /><br />My old friend Phil Harman is also giving a talk on DTrace, and I'm staying in the UK for a few days after the conference to catch up with friends and family.",
        "metadata": {
          "word_count": 73,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "UKCMG TEC 2008 is near Northampton in the UK. I'm giving two of the same presentations as I gave at the US CMG. Phil Harman is also giving a talk on DTrace."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_dfdeb237",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cloud Architecture Tutorial",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2012/03/cloud-architecture-tutorial.html",
      "content": {
        "text": "I presented a whole day tutorial at <a href=\"http://qconlondon.com/\">QCon London</a> on March 5th, and presented subsets at an AWS Meetup, a Big Data / Cassandra Meetup and a Java Meetup the same week. I updated the slides and split them into three sections which are hosted on <a href=\"http://slideshare.net/adrianco\">http://slideshare.net/adrianco</a> along with all my other presentations. You can find many more related slide decks at <a href=\"http://slideshare.net/Netflix\">http://slideshare.net/Netflix</a> and other information at the <a href=\"http://techblog.netflix.com\">Netflix Tech Blog</a>.<br /><br />The first section tells the story of why Netflix migrated to cloud, how we think about choosing AWS as our cloud supplier and what features of the Netflix site were moved to the cloud over the last three years.<br /><br /><div style=\"width:425px\" id=\"__ss_11995595\"> <strong style=\"display:block;margin:12px 0 4px\"><a href=\"http://www.slideshare.net/adrianco/cloud-architecture-tutorial\" title=\"Cloud Architecture Tutorial - Why and What (1of 3) \" target=\"_blank\">Cloud Architecture Tutorial - Why and What (1of 3) </a></strong> <iframe src=\"http://www.slideshare.net/slideshow/embed_code/11995595?rel=0\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\"></iframe> <div style=\"padding:5px 0 12px\"> View more presentations from <a href=\"http://www.slideshare.net/adrianco\" target=\"_blank\">Adrian Cockcroft</a> </div> </div><br /><br />The second section is a detailed explanation of the globally distributed Java based Platform as a Service (PaaS) we built, the open source components that we depend on, and the open source projects that we have started to share at <a href=\"http://netflix.github.com\">http://netflix.github.com</a>.<br /><br /><div style=\"width:425px\" id=\"__ss_11994976\"> <strong style=\"display:block;margin:12px 0 4px\"><a href=\"http://www.slideshare.net/adrianco/arch-tutoriallo2of3\" title=\"Cloud Architecture Tutorial - Platform Component Architecture (2of3)\" target=\"_blank\">Cloud Architecture Tutorial - Platform Component Architecture (2of3)</a></strong> <iframe src=\"http://www.slideshare.net/slideshow/embed_code/11994976?rel=0\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\"></iframe> <div style=\"padding:5px 0 12px\"> View more presentations from <a href=\"http://www.slideshare.net/adrianco\" target=\"_blank\">Adrian Cockcroft</a> </div> </div><br /><br />The final section talks about how we run these PaaS services in the cloud, and includes details of our million writes per second scalability benchmark.<br /><br /><div style=\"width:425px\" id=\"__ss_11994912\"> <strong style=\"display:block;margin:12px 0 4px\"><a href=\"http://www.slideshare.net/adrianco/cloud-architecture-tutorial-part-1of3\" title=\"Cloud Architecture Tutorial - Running in the Cloud (3of3)\" target=\"_blank\">Cloud Architecture Tutorial - Running in the Cloud (3of3)</a></strong> <iframe src=\"http://www.slideshare.net/slideshow/embed_code/11994912?rel=0\" width=\"425\" height=\"355\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\"></iframe> <div style=\"padding:5px 0 12px\"> View more presentations from <a href=\"http://www.slideshare.net/adrianco\" target=\"_blank\">Adrian Cockcroft</a> </div> </div><br /><br />If you would like to see these slides presented in person, I'm teaching a half day cloud architecture tutorial at <a href=\"http://gluecon.com/2012/\">Gluecon in Broomfield Colorado</a> in May 23-24th. I hope to see you there...",
        "metadata": {
          "word_count": 366,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The first section tells the story of why Netflix migrated to cloud, how we think about choosing AWS as our cloud supplier and what features of the Netflix site were moved to the cloud over the last three years. I presented a whole day tutorial at QCon London on March 5th, and presented subsets at an AWS Meetup, a Big Data / Cassandra Meetup and a Java Meetup the same week. The second section is a detailed explanation of the globally distributed Java based Platform as a Service (PaaS) we built. The open source components that we depend on, and the open source projects that we have started to share at http://Netflix.com. The final section talks about how we run these PaaS services in the cloud. It includes details of our million writes per second scalability benchmark. If you would like to see these slides presented in person, I'm teaching a half day cloud architecture tutorial at Gluecon in May 23-24th. I hope to see you there..."
      },
      "tags": [
        "architecture",
        "aws",
        "big data",
        "cloud",
        "netflix",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_80344475",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Automated rotating exdump",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/04/automated-rotating-exdump.html",
      "content": {
        "text": "I've added an option that automates the processing of exacct data to text file format and rotates the logs, it is hard wired to create a new log file in /var/adm/exacct (which is an empty directory created during Solaris install as standard) and it includes the hostname and the date+time in the new file.<br /><br />The previous log name is used as the base of the output file, which is written to a specified directory with a .txt extension.<br /><br />The file /etc/acctadm.conf is maintained by the acctadm command, I read the log file names from it, and if there is no log file I don't start one. i.e. you need to manually start accounting with acctadm the first time to decide which logs you want to generate. The command syntax now looks like this:<br /><br /><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"># ./exdump</span><br /><span style=\"font-family:courier new;\">Usage: exdump [-vwr] [<file> file | -a dir ]<br /></file></span></span><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"><dir></dir></span></span> <div style=\"text-align: left;\"> <span style=\"font-family:courier new;font-size:85%;\"> -v: verbose</span><span style=\"font-size:85%;\"><br /></span><span style=\"font-family:courier new;font-size:85%;\"> -w: wracct all processes first</span><span style=\"font-size:85%;\"><br /></span><span style=\"font-family:courier new;font-size:85%;\"> -r: rotate logs</span><span style=\"font-size:85%;\"><br /></span><span style=\"font-family:courier new;font-size:85%;\"> -a dir: use acctadm.conf to get input logs, and write output files to dir</span><br /></div> <span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"> <dir></dir> </span></span><br /><span style=\";font-family:georgia;font-size:100%;\"  >The usual way to run the command will be from cron as shown</span><br /><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"> <dir></dir> </span></span> <div style=\"text-align: left;\"><br /><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\">0 * * * * /opt/exdump/exdump -war /var/tmp/exacct > /dev/null 2>&1</span></span><br /><span style=\"font-size:85%;\"><span style=\"font-family:courier new;\">2 * * * * /bin/find /var/adm/exacct -ctime +7 -exec rm {} \\;</span></span><br /><br /><span style=\"font-size:100%;\"><span style=\"font-family:georgia;\">This also shows how to clean up old log files, I only delete the binary files in this example, and I created /var/tmp/exacct to hold the text files. The process data in the text file looks like this:</span></span><br /><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">timestamp  locltime duration procid ppid uid   usr   sys majf rwKB vcxK icxK sigK sycK arMB mrMB command</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1114734370 17:26:10   0.0027 16527 16526   0 0.000 0.002    0 0.53 0.00 0.00 0.00  0.1  0.7 28.9 acctadm</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1114734370 17:26:10   0.0045 16526 16525   0 0.000 0.001    0 0.00 0.00 0.00 0.00  0.1  1.1 28.9 sh</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1114734370 17:26:10   0.0114 16525  8020   0 0.001 0.005    0 1.71 0.00 0.00 0.00  0.3  1.0 28.9 exdump</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1109786952 10:09:12  -1.0000     0     0   0 0.012 118.883  131 0.00 0.33 0.00 0.00  0.0  0.0  0.0 sched</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1109786952 10:09:12  -1.0000     0     0   0 0.012 118.883  131 0.00 0.33 0.00 0.00  0.0  0.0  0.0 sched</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1109786952 10:09:12  -1.0000     0     0   0 0.012 118.883  131 0.00 0.33 0.00 0.00  0.0  0.0  0.0 sched</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1109786959 10:09:19  -1.0000     1     0   0 4.311 3.066   96 47504.69 49.85 0.18 0.34 456.2  0.9  1.0 init</span></span><br /><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\">1109786959 10:09:19  -1.0000     2     0   0 0.000 0.000    0 0.00 0.00 0.00 0.00  0.0  0.0  0.0 pageout</span></span><br /><br /><span style=\"font-size:100%;\"><span style=\"font-family:georgia;\">Since the -w option forced long running processes to write an accounting record they have not finished, and I chose to mark this by showing the duration as -1. One odd thing shown here is that the sched process generated three identical accounting records, this needs some more investigation.</span></span><br /><span style=\"font-size:100%;\"> </span><br /><span style=\"font-size:100%;\"> <span style=\"font-family:georgia;\">I'm going to run this for a while, and install it on a few other machines including SPARC Solaris 10 and Solaris 8 to make sure its robust.</span></span><br /><span style=\"font-size:100%;\"> </span><br /><br /></div> <span style=\"font-size:85%;\"><span style=\"font-family:courier new;\"> <dir><span style=\"font-size:78%;\"><span style=\"font-family:courier new;\"><span style=\"font-size:180%;\"> </span><br /></span></span></dir> </span></span>",
        "metadata": {
          "word_count": 584,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I've added an option that automates the processing of exacct data to text file format and rotates the logs. It is hard wired to create a new log file in /var/adm/exacct (which is an empty directory created during Solaris install as standard) The previous log name is used as the base of the output file. The usual way to run the command will be from cron as shown. Use acctadm.conf to get input logs, and write output files to dir. This also shows how to clean up old log files. I only delete the binary files in this example. I created /var/tmp/exacct to hold the text files. The process data in the text file looks like this. 1114734370 17:26:10   0.0114 16525  8020  0 0.001 0.005   0 1.71 0.00 0.0 28.9 exdump. The sched process generated three identical accounting records, this needs some more investigation. The -w option forced long running processes to write an accounting record they have not finished, and I chose to mark this by showing the duration as -1. <span style=\"font-size-size:180%;. mily:courier new;\"> </span><br /></span></span></dir> </span> <dir> <span style='font- size-180%;' dir= \"mily\": \"cnn.com/2013/01/29/13/world/middleeast/middle east-news/middle East/middle-east-cnn/middle\u00a0east/news/Middle East/Middle\u00a0East/Middle Eastern/MiddleEast/Mideast/Middle east/Middle Europe/Middle eastern/Middle Africa/Middle Asia/Middle Middle East/Middles East/East/Africa/Mali/"
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_9a62f771",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "NoSQL Netflix Use Case Comparison for Riak",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/11/nosql-netflix-use-case-comparison-for.html",
      "content": {
        "text": "<div>Justin Sheehy @justinsheehy of Basho kindly provided a set of answers that I have interspersed with the questions below.</div><div><br /></div><div>The original set of questions are <a href=\"http://perfcap.blogspot.com/2010/10/comparing-nosql-availability-models.html\">posted here</a>. Each NoSQL contender will get their own blog post with answers, when there are enough to be interesting, I will write some summary comparisons.</div><div><br /></div><div>If you have answers or would like to suggest additional questions, comment here, tweet me @adrianco or blog it yourself.</div><div><br /></div><div><b><span class=\"Apple-style-span\"  style=\"font-size:large;\">Use Case Scenario for Comparison Across NoSQL Contenders </span></b></div>While each NoSQL contender has different strengths and will be used for different things, we need a basis for comparison across them, so that we understand the differences in behavior. Here is a sample scenario that I am publishing to put to each vendor to get their answers and will post the results here. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. I assume that is running on AWS and use that terminology, but the concepts are generic.<br /><br /><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><b>Use Case</b></span><span class=\"Apple-style-span\"  style=\" ;font-size:large;\"><br /></span>A TV based device calls the API to add a movie to its favorites list (like the Netflix instant queue, but I have simplified the concept here), then reads back the entire list to ensure it is showing the current state. The API does not use cookies, and the load balancer (Amazon Elastic Load Balancer) is round robin, so the second request goes to a different API server, that happens to be in a different Amazon Availability Zone, and needs to respond with the modified list.<div><br /><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Favorites Storage<br /></b></span>Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads.</div><div><br /></div><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 1: Availability Zones<br /></b></span>When an API reads and writes to a queue store using the NoSQL mechanism, is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones, is the initial write local or spread over the zones, is the write replication zone aware so data is repl<span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\">icated to more than one zone?</span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></div><blockquote><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><br /></i></span></span></div><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>There are two possibilities with Riak.  The first would be to spread a </i></span></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>single Riak cluster across all three zones, for example one node in </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>each of three zones.  In this case, a single replica of each item </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>would exist in each zone.  Whether or not a response needed to wait on </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>cross-zone traffic to complete would depend on the consistency level </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>in the individual request.  The second option would require Riak </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>EnterpriseDS, and involves placing a complete cluster in each zone and </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>configuring them to perform inter-cluster replication.  This has </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>multiple advantages.  Every request would be satisfied entirely </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>locally, and would be independent of latency or availability </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>characteristics across zone boundaries.  Another benefit is that </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>(unlike either the first scenario or some other solutions that spread </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>clusters and quorums over a long haul) read requests would not </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>generate any cross-zone traffic at all.  For an application with a </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>high percentage of reads, this can make a large difference.</i></span></p></span></div></blockquote><div><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p></span><blockquote><i><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"></span></p></i></blockquote><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 2: Partitioned Behavior with Two Zones<br /></b></span>If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, but traffic between zones is lost, what happens? In particular, which of these outcomes does the NoSQL service support?<br /><ul><li>one zone decides that it is still working for reads and writes but half the size, and the other zone decide it is offline</li><li>both zones continue to satisfy reads, but refuse writes until repaired</li><li>data that has a master copy in the good zone supports read and write, slave copies stop for both read and write</li><li>both zones continue to accept writes, and attempt to reconcile any inconsistency on repair</li></ul><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"></span></i></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">As write-availability is a central goal achieved in Riak, the fourth </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">option will be the observed behavior.  This is the case regardless of </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">the strategy chosen for Question 1.  In the first strategy, local </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">nodes other than the canonical homes for given data will accept the </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">writes instead, using the hinted-handoff technique.  In the second </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">strategy, the local cluster will accept the write, those changes will </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">be replayed across the replication link when the zones are </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">reconnected.  In all cases, vector clocks provide a clean way of </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">resolving most inconsistency, and various reconciliation models are </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">available to the user for those cases which cannot be syntactically </span></i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">resolved.</span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">For more information on vector clocks in Riak, see:</span></i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><br /></span></i></span></p> <p  style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; color:#1738f5;\"><span style=\"text-decoration: underline\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">http://blog.basho.com/2010/01/29/why-vector-clocks-are-easy/</span></i></span></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">and</span></i></span></p> <p  style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; color:#1738f5;\"><span style=\"text-decoration: underline\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><i><span class=\"Apple-style-span\"  style=\"font-size:medium;\">http://blog.basho.com/2010/04/05/why-vector-clocks-are-hard/</span></i></span></span></p><p  style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; color:#1738f5;\"></p></blockquote><p color=\"#1738f5\" style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; \"><span style=\"text-decoration: underline\"><br /></span></p></div><div><div></div></div><div><b>Question 3: Appending a movie to the favorites list</b></div>If an update is performed by read-modify-write of the entire list, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/values, and are queries by attribute/value supported?</div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Riak will use vector clocks to recognize causality in race conditions. </i></span></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>In the case of two overlapping writes to the same value, Riak will </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>retain both unless explicitly requested to simply overwrite with the </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>last value received.  If one client changes A to B and another changes </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>A to C, then (unless told to overwrite) Riak will return both B and C </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>to the client.  When that client then modifies the object again, the </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>single descendant \"D\" that they created will be the new value.  For </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>applications such as sets which are mostly added to and rarely deleted </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>from, the application code to perform this reconciliation is trivial </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>and in some cases is simply a set union operation.  This would look a </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>bit like this in terms of vector clock ancestry:</i></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; color:#1738f5;\"><span style=\"text-decoration: underline\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>http://dl.dropbox.com/u/751099/ndiag1.png</i></span></span></span></p><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://dl.dropbox.com/u/751099/ndiag1.png\"><img style=\"cursor:pointer; cursor:hand;width: 445px; height: 365px;\" src=\"http://dl.dropbox.com/u/751099/ndiag1.png\" border=\"0\" alt=\"\" /></a><br /><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i><br /></i></span></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Riak allows values to be of any arbitrary content type, but if the </i></span></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>content is in JSON then a JavaScript map/reduce request can be used to </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>query by attribute/value.</i></span></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p></div><div><div></div></div><div><b>Question 4: Handling Silent Data Corruption</b></div><div>When the storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously?</div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>Many layers of Riak perform consistency checking, including CRC </i></span></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>checking in the persistence engine and object equality in the </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>distributed state machines handling requests.  In most cases where </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>corruption can be detected in a given replica of some item, that </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>replica will immediately but asynchronously be fixed via read-repair.</i></span></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p></div><div><span class=\"Apple-style-span\"  style=\"font-size:large;\"><b>Question 5: Backup and Restore</b></span></div><div>Without stopping incoming requests, how can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? For cases such as roll-back after a buggy application code push, how is a known good version of the dataset restored, how is it made consistent, and what is the performance and availability impact during the restore? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen?</div><div><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p><blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i>There are two approaches to back up Riak systems: per-node or </i></span></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>whole-cluster.  Backing up per-node is the easiest option for many </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>people, and is quite simple.  Due to bitcask (the default storage </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>engine) performing writes in an append-only fashion and never </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>re-opening any file for writing once closed, Riak nodes can easily be </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>backed up via the filesystem backup method of your choice.  Simply </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>replacing the content of the data directory will reset a node's stored </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>content to what it held at the time.  Alternately, a command line </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>backup command is available which will write out a backup of all data </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>on the cluster.  This is fairly network and disk intensive and </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>requires somewhere to put a whole-cluster backup, but is very useful </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>for prototyping situations which are not holding enormous amounts of </i></span><span class=\"Apple-style-span\" style=\"font-family: 'times new roman'; \"><i>data.</i></span></p></blockquote><p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas\"><span class=\"Apple-style-span\"  style=\"font-family:'times new roman';\"><span class=\"Apple-style-span\"  style=\"font-size:medium;\"><i></i></span></span></p> <p style=\"margin: 0.0px 0.0px 0.0px 0.0px; font: 13.0px Consolas; min-height: 15.0px\"><br /></p></div><i></i>",
        "metadata": {
          "word_count": 2001,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Each NoSQL contender will get their own blog post with answers. When there are enough to be interesting, I will write some summary comparisons. Here is a sample scenario that I am publishing to put to each vendor to get their answers. The example is non-trivial and is based on a simplified Netflix related scenario that is applicable to any web service that reliably collects data from users via an API. ill post the results here. Favorites store is implemented using a NoSQL mechanism that persistently stores a single key=user value=movielist record on writes, and returns the movielist on reads. Is the traffic routing Availability Zone aware? Are reads satisfied locally, or spread over all zones? Is the write replication zone aware so data is replicated to more than one zone? There are two possibilities with Riak. The first would be to spread a. Riak cluster across all three zones, for example one node in. each of three zones. In this case, a single replica of each item. would exist in each zone. The second option would require Riak, and involves placing a complete cluster in each zone and.configuring them to perform inter-cluster replication. This has multiple advantages. Every request would be satisfied entirely. Apple's font-family is the same as that of the Times New roman. The roman is used to represent a large percentage of read requests. This can make a large difference when dealing with large groups of reads. If the connection between two zones fails, and a partition occurs so that external traffic coming into and staying within a zone continues to work, what happens? In particular, which of these outcomes does the NoSQL service support? Riak is an open-source, free-to-use programming language. Riak is written in the roman language. The language is based on the Roman alphabet. In the second strategy, the local cluster will accept the write, and those changes will be replayed across the replication link. In all cases, vector clocks provide a clean way of resolving inconsistency. available to the user for those cases which cannot be syntactically. <span class=\"Apple-style-span\"  style=\"font-size:medium;\">For more information on vector clocks in Riak, see:. \"font-family:'times new roman'; \"font-size:medium\"; \"color: 1738f5\"; \"font: 13.0px Consolas\"; \"text-decoration: underline\"; \"padding-bottom: 10pt\"; \"line-height\": 10pt; \"width: 12pt;\" \"height\": 12pt\"; \"margin-top: 0.0%; text-decorator: bold; font-family: TimesNewRoman;\" \"source\": \"http://blog.basho.com/2010/01/29/why- vector-clocks-are-easy/,\" \"source: \"http:://www.mailonline.co.uk/article If an update is performed by read-modify-write, what mechanisms can be used to avoid race conditions? If multiple attribute/values are supported for a key, can an additional value be written directly without reading first? What limits exist on the size of the value or number of attribute/ values, and are queries by attribute/value supported? Riak will use vector clocks to recognize causality in race conditions. If one client changes A to B and another changes. A to C, then (unless told to overwrite) Riak will return both B and C. E-span: \"D\" that they created will be the new value. For. </i></span><span class=\"Apple-style-span\"  style=\" ;font-family:'times new roman';\"><i>and in some cases is simply a set union operation.  This would look a bit like this in terms of vector clock ancestry. Riak allows values to be of any arbitrary content type, but if the. content is in JSON then a JavaScript map/reduce request can be used to. When storage or network subsystem corrupts data without raising an error, does the NoSQL service detect and correct this? When is it detected and corrected, on write, on read or asynchronously? Many layers of Riak perform consistency checking, including CRC. In most cases where corruption can be detected in a given replica of some item, that replica will immediately be fixed via read-repair. Question 5: Backup and Restore. How can a point in time backup of the entire dataset be performed? What is the performance and availability impact during the backup? Are there any scalability limits on the backed up dataset size, what's the biggest you have seen? iak systems: per-node or Whole Cluster. Backing up per- node is the easiest option for many. Riak nodes can easily be backed up via the filesystem backup method of your choice. A command line is available which will write out a backup of all data. This is fairly network and disk intensive and is very useful for prototyping situations.  <p style=\"margin: 0.0%.0.0%; font: 13.0. Consolas; min-height: 15.0px\";. span></p> <i style=\"padding: 10%.0%.10%.10.0%\";. spans=\"1.0\";. border-top: 1.0,. border-bottom: 1%.0%;. margin-bottom 10%.10%%;. font-family: \"Consolas;\". span: \"13.0.\";. border: \"1.1\".,. border: 1}. span: 1,. margin-top 10%.1%;. padding-right 10%.5%.0px;. font size: 12.0"
      },
      "tags": [
        "aws",
        "netflix",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_b14cfd3f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Capacity Planning for Cloud Computing",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/12/capacity-planning-for-cloud-computing.html",
      "content": {
        "text": "<div style=\"width:425px;text-align:left\" id=\"__ss_840014\"><a style=\"font:14px Helvetica,Arial,Sans-serif;display:block;margin:12px 0 3px 0;text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco/capacity-planning-for-cloud-computing?type=powerpoint\" title=\"Capacity Planning for Cloud Computing\">Capacity Planning for Cloud Computing</a><object style=\"margin:0px\" width=\"425\" height=\"355\"><param name=\"movie\" value=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=capacity-planning-for-cloud-computing-2176&stripped_title=capacity-planning-for-cloud-computing\" /><param name=\"allowFullScreen\" value=\"true\"/><param name=\"allowScriptAccess\" value=\"always\"/><embed src=\"http://static.slideshare.net/swf/ssplayer2.swf?doc=capacity-planning-for-cloud-computing-2176&stripped_title=capacity-planning-for-cloud-computing\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"425\" height=\"355\"></embed></object><div style=\"font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;\">View SlideShare <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/adrianco/capacity-planning-for-cloud-computing?type=powerpoint\" title=\"View Capacity Planning for Cloud Computing on SlideShare\">presentation</a> or <a style=\"text-decoration:underline;\" href=\"http://www.slideshare.net/upload?type=powerpoint\">Upload</a> your own.</div></div><br /><br />At the recent CMG conference I volunteered to lead a panel session on Capacity Planning for Cloud Computing, and <a href=\"http://www.slideshare.net/adrianco/capacity-planning-for-cloud-computing\">these slides</a> were the result.<br /><br />Now, this was a conference full of people who already know how to do capacity planning, so they were interested in what was new about Cloud Computing (or even what it was all about from first principles), so the slides do not explain how to do capacity planning in the cloud, they talk about what changes for the capacity planner in this new world.<br /><br />I'm going to be developing more material in this area over the coming year. I have a head start, in 2002 and 2003 I led projects at Sun where I researched how to do Capacity Planning in Virtualized Datacenters, built some tools, filed a couple of patents, presented papers at conferences and failed to get Sun's N1 project to implement some of it. Well, it seems I was a few years ahead of my time, so I'm going to start by digging out the papers I published at the time.",
        "metadata": {
          "word_count": 235,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "\"Capacity Planning for Cloud Computing\" is a powerpoint-style presentation for cloud computing users. The presentation includes a movie and a slide show. At the recent CMG conference I volunteered to lead a panel session on Capacity Planning for Cloud Computing. These slides were the result. They do not explain how to do capacity planning in the cloud, they talk about what changes are coming. In 2002 and 2003 I led projects at Sun where I researched how to do Capacity Planning in Virtualized Datacenters. I filed a couple of patents, presented papers at conferences and failed to get Sun's N1 project to implement some of it."
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_0e8e832a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Cockcroft Headroom Plot - Part 3 - Histogram Fixes",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/cockcroft-headroom-plot-part-3.html",
      "content": {
        "text": "I found that I had some scaling issues with the histograms that needed fixing. Ultimately this made the code look a lot more complex, but it now deals with scaling the plot and the histogram with a fixed zero origin on both axes. I think its important to maintain the zero origin for a throughput vs. response time plot.<br /><br />The tricky part is that the main plot is automatically oversized from its data range by a few percent, and the units used in the histogram are completely different. A histogram with 6 bars is scaled to have the bars at unit intervals and is 6 wide plus the width of the bars etc. After lots of trial and error, I made the main plot use the maximum bucket size of the histogram as its max value, and artificially offset the histograms by what looks like about the right amount. The plot below uses fixed data as a test. You can see that the first bar includes two points, thats due to the particular algorithm used by R. Some alternative histogram algorithms are available, but this one seems to be most appropriate to throughput/response time data.<br /><pre><br />> chp(5:10,5:10)<br /></pre><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/432206/Picture%2012.png\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/400/850459/Picture%2012.png\" border=\"0\" alt=\"\" /></a><br /><br />The updated code follows.<br /><pre><br />chp <- function(x,y,xl=\"Throughput\",yl=\"Response\",tl=\"Throughput Over Time\",<br />ml=\"Cockcroft Headroom Plot\") {<br />       xhist <- hist(x,plot=FALSE)<br />       yhist <- hist(y, plot=FALSE)<br />       xbf <- xhist$breaks[1]                          # first<br />       ybf <- yhist$breaks[1]                          # first<br />       xbl <- xhist$breaks[length(xhist$breaks)]       # last<br />       ybl <- yhist$breaks[length(yhist$breaks)]       # last<br />       xcl <- length(xhist$counts)                     # count length<br />       ycl <- length(yhist$counts)                     # count length<br />       xrange <- c(0,xbl)<br />       yrange <- c(0,ybl)<br />       nf <- layout(matrix(c(2,4,1,3),2,2,byrow=TRUE), c(3,1), c(1,3), TRUE)<br />       layout.show(nf)<br />       par(mar=c(5,4,0,0))<br />       plot(x, y, xlim=xrange, ylim=yrange, xlab=xl, ylab=yl)<br />       par(mar=c(0,4,3,0))<br />       barplot(xhist$counts, axes=FALSE,<br />               xlim=c(xcl*0.03-xbf/((xbl-xbf)/(xcl-0.5)),xcl*0.97),<br />               ylim=c(0, max(xhist$counts)), space=0, main=ml)<br />       par(mar=c(5,0,0,1))<br />       barplot(yhist$counts, axes=FALSE, xlim=c(0,max(yhist$counts)),<br />               ylim=c(ycl*0.03-ybf/((ybl-ybf)/(ycl-0.5)),ycl*0.97),<br />               space=0, horiz=TRUE)<br />       par(mar=c(2.5,1.7,3,1))<br />       plot(x, main=tl, cex.axis=0.8, cex.main=0.8, type=\"S\")<br />}<br /></pre>",
        "metadata": {
          "word_count": 337,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The code for the histograms is a bit more complex. The main plot is automatically oversized from its data range by a few percent. A histogram with 6 bars is scaled to have the bars at unit intervals and is 6 wide plus the width of the bars etc. The updated code follows. to be most appropriate to throughput/response time data. The code is as follows: chp(5:10, 5:10) chp('Cockcroft Headroom Plot') chp ('Throughput',yl=\"Response',tl=\"Throughput Over Time',ml=\"Cockcrow Headroom plot\")  ycl <- length(yhist$counts) nf <- layout(matrix(c(2,4,1,3),2,2,byrow=TRUE, c(3,1), c(1,2), TRUE) layout.show(nf) par(mar=c(5,4,.0,0), space=0) barplot(xhist$Counts, axes=FALSE, xlim=c) plot(x, y, x lim=xrange, ylim=yrange, xlab=xl, ylab=yl)"
      },
      "tags": [
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_54e6a26b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Experiences using an iPad as my primary work laptop",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/experiences-using-ipad-as-my-primary.html",
      "content": {
        "text": "I spent the last week carrying my iPad with me to meetings and using it as my main system at home an in my cube. My laptop was used for a few things, like the performance monitoring tool AppDynamics - which needs a bigger screen and is flash based, but just about everything else was usable on the iPad.<br /><br />I have the apple case, which I think is essential, as it holds the iPad up at the right angle to use on a desk or lap, as well as protecting it and making it less slippery to carry around. I got the Video out cable but haven't had a chance to use it yet. After intensive use, the battery is still at 60% or more each evening.   <br /><br />I like the speed at which apps like mail and calendar come up, and they are both very nice to use, I prefer them to entourage on my Mac laptop. I spend a lot of my day reading writing and deleting email. Safari is also fast and worked well for gmail. I imported a PowerPoint presentation to keynote on the iPad and it needed adjustments to font sizes in diagrams, and lost some color coding, but seems like a usable tool. I will switch to keynote on my Mac as well and hopefully it will be compatible.<br /><br />I have a VNC app, but moving the mouse cursor around is a a pain so it's only useful for emergency access to my laptop. <br /><br />The other app I use a lot is NewsRack, which tracks about 100 random rss feeds for me by keeping in sync with google reader. I get most of my news and keep up with friends blogs this way. Compared to the iPhone version it seems to update much faster and the extra screen space is well used. The BBC app provides a more visual way to track stories, and the Guardian Eyewitness photography app contains stunning images, it's one of the most popular free apps at the moment. <br /><br />Typing on the iPad is better than expected with two caveats. I keep hitting the b or n keys when I mean to hitbspace (like that), and the auto correction system is a bit over-active so I have to proofread what I write more than usual.<br /><br />Of course I have been watching movies with the Netflix app, and have used the eBay and Pandora iPad apps, which look good. I'm still using the iPhone version of echofon as my Twitter reader, but I will try out the official twitter app when it comes out.",
        "metadata": {
          "word_count": 441,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I like the speed at which apps like mail and calendar come up, and they are both very nice to use. I spend a lot of my day reading writing and deleting email. Safari is also fast and worked well for gmail. The BBC app provides a more visual way to track stories. The Guardian Eyewitness photography app contains stunning images. NewsRack tracks about 100 random rss feeds for me by keeping in sync with google reader.  een watching movies with the Netflix app, and have used the eBay and Pandora iPad apps. I'm still using the iPhone version of echofon as my Twitter reader, but I will try out the official twitter app when it comes out."
      },
      "tags": [
        "monitoring",
        "netflix",
        "performance",
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_14dbc33b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Fractal Gig - Saturday Jan 17th - Hotel Utah San Francisco",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/fractal-gig-saturday-jan-17th-hotel.html",
      "content": {
        "text": "I'm inflicting some new music by Fractal on people who visit my blog... They have just released their new CD Sequitur, which contains a varied mix of quite unique and hard to categorize music. I hope to see people at the Hotel Utah in San Francisco on Saturday, and for those of you in Seattle, they are playing Thursday 22nd, and Portland OR on Friday 23rd. Please connect to Fractal via <a href=\"http://www.myspace.com/fractalcontinuum\">Myspace</a> or <a href=\"http://www.facebook.com/group.php?gid=46302466649\">Facebook</a> or <a href=\"http://www.bebo.com/fractal\">Bebo</a>.",
        "metadata": {
          "word_count": 79,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Fractal have just released their new CD Sequitur. The CD contains a varied mix of quite unique and hard to categorize music. I hope to see people at the Hotel Utah in San Francisco on Saturday. For those of you in Seattle, they are playing Thursday 22nd, and Portland OR on Friday 23rd."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_e4b9c728",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Millicomputing at BIL this weekend",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2008/02/millicomputing-at-bil-this-weekend.html",
      "content": {
        "text": "I'm presenting (and generally helping out) at the <a href=\"http://www.bilconference.com\">BIL un-conference</a> this weekend in Monterey. I have posted some slides on my <a href=\"http://millicomputing.blogspot.com/2008/02/bil-talk-millicomputing-future-in-your.html\">Millicomputing</a> blog, on the subject of \"The Future in Your Pocket\", that speculates on what could happen over the next few years in the mobile space, and starts a discussion of what we might be able to use it for.",
        "metadata": {
          "word_count": 63,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I'm presenting (and generally helping out) at the <a href=\"http://www.bilconference.com\">BIL un-conference</a> this weekend in Monterey. I have posted some slides on my blog, on the subject of \"The Future in Your Pocket\" that speculates on what could happen."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_4dc0bc11",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "the garage saga, it gets complicated",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/02/garage-saga-it-gets-complicated.html",
      "content": {
        "text": "We live out of town, on top of the Santa Cruz mountains. This makes a few things easier, we only have Santa Clara County to deal with, no city rules. We also have more space. The downside is that it can be hard to find people to work on the house, and it can cost more to deliver heavy stuff like gravel than it costs to buy it. <br/><br/>The complications start with a scenic highway. We can't build anything within 100ft of the road, Then there is a well, we can't do things within a 100ft radius of that. Then there is a septic tank and leach field. The leach field is two banks of 250ft long buried perforated pipes which occupies a lot of space, including the space that we want to put a garage on. Then a driveway has to find it's way through and have a turn around area big enough to take a fire engine. All of a sudden, a few acres of land is full.<br/><br/>A few years ago we got a plan drawn up for a modified leach field layout. This removed the upper 250ft field so that we could put the garage there, and added a new 250ft field below. To comply with planning rules we had to show that we could on theory add another 500ft of leach field if the first one failed. So that took up almost all the space.<br/><br/>After all that the biggest possible garage we could fit is a free standing building 32ft wide and 36ft deep. It will have one single and one double door and sit alongside the house about 20ft away and slightly downhill.<br/><br/>To prove that the space existed I got my local ground moving specialist Bob Whalen to use an excavator to dig holes and locate the leach field pipes, and made a big flat area in the right place. Along the way we destroyed the upper leach field before I got permission, we got told off but since the rest of the plan made sense we got away with it and were able to move forward. <br/><br/>At some point the county realized that the plan they had for my driveway bore no relation to what had been put in by the people who built the house, this raised the spectre of a grading violation and possible grading permit, which would be needed if we had to move more than 150 cubic yards to get a driveway that conformed to code.<br/><br/>With a lot of help from Terry and crew at TS Civil Engineering we ended up with a plan for the driveway and leach field that met code, didnt raise a violation and didn't need over 100 cubic yards of grading. With a few tweaks the county finally approved the plan. <div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a> from my iPhone]</p><br/></div>",
        "metadata": {
          "word_count": 476,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "We live out of town, on top of the Santa Cruz mountains. This makes a few things easier, we only have Santa Clara County to deal with. The downside is that it can be hard to find people to work on the house. It can cost more to deliver heavy stuff like gravel than it costs to buy it. The biggest possible garage we could fit is a free standing building 32ft wide and 36ft deep. It will have one single and one double door and sit alongside the house about 20ft away and slightly downhill. With a lot of help from Terry and crew at TS Civil Engineering we ended up with a plan for the driveway and leach field that met code. With a few tweaks the county finally approved the plan."
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_file_267812b6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "eBay Wireless WAP Access | by Adrian Cockcroft | June 12th, 2006",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/06/ebay-wireless-wap-access-by-adrian.html",
      "content": {
        "text": "I was at the eBay developers conference showing some proof of concept prototypes of new mobile applications, and I found that hardly anyone knew that eBay already has a WAP based mobile version of the site. It loads in seconds on any phone that has any kind of web browser, but there is no automatic redirect from the main eBay site. You should bookmark this on your phone's web browser:<br /><br /><a href=\"http://wap.ebay.com\">http://wap.ebay.com</a><br /><br />You can also use this site to backend other mobile applications. If your own code helps a user find an item on eBay then you can form a URL that contains the item id and go directly into the official eBay WAP based site. It handles user login, MyEbay, watchlists etc. The main problem with the wap site is that the search functionality is too simplistic. The prototypes we were showing (no, you can't access them, you should have been there...) are aimed at fixing the finding experience.",
        "metadata": {
          "word_count": 162,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Ebay already has a WAP based mobile version of the site. It loads in seconds on any phone that has any kind of web browser. There is no automatic redirect from the main eBay site."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_da3b46e6",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Latest Local Music - Fractal, Beautiful Machines, Cash Pony, Moetar and Atomic Ape... and Mirthkon!",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2014/01/latest-local-music-fractal-beautiful.html",
      "content": {
        "text": "There is a huge amount of good original new music that is looking for an audience. Most of what I listen to is from bands you've probably never heard of, so I'm going to share some updates and some local gigs that are happening over the next few days in the SF Bay Area. I encourage you to go to the gigs and buy a CD, or buy the album online to support local musicians. There is a <a href=\"https://www.facebook.com/groups/203618604214\" target=\"_blank\">Facebook group for Bay Area Prog rock</a> that's worth tracking.<br />\n<br />\nEarlier this week <a href=\"https://www.facebook.com/fractalcontinuum\" target=\"_blank\">Fractal released the first track of a free album of improvisations</a>. They plan to release the <a href=\"https://soundcloud.com/nicroozeboom/sets/fractiles\" target=\"_blank\">tracks on soundcloud around once a month</a>. It's hard to categorize what they sound like other than excellent musicians composing instrumentals in real time. They tend to have interesting sound effects, looped phrases and are known for using odd time signatures. I've been a non-playing member of this band since they started, helping in various ways including naming this album \"Fractiles\".<br />\n<br />\nI've been meaning to write this for a while, and my deadline is that <a href=\"https://www.facebook.com/events/468529613251183/\" target=\"_blank\">tonight (Thursday Jan 30th) there is a gig</a> including <a href=\"https://www.facebook.com/BeautifulMachines\" target=\"_blank\">Beautiful Machines</a> in San Francisco. I can't attend, but I do have their first album and have been listening to it a lot over the last few months. They write songs with influences from 80's synth-pop bands, and remind me of Japan or Duran Duran perhaps. Their bass player Van is one of my favorite local musicians, he used to play in <a href=\"http://www.youtube.com/watch?v=v4xqehWGgKc\" target=\"_blank\">Headshear and has a unique and interesting style on fretless bass</a>. Here's a trailer for a video of Animammal from their album \"Disconnect :: Reconnect\".<br />\n<br />\n<iframe allowfullscreen=\"\" height=\"281\" mozallowfullscreen=\"\" src=\"//player.vimeo.com/video/85324703\" webkitallowfullscreen=\"\" width=\"500\"></iframe>\n\n<br />\n<br />\nOn <a href=\"https://www.facebook.com/events/493240727463599/\" target=\"_blank\">Saturday I will be in Oakland for a gig with three of my favorite local bands</a>. Please come along!<br />\n<br />\n<a href=\"https://www.facebook.com/MoeTar\" target=\"_blank\">Moetar are a pop band</a> with a unique sound that blends jazzy female vocals and prog rock instrumentation into some very commercial sounding songs. They don't sound like Muse, but if you like Muse I think you will like Moetar. They have a unique mix, and I've played their first album regularly over the last year or two. Their second album has been recorded and is due for release soon. I've seen them live several times, and try not to miss a gig, they are awesome and deserve to be a big hit. Here's a video they made for <i>Butchers of Baghdad</i> from their first album <i>From These Small Seeds</i>.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/a5bYz78ZpWk/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/a5bYz78ZpWk&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/a5bYz78ZpWk&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\n<a href=\"https://www.facebook.com/cashponyband\" target=\"_blank\">Cash Pony</a> have played with Moetar several times, and I bought their CD at a gig, then just kept playing it. It's an interesting mix of instruments, with a bass sax and an electric sitar on some songs. It's interesting uptempo pop songs, and they have a very smooth sound on the CD, but I can't figure out what to compare it to. Take a listen to a live version of&nbsp;<i>1000 Layer Curry Dawg</i> and get their&nbsp;<i>Carpal Tunnel Vision Quest</i> album.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/JrEcVIhgkdM/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/JrEcVIhgkdM&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/JrEcVIhgkdM&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\nYears ago, there was an amazing local band called Estradasphere. They released several albums that I still listen to regularly. They have dispersed, and the guitarist Jason Schimmel has a <a href=\"http://www.atomicapeband.com/\" target=\"_blank\">new band called Atomic Ape</a> with a new album, <i>Swarm</i>. I haven't heard them before, but am looking forward to seeing them live for the first time on Saturday. <a href=\"http://www.youtube.com/watch?v=A5sIXUUMQGw\" target=\"_blank\">Here's a rather dark video of them</a>&nbsp;playing for a radio show.<br />\n<br />\nUpdate - I just found out that on Feb 15th MirthKon and Secret Chiefs 3 are <a href=\"https://www.songkick.com/events/18750234\" target=\"_blank\">playing a gig in San Francisco</a>. Not yet sure if I can make it to attend. Yet another of my favorite local bands, intensely complex virtuoso playing. Matt the bass player in Mirthkon is also the keyboard player in Moetar. If you like Frank Zappa you'll like MirthKon, and you should give the video below a listen.<br />\n<br />\n<div class=\"separator\" style=\"clear: both; text-align: center;\">\n<object class=\"BLOGGER-youtube-video\" classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\" codebase=\"http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0\" data-thumbnail-src=\"https://ytimg.googleusercontent.com/vi/NK-oo-tbFvM/0.jpg\" height=\"266\" width=\"320\"><param name=\"movie\" value=\"https://youtube.googleapis.com/v/NK-oo-tbFvM&source=uds\" /><param name=\"bgcolor\" value=\"#FFFFFF\" /><param name=\"allowFullScreen\" value=\"true\" /><embed width=\"320\" height=\"266\"  src=\"https://youtube.googleapis.com/v/NK-oo-tbFvM&source=uds\" type=\"application/x-shockwave-flash\" allowfullscreen=\"true\"></embed></object></div>\n<br />\n<br />\n<br />\n<br />\n<br />",
        "metadata": {
          "word_count": 790,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There is a huge amount of good original new music that is looking for an audience. I encourage you to go to the gigs and buy a CD, or buy the album online to support local musicians. There is a group for Bay Area Prog rock that's worth tracking. I've been meaning to write this for a while, and my deadline is that tonight (Thursday Jan 30th) there is a gig in San Francisco. I can't attend, but I do have their first album and have been listening to it a lot over the last few months. They write songs with influences from 80's synth-pop bands, and remind me of Japan or Duran Duran perhaps. Moetar are a pop band with a unique sound that blends jazzy female vocals and prog rock instrumentation into some very commercial sounding songs. They don't sound like Muse, but if you like Muse I think you will like Moetar. Their second album has been recorded and is due for release soon. Cash Pony have played with Moetar several times, and I bought their CD at a gig, then just kept playing it. It's an interesting mix of instruments, with a bass sax and an electric sitar on some songs. Take a listen to a live version of 1000 Layer Curry Dawg's \"Carpal Tunnel Vision Quest\" The band has a very smooth sound on the CD, but I can't figure out what to compare it to. MirthKon and Secret Chiefs 3 are playing a gig in San Francisco on Feb 15th. Not yet sure if I can make it to attend. If you like Frank Zappa you'll like Mirthkon, and you should give them a listen. \" gn: center;\"> \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0. gn:center; gn: right; Gn: left; gn : center; gn.: right; gn. gn : left. Gn: right, gn: left, gn.gn: center, gn.: center; Gn. gn.: left,gn: right. Gn. Gn.: center, Gn.gn, Gn.: right, Gn: center,. gn:left,gn. gn, gn : right. gn; gn = \"true\"; gn: fullscreen; gn= \"false\"; gn. fullscreen: true; gn? fullScreen: false; gn?: \"false\", gn: \"false,\" gn: true, gn? \"true\" : \"false"
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_bf5f1048",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "What&#39;s on my iPhone: apps I don&#39;t use",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/01/what-on-my-iphone-apps-i-don-use.html",
      "content": {
        "text": "Last.fm, VNC, Remote, Weatherbug. I installed these but never use them. I tend to listen to Pandora, don't need a remote console, find the Apple remote inconvenient to start up and prefer other weather apps. <br/><div class=\"iblogger-footer\"><br clear=\"all\"/><p style=\"text-align:right;font-size:10px;\">[Posted with <a href=\"http://illuminex.com/iBlogger/index.html\">iBlogger</a>]</p><br/></div><br />",
        "metadata": {
          "word_count": 43,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Last.fm, VNC, Remote, Weatherbug. I installed these but never use them. I tend to listen to Pandora, don't need a remote console, find the Apple remote inconvenient to start up and prefer other weather apps."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_645cdf0b",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Performance Testing of NetflixOSS for the Cloud Prize",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/09/performance-testing-of-netflixoss-for.html",
      "content": {
        "text": "<br />\nWhere is everyone? I've been writing blogging and tweeting about performance and capacity planning for many years, so I thought I was connected to one or two performance folks....<br />\n<br />\nWhen we setup the <a href=\"https://github.com/Netflix/Cloud-Prize\" target=\"_blank\">NetflixOSS cloud prize</a> I created a category just for you:<br />\n<br />\n<table cellpadding=\"0\" cellspacing=\"0\" dir=\"ltr\" style=\"font-family: arial,sans,sans-serif; font-size: 13px; table-layout: fixed;\"><colgroup><col width=\"362\"></col></colgroup><tbody>\n<tr style=\"height: 17px;\"><td style=\"border-color: rgb(0, 0, 0) rgb(204, 204, 204) rgb(204, 204, 204); border-style: solid; border-width: 1px; direction: ltr; padding: 0px 3px; text-align: left; vertical-align: bottom;\">Best Contribution to Performance Improvements</td></tr>\n<tr style=\"height: 17px;\"><td style=\"border-bottom: 1px solid #ccc; border-left: 1px solid #ccc; border-right: 1px solid #ccc; padding: 0 3px; vertical-align: bottom;\"></td></tr>\n<tr style=\"height: 17px;\"><td style=\"border-bottom-color: rgb(0, 0, 0); border-bottom-style: solid; border-bottom-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; border-left-width: 1px; border-right-color: rgb(204, 204, 204); border-right-style: solid; border-right-width: 1px; direction: ltr; padding: 0px 3px; text-align: left; vertical-align: bottom;\">New performance testing tools, results of performance tests and performance oriented bugs. Since this may be aggregated across many pull requests, they should be listed and documented in the Submission.</td></tr>\n</tbody></table>\n<br />\nWhile there are some interesting contestants in other categories, I've seen very little activity in this one.<br />\n<br />\nThere's $10K cash and $5K of AWS credits waiting for someone to do enough work to claim them!<br />\n<br />\nIt takes a bit of resource to run tests, so I also have a $100 AWS gift card that I will pass on to the first person to make an entry in this category, regardless of whether they win the prize or have generated any useful data at that point. Make a valid submission to the Cloud Prize by forking the github repo, editing the Submission.md file to describe your entry, entering your details (email/address etc) in the Mailchimp form, and then post a comment here claiming the $100 AWS card, and I'll mail it to you.<br />\n<br />\nAfter the first one, if you make a case for having generated lots of good data, I'll get some more $100 AWS cards.<br />\n<br />\nHere's some performance project ideas.<br />\n<br />\n<ul>\n<li>Run throughput and latency stress tests on the Zuul API gateway on various instance types</li>\n<li>Run stress tests for sizing data against other service level projects - Eureka, Edda, Ice, etc.</li>\n<li>Make the cass_jmeter project better in some way</li>\n<li>Analyze the code paths in Astyanax and find ways to make it run faster and use less memory</li>\n</ul>\n<div>\nGo for it, only a few days left. Contest ends at midnight Pacific time Sunday Sept 15th.</div>\n<div>\n<br /></div>\n<div>\nBest wishes</div>\n<div>\nAdrian</div>",
        "metadata": {
          "word_count": 442,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "When we setup the. NetflixOSS cloud prize, I created a category just for you: Performance Improvements. I've been writing blogging and tweeting about performance and capacity planning for many years, so I thought I was connected to one or two performance folks. There's $10K cash and $5K of AWS credits waiting for someone to do enough work to claim them! I also have a $100 AWS gift card that I will pass on to the first person to make an entry in this category. Make a valid submission to the Cloud Prize by forking the github repo, editing the Submission.md file to describe your entry, entering your details (email/address etc) in the Mailchimp form. Then post a comment here claiming the $100 AWS card, and I'll mail it to you. Adrian. div>Adrian </div> Adrian.div> Adrian. div\u00a0Adrian\u00a0div\u00a0=\"Adrian\" Adrian.div\u00a0\"Adrian\", \"Adrian,\" \"Adrians\" Adrian, \"Adria\", \"Arian\" Adria, \"Arians\" Adrian,  \"Aries\", \"Cyrus\", \"Dorothy\", \"Gloria\", \"Hercules\", \"Johannes\""
      },
      "tags": [
        "aws",
        "cloud",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_file_11b6cbab",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "ps3 Marketplace Research on eBay",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/11/ps3-marketplace-research-on-ebay.html",
      "content": {
        "text": "<a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"http://photos1.blogger.com/x/blogger2/3864/907/1600/697839/Picture%203.png\"><img style=\"margin: 0pt 10px 10px 0pt; float: left; cursor: pointer;\" src=\"http://photos1.blogger.com/x/blogger2/3864/907/320/106457/Picture%203.png\" alt=\"\" border=\"0\" /></a><br />Over at <a href=\"http://datamining.typepad.com/data_mining/2006/11/playstation_3_b.html\">Data Mining</a> there is some interesting info on ps3's.<br /><br />However,  there is no need to do manual scraping of<br />eBay, here is a screenshot from the marketplace research function<br />that is bundled with my eBay store subscription. For $2.99 for 2 days<br />access anyone can get at this.<br /><br /><a onclick=\"return top.js.OpenExtLink(window,event,this)\" href=\"http://pages.ebay.com/marketplace_research/\" target=\"_blank\">http://pages.ebay.com/marketpl<wbr>ace_research/</a>",
        "metadata": {
          "word_count": 77,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Over at Data Mining there is some interesting info on ps3's. There is no need to do manual scraping of eBay, here is a screenshot from the marketplace research function. For $2.99 for 2 days anyone can get at this."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_0c69b5f5",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Joost and The Venice Project Beta Testing",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/01/joost-and-venice-project-beta-testing.html",
      "content": {
        "text": "I signed up for TVP's beta program a month or two ago, and got accepted recently, so I've been playing with it. A few days ago they launched under the name Joost, which I find amusing since I have a very good Dutch friend called Joost (pronounced yoast) and I'm not sure if I'm supposed to say it as yoast or juiced....<br /><br />I've had a TiVo for years now, and Joost has a very nice user interface that works for me like a TiVo that I can run anywhere, and which has tens of terabytes of stored programs, not just the 30GB or whatever that I have inside my TiVo. The other thing that is better than TiVo is the finding experience. I was able to type \"Lotus\" into a search box, and find two episodes from \"5th Gear\" where they were track testing my favorite car. The content is largely European TV at present, and they have enough programs to keep beta testers happy for a while, but are going to need a lot more as the program launches.<br /><br />The display quality is good, and there is a short delay when a program is first selected, especially if its one of the more obscure programs. The networking is something like a securely encrypted in-order bittorrent. You don't have to wait for the whole program to load before you start watching it.<br /><br />I posted last year on disruptive innovation as it applies to the moving pictures industry, discussing <a href=\"http://perfcap.blogspot.com/2006/06/part-1-disruptive-innovation-in-path.html\">the concept</a> of a maturity model for innovation, <a href=\"http://perfcap.blogspot.com/2006/06/part-2-moving-pictures-disruptive.html\">evolution</a> from the cinema to thepiratebay, and a more <a href=\"http://perfcap.blogspot.com/2006/06/part-3-disruptive-innovation-viewed-as.html\">abstract</a> maturity model. In the final part I made this statement:<br /><blockquote>The final phase in the evolution of a market occurs as the cost of replication and distribution of the product approaches zero. There is no end user cost to use the Internet to download a movie. A central utility such as YouTube can use a mixture of advertising and premium services (for a minority of power users) to offset their own costs. Peer to peer systems distribute the load so that there is no central site and no incremental cost in the system. The only service that is needed is some kind of search, so that peers can find each other's content to exchange it.</blockquote>I was talking about how bittorrent based video is inevitably going to dominate over centralized services like YouTube, and there have been reports that most of the traffic on the internet is bittorrent. The inconvenience of bittorrent is that it is basically an overnight batch job to get a program. Joost fixes the problems of bittorrent, while staying as close as possible to free distribution. Joost inserts a small number of short adverts that the Joost client figures out how to target to your interests. These are intended to be enough to pay for the central seeding servers, and to pay the content owners so good programs become available, without becoming intrusive enough to switch off the users.<br /><br />Based on my own maturity model, Joost is nicely setup to be the end game for this market. That doesn't mean that we stop going to the cinema or watching YouTube, just that Joost could end up being \"bittorrent for the masses\", and  be an order of magnitude bigger than everything else.",
        "metadata": {
          "word_count": 557,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "TVP launched under the name Joost, which I find amusing since I have a very good Dutch friend called Joost (pronounced yoast) The other thing that is better than TiVo is the finding experience. The content is largely European TV at present, and they have enough programs to keep beta testers happy for a while. There is no end user cost to use the Internet to download a movie. A central utility such as Y. is first selected, especially if its one of the more obscure programs. The networking is something like a securely encrypted in-order bittorrent. OuTube can use a mixture of advertising and premium services to offset their own costs. Peer to peer systems distribute the load so that there is no central site and no incremental cost in the system. The only service that is needed is some kind of search, so that peers can find each other's content to exchange it. Based on my own maturity model, Joost is nicely setup to be the end game for this market. That doesn't mean that we stop going to the cinema or watching YouTube, just that Joost could end up being \"bittorrent for the masses\""
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_7c85d01f",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage - Working on Saturday",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/03/garage-working-on-saturday.html",
      "content": {
        "text": "The concrete team were back today, putting in more rebar and tidying everything up. Chris Ramoutar the Electrician was also here putting in a 2-inch pipe to carry power to the garage. I'm having a high spec 220V and 200Amps dedicated circuit configured so that it's ready to recharge electric cars in the future. I got a work-out clearing up the felled fir tree and chipping the small branches, and collecting large rocks that had been dug up, and arranging them around the pond.<br /><br />Here's a view from the deck, you can see the apron on the right, and the garage foundation on the left, and Chris working in the trench that takes the power line.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXJo5-yUFKpjise3OLGEuQsCbNFWNkH3uEOwUEfhfY0gr3JA_IbJ3DWXYE9m7NZ4NmATOG9QLgcW9JIRw5hyphenhyphenXu6LzoOPKD2BI4qLFYqKCKO8OnujxliJwwtsWKnLinjMXs_mqU/s1600/deck-rebar.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXJo5-yUFKpjise3OLGEuQsCbNFWNkH3uEOwUEfhfY0gr3JA_IbJ3DWXYE9m7NZ4NmATOG9QLgcW9JIRw5hyphenhyphenXu6LzoOPKD2BI4qLFYqKCKO8OnujxliJwwtsWKnLinjMXs_mqU/s320/deck-rebar.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5453539962304368658\" /></a><br /><br />Here's a view from where the driveway will be as it joins the apron, you can see the walkway to the left of the garage.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJtLPSOCzuqfRpcI937cemhY05jwtzxD3GfKWJp1D1gsjWxRpPx5oDRNpAwfyiO_hjFlGAh8b9gZgI8eObgsi6AlhSD8XMHN17ZkdNrveKvxi8kR52eSOw6qrT81PW9hF5CVLd/s1600/drive-rebar.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiJtLPSOCzuqfRpcI937cemhY05jwtzxD3GfKWJp1D1gsjWxRpPx5oDRNpAwfyiO_hjFlGAh8b9gZgI8eObgsi6AlhSD8XMHN17ZkdNrveKvxi8kR52eSOw6qrT81PW9hF5CVLd/s320/drive-rebar.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5453539970196477474\" /></a><br /><br />Later that evening some very confused deer wandered over the foundations, since it has been built over their usual path from the forest out to the front yard.<br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgibnDn6Zn0QOcxi3balbX218y5snUAQ02ZVlWZ0eSA7DbQQcI-tO2MIiwxpToA6wFHHUXcY5YJe0dN8gjyKuf0s4NxjYGWuZCtzVAbvXZCCYeBwOXZhssk-TAf73b0zhwU9m26/s1600/deer-rebar.jpg\"><img style=\"cursor:pointer; cursor:hand;width: 320px; height: 239px;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgibnDn6Zn0QOcxi3balbX218y5snUAQ02ZVlWZ0eSA7DbQQcI-tO2MIiwxpToA6wFHHUXcY5YJe0dN8gjyKuf0s4NxjYGWuZCtzVAbvXZCCYeBwOXZhssk-TAf73b0zhwU9m26/s320/deer-rebar.jpg\" border=\"0\" alt=\"\"id=\"BLOGGER_PHOTO_ID_5453539965465413058\" /></a>",
        "metadata": {
          "word_count": 220,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The concrete team were back today, putting in more rebar and tidying everything up. Chris Ramoutar the Electrician was also here putting in a 2-inch pipe to carry power to the garage. I got a work-out clearing up the felled fir tree and chipping the small branches. Here's a view from where the driveway will be as it joins the apron. You can see the walkway to the left of the garage. Deer wandered over the foundations, since it has been built over their usual path from the forest out to the front yard. Later that evening some very confused deer wandered over it."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_343f2974",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "FLOW - a homebrew Android gumstix phone (finally)",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/05/flow-homebrew-android-gumstix-phone.html",
      "content": {
        "text": "<a href=\"http://www.gizmoforyou.com/e107_plugins/content/content.php?content.32\">See more here</a>.<br /><br />A few years ago the Silicon Valley Homebrew Mobile Phone Club (SVHMPC) formed out of frustration with the closed market for phones, and set out to build something we could program in our pocket. We made some progress but the prototype Gumstix Goliath board never made it into production, and along came the iPhone, which gave some of us the application platform we were looking for to customize. The OpenMoko project is also limping along, and Google Android is finally out there in volume. The FLOW project is open hardware running Android with a Gumstix CPU module. Nice combination for making custom devices.",
        "metadata": {
          "word_count": 108,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The FLOW project is open hardware running Android with a Gumstix CPU module. Nice combination for making custom devices. OpenMoko project is also limping along, and Google Android is finally out there in volume."
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_dde22a9a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "iPhone Copilot GPS navigator",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/10/iphone-copilot-gps-navigator.html",
      "content": {
        "text": "this excellent app is a turn by turn talking navigator that works like a dedicated Garmin or Tomtom unit but it cost $35, with no additional subscription charges, and is amazing value. It's fast, has a very clear  display and easy user interface. To mount and power it in the car I got a Belkin TuneBase FM ($89 at Frys) that plugs very firmly into the lighter socket and has a gooseneck that lets you adjust the position. The iPhone clamp works with a case fitted, and I was able to tune the car FM to hear the turn by turn directions along with the music playing. There is an output jack if you have an aux input socket on your car stereo, and a USB to charge a second iPhone on long trips or whatever else you have.",
        "metadata": {
          "word_count": 139,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "It's fast, has a very clear  display and easy user interface. To mount and power it in the car I got a Belkin TuneBase FM. The iPhone clamp works with a case fitted."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_3ce0cc00",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Garage - Pouring and Finishing the Foundation",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2010/04/garage-pouring-and-finishing-foundation.html",
      "content": {
        "text": "Last Friday we poured concrete, and then it poured rain. Here's a stop-motion video of the work. Six concrete trucks came, we are high on the mountain, so they may not have been carrying a full load each time. The foundation is very deep on the downhill side, and the slab is 6\" deep rather than the 4\" minimum used for the walkway down the side and the parking apron.<br /><br />The first few seconds of each video look messy, this was introduced when youtube encoded it, my original video is fine.<br /><br /><object width=\"420\" height=\"320\"><param name=\"movie\" value=\"http://www.youtube.com/v/m1rKRilg5BQ&hl=en_US&fs=1&\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/m1rKRilg5BQ&hl=en_US&fs=1&\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"320\"></embed></object><br /><br />On Saturday, the weather was better, and the team came back to finish the surface and tidy up.<br /><br /><object width=\"420\" height=\"320\"><param name=\"movie\" value=\"http://www.youtube.com/v/YAmjBGAV5q8&hl=en_US&fs=1&\"></param><param name=\"allowFullScreen\" value=\"true\"></param><param name=\"allowscriptaccess\" value=\"always\"></param><embed src=\"http://www.youtube.com/v/YAmjBGAV5q8&hl=en_US&fs=1&\" type=\"application/x-shockwave-flash\" allowscriptaccess=\"always\" allowfullscreen=\"true\" width=\"420\" height=\"320\"></embed></object><br /><br />Final bill was about $25K, and I would recommend Amaya Concrete of Morgan Hill, they did a good job.<br /><br />This week, we let the concrete cure, wait for the ground to dry out and do some basic grading of the ground levels and slopes. Next week the building will be delivered and installed.",
        "metadata": {
          "word_count": 198,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "Last Friday we poured concrete, and then it poured rain. Here's a stop-motion video of the work. The first few seconds of each video look messy, this was introduced when youtube encoded it, my original video is fine. This week, we let the concrete cure, wait for the ground to dry out and do some basic grading of the ground levels and slopes. Next week the building will be delivered and installed."
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_file_b118a72a",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Extracting useful exacct process data",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2005/03/extracting-useful-exacct-process-data.html",
      "content": {
        "text": "I modified the /usr/demo/libexacct/exdump.c example code to include the data structures described in previous posts, and made the code update each item in the structure. Then I added a printf from the data structure so I could check that all the data is being captured correctly. Part of the output is shown below. I hand edited the column headers a bit for the first two lines to make them line up.<br /><br /><pre><br /><span style=\"font-size:85%;\">procid ppid uid  usr  sys time      majf minf rwKB     vcxK   icxK sigK sycK  arMB mrMB command<br />1693  1679 100 40.23 3.53 416930.11 237    0 58745.32 261.17 4.22 0.00 801.7 38.4 44.6 mozilla-bin<br />procid ppid uid  usr  sys time      majf minf rwKB    vcxK icxK sigK sycK arMB mrMB command<br />1679  1647 100 0.00 0.00 416930.16    0    0 15.11   0.02 0.00 0.00  0.6  0.6 26.4 run-mozilla.sh<br />procid ppid uid  usr  sys time      majf minf rwKB vcxK icxK sigK sycK arMB mrMB command<br />1647  1646 100 0.00 0.01 416930.30    0    0 8.29 0.03 0.00 0.00  0.8  0.4 26.4 mozilla</span><br /></pre><br /><br />Running the modified exdump with the -v option shows the data objects in detail so I can check that the right data is appearing in the right place. Here is the same data as the second line above. It looks as if the minor faults counter really is zero, which is probably a Solaris bug.<br /><br /><pre><br /><span style=\"font-size:85%;\">   100  group-proc                      [group of 35 object(s)]<br /> 1000   pid                            1679<br /> 1001   uid                            100<br /> 1002   gid                            10             staff<br /> 1004   projid                         10             group.staff<br /> 1003   taskid                         54<br /> 100b   cpu-user-sec                   0<br /> 100c   cpu-user-nsec                  2861447<br /> 100d   cpu-sys-sec                    0<br /> 100e   cpu-sys-nsec                   4150875<br /> 1007   start-sec                      1109810711     03/02/05 16:45:11<br /> 1008   start-nsec                     813223175<br /> 1009   finish-sec                     1110227641     03/07/05 12:34:01<br /> 100a   finish-nsec                    968607249<br /> 1006   command                        \"run-mozilla.sh\"<br /> 100f   tty-major                      4294967295<br /> 1010   tty-minor                      4294967295<br /> 1005   hostname                       \"crun\"<br /> 1011   faults-major                   0<br /> 1012   faults-minor                   0<br /> 1014   msgs-snd                       0<br /> 1013   msgs-recv                      0<br /> 1015   blocks-in                      0<br /> 1016   blocks-out                     0<br /> 1017   chars-rdwr                     15105<br /> 1018   ctxt-vol                       21<br /> 1019   ctxt-inv                       0<br /> 101a   signals                        0<br /> 101b   swaps                          0<br /> 101c   syscalls                       586<br /> 101d   acctflags                      0<br /> 101f   ppid                           1647<br /> 1020   wait-status                    0              exit<br /> 1021   zone                           \"global\"<br /> 1022   memory-rss-avg-k               584<br /> 1023   memory-rss-max-k               27004<br />procid ppid uid  usr  sys time      majf minf rwKB  vcxK icxK sigK sycK arMB mrMB command<br />1679  1647 100 0.00 0.00 416930.16    0    0 15.11 0.02 0.00 0.00  0.6  0.6 26.4 run-mozilla.sh</span><br /></pre>",
        "metadata": {
          "word_count": 417,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I modified the /usr/demo/libexacct/exdump.c example code to include the data structures described in previous posts. I added a printf from the data structure so I could check that all the data is being captured correctly. Part of the output is shown below. Running the modified exdump with the -v option shows the data objects in detail so I can check that the right data is appearing in the right place. It looks as if the minor faults counter really is zero, which is probably a Solaris bug. 1009   finish-sec    1110227641   03/07/05 12:34:01 <br /> 1006   command  \u00a0 \"run-mozilla.sh\"<br /> 1011   faults-major \u00a0 \u00a0 4294967295\u00a0\u00a0\u00a0 1010   tty-minor \u00a0\u00a0 429 4967295 \u00a0 1013   msgs-recv   \u201chttp://www.mozilla.org/mozilla/msgs/snd\"\u00a0   1014  msgs-snd  \ufffd\u00a0 \u201c http://msgs.msg.com/msg/s procid ppid uid  usr sys time    majf minf rwKB  vcxK icxK sigK sycK arMB mrMB command. 1679  1647 100 0.00 0.000 416930.16 0.11 0.02 0.0 0.6 26.4 run-mozilla.sh."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_432e95c4",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "A. A. Michelson Award Acceptance Speech",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/12/a-michelson-award-acceptance-speech.html",
      "content": {
        "text": "<div>Last week <a href=\"http://perfcap.blogspot.com/2007/12/cmg07-and-a-michelson-award.html\">I was selected as this year's winner of the A. A. Michelson Award</a> for Lifetime Achievement and Contribution by the Computer Measurement Group. Here is a transcript of my acceptance speech.</div><div><br /></div><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6SeP8VO6BtwaFtMXptvscbrxNXOtUbGcpsM0eHOAHiK7p8oxWhGdaTPMiFNF81iQUSJUSLnfbJCvkLPCmPOmYrwrfUDF365Fr8Y6n0A39UKNSYa1jwZH5mT1ppWaszwAwR-UR/s1600-h/Michelson.001.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg6SeP8VO6BtwaFtMXptvscbrxNXOtUbGcpsM0eHOAHiK7p8oxWhGdaTPMiFNF81iQUSJUSLnfbJCvkLPCmPOmYrwrfUDF365Fr8Y6n0A39UKNSYa1jwZH5mT1ppWaszwAwR-UR/s400/Michelson.001.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142035630792127426\" /></a><br /><div>Thanks to CMG, the Michelson Award winners who voted to add me to their ranks, and everyone who I have met here since I first attended CMG in 1994. I treat the CMG conference as my annual training class, a chance to mingle with my peers, learn and share the things I have learned.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEsUR5hYOssoBFuYKostSKAP7SVFVVQ6dbNVH2HMO9-bh_ppGXeRKJo3vFDP2oBW1iyqWZfJYSoOXP1XvdBKLmQWd9EEsrpz7NQhs4ZRMOLOrR7Wpk9EogFnTyC1cDpJ-jX0iW/s1600-h/Michelson.002.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgEsUR5hYOssoBFuYKostSKAP7SVFVVQ6dbNVH2HMO9-bh_ppGXeRKJo3vFDP2oBW1iyqWZfJYSoOXP1XvdBKLmQWd9EEsrpz7NQhs4ZRMOLOrR7Wpk9EogFnTyC1cDpJ-jX0iW/s400/Michelson.002.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142035480468272050\" /></a><br /></div><div>There are some people who I have met at CMG or while working in this field who have become special friends, not just people I see at the conference each year. Cathy Nolan is not only national CMG president this year, she is president of my local CMG group in Northern California and was also instrumental in the process of getting me nominated for this award. We have all learned a lot over the years from Neil Gunther. Neil and I have jointly presented many training classes so I have had plenty of opportunity to learn his material. He has made queuing theory accessible and useful for very many people. Mario Jauvin has been my CMG conference buddy for many years, and for the last three years we have jointly presented the Capacity Planning with Free Tools workshop. Yefim (Fima) Somin encouraged me to attend CMG in the first place, we met in 1994 while working to port BGS Best/1 to Solaris, and we have kept in touch over the years, even though he doesn't get to come to CMG nowadays. Fima and Henry Newman were part of the Universal Measurement Architecture standards body that I joined in 1995, and I have also kept in touch with Henry. We worked to get some of his ideas (like extended system accounting) to be implemented in Solaris. Bob Sneed worked with me\u00a0at Sun and is now the main contact point and conscience for CMG at Sun.<div><br /></div><div>Outside CMG there are some very significant people I would also like to thank. Brian Wong is one of my closest friends, responsible for getting me to move to the USA in 1993, and is also author of the Capacity Planning and Configuration for Solaris Servers book. Brian introduced me to Rich Pettit, and we worked together for many years to build and extend the SE Toolkit, the vehicle for most of my performance measurement ideas. Allan Packer was the co-author of my first CMG paper on Database Sizing. He knows everything about database performance and wrote the book Configuring and Tuning Databases on the Solaris Platform. We borrowed Allan from Sun's office in Adelaide Australia for a month or two, and he introduced me to Richard McDougall, who we also borrowed from Adelaide. Allan and Richard both ended up working for Sun's Performance group in California. Richard built the first tools that could measure memory usage, fixed the Solaris 8 memory system, and co-authored the Solaris Internals books. He became a Sun Distinguished Engineer and recently joined VMware as Chief Performance Architect. Jim Mauro is the other author of Solaris Internals, we borrowed him from Sun's New Jersey office a few times before he also joined the performance group full time. Finally Dave Fisk worked with Brian Wong and I at Sun, and became one of my closest friends. He knows more about Unix storage performance than anyone else I've met, and I've called on him many times (he's now a consultant)\u00a0to help figure out what's really going on.</div><div><br /></div><div>I would also like to thank my family. My father is a retired Statistics Professor from the University of Hertfordshire (a.k.a. Hatfield Polytechnic) in the UK. He was programming himself in the 1960's and gently encouraged me to tinker with computers from an early age. My son recently graduated with his BSc in Computer Science from the same University, and now works as a project manager at eBay in California. My daughter has just started a degree course in European Literature at Royal Holloway College, London, so I'm no help at all with her assignments :-) \u00a0I'm very proud of them. I moved to the USA on my own a few years ago and met my lovely wife Laurel in 2003. I'm very happy that she has also been able to attend CMG with me and meet many of you.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXr7Puqm6vGCAAqqhYEzhaLsZEx4Cu5zQ65_daeNptG6g1CiOca_N0KBH_GifCt2sqCAuc6In9DniVNbrtSyD5yn3y8VrfEsrg0eF5c0t_Wr4s0h263IzUvVqu8eSJAMKc9I0s/s1600-h/Michelson.005.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXr7Puqm6vGCAAqqhYEzhaLsZEx4Cu5zQ65_daeNptG6g1CiOca_N0KBH_GifCt2sqCAuc6In9DniVNbrtSyD5yn3y8VrfEsrg0eF5c0t_Wr4s0h263IzUvVqu8eSJAMKc9I0s/s400/Michelson.005.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142035244245070754\" /></a><br /></div><div>I have always been glad that I studied Physics, the work we do in performance and the diagnosis and measurement of performance leans heavily on my training as a Physicist and I think that pure Computer Scientists are lacking some of the tools they need to understand measurement and performance issues. Since A. A. Michelson was a Physicist this award is especially gratifying for me.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhatHcJHdyboKoAgZ9SA3x8dXzDjbPHAh54iQC305ScyEU3HWp2qxOX9J6ny8ji42xjgouduqcNBLZyHCSxNtLPah7hEJmh4zjEy5eHmRujD0uWAL9_5JuPrTGlPsGYdEaMQHyJ/s1600-h/Michelson.006.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhatHcJHdyboKoAgZ9SA3x8dXzDjbPHAh54iQC305ScyEU3HWp2qxOX9J6ny8ji42xjgouduqcNBLZyHCSxNtLPah7hEJmh4zjEy5eHmRujD0uWAL9_5JuPrTGlPsGYdEaMQHyJ/s400/Michelson.006.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142034724554027922\" /></a><br /></div><div>In 1972 my High School had a link to the computer that my Father used at Hatfield Polytechnic. I taught myself BASIC and Algol during recess....</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKXDbDk4TSwguq-QBiVezcLVaNLkSXGN-_Ew1ByS4Q8rTV4V9Lg4JnIPKIZo4ggkI2pOxtK58nJfcPu-lVMICXnsCZYjADA58fT3Lh2bFxI_l0YfGrjzVsNF6JAguyAIYWnYSn/s1600-h/Michelson.007.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhKXDbDk4TSwguq-QBiVezcLVaNLkSXGN-_Ew1ByS4Q8rTV4V9Lg4JnIPKIZo4ggkI2pOxtK58nJfcPu-lVMICXnsCZYjADA58fT3Lh2bFxI_l0YfGrjzVsNF6JAguyAIYWnYSn/s400/Michelson.007.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142034247812658050\" /></a><br /></div><div>This is what most people know me for, its getting a little out of date now, since the second edition was published in 1998, but a big thank-you to my co-author Rich Pettit and everyone that bought a copy!</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC97gSmL4uMl-StTzLV3MSvIMvR5n_LvVwLH4YtiyVY2amQ55QSAX-mb76gddIZibkGvDB9oshoS0tnYAILnVEWDxkQ2fFmV95SZKWoiRjlMpnp5NBHHJHVYLhDWNv7IqgMV20/s1600-h/Michelson.008.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiC97gSmL4uMl-StTzLV3MSvIMvR5n_LvVwLH4YtiyVY2amQ55QSAX-mb76gddIZibkGvDB9oshoS0tnYAILnVEWDxkQ2fFmV95SZKWoiRjlMpnp5NBHHJHVYLhDWNv7IqgMV20/s400/Michelson.008.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142034157618344818\" /></a><br /></div><div>The book documented metrics and performance rules, and they were implemented as an SE Toolkit script called Virtual Adrian. I spent a lot of time with all the tools vendors helping them to manage Solaris better, and with Solaris engineering getting them to make Solaris more manageable. This is what justified my involvement with CMG during my time at Sun, and its the legacy that I am most proud of.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbWw5urnt0wYY3yCYQmvEu-ifx9bCPWWLMA1yIdNSYtfVDRveqqIQ8-lAPfZKyJwAWScg00szf8myKZuSsBLaJn3vOlTO2topu4zclLtiKrpFmG2yuyvCfYIQHi_Z4QqUjpAfy/s1600-h/Michelson.009.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbWw5urnt0wYY3yCYQmvEu-ifx9bCPWWLMA1yIdNSYtfVDRveqqIQ8-lAPfZKyJwAWScg00szf8myKZuSsBLaJn3vOlTO2topu4zclLtiKrpFmG2yuyvCfYIQHi_Z4QqUjpAfy/s400/Michelson.009.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142033779661222754\" /></a><br /></div><div>To finish, some of the things I have learned over the years that you may find useful...</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigN9oW6uLVTbnLUG7SGEV4EBoMG3U3jVBWDF9NOs61-L3YtG4FmxrO6AaW0YbCabOmBodnKg825PWVMQmd3vZFu2md9qA2_V9sidGY9HHbLtGaOdLNYGL3koTAl8Ekyzx32Q1w/s1600-h/Michelson.010.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEigN9oW6uLVTbnLUG7SGEV4EBoMG3U3jVBWDF9NOs61-L3YtG4FmxrO6AaW0YbCabOmBodnKg825PWVMQmd3vZFu2md9qA2_V9sidGY9HHbLtGaOdLNYGL3koTAl8Ekyzx32Q1w/s400/Michelson.010.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142033693761876818\" /></a><br /></div><div>If you ever try to write a book, you need to pay attention to Cockcroft's Law of Book Writing. It seems counter-intuitive to people who have never attempted to write a book, but it is the most important thing to know if you ever intend to finish a book. For example, you scope a 200 page book and start writing. After you have written 50 pages you re-scope and find your book is now a 300 page book. You write another 50 pages and find you are now writing a 400 page book. The number of pages you have left to write to finish the book is increasing! You have to reduce scope continuously and plot a trend line that slopes downwards if you ever hope to be published.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsGcGvqU3xbHU1Tiyuhf6iPupANVtJtOMz2xoM2LzGBFyNITmlrPp2UFus3QWk_yAtcS1a4fpidrzixSgN8uFqc39rIurHK98tq-7HJPHULKZEBj38pqiY0hfzSDTexizOFZEm/s1600-h/Michelson.011.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhsGcGvqU3xbHU1Tiyuhf6iPupANVtJtOMz2xoM2LzGBFyNITmlrPp2UFus3QWk_yAtcS1a4fpidrzixSgN8uFqc39rIurHK98tq-7HJPHULKZEBj38pqiY0hfzSDTexizOFZEm/s400/Michelson.011.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142033281445016386\" /></a><br /></div><div>If you collect information and share it freely, you will become an expert. You become a magnet for questions, issues and information. Some people tightly control their expertise, this is a huge mistake as it gives others incentives to look for alternative experts or to become experts themselves. If you give out your expertise freely, you become the go-to person in your field, and gather many more recommendations from the people you help. Try it!</div><div><br /></div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtE9A7W3XtxV0XUlf4ZzM2_8MFGXqRFlQDT7ruRKiZFBOhQHM-PHXKcPVg-F5L6RZOh_ljnc-VX7E0p7qoqgDcce62kTy-wp_2e-r67X2NhxcmJgQoW1JFjjg-Gue2lHAFcbcl/s1600-h/Michelson.012.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjtE9A7W3XtxV0XUlf4ZzM2_8MFGXqRFlQDT7ruRKiZFBOhQHM-PHXKcPVg-F5L6RZOh_ljnc-VX7E0p7qoqgDcce62kTy-wp_2e-r67X2NhxcmJgQoW1JFjjg-Gue2lHAFcbcl/s400/Michelson.012.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142033053811749682\" /></a><br /></div><div>Some observations that may help you deal with executive level management. The first one is a big problem that Sun and other manufacturers have, because the things they build take many years to go from conception to delivery. If the managers in charge of these projects don't stick around then the project is likely to be tinkered with and delayed further or dropped. Its nice being at Netflix, where the executives have been there for years and the projects take months to deliver. The second observation is that Capacity Planners have a vertical role in the organization, they gather the lowest level metrics from operations and have to present findings to upper management. We love our detailed metrics, but the only metric that upper management should have to deal with is the dollar. If you can present everything in terms of return on investment (spend $100K to save or grow by $1M) then your executive presentations will be much more effective.</div><div><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9MQvk6gArdbjPSfGaXZYdGlcrNYZtb965mux0k7uDSmklPStrX-zenHhAAHVPEP6z0A2PzmmggJkfOgwNNB04j6QlWRE5izSdGxqHTWDj6qP1Te2-iV4gljOpnp69T4AwVJuT/s1600-h/Michelson.013.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9MQvk6gArdbjPSfGaXZYdGlcrNYZtb965mux0k7uDSmklPStrX-zenHhAAHVPEP6z0A2PzmmggJkfOgwNNB04j6QlWRE5izSdGxqHTWDj6qP1Te2-iV4gljOpnp69T4AwVJuT/s400/Michelson.013.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142032718804300578\" /></a><br /></div><div>What am I working on now? I'm managing a team that develops the Netflix web site. Its a personalized web service and I think that is the future of the computer industry. The scalability and performance challenges are very interesting. I'm also pushing an idea I call \"Millicomputing\" which could have a disruptive effect on the industry as storage moves from disks (spinning rust) to flash, and the computers move to ultra low power system-on-a-chip devices. I gave a paper on Millicomputing at CMG07 and <a href=\"http://www.millicomputing.com/\">have a separate blog on the subject</a>.</div><div><br /><br /><a onblur=\"try {parent.deselectBloggerImageGracefully();} catch(e) {}\" href=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYzLO70EARRIPsOeCxRXpmtFu1jH7s3u0aPxgjoD8_UYhoWjq3HWrK4jGo3AZ-zQy1ZYmAthbrJ46hucRq6bs45f12Gix5Mm5ImS6Y9wKpVf_Dj3zmmFGQc3mm7zg9FkyJwRn/s1600-h/Michelson.014.jpg\"><img style=\"display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;\" src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEirYzLO70EARRIPsOeCxRXpmtFu1jH7s3u0aPxgjoD8_UYhoWjq3HWrK4jGo3AZ-zQy1ZYmAthbrJ46hucRq6bs45f12Gix5Mm5ImS6Y9wKpVf_Dj3zmmFGQc3mm7zg9FkyJwRn/s400/Michelson.014.jpg\" border=\"0\" alt=\"\" id=\"BLOGGER_PHOTO_ID_5142032353732080402\" /></a></div><div><br /></div><div>My paper at last year's CMG was called \"Utilization is Virtually Useless as a Metric\", and this answer sums it up. Those who ask questions about utilization don't understand that their questions have no meaning so the answers are irrelevant :-)</div><div><br /></div><div>Thanks again!</div><div><br /></div><div>Adrian<br /><br /></div></div>",
        "metadata": {
          "word_count": 1661,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "I was selected as this year's winner of the A. A. Michelson Award for Lifetime Achievement and Contribution by the Computer Measurement Group. Here is a transcript of my acceptance speech. I treat the CMG conference as my annual training class, a chance to mingle with my peers, learn and share the things I have learned. ichelson Award winners who voted to add me to their ranks, and everyone who I have met here since I first attended CMG in 1994. Cathy Nolan is not only national CMG president this year, she is president of my local CMG group in Northern California. Yefim (Fima) Somin encouraged me to attend CMG in the first place. Mario Jauvin has been my CMG conference buddy for many years. Bob Sneed worked with me at Sun and is now the main contact point and conscience for CMG at Sun. Brian Wong is one of my closest friends, responsible for getting me to move to the USA in 1993. Allan Packer was the co-author of my first CMG paper on Database Sizing. I would also like to thank my family. My father is a retired Statistics Professor from the University of Hertfordshire (a.k.a. Hatfield Polytechnic) in the UK. Michelson's son recently graduated with his BSc in Computer Science. He now works as a project manager at eBay in California. His daughter has just started a degree course in European Literature. I have always been glad that I studied Physics, the work we do in performance and the diagnosis and measurement of performance leans heavily on my training as a Physicist. I think that pure Computer Scientists are lacking some of the tools they need to understand measurement and performance issues. Since A. A. Michelson was aphysicist this award is especially gratifying for me. In 1972 my High School had a link to the computer that my Father used at Hatfield Polytechnic. I taught myself BASIC and Algol during recess. This is what most people know me for, its getting a little out of date now, since the second edition was published in 1998. A big thank-you to my co-author Rich Pettit and everyone that bought a copy! The book documented metrics and performance rules, and they were implemented as an SE Toolkit script called Virtual Adrian. I spent a lot of time with all the tools vendors helping them to manage Solaris better. This is what justified my involvement with CMG during my time at Sun, and its the legacy that I am most proud of. If you ever try to write a book, you need to pay. If you ever want to know how to do something, ask a friend. Cockcroft's Law of Book Writing. It is the most important thing to know if you ever intend to finish a book. You have to reduce scope continuously and plot a trend line that slopes downwards. If you collect information and share it freely, you will become an expert. You become a magnet for questions, issues and information. Some people tightly control their expertise, this is a huge mistake as it gives others incentives to look for alternative experts or to become experts themselves. Some observations that may help you deal with executive level management. The things they build take many years to go from conception to delivery. Capacity Planners have a vertical role i The only metric that upper management should have to deal with is the dollar. Present everything in terms of return on investment (spend $100K to save or grow by $1M) Then your executive presentations will be much more effective. I'm managing a team that develops the Netflix web site. Its a personalized web service and I think that is the future of the computer industry. I'm also pushing an idea I call \"Millicomputing\" which could have a disruptive effect on the industry. My paper at last year's CMG was called \"Utilization is Virtually Useless as a Metric\" Those who ask questions about utilization don't understand that their questions have no meaning so the answers are irrelevant."
      },
      "tags": [
        "architecture",
        "engineering",
        "metrics",
        "netflix",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_file_1dd11206",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Computer Measurement Group - CMG07 - San Diego - December 2-7",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2007/11/computer-measurement-group-cmg07-san.html",
      "content": {
        "text": "As usual I will be presenting at <a href=\"http://www.cmg.org\">CMG07</a>. I have attended every year since 1994, and its the place where I keep track of new developments in the performance management tools industry, as well as learning and sharing with my peers.<br /><br />CMG moves around the US, last year it was in Reno NV, before that in Orlando FL.<br /><br />On Sunday 2nd December there is a workshop day. I'm co-presenting (with Mario Jauvin) a half day tutorial on Capacity Planning and Performance Monitoring with Free Tools (for the third year). We updated our slides a bit more this year to track the latest versions of the free tools.<br /><br />During the week I'm giving a new paper on Millicomputing, that introduces the concepts I have been discussing on my <a href=\"http://www.millicomputing.com/\">Millicomputing blog</a>, and talks about some of the management challenges of managing large numbers of small computers.<br /><br />I'm also giving a half day training class on Unix/Linux Performance Management, which is based on the materials I developed over many years in my books and talks. It is still somewhat biased towards Solaris since I know that better, but I have been adding more on Linux and AIX as that is the world I currently live in at work (Netflix runs RedHat Linux and AIX on IBM P-series) and in my Millicomputing sideline (I run Ubuntu on my development host and linux 2.6 on my gumstix).<br /><br />The conference contains a wide range of content, there is:<br /><ul id=\"\"><li>a large vendor exhibition showing the latest tools, vendor presentations and user group meetings to talk about their next steps.<br /></li><li>everything from beginner level introductions to deep mathematical theory.</li><li>coverage of networking, storage, CPUs, application level performance.<br /></li><li>discussion of Linux, Solaris, AIX, Windows, Mainframe and all combinations</li><li>panel discussions on virtualization, service oriented architectures, scaling large sites</li></ul><div>I hope to see some of my readers there. I used to give training classes for Sun on a regular basis, and this is now the main venue for me to present updates versions of those classes.</div><div><br /></div>",
        "metadata": {
          "word_count": 342,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "As usual I will be presenting at <a href=\"http://www.cmg.org\">CMG07</a>. I have attended every year since 1994, and its the place where I keep track of new developments in the performance management tools industry. The conference contains a wide range of content, there is. a large vendor exhibition showing the latest tools, vendor presentations and. user group meetings to talk about their next steps. Everything from beginner level introductions to deep mathematical theory. Panel discussions on virtualization, service oriented architectures, scaling large sites. I used to give training classes for Sun on a regular basis, and this is now the main venue for me to present updates versions of those classes. re."
      },
      "tags": [
        "monitoring",
        "netflix",
        "performance",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_file_31b53102",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Netflix App Gallery",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2009/05/netflix-app-gallery.html",
      "content": {
        "text": "The Netflix API now has an <a href=\"http://www.netflix.com/AppGallery\">App Gallery http://www.netflix.com/AppGallery</a> which features my Instant Queue Add for Netflix iPhone app.",
        "metadata": {
          "word_count": 20,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The Netflix API now has an App Gallery. This features my Instant Queue Add for Netflix iPhone app. The Netflix API is available at: http://www.netflix.com/AppGallery."
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_3a1ff94d",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Web vs. Skype, a paradigm shift",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2006/08/web-vs-skype-paradigm-shift.html",
      "content": {
        "text": "<div xmlns=\"http://www.w3.org/1999/xhtml\"><p>The essential characteristics of the http based web are that by default everyone is anonymous, and everyone can get to everything. Its \"free\" and the trend is for the parts that are not free and anonymous to move in that direction. For example, you can now buy stuff on <a href=\"http://www.ebayexpress.com\">eBay Express</a> without having to sign up for an eBay account, and there are fewer newspaper sites requiring paid subscriptions, since they are losing audience to the free sites.</p>  <p>However, the essential characteristics of the Skype peer to peer network are the opposite of the Internet. Everyone has a clear identity and no-one can get to anything without asking for permission or being invited. I think this truly a different paradigm for building systems.<br/> </p>  <p>Everyone on Skype is plugged into the public key encryption infrastructure (PKI) which provides a secure identity as well as secure communications between peers. However, to communicate with other peers you need to know them and have permission. For me the most interesting capability on Skype is the <a href=\"https://developer.skype.com/Docs\">application to application messaging API (ap2ap)</a> that enables a <a href=\"http://useetoo.com\">new class of distributed applications</a> that leverage the social network formed by the mesh of Skype contact lists.<br/> </p>  <p>The upshot of this is that some things that are easy on the Internet are difficult on Skype, and vice versa. There is a temptation to take something that we know works on the web, and try to make something similar on Skype ap2ap, but that is pointless, just use the web! Look for things that really don't work well on the web, or look for web-based systems that connect a few people but need an expensive back-end or don't scale. This is the start of something interesting....<br/> </p>  <p style=\"font-size:10px;text-align:right;\">technorati tags:<a href=\"http://technorati.com/tag/skype\" rel=\"tag\">skype</a>, <a href=\"http://technorati.com/tag/ap2ap\" rel=\"tag\">ap2ap</a>, <a href=\"http://technorati.com/tag/scalability\" rel=\"tag\">scalability</a></p><p style=\"text-align: right; font-size: 8px\">Blogged with <a href=\"http://www.flock.com\" target=\"_new\" title=\"Flock\">Flock</a></p></div>",
        "metadata": {
          "word_count": 313,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "The essential characteristics of the http based web are that by default everyone is anonymous, and everyone can get to everything. The trend is for the parts that are not free and anonymous to move in that direction. Some things that are easy on the Internet are difficult on Skype, and vice versa. Look for things that really don't work well on the web, or look for web-based systems that connect a few people but need an expensive back-end or don't scale. This is the start of something interesting.... \"Flock\" is a free, open-source software that allows users to write their own code. It was created by a group of researchers at the University of California, Berkeley."
      },
      "tags": [
        "infrastructure"
      ]
    },
    {
      "id": "virtual_adrianco_file_1e51f56c",
      "kind": "file",
      "subkind": "blogger_post",
      "title": "Tutorials and Training on Cloud Architecture and NetflixOSS",
      "source": "Blogger Archive",
      "published_date": "2014",
      "url": "https://perfcap.blogspot.com/2013/04/tutorials-and-training-on-cloud.html",
      "content": {
        "text": "There are two places that I'm giving in-depth training on cloud architecture and NetflixOSS this year.<br />\n<br />\nOn May 22nd-23rd at <a href=\"http://www.gluecon.com/2013/\" target=\"_blank\">Gluecon 2013</a> in Broomfield Colorado I'm giving one of the opening keynote talks that introduces the concepts of a Cloud Native Architecture, then spending all afternoon on a tutorial to go into the details of how to get there and how to use NetflixOSS tools as an on-ramp to accelerate the process.<br />\n<br />\nFor people in Europe, I'm teaching at a summer school on \"<a href=\"http://laser.inf.ethz.ch/2013/\" target=\"_blank\">Software for the Cloud and Big Data</a>\" September 8th-14th in Italy. It's organized by ETH Zurich. I'll be giving six one-hour talks over a week, and there are seven other speakers.<br />\n<br />\n<a href=\"http://laser.inf.ethz.ch/2013/\" target=\"_blank\" title=\"LASER 2013: Software for the Cloud and Big Data\"><img alt=\"LASER 2013 banner\" src=\"http://laser.inf.ethz.ch/2013/img/banner.png\" /></a><br />\n<br />\nWhen I get back from LASER, it will be time to start evaluating nominations for the NetflixOSS Cloud Prize, and these are both opportunities to figure out how to build a strong entry.<br />\n<br />",
        "metadata": {
          "word_count": 183,
          "processing_status": "success",
          "processing_errors": []
        },
        "summary": "There are two places that I'm giving in-depth training on cloud architecture and NetflixOSS this year. On May 22nd-23rd at Gluecon 2013 in Broomfield Colorado. On September 8th-14th in Italy at a summer school organized by ETH Zurich. When I get back from LASER, it will be time to start evaluating nominations for the NetflixOSS Cloud Prize, and these are both opportunities to figure out how to build a strong entry."
      },
      "tags": [
        "architecture",
        "big data",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_36",
      "kind": "youtube",
      "subkind": "",
      "title": "Adrian Cockcroft Appearance Playlist on YouTube",
      "source": "Many different accounts - technical content",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8Ce-3VPplFg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_37",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Enterprise Indigestion at NVIDIA GTC",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_38",
      "kind": "story",
      "subkind": "",
      "title": "SC23 update",
      "source": "InsideHPC",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/sc23-top500-trends-the-ai-hpc-crossover-chiplet-standardization-the-emergence-of-ucie-and-cxl-advancements/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_39",
      "kind": "story",
      "subkind": "",
      "title": "SC22 predictions",
      "source": "InsideHPC",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/12/sc22-cxl3-0-the-future-of-hpc-interconnects-and-frontier-vs-fugaku/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_40",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Why did Docker Catch on Quickly and Why is it so Interesting",
      "source": "The New Stack",
      "published_date": "2014",
      "url": "https://thenewstack.io/why-did-docker-catch-on-quickly-and-why-is-it-so-interesting/",
      "content": {
        "text": "\n\nDocker has rapidly become \u201cthe new thing\u201d for computing in 2014. Why did that happen so quickly, and how has a Linux-application container become so popular? Broadly, I think, Docker is a great example of how to build a viral, developer-oriented product.\n\nA developer can figure out what Docker does, install it and do something useful with it in 15 minutes. I first heard this \u201crule\u201d from Marten Mickos when talking about why MySQL was so successful: low friction to try it out, a simple concept and useful functionality.\n\nDocker is a great name and it has a cute logo. It resonates with what the product does and is easy to remember. Engineering-oriented founders sometimes seem to think that names and logos don\u2019t matter if the product is good enough, but a great name can turbocharge adoption and build a valuable brand.\n\nThe Docker product came from a non-threatening source, a small startup (DotCloud) that was able to broadly partner across the whole industry. If the same product had come from an established enterprise technology player, there would have been much more push-back from that player\u2019s competitors, and the market would probably have split into several competing technologies.\n\nThe rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. So everyone is having to cooperate around a single container solution. This is great for the developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny, somewhat, as Docker did an end-run around their strategy.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nGet ubiquity first, become the category leader, then convert that into business value and revenue opportunities later.\n\nThat leaves the remaining question of, what does Docker do that is interesting, and who might it compete with? I have four separate answers:\n\nPortability : Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. This is a similar benefit to the Java \u201cwrite once, run anyware\u201d idea, but is more robust and is generalized to \u201cbuild anything once, run anywhere.\u201d\n\nSpeed : Start-up time for a container is around a second. Public-cloud virtual machines (VMs) take from tens of seconds to several minutes, because they boot a full operating system every time, and booting a VM on a laptop can take minutes. To counter this advantage, VMware has just announced (but not shipped) a technology called Fargo, that clones an existing VM in a second or so.\n\nConfiguration : The Docker container captures the exact configuration of a version of an application. To upgrade the application in production, the container is usually replaced with a new version, which takes a few seconds. The layers of components that go into the configuration are kept separate and can be inspected and rebuilt easily. This changes configuration management to be largely a build-time activity, so, for example, a Chef recipe might be used to build a Docker container, but at runtime there is no need to use the Chef services to create many identical copies of a Docker container. Used in this way, Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack.\n\nDocker Hub App-store : Docker containers are shared in a public registry at hub.docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Because containers are very portable, this provides a very useful cross platform \u201capp store\u201d for applications and component microservices that can be assembled into applications. Other attempts to build \u201capp stores\u201d are tied to a specific platform (e.g., the AWS Marketplace or Ubuntu\u2019s Juju Charms) or tool (e.g., the Chef Supermarket), and it seems likely that Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities.\n\nOne reason Docker is interesting is that all four answers are each individually useful, but can be used in combination. This causes cross-pollination of ideas and patterns. For example, someone might start using Docker because they like the speed and portability, but find that they end up adopting the configuration and Docker hub patterns as well.\n\nThe Docker technology is still fairly new; work is underway to add missing features, and a large ecosystem of related projects and companies is forming around it. There\u2019s a lot of interest in the technology from the VC community, as we try to figure out whom to fund to do what, and how the story will play out in the longer term.\n\nAdrian Cockcroft is a technology fellow\u00a0at Battery Ventures.",
        "summary": "Docker is a great example of how to build a viral, developer-oriented product. A developer can figure out what Docker does, install it and do something useful with it in 15 minutes. The rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. This is great for developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny. Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack. Docker containers are shared in a public registry at hub. docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities. There\u2019s a lot of interest in the technology from the VC community.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "containers",
        "docker",
        "microservices",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_41",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "A Conference to Discuss Developer-Driven Infrastructure",
      "source": "The New Stack",
      "published_date": "2015",
      "url": "https://thenewstack.io/a-conference-to-discuss-developer-driven-infrastructure/",
      "content": {
        "text": "\n\nCompetitive pressures have pushed speed of development to be one of the highest\u00a0priorities for business today. Improved tools and techniques have moved the state\u00a0of the art in agile development from monolithic updates every few weeks, to\u00a0continuous delivery of microservices several times a day. The move to \u201crun what\u00a0you wrote\u201d and developer-driven infrastructure means that developers are not only\u00a0delivering products faster, but are also responsible for the efficiency and safety of those\u00a0products in production. Lean development techniques that take waste out of the\u00a0process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. The security blanket approach of wrapping a firewall around an\u00a0insecure monolithic product has been replaced by penetration testing of every\u00a0microservice, sophisticated use of identity and access management mechanisms,\u00a0encryption as the default and fine-grain security key management.\n\nThese three core areas form a story arc through GOTO London: Agile, Lean and Rugged. In each of them we will discuss the state of the art and emerging directions\u00a0that will set you up with a broad view of these key concerns for developers in 2016.\n\nThis is the first ever GOTO London conference. The core story is being structured as\u00a0a single track. Many conferences have an initial keynote session for everyone,\u00a0followed by a wide selection of talks running at the same time. This forces you to\u00a0decide what to miss and move from room to room. It also means that an individual\u00a0track has trouble building and maintaining context, and encourages repetitive, self-contained presentations. The approach is great for variety, but can be frustrating.\n\nFor GOTO London, we will start the week with two optional training days on Monday and Tuesday, Sept. 14-15, then kick off the event with two full days of single-track talks Wednesday and Thursday, Sept. 16 -17. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences,\u00a0presentations about open source tools and talks about the latest hot products.\n\nFor the single-track days, everyone will be in the same room, building up a shared\u00a0context as the curated story arc unfolds. You won\u2019t miss anything, you won\u2019t waste\u00a0time moving from room to room. You will get to know each other and the speakers\u00a0much better. The talks will be a bit shorter, more focused, and will build on each other, reducing repetition and elementary introductions.\n\nThere is a repeated pattern for the single-track days. In each half day, there will be\u00a0four half-hour talks, followed by a half-hour panel session with all four speakers\u00a0discussing each other\u2019s talks with each other and the audience.\n\nAfter leading a workshop on microservices, I\u00a0will also introduce the\u00a0conference as a whole on Wednesday morning.\n\nIntroducing Agile , Dan North will\u00a0focus on the impact of new ideas and tooling supporting faster development.\n\nNicole Forsgren, Ph.D., will introduce\u00a0Lean, showing how DevOps culture and practices\u00a0reduce waste and improve outcomes.\n\nIntroducing Rugged , Joshua Corman will discuss how developers can build the core concepts of security into their\u00a0applications to protect information from attack. These four speakers will end the\u00a0morning with the first panel session.\n\nThe afternoon continues with four speakers digging deeper into the relatively\u00a0unfamiliar territory of Rugged systems.\n\nWhat kind of attacks are we seeing? What\u00a0tools are available for developers to automate security testing?\n\nHow can we\u00a0manipulate keys, identity and access, safely and easily?\n\nThe afternoon ends with\u00a0Joshua Corman joining the four speakers for a panel discussion.\n\nThe first evening is the conference party, with an entertaining and informative\u00a0keynote presentation by Ines Sombra and Adrian Colyer. Ines runs the San Francisco Papers-We-Love meetup, and Adrian publishes a daily blog post called The Morning\u00a0Paper. Over the last year, there has been a rush of interest in academic research\u00a0papers resulting in Papers-We-Love meetups all over the world. People are finding\u00a0hidden gems, radical ideas, or fundamental turning points in computer science, and\u00a0having fun presenting their own interpretations. We hope you will be inspired to\u00a0attend your local meetup, read The Morning Paper and share your own ideas.\n\nThe second day starts with four presentations on Lean. The Lean Enterprise book\u00a0has become required reading as these ideas spread into the mainstream. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer as their top priority. We\u00a0will also look at developer-oriented techniques for measurement, analysis and\u00a0optimization. Nicole will return to lead the panel session.\n\nDigging into Agile, we will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave , and see what it takes for a startup to \u201ccatch and surf the\u00a0wave\u201d when a product goes viral and everyone else dives in. At the other extreme, we will examine the latest ideas to speed up development at a financial institution\u00a0and for embedded hardware, and finish with Rachel Davies on Extreme\u00a0Programming in the 21st Century. Dan North then leads the closing panel.\n\nWe wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups.\n\nThe final day of the conference puts the concepts into practice, with deep dives into\u00a0open source tools and the latest products. The multi-track talks are longer, with\u00a0more time between them to switch rooms and for hallway conversations.\n\nWe expect that GOTO London attendees will have an enjoyable and memorable\u00a0experience learning the concepts and tools needed to be agile, lean and rugged for\u00a02016 and beyond.\n\nWeaveworks is a sponsor of The New Stack.\n\nFeatured image via Flickr Creative Commons.",
        "summary": "Developers are responsible for the efficiency and safety of products in production. Lean development techniques that take waste out of the process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. This is the first ever GOTO London conference. The core story is being structured as a single track. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences, presentations about open source. There is a repeated pattern for the single-track days. In each half day, there will be four half-hour talks. The talks will be a bit shorter, more focused, and will build on each other. The first evening is the conference party, with an entertaining and informative presentation by Ines Sombra and Adrian Colyer. Over the last year, there has been a rush of interest in academic research papers resulting in Papers-We-Love meetups all over the world. The second day starts with four presentations on Lean. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer. We will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave. We wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups. The final day of the conference puts the concepts into practice, with deep dives into open source tools and the latest products.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "agile",
        "architecture",
        "devops",
        "docker",
        "infrastructure",
        "lean",
        "microservices",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_story_42",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "The Rise of Continuous Resilience",
      "source": "The New Stack",
      "published_date": "2020",
      "url": "https://thenewstack.io/the-rise-of-continuous-resilience/",
      "content": {
        "text": "\n\nAmazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21.\n\nI sometimes ask a CIO whether they have a backup datacenter. Most will say yes, as it\u2019s a normal part of a business continuity plan for disaster recovery. In some industries, like financial services, it\u2019s a regulated requirement and there\u2019s an annual visit from an auditor to make sure it\u2019s in place.\n\nWhen I ask how often they test their failover process, people start to look uncomfortable. Some admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. When I ask what the failover test looks like, it\u2019s normally one application at a time, in a planned migration from the primary data center to the backup. It\u2019s rare for people to test an entire data center by cutting its power or network connections at an arbitrary time. I did hear once from a bank that had two data centers and switched between them every weekend so that one data center was primary on even-numbered weeks and the other was primary on odd-numbered weeks. If they ever had a problem mid-week, they knew what to do and that they could rely on it working smoothly. I\u2019ve asked this question a lot over several years, and have had only a handful of good answers.\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nDisasters don\u2019t happen very often, but unfortunately, when data centers lose power, drop network connectivity, lose their cooling system, catch fire or are flooded, the whole data center goes offline. Usually at an inconvenient time, with little or no warning. During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater, and they still don\u2019t work once the water recedes \u2014 as they are then full of mud and other debris. Even if the data center isn\u2019t in the basement, sometimes the backup generators are, or the fuel tanks for the generators, or some critical network equipment. There are lots of examples of disasters like this in the press, so why are so many companies that have a business continuity plan in the news when disaster strikes?\n\nThe short answer is that it\u2019s hard to get a disaster recovery data center implemented, and too much work and too risky to test it frequently. Each installation is a very complex, fully customized \u201csnowflake.\u201d The configuration of the two data centers drifts apart, so that when the failover is needed the failover process itself fails and the applications don\u2019t work. Even such basic things as backups for data need to be tested regularly, by attempting the restore process. I\u2019ve heard of some embarrassing data loss situations, where the backups failed and this wasn\u2019t discovered until months later when a database failed and a restore was needed.\n\nI call this \u201cavailability theater\u201d \u2014 everyone is going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nSo how can we make this better? There are two technology trends coming together to create a more productized solution, that is tested frequently enough to be sure it\u2019s going to work when it\u2019s needed. The combination of cloud computing and chaos engineering is leading to \u201c continuous resilience. \u201d\n\nThe last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. They ended up doing region evacuation testing about once every two weeks.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nIn these tests, they drain all the traffic from a region and show that they can still run Netflix on the two remaining regions; and no-one notices! (Netflix operates from AWS regions in Virginia, Oregon and Dublin). The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope \u2014 a \u201c Chaos first \u201d policy.\n\nThe reason Netflix was able to implement this is that cloud regions are different to data centers in two critical ways. Firstly, they are completely API driven, and the entire state of an AWS account can be extracted and compared across regions. Secondly, the versions and behaviors of the AWS services in each region don\u2019t drift apart over time the way data centers do.\n\nMost customers will have a mixture of multiregion workloads. Some customer-facing services, like a mobile back-end that needs to be online all the time, can be built active-active, with traffic spread across multiple regions. Workloads like a marketplace, which needs a consistent view of trades, are more likely to be operating in a primary region \u2014 with failover to a secondary, after a hopefully short outage.\n\nNetflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong, and to make sure that the system has enough resilience to absorb failures without causing customer-visible problems. Nowadays more companies are setting up chaos engineering teams, hardening their systems, running game day exercises, and using some of the chaos engineering tools and services that are developing as the market matures.\n\nAWS has been investing in our services to provide support for multiregion applications, for both active-active operation and primary-secondary failover use cases. In the last few years, we\u2019ve added cross-region data replication and global table support to Amazon S3, DynamoDB, Aurora MySQL and Aurora Postgres . AWS also acquired a disaster recovery service called CloudEndure , which continuously replicates application instances and databases across regions. We\u2019ve also extended AWS Cloudwatch to support cross-account and multiregion dashboards .\n\nAs usual, we are listening to what our customers ask for, and our partners and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.\n\nReaders might also like the Architecting resilient systems on AWS session from AWS re:Invent 2019.\n\nFeature image via Pixabay.",
        "summary": "Amazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21. Some CIOs admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. The combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater. It's hard to get a disaster recovery data center implemented. Each installation is a very complex, fully customized \u201csnowflake.\u201d Even such basic things as backups for data need to be tested regularly. The last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. Netflix operates from AWS regions in Virginia, Oregon and Dublin. The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope. Netflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong. Nowadays more companies are setting up chaos engineering teams. AWS Cloudwatch extended to support cross-account and multiregion dashboards. Partner and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "netflix",
        "resilience"
      ]
    },
    {
      "id": "virtual_adrianco_story_43",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Nvidia GTC: Hyperscaler Happiness and Enterprise Indigestion",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_44",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Sustainability: Comparing AWS, Azure and Google",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/sustainability-how-did-amazon-azure-google-perform-in-2023/",
      "content": {
        "text": "\n\nThe leading cloud providers and the mammoth global companies that run them are adopting environmentally sustainable practices, but it is a complicated work in progress.\n\nAs Amazon , Google and Microsoft continue to grow, they\u2019re all grappling with the challenges of reducing their carbon emissions in the f ace of increased resource usage of AI, while making the right investments in renewable energy infrastructure around th\u200b\u200be world.\n\nCustomers of cloud providers need detailed information to understand and optimize the sustainability of their own workloads . While Amazon is making the biggest sustainability investments and getting some good results, it has historically been the least transparent cloud provider.\n\nAmazon released its sustainability report covering the whole of 2023 on July 9, Google released its report on July 2 , and Microsoft released its in May .\n\nI\u2019ve been leading the Real Time Cloud project at the Green Software Foundation for the last year or so. We have made a deep dive into the information available, identifying gaps and subtle differences in the data from each cloud provider.\u00a0 I\u2019m going to compare these three reports to see what\u2019s new since last year; my analysis here is my own, not the project\u2019s.\n\nAmazon Leads in Carbon Output Reductions\n\nAmazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 MmTCO2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ (grams of carbon dioxide per gross merchandise dollar) to 80.8 gCO2/$.\n\nThis continues a trend that started last year, of a small reduction in total carbon despite growing the business, as more of the internal projects started over the last few years mature and begin to have material impact. There\u2019s a long way still to go but it\u2019s a commendable result.\n\nGoogle however, increased its total carbon by 13% to 14.3 MmTCO2e for 2023, on top of an increase the previous year, slipping on carbon progress due to investment in data centers for AI, which was also the story at Microsoft, up from 16.5 to 17.2 MmTCO2e.\n\nThe physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors, and Amazon\u2019s revenue growth rate is a bit lower. But over the years, Amazon has also been more aggressive in buying renewable energy than Google and Microsoft.\n\nAmazon\u2019s carbon footprint was reduced for all categories apart from fossil fuels, as its delivery fleet continues to grow as a business, faster than those vehicles are being electrified. This trend is driven in part by Amazon bringing more deliveries in-house; in the report, the category that includes third-party delivery carbon declined 4%.\n\nIncreased focus on rail-based delivery in Europe and India is reducing delivery carbon. The electric delivery fleet grew more than sevenfold, to 19,000 vans, but still has a long way to go before it makes a material difference.\n\nOver time, the electricity use of Amazon for retail and delivery is adding a lot more demand in addition to the growth of data centers, compared to\u00a0 Microsoft and Google. This is why Amazon is investing in so many more energy projects.\n\nRenewable Energy Projects: Making Good Progress\n\nThe most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon , including retail and deliveries.\n\nThis is much more than just the AWS data center usage. Energy matching means that renewable energy is being generated and put into a grid somewhere in the world in an amount that matches any non-renewable energy that\u2019s being used.\n\nThe energy projects that make up the portfolio are geographically positioned as close as possible to where the power is needed. But it\u2019s not always possible for them to be located nearby, and the amount of energy generated doesn\u2019t exactly match how much is currently being used locally.\n\nThe Amazon generation portfolio increased from 20GW to 28GW, making it the world\u2019s largest purchaser of renewable energy for the fourth year in a row. Google added half as much, 4GW in 2023, which is still a record for the company.\n\nForty-two new utility-scale projects came online for Amazon in 2023. The Amazon sustainability report also states that the company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has also purchased a nuclear-powered data center in Pennsylvania.\n\nAmazon\u2019s energy storage increased from 445MW in 2022 to 1.3GW in 2023 to help smooth out the daily supply and demand mismatch as solar and wind come and go. It\u2019s now become common for new solar power developments to include battery storage, as the peak daily output could be curtailed and the excess can be stored then supplied to the grid in the evening.\n\n100% Carbon-Free Energy Match Still Emits Carbon\n\nThe 100% carbon-free energy match metric uses the annual \u201cmarket-based\u201d method, which is based on energy purchases, averaged over the whole year. As we noted previously, the in-house generating capacity won\u2019t ever match the right location at the right time, so there\u2019s a market for trading excess renewable energy.\n\nRenewable energy certificates (RECs) can be traded for up to a year, so when companies have final data for 2023, additional \u201cunbundled RECs\u201d can be purchased as needed to cover for the non-renewable energy that was used, and obtain any desired percentage, at a cost.\n\nAmazon committed to reach 100% by 2030, with a goal of 2025, and is proud to get to 100% in 2023. It looks less impressive to realize that Amazon has finally caught up with Google, which has been at 100% carbon-free energy match using the same annual market-based method for the last seven years. The Amazon sustainability report states that there was 2.79 MmTCO2e of energy-related carbon emissions (known as Scope 2). In other words, when Amazon reached 100% carbon-free energy, there were still 2.79 million tons of carbon equivalent emitted.\n\nA few years ago Google started measuring and reporting a much more stringent \u201c24\u00d77\u201d hourly matching location-based method. Microsoft has dabbled with 24\u00d77, but only for the Azure Sweden region, and AWS hasn\u2019t done anything with it.\n\nAmazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report : \u201cAmazon\u2019s Scope 2 (Indirect) GHG emissions: Location-based method (LBM) are 15.67 MmTCO2e.\u201d Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by the location-based method. When the methods were defined, the report\u2019s authors didn\u2019t consider that large end users of energy could also be large generators of energy across multiple locations.\n\nGoogle discloses location-based data to customers for all its cloud regions individually, and AWS needs to follow suit and start to report Scope 2 location-based data on a per-region basis.\n\nGiven the locations and sizes of the Amazon renewable power projects, I think AWS will often have lower location-based method carbon than Google and Azure. But AWS needs to publish its results \u2014 regardless of whether those numbers are better or not.\n\nAmazon Is Leaning Into Future Energy Capacity\n\nThe new Amazon report includes a statement that the company is \u201cpurchasing additional environmental attributes (such as renewable energy credits) to signal our support for renewable energy in the grids where we operate, in line with the expected generation of the projects we have contracted.\u201d\n\nIt takes two to three years to bring a wind or solar farm online, so Amazon buys RECs on the open market that match the future generation capacity that they have committed to building. This is a good policy, as unbundled REC purchases have a bad reputation when they are used across regions and countries as a cheap substitute for actual investments. But here they are being purchased to match investments in new generating capacity.\n\nHowever, there is also a footnote on page 26 of the sustainability report:\n\n\u201cAWS aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases (e.g., renewable energy in the same grid is not available), AWS may procure renewable energy attributes in other locations.\u201d\n\nA similar footnote in the audit report on energy also reads:\n\n\u201cAmazon takes a global approach to calculating the percentage of electricity consumed by Amazon\u2019s global operations matched by renewable energy sources. Amazon aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases, Amazon may procure renewable energy in other locations.\u201d\n\nThis is an unfortunate fact of life: there just isn\u2019t enough renewable energy in regions in Asia in particular, so cross-border RECs are used. This is the first time I\u2019ve seen Amazon make this statement, and Amazon joins Google and Microsoft, who\u2019ve been buying cross-border RECs and carbon offsets for many years.\n\nAmazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. This covers all of the Canada, China, Europe, India, Japan and U.S. regions. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Indonesia, Israel, Korea, Singapore, South Africa, and the United Arab Emirates. The difference since 2022 is that the \u201cin-market\u201d wording was added and Japan was added to the previously published 100% list.\n\nHowever Amazon has far more generating capacity in Asia than either Google or Microsoft, and Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances.\n\nThe problem with this is that by the market method, cloud energy use in Asia will be reported as zero carbon, because cloud providers will pay extra by buying RECs elsewhere. However, increased use of regions in Asia will cause extra carbon to be emitted compared to the same use in Europe or the U.S., and to measure the difference the location-based method needs to be used and reported.\n\nMore Investments in De-Carbonizing Supply Chains\n\nAmazon announced a new program to engage major suppliers to decarbonize its supply chain with Amazon Sustainability Exchange , sharing internally developed playbooks and sustainability science models for things like renewable concrete and hydrogen. There\u2019s some interesting work and good advice here; it was put together by sustainability scientists in the Amazon central sustainability team.\n\nLow-carbon concrete and steel for construction is a good investment and is growing fast, up from 16 AWS data centers to 36 in 2023, but it\u2019s still a small impact on the carbon total. Efficiency and electrification upgrades and the use of local solar power on existing buildings are having a bigger effect.\n\nAWS, Azure and Google have all started to transition to bio-diesel (mostly from recycled cooking oil) for backup generators, and AWS has reduced the use of air freight for deliveries and started to use electric vehicles for trucking equipment to data centers. This is worthwhile, as it helps develop a market for the supply of biodiesel globally, although this is currently a very small part of the total carbon footprint.\n\nAmazon is correctly positioning investment in nature-based solutions and carbon capture as a second priority to direct reductions in carbon emissions. But it\u2019s a worthwhile additional investment, as it\u2019s expected to be necessary to scale these solutions over time to meet the 2040 Net Zero Climate Pledge goal, as the remaining direct emissions become harder to eliminate.\n\nSustainable Water Resources: Good Progress\n\nSustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers, which has increased a lot in recent years. A few years ago the cloud providers started to measure and report their water usage, and they all have targets to use less, and to return more clean water to the locations where they operate.\n\nThere are two ways to measure water, one is how much water is used as a proportion of the energy being used in a data center. Water usage effectiveness (WUE) is measured in liters per kilowatt hour. The other is how much water is replenished as a proportion of what is used.\n\nAWS\u2019s WUE improved by 5% from 0.19 to 0.18 Liters/kWh in 2023, averaged across all the facilities AWS runs globally. It\u2019s unfortunate that AWS only provided a global average, which removes any opportunity to optimize via workload placement.\n\nMicrosoft Azure provided WUE on a region-by-region basis, varying from zero to over 2 Liters/kWh. Google did not report WUE figures.\n\nAWS reports 41% \u201cwater positive\u201d as a new metric in 2023, with a 100% goal. This appears to be another name for \u201cwater replenishment.\u201d\n\nGoogle reports improving from 6% to 18% water replenishment in 2023, but the company has a goal of eventually returning more water than it uses. Google was dinged for poor water use a few years ago, and is clearly playing catch up, while AWS has already made good progress.\n\nAzure doesn\u2019t provide a replenishment figure. This is the kind of thing the Green Software Foundation\u2019s Real Time Cloud project is working on, trying to get all the cloud providers to standardize on the same metrics with the same names by pointing out the gaps.\n\nAmazon\u2019s Reporting Needs More Transparency.\n\nAWS had previously published data center versus cloud carbon reduction comparison reports for Asia, Europe and the U.S. that were used as the basis for a lot of marketing claims over the years. They were getting a bit old and were updated by a new carbon reduction comparison report in June.\n\nAccenture was commissioned to write the report, and while it\u2019s a helpful document and the claims it makes do seem reasonable, there\u2019s no transparency in the calculations, and a lot of private AWS data was used by Accenture that isn\u2019t available for customers to use to model their own specific situations.\n\nCustomers need more than marketing claims, and the report could have included a lot more of the underlying data and calculations, not just the headline results.\n\nAWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago and releases quarterly updates. Microsoft Azure publishes PUE data for all its regions.\n\nWhen you measure energy use for a workload in a cloud region (e.g. by collecting NVIDIA GPU energy use metrics), you need to multiply by PUE to account for how much energy was supplied to the data center to cover cooling and transmission inefficiencies then multiply by the carbon content of that energy.\n\nPUE is much higher in hot and humid climates. Of course, some workloads have to be in a specific region. But a significant amount of compute and storage capacity (like backup/archives) could be located anywhere. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data.\n\nThe other data provided by both Google and Azure is the companies\u2019 carbon-free energy (CFE) proportion on a per-region basis. This takes the local method grid carbon mix into account, then subtracts out the locally generated capacity from renewable projects that are up and running. It\u2019s a good way of indicating which regions are benefiting from low-carbon energy that can be used for optimizing global workload placement.\n\nThere is still no mention of Scope 3 supply chain reporting for AWS customers (Scope 3 refers to emissions from assets not owned or controlled by an organization, but that the organization\u2019s supply chain directly affects.)\n\nIt has been many years since Azure and GCP started to report Scope 3 and it is now years since AWS promised that Scope 3 would be provided. They\u2019ve told me they have a team working on it, but until something is released, this is a huge gap. Microsoft released a good white paper on their Scope 3 methodology back in 2021.\n\nThe AWS Customer Carbon Footprint Tool (CCFT) was embarrassing when it was initially released in 2022, and it has made no progress in the years since then. It is going to report zero for everyone for their Scope 1 and 2 carbon footprint, according to the market methodology, has no Scope 3 data, and aggregates too much data together.\n\nI recently tried to use the CCFT data to track progress for a company, and the three usage categories EC2, S3 and Other combined with three geographies EU, Americas and Asia made it impossible to figure out what was going on. The Other category in the Americas is dominating, but that\u2019s all it tells you.\n\nCustomer carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. Completely useless.\n\nIn summary, Amazon is making decent progress towards a reduced carbon footprint, while Google and Microsoft are slipping. However, someone at AWS needs to read and follow the advice given in the Amazon Exchange Carbon Measurement Guide . The guidance says that metrics and transparency are needed, and AWS is still at ground zero, with no transparency, and no progress on metrics that their customers have been asking for for years.\n\nAdditional Resources\n\nAmazon Reporting website\n\nAmazon Carbon Methodology\n\nAmazon Renewable Energy Methodology\n\nCarbon Free Energy\n\nAWS Sustainability Reporting Framework Summary\n\nAmazon Exchange Carbon Measurement and Reporting guidelines\n\nWater Positive Methodology\n\nAccenture report on AWS carbon reduction\n\nAmazon Energy Audit\n\nAmazon Scope 1 and 2 Audit\n\nAmazon Scope 3 Audit",
        "summary": "Amazon released its sustainability report covering the whole of 2023 on July 9. Google released its report on July 2 and Microsoft released its in May. Customers of cloud providers need detailed information to understand and optimize their own workloads. Amazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 Mm TCO 2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ to 80.8 g CO2/$. This continues a trend that started last year, of a small reduction in total carbon despite growing the business. The physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors. Amazon has also been more aggressive in buying renewable energy than Google and Microsoft. The most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon. Amazon generation portfolio increased from 20GW to 28GW. Forty-two new utility-scale projects came online for Amazon in 2023. The company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has finally caught up with Google, which has been at 100% carbon-free energy match for the last seven years. Amazon committed to reach 100% by 2030, with a goal of 2025. Amazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report. Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by theLocation-based method. It takes two to three years to bring a wind or solar farm online. Amazon buys RECs on the open market that match the future generation capacity. This is a good policy, as unbundled REC purchases have a bad reputation. Amazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Japan and U.S. Amazon has far more generating capacity in Asia than either Google or Microsoft. Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances. Sustainability scientists in the Amazon central sustainability team. Low-carbon concrete and steel for construction is a good investment and is growing fast. Amazon is correctly positioning investment in nature-based solutions. Sustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers. Cloud providers have targets to use less, and to return more clean water to the locations where they operate. In 2023, water use in the U.S. is expected to rise from 0.3% to 0.4%. The U.N. has set a goal of 0.5% water use by the end of the year. AWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago. PUE is much higher in hot and humid climates. Microsoft Azure publishes PUE data for all its regions. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data. There is still no mention of Scope 3 supply chain reporting for AWS customers. Scope 3 refers to emissions from assets not owned or controlled by an organization. It has been many years since Azure and GCP started to report Scope 3. AWS carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. AWS Sustainability Reporting Framework Summary. Water Positive Methodology. Accenture report on AWS carbon reduction. Amazon Exchange Carbon Measurement and Reporting guidelines. Amazon Energy Audit.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "metrics",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_45",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Cloud PUE: Comparing AWS, Azure and GCP Global Regions",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/cloud-pue-comparing-aws-azure-and-gcp-global-regions/",
      "content": {
        "text": "\n\nIn December 2024 Amazon released Power Usage Effectiveness (PUE) data for many of their AWS cloud regions, and along with existing data from Microsoft Azure and Google Cloud , there is finally enough data to make some comparisons for regions spread around the world and see what changed from 2022 to 2023.\n\nPUE is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center, and losses in the transmission and conversion of electricity on its way to the computers. Energy is measured (and paid for) at the meter as it enters the building, and a PUE of 1.15 means that an extra 15% of the total energy used by the computers is used for cooling and overheads. PUE varies between about 1.04 and 2.0 in practice.\n\nAll the cloud providers run very efficient data center hardware configurations, and they have, in general, become more efficient over time. However, it is harder to cool data centers in warm and humid environments, so PUE for data centers in the tropics will tend to be at the high end of the range, and PUE for data centers in cold and dry parts of the world are at the low end of the range.\n\nOne way to use less energy for cooling is to use more water, so there is also a natural tension between PUE and Water Usage Effectiveness (WUE in liters/kWh). The cloud providers have all recently invested heavily in optimizing for WUE as well, so the latest data centers tend to have good PUE and good WUE, but this requires the best and latest technology. Lower-cost and older data centers typical of the kind that enterprises own tend to be closer to 2.0 PUE and have a high WUE as well. WUE was explained and compared across cloud providers in the story I wrote for The New Stack in July 2024.\n\nSome of the regions deployed by cloud providers around the world are hosted by local service providers rather than in dedicated data centers built by the cloud providers. In this case the PUE is not included in the public numbers for two reasons, one is that it\u2019s hard to attribute and allocate part of a shared resource when there is no information about what is in the rest of the datacenter, and the other is that the over-all PUE is often proprietary information that is owned by the service provider and they don\u2019t allow it to be shared openly. Some additional PUE estimates may be available privately under a non-disclosure agreement so it\u2019s worth asking your provider if you are operating in a region that doesn\u2019t have a public PUE number.\n\nAmazon/AWS PUE Data\n\nThe new PUE information from Amazon is described on their sustainability page . They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. There is a short PUE Methodology pdf that basically says that they are following the relevant international (ISO) and European (CEN) standards. The global average 2023 PUE for AWS is 1.15, with AMER at 1.14, EMEA at 1.12 and APAC at 1.28. They say their best individual data center facility in Europe has a PUE of 1.04. Still, their best region is Melbourne, Australia, with a PUE of 1.08, and their worst is Hyderabad, India, with a PUE of 1.50. From year to year, AWS shows improvements in many regions.\n\nAWS PUE data as stated in January 2025.\n\nMicrosoft Azure PUE Data\n\nMicrosoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions, but when they were updated in December 2024 the numeric data was omitted. Instead, a webpage provides a summary that provides information for only 11 out of 27 regions, that is offset from the calendar year and doesn\u2019t cover all of 2023. We contacted the currently responsible teams within Microsoft and found that whoever disclosed the original 2022 data had moved on, and the 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table .\n\nThe Microsoft web page PUE data as stated in January 2025.\n\nMicrosoft discloses their best PUE of 1.11 in Wyoming, USA, and their worst at 1.35 in Illinois. Their sustainability targets don\u2019t mention PUE.\n\nGoogle Cloud Platform GCP PUE Data\n\nGoogle has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. For the same reasons as AWS and Azure discussed above, they don\u2019t report public PUE data on every region they operate in.\n\nGCP PUE data as stated in January 2025.\n\nGoogle\u2019s best PUE result is 1.07 in Oregon, and its worst is 1.19 in both Singapore and Nevada. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while until they start to fill up with equipment. This can be seen in the GCP regional data. Some locations support Google products that are not part of GCP regions, and there is a mapping of GCP cloud regions to the Google PUE data that we performed with help from Google engineers as part of producing the GSF cloud region metadata table .\n\nComparisons Between AWS, GCP and Azure\n\nWhile all the major cloud providers have industry-leading PUE numbers that are likely to be much better than local data center alternatives, the numbers above show that GCP has the best transparency, with more data over a longer time period, and the best overall PUE for the regions being disclosed. Microsoft went from providing the most data for 2022 to the least data for 2023 and also has the highest PUE values overall. AWS sits in-between, with two years of data, and PUE values that are worse than GCP but mostly better than Azure overall. AWS includes data for regions like Hyderabad with higher PUE that aren\u2019t disclosed at all by other cloud providers, and they have an excellent stated goal of 1.08 for their new data center builds.\n\nRegional Comparisons\n\nIf you can choose which cloud provider to use in a particular region, and you\u2019d like to make the most efficient use of the energy needed, then the above PUE data provides data for some scenarios.\n\nVirginia is the biggest cloud region in the world, AWS PUE is 1.15, Azure is 1.14 and GCP is 1.08. For the nearby regions in Ohio AWS PUE is 1.12, GCP is 1.10.\n\nSingapore is a major region in Asia for all the cloud providers, with a challenging tropical climate. AWS PUE is 1.30, Azure is 1.34, and GCP is 1.13 or 1.19 for each of their two facilities.\n\nIreland is one of the largest regions in Europe. AWS PUE is 1.10, Azure is 1.19, and GCP is 1.08.\n\nWhat Is a Good PUE Assumption for the GPUs Powering the AI Boom?\n\nThere is a lot of concern about how much power the massive build-out of GPU capacity is going to need. Whatever power the GPUs use directly needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. Looking at the above data, I think there are two situations to think about. GPUs that are put into older enterprise data centers tend to overwhelm the existing infrastructure that wasn\u2019t designed for very high power density, so the PUE is likely to be bad, I\u2019d guess 1.5 or higher. However, the massive new data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. The latest, most efficient designs will allow more GPUs to be powered and cooled in a given location, and I\u2019d assume a PUE of 1.08, regardless of who is building it.\n\nGreen Software Foundation Real-Time Cloud Project\n\nI\u2019ve been leading the GSF real-time-cloud project for the last year or so. We\u2019ve spent a lot of time discovering many sources, interfaces and products that collect and report energy and carbon data and documented their relationships in a large Miro flow chart. We\u2019ve also published regional metadata collected from GCP, AWS and Azure and summarized it into a single table covering the 2022 data sets. We\u2019re in the process of updating it to include the latest data releases to cover 2023 and are planning to produce estimates for 2024 and 2025 before the cloud providers disclose their data so that workloads that are running today can have some data to use that we think is the best guess available.",
        "summary": "Power Usage Effectiveness (PUE) is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center. PUE varies between about 1.04 and 2.0 in practice. PUE for data centers in the tropics will tend to be at the high end of the range. PUE for centers in cold and dry parts of the world are at the low end. Lower-cost and older data centers typical of the kind enterprises own tend to have a high WUE as well. The new PUE information from Amazon is described on their sustainability page. They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. Some additional PUE estimates may be available privately under a non-disclosure agreement. Microsoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions. The 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table. Google has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while. GCP has the best transparency, with more data over a longer time period. Microsoft went from providing the most data for 2022 to the least data for 2023. AWS sits in-between, with two years of data, and PUE values that are worse than GCP.  AWS PUE is 1.15 in Virginia, Azure 1.14 in Ireland and GCP 1.08 in Ohio. PUE needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. New data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. I\u2019d assume a PUE of 1.08, regardless of who is building it. . at are running today can have some data to use that we think is the best guess available. at the time of this article. We are happy to provide the best data we can.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "platform",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_46",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "NVIDIA Unveils Next Gen Rubin and Feynman Architectures",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/nvidia-unveils-next-gen-rubin-and-feynman-architectures-pushing-ai-power-limits/",
      "content": {
        "text": "\n\nSAN JOSE, Calif \u2014 NVIDIA has cemented its central position in the technology industry over the last few years. It has the most in-demand products and amongst the biggest revenue and biggest market capitalization. It\u2019s a once-in-a-lifetime moment for everyone involved, as the Talking Heads once sang, but the torrent of product releases is extremely hard to keep up with and make sense of.\n\nThere is an impressive amount of business agility and relentless progress, but sometimes headline-making announcements are quietly forgotten or superseded by even better products before people have had a chance to get their hands on them. NVIDIA is announcing new hardware that effectively obsoletes its previous products well before they are available, but customers are buying whatever they can get, as it appears.\n\nMeanwhile \u201cthe new stack\u201d of AI software is maturing rapidly, easy-to-use, pre-tuned, high-performance open source models are available, and a plethora of AI-based services and tools are competing for everyone\u2019s attention. Development is also being accelerated by a variety of AI-backed tools that are improving rapidly.\n\nLast year, I predicted that enterprise customers would find it extremely difficult to manage the pace of change, and get \u201centerprise indigestion,\u201d but that hyperscalers would be happy. This year, I think the hyperscaler customers and cloud providers have indigestion as well. It takes a few years to plan and build a data center, but the specification of the racks and the power density keeps going up, so they must be replanning before they are finished building. NVIDIA is responding by releasing a very wide range of packaging options to target various points in the market and providing more information about its future roadmap, which helps but is making it even harder to keep track of what\u2019s going on.\n\nHow Did We Get Here?\n\nBefore we can make sense of what was announced by Jensen Huang , the CEO of NVIDIA, in his annual GTC keynote Tuesday , we need to establish some context by looking back at previous keynotes and additional announcements over the last year. There is a big disconnect between the GPUs that most people are using now (the ones that just started shipping, but that most people haven\u2019t used yet) and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail.\n\nThe GPU servers that most people are familiar with right now have two Intel CPUs and eight NVIDIA Hopper H100 GPUs as the nodes that can be clustered. They were announced three years ago and are the mainstream workhorse underpinning most of the AI-based products and services we use today.\n\nTwo years ago in his keynote, Jensen announced the Grace Hopper GH200 combined CPU/GPU architecture. Grace was the first CPU designed by NVIDIA and uses ARM architecture rather than Intel, with an NVlink interface to couple it directly to a slightly upgraded Hopper H200 GPU. While it shipped in volume, the GH200 one-to-one mix of CPU to GPU is the wrong ratio for many AI training customers, who wanted more of their dollars and power budget to be spent purely on additional GPU capacity. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as the H100s.\n\nGH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. This is reversed from conventional architectures where the CPUs are in the center (that\u2019s what the C in CPU stands for!) driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when an H200 GPU wants to send data to another H200 GPU in the same system in a cluster, it goes directly across NVlink. This is far faster than a traditional GPU sending via a PCI bus to an Intel CPU then over a network to another Intel CPU, then via PCI to the other GPU. However, when one Grace CPUs wants to communicate with another, the data passes through at least two Hopper GPUs to get there.\n\nGH200 systems seem to be more suited to the HPC Supercomputer market than the much bigger AI training and inference market. NVIDIA told me in 2024:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nIn 2024, Jensen announced the Blackwell architecture GPU and the GB200 system. To get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (addressing the issue with the 1:1 CPU/GPU ratio in GH200).\n\nBlackwell GB200 modules have around four times the silicon area dedicated to GPU, and the same area for CPU, compared to GH200 modules. Two modules fit on a 1 rack-unit-high, water-cooled motherboard, and 18 compute boards fit in a 72 GPU NVL72 rack, along with switchboards to interconnect them.\n\nNVIDIA has introduced several variations of the Blackwell and packaging options since launch, including a reduced-performance single-chip B200A variation that can be configured to be air-cooled as part of its MGX line of products. In the MGX GB200A NVL36 rack, each board takes two rack units and has a single Grace CPU resulting in a 9-CPU/36-GPU rack that needs 40kW of cooling, which is still a challenge but is easier to deploy in existing data centers.\n\nAt the end of 2024, it was disclosed that a new process from chip supplier TSMC allows for 50% higher performance Blackwell Ultra \u2013 B300 GPUs. These also use even more power, pushing the NVL72 rack from 132kW to 163kW, but it appears that many of the orders and plans that were in place for B200 have now switched to Blackwell Ultra.\n\nIt also has more flexibility in its design options, allowing customers like AWS to customize its deployments to integrate with its custom packaging and network architecture. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72 so that existing installations can extend their lifespan, and work done on productizing data centers to support the original NVL72 rack isn\u2019t wasted.\n\nNVIDIA revealing Blackwell Ultra NVL72 onstage at GTC.\n\nThe highly structured AWS cloud architecture doesn\u2019t naturally support dropping in a product like NVL72, and while Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave , the latest AWS GPU that is generally available as of March 18, 2025 is the p5 that has eight H200 with two Intel CPUs . Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. In the header shot for this story, you can see the variants of Blackwell-based racks that are deployed by each provider.\n\nLooking closely, it appears that most of the racks including Azure and Oracle cloud are NVL72, but AWS and GCP are using the air-cooled MGX36, and HP has its own custom Cray architecture rack for HPC that hosts 244 Blackwell GPUs. I haven\u2019t seen any new announcements from AWS at GTC and confirmed that it has no announced Blackwell-based products right now after visiting its expo booth. And you may ask yourself, how did they get here?\n\nThere was already some disclosure in mid-2024 but Jensen provided more details of the next-generation 2026 Vera CPU and Rubin GPU , which has even higher per-rack power requirements. To reduce \u201chyperscaler indigestion,\u201d the first version of Rubin also delivers into the same NVL72 rack architecture, with another bump in power consumption and a change in naming convention to Vera Rubin NVL144. Jensen said that they are reverting to calling a two-die module two GPUs for Rubin rather than one, as Blackwell was too confusing.\n\nNVIDIA revealing Vera Rubin NVL144 onstage at GTC.\n\nThe Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance, up from 1.1 exaflops for Blackwell Ultra GB300, and 1.2 exaflops of FP8 for training, up from 0.36, which is 3.3x the performance. It\u2019s targeted to ship in the second half of 2026.\n\nEscalating per-rack power requirements are just one of the problems, but the next-generation Rubin Ultra racks are up to 600kW according to Jensen, containing 576 GPUs in 144 modules that each have four Rubin GPUs sharing 1 terabyte of high-bandwidth memory. This is useful information for people designing for data center builds completing 2-3 years out who want to take these next-generation racks. However, it emphasizes that running an AI factory data center is a new set of challenges, even for the hyperscalers and cloud providers, let alone the more traditional enterprise customers.\n\nNVIDIA revealing Robin Ultra NVL576 onstage at GTC.\n\nLooking beyond Rubin Ultra in 2027, the next-generation architecture will be named Feynman in 2028, so we can expect to hear more about the performance of that GPU architecture next year.\n\nNVIDIA showing its next-gen architecture onstage at GTC.\n\nAt the lower end of the scale, NVIDIA is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. These are particularly useful for accelerated computer-aided engineering (CAE) workloads and Jensen mentioned that many of the standard CAE tools for chip simulation and design, mechanical simulation, etc., are now accelerated.\n\nNVIDIA revealing AI infrastructure for enterprise computing onstage at GTC.\n\nBenchmarking\n\nLast year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. Blackwell adds a new FP4 capability for inference that doubles the performance and there are additional optimizations for inference that give rise to a 30x improvement claim. I analyzed the 2024 benchmark claims in detail and decided that a more reasonable expectation for configurations of up to eight Blackwell GPUs that most people actually use would be 2.5x the H100 (not 4x) for training, and 8-10x the H100 (not 30x) for inference. These are good improvements in any case. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.\n\nThe way Jensen positioned performance was in the context of an AI Factory, which needs to operate in a sweet spot between large efficient batches of work vs. quick responses to interactive queries. That\u2019s where the extra memory and memory bandwidth makes the GPU operate closer to its maximum potential capacity. To further optimize this workload, NVIDIA will release a new open source distributed inference serving library called Dynamo (which isn\u2019t on GitHub as I write this). This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.\n\nNVIDIA announcing Dynamo onstage at GTC.\n\nThe latest performance claim is that with Dynamo and scale-up benefits from the larger NVL72, Blackwell is \u201c40x better\u201d than Hopper at AI Factory hyperscale inference serving. I think the benchmark was explained better this year, than last year, and it\u2019s a relevant application-level benchmark, but I also hope that people don\u2019t assume that a single Blackwell is 30x or 40x faster than a single H100.\n\nRepeated attempts to architect the largest-scale SuperPods at GTC.\n\nWater-Cooled Optical Switches\n\nA big announcement from two years ago was that NVlink could replace the network for up to 256 GH200s combined into a single system connected by 900GB/s interfaces to NVswitch chips. That would have contained 120TB of CPU-hosted memory and 24TB of GPU-hosted high-bandwidth memory, but this configuration seems to have never shipped in volume.\n\nNVlink is a copper-cabling system that operates inside a rack and can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. So in addition to the over-provisioning of CPU capacity with GH200, an additional issue was the high cost of NVlink optical transceivers for building such a big system. For AI training workloads, it\u2019s common to arrange GPUs in a \u201crail\u201d architecture where they are connected in parallel lines that don\u2019t need to cross-connect. This seems plausible to configure using copper NVlink from one rack to the next on each side, but a system configured this way is only useful for training workloads.\n\nThere was an update in 2024 to the shared memory cluster plans, abandoning the GH200 approach and focusing on a GB200-based configuration where the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576. Again, this seems like a theoretical architecture at this point.\n\nI haven\u2019t been able to find anyone talking about more than 36 or 72 GPUs, and Coreweave \u2019s documentation implies that they configure the NVL72 racks as individual 4-GPU boards networked together. The previously announced deal with AWS to deploy a cluster of GH200s for NVIDIA called Project Ceiba was upgraded (and delayed somewhat) to be a GB200 NVL72-based system with more than 20,000 GPUs instead, as of October 2024. It was not mentioned this year at GTC. I expect that to switch again to be based on GB300 if it\u2019s ever completed.\n\nA key new announcement for 2025 is a very efficient, highly integrated water-cooled optical switch, NVIDIA Photonics. This is going to take out a lot of the complexity, cost and power consumption of larger clusters. The optics terminate directly on the switch chip without needing expensive transceivers. This was probably the biggest surprise announcement in the GTC keynote.\n\nThe latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d and it can all be wired together with low-latency copper cables. This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. A prototype of the packaging using Blackwell GPUs and current switch chips was shown in the expo and I took a few pictures.\n\nCan You Afford To Own GPUs?\n\nSame as it ever was. The challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years. That makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation.\n\nOld GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU. Jensen said during his keynote that H100 GPUs aren\u2019t generally useful at this point for building AI factories, and sales of Blackwell are much higher than Hopper already.\n\nNew Capabilities for Building AI Apps and More\n\nJust like last year, there were many more announcements for markets like robotics, automotive, healthcare, quantum computing, digital twins and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new GPU hardware specifications and the optical interconnect.\n\nThe new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU, the new Dynamo software to operate inference efficiently, optimized models, a large range of third-party packages to get started with and a choice of development tools to help write the remaining code for you.\n\nJensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors at building the software that makes it easy to build applications and has open sourced most of it. Letting the days go by, once in a lifetime.\n\nNVIDIA onstage at GTC showing CUDA-X for every industry.",
        "summary": "NVIDIA is announcing new hardware that effectively obsoletes its previous products. Customers are buying whatever they can get, as it appears. \"The new stack\" of AI software is maturing rapidly. Last year I predicted that enterprise customers would find it extremely difficult to manage the pace of change. This year, I think the hyperscaler customers and cloud providers have indigestion as well. Development is also being accelerated by a variety of AI-backed tools. There is a big disconnect between the GPUs that most people are using now and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail. GH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as H100s. The largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019 and the J\u00fclich Supercom computing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Blackwell is made from two of the largest possible chips, connected directly together. Two modules fit on a 1 rack-unit-high, water-cooled motherboard. 18 compute boards fit in a 72 GPU NVL72 rack. Blackwell Ultra \u2013 B300 has 50% higher performance than B200. New process from chip supplier TSMC allows for even more power. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72. Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave. The Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance. It\u2019s targeted to ship in the second half of 2026. The next-generation Rubin Ultra racks are up to 600kW. The next-generation architecture will be named Feynman in 2028. The company is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. Last year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.  NVIDIA will release a new open source distributed inference serving library called Dynamo. This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.  NVlink is a copper-cabling system that operates inside a rack. It can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. The NVL72-based system with more than 20,000 GPUs will be deployed by October 2024. GPUs in the shared memory cluster more than doubles from 256 to 576. The latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. The new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU. Jensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "engineering",
        "gcp",
        "hpc",
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_48",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable: Chips, AI, Search, Quantum, Bitcoin, Fusion \u2013 OXD26",
      "source": "OrionX Download",
      "published_date": "2025",
      "url": "https://orionx.net/2025/03/analyst-roundtable-chips-ai-search-quantum-bitcoin-fusion-oxd26/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_49",
      "kind": "podcast",
      "subkind": "",
      "title": "SC24, Supercomputing, CryptoSuper500, Quantum, RISC-V Summit \u2013 OXD25",
      "source": "OrionX Download",
      "published_date": "2024",
      "url": "https://orionx.net/2024/12/sc24-supercomputing-cryptosuper500-quantum-risc-v-summit-oxd25/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_50",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Scaling AWS Costs to Match the Business",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/scaling-aws-costs-to-match-the-business-f9k",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_story_51",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Why are services slow sometimes?",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/why-are-services-slow-sometimes-mn3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_52",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "If at first you don't get an answer...",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/if-at-first-you-don-t-get-an-answer-3e85",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_53",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Sustainability transformation and DevSusOps",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/what-is-sustainability-transformation-32hi",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_54",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Measuring energy usage",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/adrianco/measuring-energy-usage-5ip",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_55",
      "kind": "youtube",
      "subkind": "",
      "title": "Adrian's Youtube Channel, cars, music and random stuff",
      "source": "Mostly non-technical content",
      "published_date": "2025",
      "url": "https://www.youtube.com/@adriancockcroft/videos",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_cadcf9fb",
      "kind": "file",
      "subkind": "presentation",
      "title": "Systems for Innovation Villanova",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Systems for Innovation Villanova.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5bcbe0dc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Speeding Innovation Singapore 2019",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Speeding Innovation Singapore 2019.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b665be9d",
      "kind": "file",
      "subkind": "presentation",
      "title": "YOW22-Greatest-Hits-Brisbane",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/YOW22-Greatest-Hits-Brisbane.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_76a699cd",
      "kind": "file",
      "subkind": "presentation",
      "title": "ChoosingUsingLosing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/ChoosingUsingLosing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_27b829e5",
      "kind": "file",
      "subkind": "presentation",
      "title": "Cloud Trends 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Cloud Trends 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_60526469",
      "kind": "file",
      "subkind": "presentation",
      "title": "OSSV Lock-In 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/OSSV Lock-In 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ce8da570",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices TempleUniv Cool AWS Stuff",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices TempleUniv Cool AWS Stuff.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_7b574db1",
      "kind": "file",
      "subkind": "presentation",
      "title": "QConLondon Netflix Retrospective",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/QConLondon Netflix Retrospective.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_7d8d13fc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Gophercon 2016 CSG",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Gophercon 2016 CSG.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f0b33bd4",
      "kind": "file",
      "subkind": "presentation",
      "title": "NetflixWorkshop",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/NetflixWorkshop.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d90ba45d",
      "kind": "file",
      "subkind": "presentation",
      "title": "CMG-cloud-computing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CMG-cloud-computing.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_db153f9c",
      "kind": "file",
      "subkind": "presentation",
      "title": "bil2010-millicomputing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bil2010-millicomputing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d02dc96d",
      "kind": "file",
      "subkind": "presentation",
      "title": "netflixoncloudfordevandops",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/netflixoncloudfordevandops.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_c6fb1e62",
      "kind": "file",
      "subkind": "presentation",
      "title": "bottleneckanalysis",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bottleneckanalysis.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_821640e2",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices Workshop 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices Workshop 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_f150f910",
      "kind": "file",
      "subkind": "presentation",
      "title": "Resiliency - Failure Modes and STPA",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Resiliency - Failure Modes and STPA.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_100f72ca",
      "kind": "file",
      "subkind": "presentation",
      "title": "migratingnetflixtocassandra",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/migratingnetflixtocassandra.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fe8671ea",
      "kind": "file",
      "subkind": "presentation",
      "title": "CloudNative",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CloudNative.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_bafc0c54",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn t learn from Netflix",
      "source": "infoqvideo",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/infoqvideo/What_we_learned_and_didn_t_learn_from_Netflix.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_951fa883",
      "kind": "youtube",
      "subkind": "",
      "title": "061 Open source  open data  open standards with Adrian Cockcroft and Zaheda Bhorat  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htUGrDBKAMg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_eeaa0d1d",
      "kind": "youtube",
      "subkind": "",
      "title": "086 Keynote  Cloud Trends  DevOps and Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=c0wSmr-u5vQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6c6d2a41",
      "kind": "youtube",
      "subkind": "video",
      "title": "Lotus Elise S1 - Laguna Seca - GGLC 2025",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SA7xjh1TTNk",
      "content": {
        "metadata": {
          "video_id": "SA7xjh1TTNk",
          "playlist_id": null,
          "playlist_index": 1,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_f286801c",
      "kind": "youtube",
      "subkind": "video",
      "title": "Tesla Telemetry Test Video for rs-tesla-telemetry viewer",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=wMQZDMQ8bLQ",
      "content": {
        "metadata": {
          "video_id": "wMQZDMQ8bLQ",
          "playlist_id": null,
          "playlist_index": 21,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "video"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_80cdf542",
      "kind": "youtube",
      "subkind": "",
      "title": "040 Innovating at speed   AWS and Formula 1",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=D7usPAR9a1k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3733c7ad",
      "kind": "youtube",
      "subkind": "",
      "title": "001 SC24  Supercomputing  CryptoSuper500  Quantum  RISC V Summit   OXD25",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=d5mr6Ib5ygQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_787fb720",
      "kind": "youtube",
      "subkind": "",
      "title": "015 Ep 113 Platform Engineering Teams Done RIGHT with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_16014850",
      "kind": "youtube",
      "subkind": "",
      "title": "060 AWS Summit Series 2017   Chicago  Keynote",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=tMUKC6eBjL8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_08c5f116",
      "kind": "youtube",
      "subkind": "video",
      "title": "Thunderhill Raceway - West Circuit - Spinning in Turn 3",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Zs9f700DgJM",
      "content": {
        "metadata": {
          "video_id": "Zs9f700DgJM",
          "playlist_id": null,
          "playlist_index": 30,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_c3f015ec",
      "kind": "youtube",
      "subkind": "",
      "title": "035 AWS Executive Forum 2019   Positioning Large Transformation Efforts to the Board",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=L2N9y9ovKuk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_e8c8e73a",
      "kind": "youtube",
      "subkind": "",
      "title": "009 Adrian Cockcroft   A Journey from Chaos Monkey to Sustainability",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jwRVtUEND8c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_11345db5",
      "kind": "youtube",
      "subkind": "video",
      "title": "Avi Ronen exploring the sounds of the Boss OD-200 programmable distortion pedal",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=uVglE4kiYDE",
      "content": {
        "metadata": {
          "video_id": "uVglE4kiYDE",
          "playlist_id": null,
          "playlist_index": 22,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_d5f9baf9",
      "kind": "youtube",
      "subkind": "",
      "title": "023 Coffee with Mr  IoT  Adrian Cockcroft  Amazon s pathway to sustainability through technology",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hFVxkdHh9To&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_038c23c8",
      "kind": "youtube",
      "subkind": "video",
      "title": "Tesla Model 3 Performance at Laguna Seca",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4k-yyMh6eMY",
      "content": {
        "metadata": {
          "video_id": "4k-yyMh6eMY",
          "playlist_id": null,
          "playlist_index": 23,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_67c72125",
      "kind": "youtube",
      "subkind": "",
      "title": "088 Systems for Innovation   Adrian Cockcroft  at USI",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=-vlOG3UIp9c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a412e1c7",
      "kind": "youtube",
      "subkind": "video",
      "title": "Aftermath - Fractal at Skyline",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=QcpdiPOGzo8",
      "content": {
        "metadata": {
          "video_id": "QcpdiPOGzo8",
          "playlist_id": null,
          "playlist_index": 19,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_71a9366a",
      "kind": "youtube",
      "subkind": "",
      "title": "051 Migrating to Cloud   Lessons from Netflix  Brought Up to Date",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=XrWII4ewrXA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_54b715c7",
      "kind": "youtube",
      "subkind": "",
      "title": "062 AWS Summit Stockholm 2017  Opening Keynote with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=GgkAhTtZugc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_cb35d110",
      "kind": "youtube",
      "subkind": "",
      "title": "063 AWS Summit Seoul 2017   Day2 \uae30\uc870\uc5f0\uc124 Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=061bsq0jVYU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_115dc1fa",
      "kind": "youtube",
      "subkind": "",
      "title": "029 Keynote   Modern App Development   Adrian Cockcroft  VP Cloud Architecture Strategy  AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=wYCLbLrEoqs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7eb1f382",
      "kind": "youtube",
      "subkind": "",
      "title": "045 Open Source in the enterprise with Zaheda Bhorat and Adrian Cockcroft  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=YMKsEmlw6V4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_1b3f4a2e",
      "kind": "youtube",
      "subkind": "",
      "title": "From Netflix to the Cloud DevOps Microservices and Sustainability",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/From_Netflix_to_the_Cloud_DevOps_Microservices_and_Sustainability.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_92754b58",
      "kind": "youtube",
      "subkind": "",
      "title": "080 DOES15   Adrian Cockcroft   Systems for Innovation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SaMIiLF1w20&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_42a586a4",
      "kind": "youtube",
      "subkind": "",
      "title": "008 Speeding Up Innovation   Adrian Cockcroft   YOW  2019",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=QlqQmHLXNiY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_804c4369",
      "kind": "youtube",
      "subkind": "",
      "title": "042 Developing a Chaos Architecture Mindset   Adrian Cockcroft   GOTO 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vHl7EZ5o0uY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_42d534fd",
      "kind": "youtube",
      "subkind": "",
      "title": "017 Mik   One  Adrian Cockcroft  Episode 24 ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=fDgqiA2yZ7g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_881cfc28",
      "kind": "youtube",
      "subkind": "video",
      "title": "Fireworks at Weymouth August 29th 2022 - End of Summer Bank Holiday",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=duYujRI5dFk",
      "content": {
        "metadata": {
          "video_id": "duYujRI5dFk",
          "playlist_id": null,
          "playlist_index": 12,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_d8950de9",
      "kind": "youtube",
      "subkind": "",
      "title": "043 Rapid Development   Why Serverless First Is Like Building with Lego",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5siD210Grr4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_9192c555",
      "kind": "youtube",
      "subkind": "",
      "title": "049 Adrian Cockcroft on Chaos Architecture",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ja6n5etN8hk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_98dafaa1",
      "kind": "youtube",
      "subkind": "",
      "title": "010 Don t Miss   How Cloud Architect Adrian Cockcroft Transformed Tech   A Candid Conversation with Jani",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vNIDssi4FK4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_ae43484b",
      "kind": "youtube",
      "subkind": "",
      "title": "082 Panel   Nicole Forsgren  Joshua Corman  Dan North   Adrian Cockcroft   GOTO 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=1qyljMck9Ng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_190f19af",
      "kind": "youtube",
      "subkind": "",
      "title": "012 2023 Monitorama Live Stream Day 1",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CRJcc1TqBhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1354b4cb",
      "kind": "youtube",
      "subkind": "",
      "title": "048 Adrian Cockcroft on Mapping Your Stack",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mzIdKGCOf1g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_64253f0e",
      "kind": "youtube",
      "subkind": "",
      "title": "028 Failing Over without Falling Over   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R3_ccsuPoD8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_7c0e1852",
      "kind": "youtube",
      "subkind": "video",
      "title": "Elise K20A Dyno test",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=S9JaYNz4xZo",
      "content": {
        "metadata": {
          "video_id": "S9JaYNz4xZo",
          "playlist_id": null,
          "playlist_index": 17,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_48a8bc62",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/Platform_Engineering_Done_Right.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c272be4e",
      "kind": "youtube",
      "subkind": "",
      "title": "005 From Code to Climate with Adrian Cockcroft  Clip ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5YF68KFGSzk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a5299866",
      "kind": "youtube",
      "subkind": "",
      "title": "018 Mik   One  Adrian Cockcroft Episode 15",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=nFiauu-qMV4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_83742d02",
      "kind": "youtube",
      "subkind": "video",
      "title": "Eric Tessmer closing the show with Voodoo Chile",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=YOqa9Fv7tXA",
      "content": {
        "metadata": {
          "video_id": "YOqa9Fv7tXA",
          "playlist_id": null,
          "playlist_index": 26,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_65620fe2",
      "kind": "youtube",
      "subkind": "",
      "title": "036 Speeding Up Innovation   Adrian Cockcroft   Craft 2019",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=rnygCAvVBj8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_f9cf2dda",
      "kind": "youtube",
      "subkind": "",
      "title": "084 Welcome Keynote   Adrian Cockcroft   GOTO 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SBp7AWelOhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_373e2d35",
      "kind": "youtube",
      "subkind": "",
      "title": "078 microXchg 2016   Adrian Cockcroft   Analyzing Response Time Distributions for Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5DPr4x76nvQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_24288b27",
      "kind": "youtube",
      "subkind": "",
      "title": "033 Adrian Cockcroft   Speeding Up Innovation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4jYYrkmAnS0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_165bbd84",
      "kind": "youtube",
      "subkind": "video",
      "title": "Install rs-tesla-telemetry",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pJ1_Kv36w4M",
      "content": {
        "metadata": {
          "video_id": "pJ1_Kv36w4M",
          "playlist_id": null,
          "playlist_index": 20,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1679c42f",
      "kind": "youtube",
      "subkind": "",
      "title": "032 Episode 216  Adrian Cockcroft on the Modern Cloud based Platform",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qQzPmzVfmYA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_1e6bfc50",
      "kind": "youtube",
      "subkind": "",
      "title": "054 Adrian Cockcroft   Migrating to Cloud  Netflix Cloud Journey ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=U88QkaDGx6k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c4c389c9",
      "kind": "youtube",
      "subkind": "",
      "title": "068 Paul Borrill on Time clocks and the reordering of events",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CWF3QnfihL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_e3b6b22d",
      "kind": "youtube",
      "subkind": "video",
      "title": "Thunderhill Raceway West - September 2016 - Spinning in Turn 2",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=03dqCR9xX0I",
      "content": {
        "metadata": {
          "video_id": "03dqCR9xX0I",
          "playlist_id": null,
          "playlist_index": 29,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_57c52234",
      "kind": "youtube",
      "subkind": "",
      "title": "020 ACD22 1 01 Adrian Cockcroft AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Qu1MLtHg2so&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c03c94b2",
      "kind": "youtube",
      "subkind": "",
      "title": "094 Fast Delivery   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hvFo3Q2PIQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_07d6530b",
      "kind": "youtube",
      "subkind": "",
      "title": "073 Purpose Driven Development   Adrian Cockcroft   SpringOne Platform",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=sll9RNHz3t0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_500e5ef2",
      "kind": "youtube",
      "subkind": "",
      "title": "007  HPCpodcast 77  Adrian Cockcroft on Future Architectures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=2jrCvon9oTE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0cac4e48",
      "kind": "youtube",
      "subkind": "",
      "title": "096 Fast Delivery   Adrian Cockcroft   adrianco ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=icvtIK83I_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8d6db02e",
      "kind": "youtube",
      "subkind": "",
      "title": "090 Reflections on Monitorama 2015 with Adrian Cockcroft and James Turnbull",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LuA1AjorCQs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_95ce6b03",
      "kind": "youtube",
      "subkind": "",
      "title": "011 Microservices Retrospective   What We Learned  and Didn t Learn  from Netflix",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=TOM6UhCetQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_4377a633",
      "kind": "youtube",
      "subkind": "",
      "title": "069 Simplifying the Future   Adrian Cockcroft  Battery Ventures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DGK6jjamzfY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_2482bda5",
      "kind": "youtube",
      "subkind": "",
      "title": "044 Top Technical Talent Programs   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CSL-vNQW3SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_777deef1",
      "kind": "youtube",
      "subkind": "",
      "title": "025 Day 1 Keynote by  Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=O2UzvvtHkL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1420d740",
      "kind": "youtube",
      "subkind": "",
      "title": "019 Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=6aC_nes9LIk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_35576314",
      "kind": "youtube",
      "subkind": "",
      "title": "052 Developing a Chaos Architecture Mindset   Adrian Cockcroft  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4VleTKY0QAM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c7b196b1",
      "kind": "youtube",
      "subkind": "video",
      "title": "Baby humpback whale with it's mother off Maui",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=B4meOfmcgMU",
      "content": {
        "metadata": {
          "video_id": "B4meOfmcgMU",
          "playlist_id": null,
          "playlist_index": 7,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_40ac5b1c",
      "kind": "youtube",
      "subkind": "video",
      "title": "Elise S1 K20A Thunderhill West",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=B3KQ_-X5E74",
      "content": {
        "metadata": {
          "video_id": "B3KQ_-X5E74",
          "playlist_id": null,
          "playlist_index": 16,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_3d0218df",
      "kind": "youtube",
      "subkind": "",
      "title": "098 Deep Dive into the Cloud Native Open Source with NetflixOSS   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R2kKmMyqTfc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_89895a4e",
      "kind": "youtube",
      "subkind": "",
      "title": "050 Adrian Cockcroft  AWS   KubeCon   CloudNativeCon EU 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=iqVtGmNgSJk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_302761eb",
      "kind": "youtube",
      "subkind": "",
      "title": "014 Chaos Carnival 2023 Day 2 Keynote 1   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=noKPM4UlJDk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8d02fc0c",
      "kind": "youtube",
      "subkind": "video",
      "title": "First ride in rebuilt Elise with K20A2 engine",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=1s-dp6k2uBc",
      "content": {
        "metadata": {
          "video_id": "1s-dp6k2uBc",
          "playlist_id": null,
          "playlist_index": 18,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_ea2d012f",
      "kind": "youtube",
      "subkind": "",
      "title": "041 Adrian Cockroft   Chaos Engineering   What is it  and where it s going    Chaos Conf 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=cefJd2v037U&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_377868ec",
      "kind": "youtube",
      "subkind": "",
      "title": "030 Managing Failure Modes in Microservice Architectures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=v5Gwi9AYvm4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9d0e9948",
      "kind": "youtube",
      "subkind": "",
      "title": "038 Safety Margins and Availability with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NRCQngvRNng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_4449e385",
      "kind": "youtube",
      "subkind": "",
      "title": "079 Adrian Cockroft   DevOps Enterprise Summit 2015   theCUBE    DOES15",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pfEKVDWUknU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_cd125e22",
      "kind": "youtube",
      "subkind": "",
      "title": "056 Keynote  Cloud Native at AWS   Adrian Cockcroft  Amazon Web Services",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5U-6sxR5DaQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_a20ac9db",
      "kind": "youtube",
      "subkind": "video",
      "title": "Two male humpback whales competing for a female near Maui",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=t09PgDzkM7g",
      "content": {
        "metadata": {
          "video_id": "t09PgDzkM7g",
          "playlist_id": null,
          "playlist_index": 8,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_b23f3d66",
      "kind": "youtube",
      "subkind": "",
      "title": "046 Dynamic Non Events   Adrian Cockcroft   GOTO 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mFQRn_m2mP4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_259d42a3",
      "kind": "youtube",
      "subkind": "",
      "title": "067 GopherCon 2016  Communicating Sequential Goroutines   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=gO1qF19y6KQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0106dd69",
      "kind": "youtube",
      "subkind": "video",
      "title": "DIYrobocar race 2 and crash",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7Txm0Dkc-TY",
      "content": {
        "metadata": {
          "video_id": "7Txm0Dkc-TY",
          "playlist_id": null,
          "playlist_index": 14,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_20107970",
      "kind": "youtube",
      "subkind": "",
      "title": "057 Cloud Trends   Adrian Cockcroft   GOTO 2017",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=EDZBYbEwhm8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_43bd54d8",
      "kind": "youtube",
      "subkind": "video",
      "title": "Lotus Elise S1 VTEC Autocross 2016",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=y-55GnL7SGc",
      "content": {
        "metadata": {
          "video_id": "y-55GnL7SGc",
          "playlist_id": null,
          "playlist_index": 28,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6466ddfb",
      "kind": "youtube",
      "subkind": "",
      "title": "097 Fast Delivery  Adrian Cockcroft  nginxconf 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5qJ_BibbMLw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_3b583ca2",
      "kind": "youtube",
      "subkind": "",
      "title": "099 AWS re Invent 2014    ARC201  Cloud Native Cost Optimization",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LZwlkqERv2g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_f2d154d4",
      "kind": "youtube",
      "subkind": "",
      "title": "075 OSCON Roundtable  The Future of IT Infrastructure",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=9q08veg5WkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "infrastructure"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_a6928be7",
      "kind": "youtube",
      "subkind": "video",
      "title": "GGLC Autocross at Crows Landing - Oct 31, 2021",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=yQ5vQxflDk0",
      "content": {
        "metadata": {
          "video_id": "yQ5vQxflDk0",
          "playlist_id": null,
          "playlist_index": 13,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_befeedd7",
      "kind": "youtube",
      "subkind": "",
      "title": "053 Adrian Cockcroft on The New De Normal   Untangling  Kitchen Sink  Database Schemas",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Y6nKD-sK6tg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_30fc2a90",
      "kind": "youtube",
      "subkind": "",
      "title": "059 Live from the NY Summit   Interview with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7CJrAhpZlXE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a926f53d",
      "kind": "youtube",
      "subkind": "",
      "title": "002 It s Complicated    Adrian Cockcroft   YOW  2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NJ-3eNx8iBo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_68114147",
      "kind": "youtube",
      "subkind": "",
      "title": "066 The Evolution of Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=b8TDodu5E0k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6ef2b329",
      "kind": "youtube",
      "subkind": "",
      "title": "085 OpenStack Silicon Valley 2015   Web Services and Microservices  The Effect on Vendor Lock In",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ewuw1s4cnJA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3aafdc07",
      "kind": "youtube",
      "subkind": "",
      "title": "022 DevOpsDays Silicon Valley 2018   Dynamic Non Events by Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=C9VchTAd7AM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_2c496658",
      "kind": "youtube",
      "subkind": "",
      "title": "058 AWS Summit Tel Aviv 2017  Keynote with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Cg79AaEAPwA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c6b7b76b",
      "kind": "youtube",
      "subkind": "video",
      "title": "First try at a clear lap at Sonoma Raceway - Elise S1/K20",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=E5ueghmX2PU",
      "content": {
        "metadata": {
          "video_id": "E5ueghmX2PU",
          "playlist_id": null,
          "playlist_index": 5,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_c057b69d",
      "kind": "youtube",
      "subkind": "",
      "title": "016 Fireside Chat  Adrian Cockcroft Talks with the Authors of The Value Flywheel Effect",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=713_6MBW7q0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_3d7bc203",
      "kind": "youtube",
      "subkind": "",
      "title": "087 OpenStack Now Podcast  Episode 5  Adrian Cockcroft  Battery Ventures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jOxFFmoRkzw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_29ad458b",
      "kind": "youtube",
      "subkind": "",
      "title": "071  The Evolution of Microservices   Adrian Cockroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Mg4Cs2K7f98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7d59869a",
      "kind": "youtube",
      "subkind": "",
      "title": "031 AWS re Invent 2019  Innovation at speed  ARC203 ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8ona5ZTu4_E&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_0aa723a6",
      "kind": "youtube",
      "subkind": "",
      "title": "095 Migrating to Cloud Native with Microservices   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DvLvHnHNT2w&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_56b5f9de",
      "kind": "youtube",
      "subkind": "",
      "title": "024 Adrian Cockcroft Presents  The Startup Lifecycle",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=uRcSFXafIWM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_5814af06",
      "kind": "youtube",
      "subkind": "video",
      "title": "WCLM22 Mount Palomar Hillclimb #1 - Lotus Elise S1",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pYcYBuwYRzo",
      "content": {
        "metadata": {
          "video_id": "pYcYBuwYRzo",
          "playlist_id": null,
          "playlist_index": 11,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_4e3a3691",
      "kind": "youtube",
      "subkind": "video",
      "title": "Eric Tessmer Band Gold Digger LA - Nov 1, 2018",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=fIAQIWp04EY",
      "content": {
        "metadata": {
          "video_id": "fIAQIWp04EY",
          "playlist_id": null,
          "playlist_index": 25,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_d6c56b5f",
      "kind": "youtube",
      "subkind": "",
      "title": "091 Adrian Cockcroft on Microservices and the Importance of Next Gen Apps for Businesses",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4ClmJxVz1SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_5f4fa47d",
      "kind": "youtube",
      "subkind": "",
      "title": "064 microXchg 2017   Adrian Cockcroft  Shrinking Microservices to Functions",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ZgxZCXouBkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_70e09c9a",
      "kind": "youtube",
      "subkind": "",
      "title": "004  022   Kubernetes for Humans with Adrian Cockcroft  Nubank ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=xSy-usyvWC4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_195319bf",
      "kind": "youtube",
      "subkind": "",
      "title": "070 Monitoring Microservices   A Challenge   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Q3-XKQbMkXg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3dd8223b",
      "kind": "youtube",
      "subkind": "",
      "title": "006 Adrian Cockcroft  OrionX   Supercloud 5",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=phZabEbs2-4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_f34c92b2",
      "kind": "youtube",
      "subkind": "",
      "title": "089 Adrian Cockcroft   DockerCon 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=K4rcnaiyc_M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6dc90cfb",
      "kind": "youtube",
      "subkind": "",
      "title": "027 Map Camp 2020   Maps  Games and Morality",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qtMHOuGw0lI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_712cfd77",
      "kind": "youtube",
      "subkind": "",
      "title": "092  Monitoring Microservices  A Challenge    Adrian Cockcroft Keynote",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=smEuX-Hq6RI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_2214dc0a",
      "kind": "youtube",
      "subkind": "",
      "title": "083 AWS re Invent 2015    SPOT304  How Adrian Cockcroft Helped Move Customers to AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LMYYJuh9t70&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_2dfd0089",
      "kind": "youtube",
      "subkind": "",
      "title": "081 Adrian Cockcroft Interview at DevOps Summit 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htIO8ydywa4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7a2d0f63",
      "kind": "youtube",
      "subkind": "",
      "title": "013 Adrian s Greatest Hits  B Sides   Re issues   Adrian Cockcroft   YOW  2022",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=kc9dyTF2PjI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0252811b",
      "kind": "youtube",
      "subkind": "",
      "title": "047 Adrian Cockcroft on Digital Transformation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7lDWXtNjVyQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a6e572a5",
      "kind": "youtube",
      "subkind": "video",
      "title": "WCLM22 Mount Palomar Hillclimb #3 (foggy) - Lotus Elise S1",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=tvATIaHZ_Uo",
      "content": {
        "metadata": {
          "video_id": "tvATIaHZ_Uo",
          "playlist_id": null,
          "playlist_index": 9,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_65efc3e6",
      "kind": "youtube",
      "subkind": "",
      "title": "021 The Lotus Elise was unlike any other car ever made   Revelations with Jason Cammisa   Ep  21",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=h0jXhOmL7Xg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_5c96d39f",
      "kind": "youtube",
      "subkind": "",
      "title": "055 Adrian Cockcroft on the Evolution of Business Logic from Monoliths  to Microservices  to Functions",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=aBcG57Gw9k0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c82e0a5c",
      "kind": "youtube",
      "subkind": "",
      "title": "026 AWS re Invent 2020  Adrian Cockcroft s architecture trends and topics for 2021",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4rLVJFHfK98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_b24ce550",
      "kind": "youtube",
      "subkind": "video",
      "title": "Fireworks on Weymouth Beach for Guy Fawkes 2024",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Dh61G9ZLaRc",
      "content": {
        "metadata": {
          "video_id": "Dh61G9ZLaRc",
          "playlist_id": null,
          "playlist_index": 2,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_2a9ef97d",
      "kind": "youtube",
      "subkind": "video",
      "title": "Humpback whales flapping flippers and tails off Maui",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=FQJ4JDpw58Q",
      "content": {
        "metadata": {
          "video_id": "FQJ4JDpw58Q",
          "playlist_id": null,
          "playlist_index": 6,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_b4111520",
      "kind": "youtube",
      "subkind": "",
      "title": "076 Microservices  What s Missing ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=p848Dr3EtQg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_f99866b7",
      "kind": "youtube",
      "subkind": "",
      "title": "037 Adrian Cockcroft on the importance of the culture at Amazon",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=eMXMJ2lV1_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6f4889c5",
      "kind": "youtube",
      "subkind": "",
      "title": "039 Adrian Cockcroft  AWS   AWS re Invent 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=lhHmXNDvd3M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_91c8b7bd",
      "kind": "youtube",
      "subkind": "",
      "title": "065 It s Simple      Adrian Cockcroft   GOTO 2016",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=JSnOk-4m7C4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0662662c",
      "kind": "youtube",
      "subkind": "video",
      "title": "Red Dirt at the Willie K blues festival",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8alqoL0UggY",
      "content": {
        "metadata": {
          "video_id": "8alqoL0UggY",
          "playlist_id": null,
          "playlist_index": 24,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9ef244b9",
      "kind": "youtube",
      "subkind": "",
      "title": "074 Mendix World 2016 Adrian Cockcroft Keynote  It s Simple",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Bn2WLIpPxX8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6d2fc92d",
      "kind": "youtube",
      "subkind": "",
      "title": "072 It s Simple    Adrian Cockcroft   GOTO 2016",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=a8Re9Cvv6nU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_48d9d7e0",
      "kind": "youtube",
      "subkind": "",
      "title": "093 The State of the Art in Microservices by Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pwpxq9-uw_0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6b905594",
      "kind": "youtube",
      "subkind": "",
      "title": "034 Open Source Force Multipliers   Adrian Cockcroft  Amazon Web Services ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=E1aAG2iftMo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9294eecb",
      "kind": "youtube",
      "subkind": "",
      "title": "100 Keynote Speakers Adrian Cockcroft and Nicholas Heller Interview at TM Forum Live 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=dWPdsd8m8KM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_7cbdb04f",
      "kind": "youtube",
      "subkind": "video",
      "title": "Heading home through the mudslides",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=bdTMAyllZj8",
      "content": {
        "metadata": {
          "video_id": "bdTMAyllZj8",
          "playlist_id": null,
          "playlist_index": 27,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_39ae9f5a",
      "kind": "youtube",
      "subkind": "video",
      "title": "Salinas AutoX GGLC Lotus Elise S1 K20 - best lap",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=6CDHNPA5-FI",
      "content": {
        "metadata": {
          "video_id": "6CDHNPA5-FI",
          "playlist_id": null,
          "playlist_index": 3,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_eb7ae537",
      "kind": "youtube",
      "subkind": "video",
      "title": "WCLM22 Mount Palomar Hillclimb #2 - Lotus Elise S1",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=tSV7LZA_oK0",
      "content": {
        "metadata": {
          "video_id": "tSV7LZA_oK0",
          "playlist_id": null,
          "playlist_index": 10,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_b0b95053",
      "kind": "youtube",
      "subkind": "video",
      "title": "Avi Ronen showing what the Boss DD-8 delay pedal can do",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=TBT5t79E3TI",
      "content": {
        "metadata": {
          "video_id": "TBT5t79E3TI",
          "playlist_id": null,
          "playlist_index": 15,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8aeb3627",
      "kind": "youtube",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Ev3rIMLujTM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_75aa4cef",
      "kind": "youtube",
      "subkind": "video",
      "title": "Salinas AutoX GGLC Lotus Elise S1 K20 - almost spin\u2026",
      "source": "Adrian Cockcroft",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=v6iLLejY5To",
      "content": {
        "metadata": {
          "video_id": "v6iLLejY5To",
          "playlist_id": null,
          "playlist_index": 4,
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_c3ac9176",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_76e7a5e0",
      "kind": "podcast",
      "subkind": "",
      "title": "Supercomputing 24 CryptoSuper500 Quantum RISC V OXD25.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Supercomputing_24_CryptoSuper500_Quantum_RISC_V_OXD25.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_3a5123e2",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/940fbd39-3936-441f-8087-4ce99b7be568/audio/050961ea-530c-49a0-ae27-046982856d34/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_363e396b",
      "kind": "podcast",
      "subkind": "",
      "title": "022 Kubernetes for Humans with Adrian Cockcroft Nubank Komodor info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/022_Kubernetes_for_Humans_with_Adrian_Cockcroft_Nubank_Komodor_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_869d1844",
      "kind": "podcast",
      "subkind": "",
      "title": "From Netflix to the Cloud Adrian Cockroft on DevOps Microservices and Sustainability.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628032&ei=YBDeZ-z8KIKRsfIPj6-B4Qo&ip=71.198.156.212&id=o-ALRdyd3qfKsy1G9HxamFg5Jlp29gbc4zvxFPOtQwDmJy&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606432%2C&mh=8F&mm=31%2C26&mn=sn-n4v7sns7%2Csn-a5msenek&ms=au%2Conr&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcMw4zqjQxITUESI3F4OdfRvFZOxrqDk-TUOZn4x7eCCs205IRITjiaZ0dzbr0GdxJw9s8JX8zL8&spc=_S3wKgfT7Xqfy-T0aMoIB3PSIXueOFKlyiIvrfsx4jJ7TPkKtHTx8VviE8JJhRrTMnY7&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAMckZWnpsB3fBlBTaiYaBnYTtPLf7dxFgvURPohpW8q6AiEA2NBOIgXi3JB21VU2yn8fZq8omdk17NCABrNukbiJMNo%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRgIhANvmiGQSkP7c4tahT-l3TG0XWq4jeEcTCua8y5FqJnCqAiEAzd4tM3x1zU7kiX_g6k48CXhwlGAIQfqZEgKqK60yRQY%3D",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_06bf9f18",
      "kind": "podcast",
      "subkind": "",
      "title": "youtube FY3asCV9qOE.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628024&ei=WBDeZ7nKBbOWsfIPhpCtgA4&ip=71.198.156.212&id=o-AJVQQivjY88dhhg9gljXdceHbQmr6Lyc6-02WD9zBIiZ&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606424%2C&mh=8F&mm=31%2C29&mn=sn-n4v7sns7%2Csn-o097znsd&ms=au%2Crdu&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcNS3vFqoapn2rXkMIxcI1MX3Z2bIKmie9TZ-dqQIiYCvHUuJdg73wmlfWCcdJ8yTntmdBmuruuL&spc=_S3wKi4z4DPgT8nu312evHyNCFxrXsReJNelRbqUtemQiLgCxg34NboTSt1hY9o94IFI&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhANKPHEhIXUjzklRAhlQU1ZhVlRkwrVgx3EH94x7mBDQFAiBJKmW4DbD5xYvMlvB-rKLq6fr3QTxp9wjfHFmTCFWWnA%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRAIgEDNtYM-MWxZhuooRi_jNP5Q51AN6oKRsEnxFblq3qJYCIBhhL1wzEun4pSgzvsfjcyq_PzgIST3xi5naspt8wASR",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4457c57a",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_AWS_Sun_eBay_Netflix_and_Others_Vet_Adrian_Cockcroft_Talks_Cloud_HPC_AI_and_the_Amazon_Sustainability_Data_Initiative_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f3e33df0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "http://orionx.net/wp-content/uploads/2022/09/036@HPCpodcas_Adrian-Cockcroft_Cloud_ESG_20220914.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9b33411",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ1MzB9fX1dfQ__&Signature=M7LCJcOysGLmo0khzuJvnby05s8k6RXaFozS0DQzzfhv41VUmroFPw5UmZJRY6CXvE4lwVXQ8kK4hnRzIqmYJijoP7f8Shxb46VOfwtKvJPjLO~Nv9zWOHkfJH0x2Z0SZvD-cMzI1zQJHYEbAMmAp3sSmy04vfQ9TMpzmHNvtHFWb~xa3hwXIlEWJQ538OSY2ahtPOnFZgrsxCQTaXC0hVihgRWlWMLSsQ0MF-4JvXc4kXl75j6GAPC7LAKWxc6mBTPrst8MDV6dgPHcUFURE5aIkQceyUjGSM5lun1T0tjyp8vcwiEe~yjt64gh6gXpIGAPJDHJmCX7aDMPgfS0Kg__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_af2c8e31",
      "kind": "podcast",
      "subkind": "",
      "title": "Podcast Adrian Cockcroft on Serverless Continuous Resilience Wardley Mapping Large Memory Systems and Sustainability info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Podcast_Adrian_Cockcroft_on_Serverless_Continuous_Resilience_Wardley_Mapping_Large_Memory_Systems_and_Sustainability_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast",
        "resilience",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_aec0d230",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_New_Monitoring_for_Services_That_Feed_from_LLMs.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_a2854b1c",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_10b527e1",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Nvidia_s_Superchips_for_AI_Radical_but_a_Work_in_Progress.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4d217b30",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/05/057@HPCpodcast_Adrian-Cockcroft_Carbon-ESG_20230503.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4ca2fe2b",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/c27ff629-b50f-4c9d-a4b8-154adb18e4a5/audio/21249ddc-8d38-4138-8e5d-612df9cfb2b5/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9e2e9e5",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://traffic.megaphone.fm/SED1616041688.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1d8d897c",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Architecture for the Cloud InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "http://ress.infoq.com/downloads/mp3downloads/interviews/infoq-12-nov-adrian-cockcroft.mp3?Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6IioiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDkxNzN9LCJJcEFkZHJlc3MiOnsiQVdTOlNvdXJjZUlwIjoiMC4wLjAuMC8wIn19fV19&Signature=Fj4ki7BoXgztHB73H9~FwnuDJgKS1LnMG2XTsJD6D8Gbk0-6sMON8rsDY1mdGExfsssrSZnvsbdfLTcmYEhd8w07G0RyJzxmFRa25asaM~MSJIOgdZjRZ3A7j0WfQ631a3YOC2blChyTN62yUQl1YFNxSFF8yQeHkMOqiugL8v4_&Key-Pair-Id=APKAIMZVI7QH4C5YKH6Q",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f131fbf4",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Schedulers_with_Adrian_Cockcroft_Software_Engineering_Daily.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7262cb55",
      "kind": "podcast",
      "subkind": "",
      "title": "Sam Newman Magpie Talkshow Episode 22 Adrian Cockcroft.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Sam_Newman_Magpie_Talkshow_Episode_22_Adrian_Cockcroft.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b9c74b59",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_764ae438",
      "kind": "podcast",
      "subkind": "",
      "title": "unknown info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/unknown_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_af469a08",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Decarbonization_ESG_w_Adrian_Cockcroft_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b2a0efa0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_Tech_Analyst_Adrian_Cockcroft_on_Trends_Driving_Future_HPC_Architectures_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_33a2686c",
      "kind": "podcast",
      "subkind": "",
      "title": "When Netflix Bet On AWS.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/62cedf3d-2540-4354-85b4-8a5e293d1104/episodes/bb12eeb0-7db8-413c-b7b1-aa3a17a1ae64/audio/91c97a87-12d7-4be9-b628-85e82601b948/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_132b3cb0",
      "kind": "podcast",
      "subkind": "",
      "title": "The Evolution of Microservices with Adrian Cockroft info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_Evolution_of_Microservices_with_Adrian_Cockroft_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_e90e6bf6",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable Chips AI Search Quantum Bitcoin Fusion OXD26.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Analyst_Roundtable_Chips_AI_Search_Quantum_Bitcoin_Fusion_OXD26.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_c264cb7b",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_ffd1f118",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDM4MTJ9fX1dfQ__&Signature=XDo1OiQ7kO0RHU3Y5WAHECtOp57K7B1AXwhxGedTt2jQibFn0Kv8dhm-Oa~YtSbBpiXaF~C2wu5qPqmOPcz~dxoxiprzRxlr-IZ1dsiT0KrCV6Z-b9uQFTsXlwXtGHmABI4oBCocspLkVf-Zi4znHEPFE7HmjnscSrw~rvmiuLY9eLHc4pR3hLQtFkg-ALmovjKh281H8Pa0RZSK3rQcNe-vKREBK6MZ7D9KyP3AhQ8RAvOEvRFhdoFDlOQ~9F1HuY3Ak06u7SNvHnk7qq1Ut~f61NAN0Fzz-4~S0844RCfzMDNQuM3z6zWQjYArarAEYMnRvOEKcFN2fjiPMOxgaw__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7bd25a6f",
      "kind": "podcast",
      "subkind": "",
      "title": "Magpie Talkshow Episode 22 Adrian Cockcroft Spaghetti Monster Simulation Edition .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/DwsToh00RUjc.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvRHdzVG9oMDBSVWpjLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ3OTR9fX1dfQ__&Signature=Gvi1r7yNumawbEeJuvVaDfsMUiBQDoJHIR1e7r~QADr7yNVD1D03NomYjlAfDXQaKNpRcomVgYN~bAJ03fJuNjTUn39oWi33Vh9-8S6J0pAXhZWXC6FyamS7ea4ZJ5DKgrhsWQn5v~FVeGQ5K-K87~lOqNDsmFUHNA2DeShEiNZz3uWjmgFNj-SVhCfFhZDQreJac117RfAZNmNB4eIifgHa806fPBl2FelYrdo17yhaHWE2Yak6FEjBtJQEjDEn3cyFikDrtbh2~~5idW6P6VyBrmWtNMJpwm5RJNrSNKIJpleK5XTZ3xhGQF6GM93tMe~ahD37nnDVTLxrvWpkWA__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_99cc1983",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Adrian_Cockcroft_on_Future_Architectures_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    }
  ]
}