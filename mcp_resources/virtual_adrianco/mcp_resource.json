{
  "metadata": {
    "author": "virtual_adrianco",
    "version": "1.0",
    "last_updated": "2025-05-26T11:21:54.329477+00:00",
    "content_count": 204,
    "content_types": {
      "youtube": 4,
      "podcast": 52,
      "infoqvideo": 2,
      "book": 10,
      "story": 18,
      "youtube_playlist": 100,
      "file": 18
    },
    "processing_stats": {
      "total_items": 204,
      "processed_items": 204,
      "failed_items": 0,
      "processing_time": 2651.445447
    }
  },
  "content": [
    {
      "id": "virtual_adrianco_youtube_0",
      "kind": "youtube",
      "subkind": "",
      "title": "From Netflix to the Cloud: DevOps, Microservices and Sustainability",
      "source": "Platform Engineering Podcast",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=FY3asCV9qOE",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1",
      "kind": "podcast",
      "subkind": "",
      "title": "Chat about open source and platforms",
      "source": "Kubernetes for Humans",
      "published_date": "3/27/2024",
      "url": "https://komodor.com/resources/022-kubernetes-for-humans-with-adrian-cockcroft-nubank/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_2",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia\u2019s Superchips for AI: Talk about GH200 and flip.ai",
      "source": "The New Stack Analysts with Alex Williams and Sunil Mallya",
      "published_date": "3/14/2024",
      "url": "https://thenewstack.io/nvidias-superchips-for-ai-radical-but-a-work-in-progress/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_3",
      "kind": "podcast",
      "subkind": "",
      "title": "Monitoring for AI",
      "source": "The New Stack Analysts with Alex Williams",
      "published_date": "3/28/2024",
      "url": "https://thenewstack.io/the-new-monitoring-for-services-that-feed-from-llms/",
      "content": {},
      "tags": [
        "ai",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4",
      "kind": "podcast",
      "subkind": "",
      "title": "From Code to Climate",
      "source": "Unlearn Podcast with Barry O'Reilly",
      "published_date": "1/31/2024",
      "url": "https://barryoreilly.com/explore/podcast/sustainable-future-for-technology/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_5",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices and Teraservices",
      "source": "Infoq with Wesley Reisz",
      "published_date": "",
      "url": "https://www.infoq.com/podcasts/adrian-cockcroft/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_6",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_7",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft-ep-24/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "John Myer",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_9",
      "kind": "podcast",
      "subkind": "",
      "title": "Netflix on AWS",
      "source": "CloudFix",
      "published_date": "",
      "url": "https://cloudfix.com/podcast/when-netflix-bet-on-aws/",
      "content": {},
      "tags": [
        "aws",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_10",
      "kind": "podcast",
      "subkind": "",
      "title": "Serverless and continuous resilience",
      "source": "Charles Humble/Container Solutions",
      "published_date": "",
      "url": "https://blog.container-solutions.com/adrian-cockcroft-on-serverless-continuous-resilience",
      "content": {},
      "tags": [
        "resilience",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_11",
      "kind": "podcast",
      "subkind": "",
      "title": "Future Architectures",
      "source": "HPC Podcastmwith Shahin/OrionX",
      "published_date": "11/1/2023",
      "url": "https://orionx.net/2023/11/hpcpodcast-77-adrian-cockcroft-on-future-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_12",
      "kind": "podcast",
      "subkind": "",
      "title": "ESG and HPC",
      "source": "HPC Podcast with Shahin/OrionX",
      "published_date": "5/1/2023",
      "url": "https://orionx.net/2023/05/hpcpodcast-57-decarbonization-renewable-energy-esg-w-adrian-cockcroft/",
      "content": {},
      "tags": [
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_13",
      "kind": "podcast",
      "subkind": "",
      "title": "Evolution of Microservices",
      "source": "ACM Bytecast",
      "published_date": "6/17/2016",
      "url": "https://learning.acm.org/techtalks/microservices",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_14",
      "kind": "podcast",
      "subkind": "",
      "title": "Scheduling",
      "source": "Software Engineering Daily",
      "published_date": "7/6/2016",
      "url": "https://softwareengineeringdaily.com/2016/07/06/schedulers-with-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_15",
      "kind": "podcast",
      "subkind": "",
      "title": "Architecting for cloud",
      "source": "Infoq interview with Michael Floyd",
      "published_date": "1/24/2013",
      "url": "https://www.infoq.com/interviews/Adrian-Cockcroft-Netflix/",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_16",
      "kind": "podcast",
      "subkind": "",
      "title": "Interview, early life etc",
      "source": "Sam Newman's Magpie Talkshow",
      "published_date": "10/8/2016",
      "url": "https://samnewman.io/blog/2016/10/08/magpie-talkshow-episode-22-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_17",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices",
      "source": "SE Radio with Stefan Tilkov",
      "published_date": "12/10/2014",
      "url": "https://se-radio.net/2014/12/episode-216-adrian-cockcroft-on-the-modern-cloud-based-platform/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_18",
      "kind": "podcast",
      "subkind": "",
      "title": "Cloud and HPC",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/09/hpcpodcast-aws-sun-ebay-netflix-and-others-vet-adrian-cockcroft-talks-cloud-hpc-ai-and-the-amazon-sustainability-data-initiative/",
      "content": {},
      "tags": [
        "cloud",
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_19",
      "kind": "podcast",
      "subkind": "",
      "title": "SC23 summary",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/hpcpodcast-tech-analyst-adrian-cockcroft-on-trends-driving-future-hpc-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_20",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn't learn from Netflix",
      "source": "Infoq - QCon London talk/video integrated together",
      "published_date": "3/28/2023",
      "url": "https://www.infoq.com/presentations/microservices-netflix-industry/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_book_21",
      "kind": "book",
      "subkind": "",
      "title": "Q&A: Serverless Development on AWS - sustainability expert section",
      "source": "O'Reilly - Sheen Brisals and Luke Hedger",
      "published_date": "3/14/2024",
      "url": "",
      "content": {},
      "tags": [
        "aws",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_book_22",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Building Green Software",
      "source": "O'Reilly - Anne Curry",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_23",
      "kind": "book",
      "subkind": "",
      "title": "Foreword1: The Value Flywheel Effect",
      "source": "IT Revolution - David Anderson",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_24",
      "kind": "book",
      "subkind": "",
      "title": "Foreword2: Reaching Cloud Velocity",
      "source": "AWS: Jonathan Allen, Thomas Blood",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_book_25",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: In Search of Certainty 2nd Edition",
      "source": "Mark Burgess",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_26",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Irresistable APIs",
      "source": "Kirsten Hunter",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_27",
      "kind": "book",
      "subkind": "1,21-187,215-233",
      "title": "Capacity Planning for Internet Services",
      "source": "Sun Press Blueprints - adrianco, Walker",
      "published_date": "2000",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/authors/virtual_adrianco/caphi.pdf",
      "content": {
        "text": "901 San Antonio Road\nPalo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131CapacityPlanningfor\nInternetServices\nQuick planning techniques for high growth rates\nbyAdrianCockcroftandBillWalker\nPart No. 806-3684-10\nMay 2000, Revision 01Sun Microsystems, Inc.\n\n1CHAPTER1\nIntroduction\nAlmost every business, from a corner shop to a multinational corporation, is faced\nwith competitive pressure to \u201cgo online\u201d and provide services via an Internet site. Inaddition, a large number of new online businesses are being implemented in a maddash to capture the attention and wallets of a huge and fast-growing number ofInternet users. Success is measured by growth in the number of pages viewed,registered users, and in some cases, by the amount of business transacted.\nSuccess comes at a cost. Rapid growth can overwhelm the ability of the site to\nprovide services with acceptable performance. There have been many reports of Websites that have suddenly attracted too many users and collapsed under the strain.\nStartup dot-com companies spend most of their investors\u2019 funds on advertising as\nthey attempt to establish their name in the collective consciousness of consumersand the media. Established companies are concerned about maintaining theirpreexisting brand image while gaining credibility as they add Internet services totheir traditional businesses. Therefore, it is important to provide enough capacity tocope with sudden increases in load.\nTraditional computer installations have a relatively static number of users, and a\ndetailed understanding of their workload patterns can be obtained. Growth rates inthis environment are relatively low, and costs can be optimized by careful capacityplanning. Internet services are available to many millions of potential users of theservice. The load on the service depends upon the whim of the users. If a high-profile advertisement or news item reaches a large number of people, there is a greatopportunity to expand the user base as long as the site can cope with the load.Growth rates for successful sites in this environment are very high, and very hard toplan for. It is normal to lurch from one crisis to the next and to throw hardwarequick fixes at the problem regardless of the cost.\nCapacity planning is an optimization process. Service level requirements can be\npredicted and balanced against their costs. Even if there are few cost constraints, it isimportant to have good estimates of how much spare capacity the site has andwhether it can survive the next load peak.\nCapacity planning is a well-known discipline, particularly for sites that have a\nmainframe-oriented background. When very high growth rates occur, timeconstraints prevent normal techniques from being applied. This Sun BluePrints book\n\n2Chapter 1    Introductioncharts a course through the available techniques and tools; examines time scales and\nreturn on investment for different methodologies; provides a framework fordecomposing big problems into solvable subproblems; and gives simple, practicalexamples that provide results in hours thanks to spreadsheet-based techniques. Ifyou wait until you have chosen and purchased an expensive tool, you will then needweeks or months to learn how to use it. These tools are useful and powerful, andtheir use is also described in detail.\nThe topics covered in this book can be divided into the following sections:\n\u25a0Principles and processes\n\u25a0Scenario planning techniques\n\u25a0The effective use of tools\nCompared to conventional capacity planning techniques, the Internet service\ncapacity planning techniques described in this book must cope with high rates ofchange, work with limited system administrator experience, and steer a paththrough confusing choices and a lack of tools.\nBecause there are also increased availability requirements, it is important to give\npriority to simple, common-sense principles that can be followed consistently.\n\n3CHAPTER2\nTheoretical Principles\nWith the exponential growth of the Internet and consumer electronic commerce on\nthe Internet, service quality has become a key component of success. Electroniccommerce and commercial portals on the Internet expose the business front officeand the related business back office systems to scrutiny by the direct consumer, aswell as by the news media.\nSun servers have entered the traditional datacenter environment where system\navailability, manageability, and service availability are key components in providinga solution to business requirements. With an established and encompassing\u201cproduction environment\u201d life-cycle plan, robust solutions can be safely and reliablyplaced into production and evolved to meet the changing and growing needs of thebusiness.\nThis chapter presents methods for managing performance and establishing service\nlevel agreements (SLAs). It also examines the IT frameworks designed to providesolutions for production environment business requirements. Additional ITframeworks from ISO FCAPS (fault, configuration, application, performance, andsecurity) models are presented, and tips are offered for implementation.\nPerformance Management\nPerformance management is the measurement, analysis, and optimization ofcomputer resources to provide an agreed-upon level of service to the end-user. Bydefining performance management and identifying the key components required tosafely and accurately implement performance management in the datacenter, youcan minimize the risks associated with high growth rates. These risks include:\n\u25a0System downtime due to unexpected overload\n\u25a0Negative customer feedback\n\u25a0Loss of potential business due to poor response times\n\u25a0Loss of customer loyalty due to perceived lack of service quality\n\n4Chapter 2    Theoretical PrinciplesThe key components that we will concentrate on to define our scope of performance\nmanagement will include:\n\u25a0Throughput\n\u25a0Latency\n\u25a0Utilization\nThroughput is defined as the number of defined actions performed in a given period\nof time. Latency is defined as the time that it takes to complete a well-defined action.\nThroughput and latency can be applied at a high level while measuring transactions\nat the end-user level. They can also be examined for discrete events such as networkpackets, disk activity, or system centerplane bandwidth consumption. Each of theselevels of detail can be measured, reported, and analyzed for impact on overallsystem performance, provided that you understand the events being monitored andthe capabilities of the resources involved in providing those actions.\nUtilization is usually expressed as the percentage of the overall capability of a given\nresource consumed during a defined action or quantity of actions. Resourceutilization and resource utilization planning are the cornerstones of capacityplanning. Utilization is a measure of system resource impact, throughput defines thequantity of services, and latency defines the quality of the services being provided.\n\n Layers of Service Architecture 5Layers of Service Architecture\nSeveral layers of resources and resource consumption can be defined, tuned,\nmeasured, and reported within the service architecture. Categorizing these layers(see\nTABLE 2-1 ) and defining the expectations for each level provides the guidelines\nfor the design and implementation of a system.\nEach of these layers of the overall service architecture affords opportunities for\ntuning, measurement, reporting, and management. Each layer will have its ownparticular scale of benefit and investment to introduce change.\nThe Business layer often provides the most significant opportunities for \u201ctuning\u201d\nand has the most significant contribution to the overall architecture. The Applicationlayer and Hardware layer can also provide a significant and obvious impact on theoverall performance of the architecture. The Operating System and Infrastructurelayers are often where administrators look for some magic cure, but these layersoften provide the least opportunity for impacting the performance of a system.TABLE 2-1 Layers of Service Architecture\nLayer Components\nBusiness Number of users\nBatch job definitionsReport schedulesBusiness hours\nApplication N-tiered architecture\nDatabase layoutSoftware architectureAccess methods\nOperating System Kernel tuning\nOS revisionsDisk volume layouts\nHardware CPU\nDiskMemory\nInfrastructure Network architecture\nEnterprise managementBackup strategies\n\n6Chapter 2    Theoretical PrinciplesPhases of Performance Management\nPerformance management can be applied in an iterative, cyclic, four-phase approach\n(see FIGURE 2-1 ).\nFIGURE 2-1 Four-Phase Approach to Performance Management\nThe output of each phase is used as input to the following phase. Each phase mustreach a steady state in which the results can be published and transferred to thefollowing phase. Once the four phases are locked in place and all results have beenpublished, those results are fed into the next generation of phases. The next sectionsdiscuss each phase in detail.\nA relatively simple change and configuration management process integrated into\nthe performance management deliverables can greatly improve the efficiency andaccuracy of performance management. This change management can be as simple asgenerating and maintaining revision numbers for the documentation and reportsthat are produced.\nHistorical revisions should be retained for future examination and change analysis.\nThese revisions also provide a history of changes to the data analyzed and theresulting conclusions. This compilation of historic data and conclusions can helpreduce repetition of effort and acts as a guide that displays the impacts (bothpositive and negative) of past load planning and capacity planning.Load\nPlanning\nCapacity\nPlanningBaselining\nResource\nManagement\n\n Phases of Performance Management 7Baselining\nBaselining creates a snapshot of a system as it currently exists and generates reports\nthat describe the system performance and the characterization of the workload beingmeasured. To baseline a system, we first describe the goals of system performance inthroughput, latency, and utilization within each level of the service architecture.\nThe business requirements of the service being provided by the established\nworkload must first be defined. These end-user service level requirements caninclude:\n\u25a0Transaction rates\n\u25a0Transactional volumes\n\u25a0Hours of operation\n\u25a0Critical time frames for processing batch loads\n\u25a0Concurrent user session requirements\nThese are the same business requirements defined in the SLA, which is explained in\ndetail in Chapter 3, \u201cSuggested Processes.\u201d\nA configuration inventory establishes a record of the current state of the five layers\nof the service architecture and provides a reference for modifying that architecture.An accurate representation including hardware, software, and operating systemversions is critical to creating an accurate inventory. This configuration inventory isconsidered \u201clocked down\u201d for the life span of the baselining process. This causes allchanges to the operating environment to be considered a tuning event that expiresthe current system state and triggers a new baselining cycle.\nAt this point, service performance is monitored and measured against the goals\ndefined in the SLA. In addition, system performance is monitored and measuredagainst the desired resource consumption thresholds defined in the key performanceindicator (KPI) document, as addressed in Chapter 3.\nIf any tuning opportunities are identified in any of the five layers of system\nperformance, the identified changes to the system or architecture are implementedand remeasured against the previous snapshot. This helps to determine the positiveor negative effects of those changes on system performance. The old snapshot isretained as a historical record of configuration and resulting performance metrics,and the new snapshot becomes the working baseline.\n\n8Chapter 2    Theoretical PrinciplesLoad Planning\nLoad planning accounts for changes or new demands on the system. Any available\ndata describing the changes and new demands is collected and analyzed. If possible,measurements are taken by baselining similar workloads or workloads in thedevelopment phase. The new workloads or changes to the existing workloads areprofiled, providing input to the capacity planning phase (see\nFIGURE 2-2 ).\nLoad Planning is a cyclical process that comprises three activities: load assessment,\nload data gathering, and load profiling. Each of these activities may need to berepeated multiple times before valid assumptions can be made about the datacollected.\nFIGURE 2-2 Load Planning\nLoad Assessment identifies how the new demand can be quantified. In the case of a\nnew pool of end-users migrating to an application, load assessment would determine\nwhere the users would be located, how they would be accessing the environment,what they would be doing in the environment, and the frequency with which theywould do it. This information could then drive the creation of benchmarks to test theeffects of this new demand on the existing environment.\nLoad Data Gathering collects raw statistics on the transactions within a benchmark.\nThis collection can be accomplished in many ways depending on the layer that isfocused on. These statistics tend to be much more detailed and over shorter intervalsthan those measured during baselining activities. These measurements usually takeplace in an isolated development environment or test environment.\nLoad Profiling creates a model based on the load-gathering data. This model will\nrepresent the typical load that is applied to a system for that particular planningcycle.Load\nPro\ufb01lingLoad Data\nAssessmentGathering\nLoad\n\n Phases of Performance Management 9Capacity Planning\nCapacity planning applies the results of the load planning analysis to the system\nbaseline and performance reports from the baselining phase. The capacity planningphase produces a new system and service configuration architecture to meet therequirements established in the load planning phase. The accuracy of the datacollected in the baselining and load planning phases is directly related to theaccuracy of the capacity planning model.\nSystem capacity and resource consumption are modelled, calibrated, and validated\nagainst the current baseline data. The model compares the relative resourceconsumptions (measured in the performance reports of the baselining process) to therelative performance characteristics of the system architecture (established duringthe configuration inventory of the baselining process). The relationship betweenresource consumption and system architecture capabilities creates the workloadcharacterization used to model and predict the effect of changes (deltas) to themeasured environment.\nSystem performance characteristics are established and projected into a model of\nscenarios that include projected workload growth, system architecture and hardwarechanges, and workload resource assignment.\nOnce the model has been established as accurate and representative of the measured\nworkload characteristics, the model is used to predict the impact of the changesintroduced in the load planning phase. The operating environment can be adjustedas necessary to meet the established performance requirements defined in the SLAand KPI.\nApplication disk layouts, disk architecture models, CPU speeds, cache sizes,\nmemory sizing, and workload mixes can be adjusted and projected in the systemmodel. The new workload is projected into the new system environment in aniterative process until a system environment that meets the business requirementsand performance objectives is found. This iterative process produces utilization,latency, and throughput projections for the new workload, as well as a systemarchitecture and operating environment that meet the planned changes and growthwhile maintaining a predictable level of service to the end-user community.\nResource Management\nResource management defines the controls, shares, quotas, and priorities necessaryto manage the workload within the implemented production environment. Sharesand quotas are enforced and monitored for accounting where necessary. In addition,system resource accounting provides input for creating the new baseline of systemperformance. System resource planning also provides additional calibration\n\n10Chapter 2    Theoretical Principlesfeedback into the next phase of load planning and capacity planning to improve\nmodel accuracy. Refer to the Sun BluePrints book Resource Management for an\nexhaustive view of resource management.\nService Level Agreements\nWith the increasing performance and RAS (reliability, availability, and serviceability)of Sun servers and software, Solaris\u2122 Operating Environment, or Solaris OE-basedservers have been encroaching on what has traditionally been the mainframe worldof mission-critical applications. This relatively new positioning of Sun servers in theenterprise has resulted in the necessity for IT departments to apply mainframedisciplines to UNIX servers. One of the staples in the mainframe environment that isstill relatively new to the Solaris OE is the service level agreement (SLA). Theseagreements outline the level of availability of mission-critical services between the ITdepartment and end-users.\nThis section describes the principles of an SLA; the section \u201cService Level\nManagement\u201d on page 26 outlines the steps that IT departments should take in theanalysis of the end-users\u2019 computing resource requirements, capabilities of the ITarchitecture, and formulation of the agreement.\nThe SLA is really just a list of expectations for a given service. Moreover, it is a tool\nfor maintaining service availability. It defines the transactional capabilities, networklatency projections, availability requirements, maintenance windows, escalationprocedures, and the review process for those services.\nWhen service levels are not explicitly defined, users will be left on their own to\nformulate their own idea of service levels. All too often, the service levels that theusers create on their own will not correspond to those of the IT department. Theunfortunate impacts of not establishing service levels are miscommunication andchaos. If proper communication does not take place between the provider andconsumer, the SLA may be seen as a weapon to be used between the two parties,thereby negating proper communications and negotiations.\nWhen used properly, the SLA becomes a bridge of understanding between the IT\ndepartment and the service consumers. These agreements should be used as a basisof understanding between all parties involved and should define key roles,responsibilities, and expectations that have been discussed and agreed upon by allparties. Furthermore, the SLA should be reviewed and modified regularly andconsidered a living document. Any proposed modifications should be discussed andagreed to by all parties, before inclusion in the SLA.\n\n Service Level Agreements 11In most established computing environments such as more traditional datacenters,\nSLAs have long been a staple of life. However, in the client/server world, theintroduction of SLAs usually poses a significant cultural change. Usually, in thissituation, the traditional technologically focused management needs to give way tomanagement based on user satisfaction.\nThe SLA should answer five critical questions:\n\u25a0What expectations do the users have for the service?\n\u25a0What expectations do the providers have for the service?\n\u25a0What do the users expect from the service provider (IT department)?\n\u25a0What does the service provider expect from the users?\n\u25a0How can the SLA improve the business process, affect the bottom line, and\nimprove availability?\nThe answers to these questions may seem deceptively simple, but in reality, they\nmay take months of negotiation to work out.\nNonexistent or poorly defined SLAs can result in many unpleasantries, not the least\nof which are:\n\u25a0The perception of poor service because the IT department fails to meet theundocumented expectations of end-users\n\u25a0Antagonistic work relationships between end-users and IT staff\n\u25a0IT staff providing unwanted support, most likely at a price that the end-user isunwilling to pay (downside of this perception is that the IT department may be\u201clooking for work\u201d and thus become a target for new budget constraints)\n\u25a0Inconsistent support and the possible impression of favoritism between the ITstaff and end-users\nCaveats and Problems with Service Level\nAgreements\nSLAs are not more widely used for a number of reasons. Some of the most common\nare derived from the lack of end-user communication and the culture changes for ITdepartments. However, the many long-term benefits of SLAs include increased end-user satisfaction, more accurate resource accounting, and more efficientprioritization. The following issues are the most common downfalls of SLAs:\n\u25a0IT department and end-user management did not commit substantial time andpersonnel.\n\u25a0Lines of communication between IT department and end-user community werenot established.\n\n12Chapter 2    Theoretical Principles\u25a0IT managers could not concisely define, qualify, and quantify IT services and\ncosts.\n\u25a0IT managers lacked the marketing skills to identify, price, and sell IT services,both internally and externally.\n\u25a0End-users did not know exactly what they wanted or when they wanted it.\n\u25a0IT department failed to set baselines and run benchmarks on existing services andcaused inaccurate predictions about future services.\n\u25a0SLAs were not realistic and caused tested numbers to fail.\nHowever, with the right IT skills, either from the internal IT department or external\nconsulting organizations, all of these hurdles can be overcome.\nThe creation of the first SLA can be a challenging task for any IT department. It\nrequires a new look at the service, which is largely from the end-user\u2019s perspective,and can be a difficult cultural change.\nHowever, the payoff of establishing and maintaining an SLA is well worth the effort\nfor several reasons. A well-conveyed SLA can help develop higher serviceavailability, greater end-user satisfaction, and more efficient use of computingresources. In addition, preferable control over IT budgets and increased accuracy inlong-term budgeting can also be direct benefits of SLAs.\nFurther, the framework created by the SLA, based on input from the end-users and\nIT department, provides a number of benefits to IT management. With the SLA, anIT manager can assign personnel to requirements more effectively. Thus, personnelrestrictions, whether due to staffing or skill limitations, can be addressed throughhiring or training. This framework also provides insight into the coordinationrequired to optimize enterprise IT spending, reinvestment, and priorities. Thisinsight consequently reveals the link between IT service delivery and costs incurredfor that service.\nProduction Environment Engineering\nSuccessful production environment implementations are the product of carefulplanning and design. Production environment engineering and the topic ofdatacenter production environments have been studied and analyzed for decades.Balancing the business objectives, the explosive growth of Internet business, and theintegrity and safety of the critical business services can certainly be classified as anart.\nThis section addresses the needs of the datacenter production environment by using\nthe ISO FCAPS model, the ITIL (information technology infrastructure library)framework, a basic IT reference model, and the SunReady roadmap to production.\n\n Production Environment Engineering 13This discussion provides an encompassing view of the infrastructure requirements\nand timelines necessary to successfully launch, manage, and maintain a newdatacenter service or refresh an existing one.\nOverview\nTraditional \u201cglass house\u201d datacenter managers are generally familiar with theconcepts of production environment standards for operational policies andprocedures, but are unprepared for the exponential growth experienced by the fast-paced dot-com consumer markets. These glass house managers must learn to:\n\u25a0Adapt to the fast-paced Internet economy\n\u25a0Be prepared to quickly accommodate the new model of instantaneous and directcustomer contact\n\u25a0Conform to an environment where exponential growth has become a standard insuccessful ventures\nThey must also realize that the traditional model of multi-layered sales and\ndistribution retail markets is being challenged by the new access model that includesdirect interaction between the corporate IT systems and the customer, withcompetition just a mouse click away.\nThe dot-com startup Internet business has a related challenge. While their business\nmodel relies on exponential growth for survival in the highly competitive Internetbusiness environment, they often lack the datacenter policies, procedures, andexperience to manage a datacenter production environment through the growth thatsome successful Internet businesses have displayed in recent years. By learning fromthe cumulative experience of their more traditional predecessors, the dot-comcompanies can implement best practices for the production environmentarchitecture, while retaining the flexibility necessary to thrive in the expandingmarket.\nIT Frameworks\nAn IT framework defines the scope of coverage in the production environment andconsists of generalized topics to be designed and implemented in that environment.Frameworks also help determine staffing and organizational requirements based onbroad categories of IT services. Examples of some traditional topics include:\n\u25a0Backup and restore\n\u25a0Change control\n\u25a0Security\n\u25a0Performance/capacity management\n\n14Chapter 2    Theoretical Principles\u25a0System management\n\u25a0Application management\n\u25a0Network management\n\u25a0Disaster recovery\n\u25a0Facilities management\nSome of the categories defined in the framework are company-wide, some are\ndatacenter-specific, some are system-specific, and others are application-specific. Forexample, the security group could manage security policy and administration for theentire company, while a facilities management resource could be designated toseveral disparate datacenters. In addition, the application management resourcecould be dedicated to a single server or a set of service instances throughoutmultiple datacenters.\nThe ISO FCAPS IT Framework\nThe International Organization for Standardization (ISO) defined a framework formanaged objects in a networked enterprise in their Guidelines for the Definition of\nManaged Objects contained in the Open Systems Interconnection (OSI): Structure of\nManagement Information standard. The OSI structure defined five categories of\nmanaged objects, covering the basic categories of enterprise support operations (see\nFIGURE 2-3 ).\nFIGURE 2-3 FCAPS High-Level IT Framework\nThese categories can be applied to network management systems, which is theintended purpose, or they can be expanded to include the skills, tools, andcapabilities in datacenter operations and management.Support Operations\nSecurity Performance\nManagementFault Con\ufb01guration Accounting\nManagement Management Management Management\n\n Production Environment Engineering 15IT Extended Frameworks\nEach of the categories in an IT framework can be broken down into smaller pieces,\ndescribing the responsibilities of the group providing the service associated withthat category. This expanded list of IT capabilities is called the IT extendedframework (ITEF).\nTABLE 2-2 shows an example.\nThe ITEF provides an excellent checklist for datacenter capabilities, skill sets, and\nproduct coverage. It also provides a framework for establishing a logical structurefor datacenter policies and procedures that is independent of operating system,server type, or application type. Several levels of criticality can be assigned to thecategories of IT services established by the ITEF, such as mission-critical, business-critical, business support, and incidental. These levels of criticality can helpdetermine the level of IT support needed for a particular category.\nAn ITEF can be further divided to establish functional responsibilities and to\nprovide more detailed information about the services the ITEF offers. Thesefunctional responsibilities either can be represented as a table of departmentalresponsibilities (see\nTABLE 2-3 )or they can be explicitly declared in an IT service\ncoverage document defining departmental responsibilities.TABLE 2-2 IT Extended Framework\nFault Management\nCustomer Service\nIncident ManagementBackup and RecoveryDisaster RecoverySystem AdministrationFault Protection\nTABLE 2-3 FCAPS IT Extended Framework Subdivision\nBackup and Recovery\nTape Pool ManagementUser Data RestorationSystem BackupData Backup Integrity TestsOffsite Tape Archive Rotation\n\n16Chapter 2    Theoretical PrinciplesFCAPS IT Extended Framework\nBy expanding the FCAPS IT framework to include descriptive categories of IT\nservice coverage, we can define an ITEF based on FCAPS. This ITEF is bestrepresented in a tree structure that is viewable as a series of Web pages that allowthe consumer to easily navigate to the level of detail needed. This drill-downstructure is key for implementing the ITEF in an easily accessible sharedpresentation and for including the ITEF in online system runbooks.\nFIGURE 2-4 shows the ISO FCAPS IT extended framework.\nFIGURE 2-4 ISO FCAPS IT Extended Framework\nA clear presentation of the logical architecture is important in presenting a unified IT\nmanagement and operations strategy to both the end-user service consumerorganizations and to business-level management. This presentation also helps enableSupport Operations\nSecurity Performance\nManagementFault Con\ufb01guration Accounting\nManagement Management Management Management\nCustomer\nService\nIncident\nManagement\nBackup &\nRecovery\nDisaster\nRecovery\nSystem\nAdmin\nFault\nPreventionChange\nManagement\nWorkload\nManagement\nCon\ufb01guration\nManagement\nResource\nManagement\nAsset\nTracking\nLicense\nManagementMIS Cost\nControl\nUser Account\nManagement\nSystem Usage\nManagement\nLicense\nMeteringApplication\nManagement\nMachine\nPerformance\nCapacity\nPlanning\nLoad\nPlanning\nPurchasing\nProcurementSystem\nSecurity\nUser Account\nSecurity\nAccess\nControl\nArchival &\nRetrieval\nSecurity\nPolicy\nSecurity\nAdmin\nOperational\nAcceptance\n\n Production Environment Engineering 17accurate SLA creation and service capability interpretation. The following example\nprovides an excerpt of a sample IT service coverage document for changemanagement.\nIT Service Coverage Document\nThe IT service coverage document should include a general statement describing the\ncategory of coverage, as well as including statements describing the individual servicesand areas of authority in the IT environment. The service coverage document can provideassistance in tool selection, as well as staffing and training requirements. By establishingcategories of coverage, the services provided by the IT department are well-defined, whichallows the establishment of enforcement and auditing policies across the IT operatingenvironment.\nChange Management\nThe change management system is the vehicle for recording ,authorizing ,monitoring , and\nreporting all changes to the open systems environment. The primary services and\nresponsibilities provided are:\n\u274fSingle auditable mechanism for handling all changes integrated with the enterprise change\nmanagement system\n\u274fReview process, including all interested parties, for authorizing changes\n\u274fProcess for ensuring all changes are tested and validated with tested back-out procedures\n\u274fChange process reporting procedure incorporated within the SLA reporting process\n\u274fMechanism for reviewing all changes not authorized through the change management procedure\n\u274fControl of software development and maintenance for applications and packages, including\nsource code, executable code, and parameter files, plus the provision of release management andaudit control\n\u274fSoftware distribution of applications, operating system, and packages to the distributed\nplatforms in a controlled and audited manner\n\n18Chapter 2    Theoretical PrinciplesThe IT Service Model\nThe IT service model (ITSM) introduces levels of criticality, levels of service, and\n\u201cbig rules\u201d to the ITEF. This multilevel approach extends the natural drill-downstructure for IT services (see\nFIGURE 2-5 ).\nFIGURE 2-5 Drill-Down IT Document Hierarchy\nThe ITSM can be implemented across platforms and used to create service leveldescriptions for use in SLAs between end-user consumer organizations and ITresource provider organizations. Where appropriate, a chargeback structure for ITservices can be produced for varying levels of service. Once defined, the ITSM helpsproduce an automated, standardized, repeatable, and simplified SLA creation andauditing process.Support Operations\nFault\nManagement\nBackup &\nRecovery\nDisaster\nRecovery\nUser Data\nRestoration\nPolicy\nChargeback\nService Levels\nProcedures\nEscalation\nService Levels\n\n Production Environment Engineering 19The following is a sample of a service level description for user data backup and can\nbe used as a guideline; it should be modified to adhere to local datacenter policies.\nService:\nUser Data Backup\nService Schedule:\nMaintenance Window Task\nService Levels:\nMission-Critical\nUser data will be backed up to the central backup\nmanagement system using a full backup on a daily\nschedule.\nChargeback:\n$1000 + $80 \u00f7 100 GB per month\nBusiness-Critical\nUser data will be backed up to the central backup\nmanagement system with weekly full backups and daily\nincremental backups.\nChargeback:\n$500 + $60 \u00f7 100 GB per month\nBusiness Support\nUser data will be backed up to a locally attached\ntape system with weekly full backups and daily\nincremental backups\nChargeback:\n$200 + $40 \u00f7 100 GB per month\n\n20Chapter 2    Theoretical PrinciplesApplying ITIL\nThe Central Computing and Telecommunication Agency (CCTA) of the UK began\npublishing volumes of their established best practices for IT operations in 1978. The24 volumes, titled\nInformation Technology Infrastr uctur eLibrary (ITIL), describe the\nCCTA\u2019s operational procedures and discuss their application and logistics in aproduction datacenter environment.\nThe ITIL has evolved over the years, being studied, adopted, and adapted by many\nIT organizations and vendors of management tools. The Examination Institute of theNetherlands (EXIN) has established certification programs granting foundation,practitioner, and master levels of expertise to the categories of service support,service delivery, and IT management.\nOne key benefit of the ITIL is that it has helped identify the interdependencies\ninherent in IT management organizations. The following is an example of theinterdependencies within a backup process:\n1. Backups must first be scheduled through the batch job scheduling system.2. The status and any errors of the backup must be reported through the enterprise\nmonitoring and management system.\n3. The monitoring and management system must transfer the status and any errors\nof the backup to the service desk.\n4. The service desk must notify the technology owner or system administrator of the\nstatus and any errors.\nWith a properly organized IT framework and extended framework, these\ninterdependencies and process flows can be documented. In addition, theprocedures for implementation and operation can be integrated into thedocumentation describing the service.\nThe ITIL establishes policies, not implementations. It remains independent of\noperating system, hardware platform, and application, but can be used as aguideline for creating the \u201cbig rules\u201d to be used in the ITEF. The ITEF can then beimplemented according to service levels, operating system, hardware, andapplication capabilities.\nUse the following resources to provide the knowledge necessary for implementation\nof the established policies in the ITEF:\n\u25a0Sun BluePrints program\n\u25a0Collective experience of the IT staff\n\u25a0Technical publications and conferences\n\u25a0Sun Education Services\n\u25a0Sun Professional Services\n\n Monitoring and Management 21The SunReady\u2122 Approach\nThe FCAPS framework was originally generated from the ITIL. The Sun Professional\nServices (SunPS) team has developed a project methodology for applying the FCAPSframework. This methodology creates a unified extended framework for IToperations. Using the ITIL as a guideline for best practices in mission-criticalenvironments, the SunPS team created sets of big rules for implementation. Theserules weigh the criticality of each task required to deliver the service levels promisedby the customer IT department to the end-users of the specified services.\nBy creating this multilevel approach to each IT service, the SunPS team can establish\na scoring model to predict the ability of the current IT infrastructure to satisfy therequirements of the end-user service level objectives. This IT capability analysis canthen be used to develop a high-level architecture for the IT infrastructure. This new,well-planned architecture can supplement the current established IT operationsservices before the end-user service is placed into production. Hence, the high-levelarchitecture is optimally created before the pilot phase of the applicationdevelopment life cycle.\nThe implementation of supplemental service capabilities shadows the application\ndevelopment life cycle from the architecture phase to the pilot, development,integration, deployment, and sustaining phases. The IT service capabilities can thenbe tested and assessed simultaneously with the application\u2019s functional, stress,crash, and disaster recovery test cycles. In addition, the IT service capabilities canthen be promoted into the live production environment in parallel with the newend-user application environment.\nAttaching the IT infrastructure development life cycle to the application\ndevelopment life cycle identifies the dependencies between organizations andresources. In addition, this process documents the information flow necessary toestablish unified timelines for tasks and deliverables for project management. Thisensures that the underlying IT services required by the end-user service levelobjectives are in place before end-users are introduced to the productionenvironment.\nMonitoring and Management\nService level management is the key to maintaining the production environment. Byestablishing the rules of engagement, the capabilities necessary to support theservice level objectives, and the implementations of tools in the productionenvironment, the entire production infrastructure can be monitored and measured.Violations of the scope defined in the SLAs and measured exceptions to the agreed-upon performance and management criteria can be addressed and resolved quickly,minimizing the impact on the business.\n\n22Chapter 2    Theoretical PrinciplesIn this business reference model (see FIGURE 2-6 ), the flow of information is defined\nwith the service desk established as the center of management information. Theservice desk controls information, brokers requests, audits request completion, andprovides service level reporting services to the IT service management groups andbusiness service end-user community.\nFIGURE 2-6 IT Business Reference Model - I\nThe service desk also brokers action items for the support organizations, bothinternal and external, and audits the completion, impact, and duration of serviceevents (see\nFIGURE 2-7 ). With the dispatch of support activities controlled, the\naffected end-user service levels can be monitored. Monitoring end-user service levelsenables vendor and end-user SLA and service level objective (SLO) adjustments,penalty assessment, enforcement, and arbitration where necessary.Network/System\nEvents\nSecurity\nEvents\nOperational\nEvents\nManagement Information\nReportingApplication\nEvents\nCustomer Requests\nPeripheral UpdatesCon\ufb01guration &\nAsset Feeds\nService Desk &\nService Level\nManagement\n\n Measurement Principles 23FIGURE 2-7 IT Business Reference Model - II\nMeasurement Principles\nAssume that anything that is not measured is out of control.\n\u25a0Measure the business as well as the computer systems and people.\n\u25a0Measure and record trends in performance, cost, and availability.\n\u25a0Measure things that you have no intention of tuning or controlling.\nAlthough it may not be obvious, the mere act of measuring something is beneficial.\nThere is a natural human instinct to observe measurements, and when a measurestarts to behave in an odd way, it becomes noticeable and therefore can be correctedat an early stage.\nYou should not specify all the measurements in advance; however, it is beneficial to\nencourage end-to-end measurement as a culture among the staff. There are somestable and important measurement definitions imposed by management; but inaddition, lightweight, evolving, ad hoc, and experimental measurements areextremely useful.\nIf the entire IT staff establishes how to measure everything that they do and\ndecorates their walls with plots of those measurements, a large amount of correctivefeedback will automatically blanket the systems and processes that run the business.\nA system should be built to help the IT staff log arbitrary data into performance\ndatabases, both automatically and by hand. These databases should have summariesand graph plotting that anyone can use and should be as simple and lightweight aspossible.Service Desk &\nService Level\nManagement\nApplication\nSupportThird-Party\nSupport\nSystems\nSupport\nHardware\nSupportNetwork\nSupport\nPurchasing, Acquisition, and Contract Support\n\n24Chapter 2    Theoretical PrinciplesOne important trade-off that must be made is deciding the priority of three\nconflicting ideals: fast, cheap, or safe. Make the decision with care to establish abalance. Parameterize each component separately and combine them into anavailability hierarchy. Rate each component as %fast + %cheap + %safe = 100% toget relative proportions. Combine the components into a hierarchy so that high-levelviews are automatically given the combined priority. Combining reliability andavailability metrics is well understood, but it is also important to at least estimatethe relative cost and performance implications of the choices being made.\nConclusion\nProduction environment engineering and performance management may appear tobe obscure forms of art. By studying the collective experiences of the traditionaldatacenter community and applying proven best practices through an organized andwell-defined methodology of service coverage, the art can be converted into a well-understood science, with a predictable quality of results. Creating reusabletemplates, documents, and processes for design, implementation, and integration ofdatacenter services in production environments can result in higher quality ofservices, lower service delivery costs, and less replication of effort. Defining theframework, extended framework, reference models, and timelines and enforcingthem within IT production environments can help deliver end-user services withrepeatable quality of service. Defining these frameworks, models, and timelinesdetermines what actions are necessary to keep the modern open-system datacenteras well-organized and well-managed as the best of the traditional mainframepredecessors.\nThe next chapter provides a methodology for service level management and looks at\nsome problems of capacity estimation across multiple platforms.\n\n25CHAPTER3\nSuggested Processes\nIn medicine, prevention is almost always more efficient than cure, and it is a good\nidea to optimize your outlay on preventive medicine to help reduce the frequencyand severity of illness. In the enterprise computing environment, you canaccomplish a similar purpose by establishing, monitoring, and enforcing ongoingpractices and processes.\nWherever possible, automate health monitoring and reporting for your systems.\nAutomating the collection of data for the qualitative and quantitative properties of ameasured service allows you to create baseline and trending information that isvaluable to the capacity planning and load planning processes.\nDevelopment practices make a considerable difference, so we recommend that you\ninclude performance when determining internal application developmentguidelines. Measuring and reporting key performance criteria in an applicationdevelopment environment can help set release time expectations for performanceand capacity. These practices can also help you recognize features and identifychanges of performance implications of the application being developed. Toestablish the best internal application guidelines, it is important to:\n\u25a0Define techniques and tools\n\u25a0Use a common instrumentation and availability architecture\n\u25a0Test and size applications before they reach production\nThese tools, their usage specifications, and the actions required at measured\nthresholds should be well-documented and placed in a system runbook. Thisrunbook should provide the guidelines and standard process flows for reporting andacting on system events of many types. The system runbook should contain enoughdetail to reconstruct the procedures and operating environment in the productionenvironment in the event of a disaster. This runbook should also be designed toenable an IT employee who is unfamiliar with the specific local environment tosuccessfully implement the described procedures and threshold activities.\nCreate some breathing room for planning by defining a multilevel escalation path.\nUse a strict change control process, and schedule batches of changes to manage risk.Arrange for the appropriate staff to be on site during change periods, to concentrate\n\n26Chapter 3    Suggested Processeson fire-fighting efforts. In a fast-changing business environment, the amount of\nchange will keep increasing and this acceleration must be managed and controlledwithout impeding change.\nThis chapter highlights the importance of documenting service level agreements\nbetween service providers and users. Additionally, this chapter offers best practicesfor capacity estimation and consolidating workloads.\nService Level Management\nMany client/server environments measure their service performance by CPUutilization, uptime/downtime, paging rates, I/O utilization, cache hit ratios, and thelike. Though all of this is important, still more information is needed to understandthe end-user\u2019s perspective. Users are more interested in the end-to-end availabilityof a service and the perceived notion of slow and fast. Unfortunately, these\nperceptions are difficult to quantify. The synthesis of traditional datacentermonitoring and the end-to-end availability of the service is required. This synthesisis known as service level management (SLM).\nWhen IT departments first attempt to implement SLM, the tools are not always\navailable. In such instances, it is appropriate to measure what canbe measured. The\ngoal is to eventually be able to measure from the end-user\u2019s service perspective.Once the new perspective of monitoring a service becomes more familiar to the ITdepartment, the service level objectives (SLOs) can be expanded to includeadditional metrics from the end-user\u2019s perspective.\nService Level Agreements (SLAs)\nService level agreements are negotiated between consumers and providers of ITservices. There could be multiple SLAs between a single business unit and multipleservice providers, such as network, IT infrastructure, database, and desktopworkstation support groups. Some providers can also have SLAs in place with otherproviders, including vendors and internal provider organizations whose servicesthey consume. These multilevel SLA structures can help pass business requirementsup, refining the requirements of the IT infrastructure. They can also pass capabilitiesdown to the end-user level, providing technical capabilities in a business servicecontext.\nMost SLAs follow the same basic framework:\n\u25a0A background is established for the service; it generally consists of what has been\nprovided historically, the proposed service, and what the proposed service willprovide:\n\nService Level Management 27\u25a0All parties are identified.\n\u25a0The service is identified.\n\u25a0Expected transaction load is projected.\n\u25a0Metrics are established.\n\u25a0Escalation procedures are clearly documented.\n\u25a0Arbitration, costs, consequences, and terms are all clearly defined.\nKeep in mind that one of the keys to a successful SLA is clear communication and\nrealistic, measurable goals that are stated in terms of the consumer\u2019s needs.\nThe SLA should not be mired in legalese; rather, it should be easily understood by\nanyone who is familiar with the service and the specified area of business. The SLAshould be implemented as part of the system runbook and maintained in an areaaccessible to both the business unit using the service and the IT staff maintaining the\nservice.\nThe following list names the sections most commonly found in an SLA and describes\nwhat each should include:\n\u25a0Background \u2013 Provide an overview of the SLA and historic information about the\nservice that was previously provided. Additionally, general information about theSLA, the business processes and objectives, and the impact of the new SLA on thebusiness processes should be included. This section should target readers who areunfamiliar with the service.\n\u25a0Parties \u2013 Identify key contacts from all groups and the responsibilities of each.\nClearly define the parties affected by this service. Who is the service provider andwho is the consumer? Are there other affected groups, and whose expertise isneeded?\n\u25a0Service definition \u2013 Define the service being covered, location of the service (both\nthe servers and the end-users), number of users, transaction volume expected,and service capacity. Determine coverage and schedules for the service, includingservice availability hours and maintenance windows. Be sure to includemeasurable goals for the service.\n\u25a0Key business indicators (KBIs) \u2013 List in detail the number of concurrent users,\ntransactions per unit of time (hours/minutes), transaction types, user distribution(LAN and WAN links and latency), resource contention with other applications,and workload intensities. This item should be very explicit.\n\u25a0Measurement of service \u2013 Define the method by which the KBIs are measured, and\ntrack compliance with the service definition.\n\u25a0Problem reporting and escalation \u2013 Establish the problem reporting process and\nescalation procedure.\n\u25a0Review and reporting \u2013 Establish regular service reports and review meetings.\n\u25a0Costs, chargebacks, and consequences \u2013 Establish costs for the service and chargeback\nprocesses. Define rewards or penalties for compliance/noncompliance.\n\n28Chapter 3    Suggested Processes\u25a0Resolution of disputes and arbitration \u2013 Define the arbitrators who will resolve\ndisputes.\n\u25a0Reassessment and update of SLAs \u2013 Establish a regular review cycle. Given the\ndynamic nature of IT environments (new applications, greater demand, oradditional users), regular reviews are necessary to revise and update the SLAs.\n\u25a0Term \u2013 Define valid dates of the SLA.\n\u25a0Approvals \u2013 Acquire signoff of the SLA by heads of each party.\n\u25a0Definition of terms \u2013 Provide a glossary of terms used in the SLA.\nEach SLA should be unique, based on the service being provided and the needs of\nthe users. However, this does not mean that once the first SLA has been completed itwill be necessary to start again from scratch. There will be nuances for each servicethat is being placed in an SLA.\nIdentifying a Service\nGiven the user-centric focus of an SLA, the process of defining these agreementsrequires sales and marketing skills, tact and diplomacy, and the traditional ITabilities of managing and measuring IT resources.\nWhen identifying a service for the initial implementation of an SLA, check that the\nexisting service fits two criteria:\n\u25a0Existing tools can monitor the service and the existing baseline data exists,including service availability and historic performance metrics.\n\u25a0The dependencies for the service can be clearly defined and those groups thatare\nrequired to provide components of the service are available (network, printing,help desk, etc.).\nThe implementation of an SLA is beneficial to both parties, providing resource\nconsumption and service sizing information. An SLA can assist the IT department inprojecting load planning information for system sizing, the business unit inestimating the cost of proposed changes to the service, and the accountingdepartment in assigning real costing estimates for business services provided.\nThe Service Definition\nOnce a service has been defined, the technical components of that service need to beidentified. Include the servers, networks, number of end-users, and their physicallocations. Detail the projected transaction volume and service capacity for thesystems, as well as the acceptable threshold. The goal is to include the lowest\n\nService Level Management 29acceptable and desired average per transaction latency and maximum supported\ntransaction rate for the service. List the core system availability hours and plannedmaintenance windows.\nThe service definition should be:\n\u25a0Meaningful\n\u25a0Measurable\n\u25a0Understandable\n\u25a0Cost-effective\n\u25a0Attainable\nMetrics for the service will be derived from meetings with the end-users,\nbenchmarking, and historical data that includes uptime/downtime, outage times,and performance. All of these metrics should be mutually agreed upon by both theIT department and the users, and should represent explicit numbers withmeasurable goals. The SLA should be cost-effective for both the end-users and the ITdepartment, while still meeting the demands of the business. Lastly, the goals shouldbe attainable, based on testing and historic performance.\nExample Service Definition:\nThe order processing group has 300 employees, all centrally located in theKansas City facility. They are currently connecting to a database on a SunEnterprise \uf6db6500 server, also located in the Kansas City facility, over the local\narea network (LAN). The order processing group must run monthly reports onthe first week of every month (for the previous month), quarterly reports on thefirst week of every quarter (for the previous quarter), and annual reports on thefirst week of the new fiscal year (for the previous year).\nExample Server Specification:\nThe server architecture for this environment consists of the following:\nSun Enterprise 6500 server16 CPUs, 4 Gigabytes of memory700-Gigabyte A35002 FDDI network adapters connected to the server LANNote that desktops, network infrastructure, help desk, backup, restore, and\nprinter support are covered under a separate contract.\n\n30Chapter 3    Suggested ProcessesAfter characterizing the service and defining the infrastructure, establish the service\ntime frames and windows of maintenance. Service specifications should be based onthe needs of the users and the performance data acquired from previous years (ifavailable).\nExample Service Schedule Specification:\nBusiness hours are defined as 0600 EST to 2000 EST, Monday through Sunday.\nThe maintenance windows are defined as 2030 EST until 0530 EST, Monday\nthrough Sunday. During these maintenance windows, no transactions arepermitted.\nIf transactions are required during any of the maintenance windows, then prior\napprovals from the IT manager and the end-user manager are required. Approvalmust be secured and the request submitted to the help desk at least 7 days priorto the proposed request. At no time will transaction processing needs be allowedto compromise data or backup integrity.\nIn the event of an emergency, a planned outage may need to be scheduled outside\nthe normal maintenance windows. In such a case, the IT department will seekthe users\u2019 approval for the arrangement and timing of the outages. They will alsomake a reasonable attempt to limit the impact of the planned service outage tothe end-users when possible.\nIn the event of a \u201cdisaster\u201d requiring the recovery of data or the restoration of\nsignificant portions of the system, the help desk will notify the users of theservice event and regular updates will be provided every 30 minutes until theservice is restored. Those events defined as \u201cdisaster\u201d will not be counted in theservice availability calculations, but will be documented in the monthly servicereview.\nCurrent estimations of disaster-level service restoration are two hours for system\nlevel restoration, four hours for application data recovery, and two hours fordatabase system recovery. No system service event, including disaster events,shall exceed twelve hours.\nExample Availability Specification:\nOutages are periods during defined business hours that fall outside emergencymaintenance windows and during which the system is unavailable fortransactions. Planned or scheduled emergency maintenance is not included inoutage calculations. Outage onset is the time that the outage is detected ratherthan the actual time of occurrence. Outage termination is the time that service isavailable after an outage.\nThe system shall maintain functional availability of 99.8% of business hours.\nSystem availability is determined by the following equation:\n\nService Level Management 31Coverage = Total minutes in month defined as \u201cbusiness hours\u201d\n14 hours per day \u00d760 minutes \u00d730 days = 25200 minutes\nOutage = minutes of monthly outage (33 minutes)Availability = (Coverage \u2013 Outage)/Coverage x100\n(25200 mins)\u2013(33 mins) \u00d7100 / (840 mins \u00d730 days) = 99.8%\nDuring the peak hour, 95% of queries will be completed in less than 2 seconds\n(on average) with up to 300 users online. No queries will exceed 5 seconds.\nDuring peak hour, 95% of all order transactions will be completed in less than 5\nseconds (on average) with up to 300 users online. No queries will exceed 9seconds.\nThe order processing system will sustain a transaction of 30,000 transactions\nper hour during business hours.\nBecause historical batch-related performance data is lacking, no guarantees of\nperformance for batch processing can be made at this time.\nKey Performance Indicators (KPIs)\nKPI documents have long been the yardstick we have used to measure our server\u2019sperformance. While SLAs are specified in business service terms, KPIs are defined inlanguage familiar to the system administrator. They often measure available CPU,memory, and I/O throughput, as well as more granular aspects such as mutexcounts, interrupts, and context switches. KPIs are not to be interpreted as pass/failconditions, but are meant to report on the overall performance health of the serverand to indicate when a service might not have adequate resources to meet the goalsof the SLA.\nAfter the service definition has been created, system performance goals are\nestablished to measure resource utilization against resource capacity. The KPIdocument establishes resource thresholds and identifies warning and critical levelsof resource consumption.\nWhen the capabilities of a service are being measured, a number of tests are\nperformed on the server. Benchmarking of the application should be performed toidentify the limits of the server\u2019s capabilities and to identify a baseline of theresources that the business service consumes at different thresholds of use.\n\n32Chapter 3    Suggested ProcessesTABLE 3-1 lists the kind of thresholds found in a KPI document.\nThe system architecture, both hardware and software, corresponds directly to the\ncapacity and impact on the service. As the hardware architecture and software arechanged by upgrades or patches, the KPI document should be revisited to determinenew thresholds where necessary.\nMost IT departments choose not to include KPIs in the actual SLA, because the\ndetails contained in the KPI document are intended for the IT department personneldirectly involved in the management of the server system. These KPI measurementscan be useful in budgeting, planning, and justifying upgrades and additionalresources. They can also be used as the baseline to compare performancemeasurements against measurements made after modifications to the serviceapplications, database, and desktop systems.\nKPI Measurement\nAfter identifying the KPIs, make sure that the measurement criteria and toolsets thatmonitor them are outlined in the KPI document.\nA number of tools are available to help monitor system performance. Some of the\ntools including sar,iostat ,vmstat ,mpstat ,andnetstat are integrated into the\nSolaris OE to assist with KPI measurement. Free tools such as Sun\u2122 ManagementCenter (SunMC) and the SE-Toolkit provide graphical tools to monitor singlesystems. Commercial monitoring tools like SunMC, BMC Patrol, BMC Best/1, andTeamQuest can be configured to measure, monitor, and report on performance statusand thresholds for a large number of systems. Most can then be integrated intoenterprise management tools like Solstice Enterprise Manager\u2122 software and HP\u2019sOpenView.\nIt may be necessary to create some of your own tools to monitor a service from the\nend-user\u2019s perspective. Although it is easy to monitor services from the back room,a regular snapshot of the user\u2019s response time is needed. Thus, network latencyshould be taken into account when possible. These custom tools need not be overlycomplex. In fact, a simple script that queries the server from the end-user\u2019s subnetand sends SNMP traps back to the network operation center (NOC) with responsetimes may be sufficient.TABLE 3-1 Key Performance Indicator Examples\nResource Warning Threshold Critical Threshold\nCPU >70% utilization for any\nmeasured shift>90% utilization for any\nmeasured shift\nDisk >30% \u201c%busy\u201d for any measured\nshift>50% \u201c%busy\u201d for any\nmeasured shift\nMemory Scan Rate >1000 pages per second,\nsustained for 5 minutes>4000 pages per second,\nsustained for 5 minutes\n\nService Level Management 33TABLE 3-2 lists example tool definitions that correspond to the sample thresholds in\nTABLE 3-1 .\nThe example in TABLE 3-2 also illustrates a problem: Some metrics are vendor- or\nrelease-dependent, so multiple definitions need to be maintained.\nWhen summarizing KPI data, keep in mind that there are different consumers of this\ndata. First, there are the system administrators, who are usually adept at analyzingraw performance data. Next are the IT managers and financial planners, who needsummaries of the data for budgeting. Last are the end-users, who may be interestedin the system\u2019s health and performance. These last two groups may be best servedby a Web page that provides graphical representations of system load andperformance. Charts and graphs can better represent system health for less technicalusers. Chapter 6, \u201dObservability,\u201d discusses audiences for performance informationin more detail.\nA KPI document can also include actions for immediate relief or resolution of\nperformance events. These actions might include stopping, rescheduling, orreprioritizing running batch jobs, stopping other business functions that are deemednot to be business-critical, or running utilities for analyzing the current applicationload.\nProblem Reporting and Escalation\nTo minimize impact and confusion for the users, use the existing problem-reportingprocess that is currently in place for that service (if the process exists and can bemodified appropriately). This process should list the severity levels, the primarycontacts at the help desk, and an escalation matrix for unresolved problems.Problem-reporting and escalation procedures should be well-documented in thesystem runbook. The procedures should be tested and updated as necessary toreflect changes to the IT environment.TABLE 3-2 Tool Definitions for Key Performance Indicators\n   Resource    Tool    Sample Time\n   CPU    sar \u2013  utilization is determined by the\nsum of \u201c%sys\u201d and \u201c%usr\u201d   20 mins\n   Disk    sar \u2013  utilization is determined by\n\u201c%busy\u201d   20 mins\n   Memory (to Solaris 7 OE) vmstat - scan rate as reported in the \u201c sr\u201d\ncolumn   5 mins\nMemory (from Solaris 8 OE)    vmstat - free memory as reported in the\n\u201cfree\u201d column   5 mins\n\n34Chapter 3    Suggested ProcessesSeverity of problems can range anywhere from a single user having slow response to\na global outage. To allocate resources appropriately, severity levels should bedefined, as should the priority in which those levels are organized.\nTABLE 3-3 lists a sample definition of severity levels.\nA simple call sheet may suffice for the contact information. All calls, regardless of\nseverity, should be placed through the main help desk for tracking and escalations.\nDuring Critical- and High-level problems, the help desk should contact the next tier\non the escalation list immediately in addition to the appropriate IT personnelresponsible for resolution. The contact information for different components of theproduction environment should be documented in the system runbook, includinghow to contact the help desk itself, owners of different categories of problems, andvendors responsible for different components of the environment.TABLE 3-3 Sample Severity Levels\nLevel Possible Impact\n1 \u2013 Critical Potential loss of customers\nKey business functions unavailable for one or morelocationsMajor network outageMore than 50% of workforce affectedPossible revenue loss of >$100,000/hourPossible cumulative revenue loss of >$250,000 / hour\n2 \u2013 High Key business functions not working properly and is\nseverely impacting one or more locationsMore than 25% of workforce affectedPossible revenue loss of $50,000 \u2013 $100,000/hourPossible cumulative revenue loss of $50,000 \u2013 $250,000/hour\n3 \u2013 Medium A key business function experiencing problems and\nlimiting the efficiency of a single group or locationLoss of productivity for at least 25 peoplePossible revenue loss of between $20,000 \u2013 $50,000/hour\n4 \u2013 Low Fewer than 25 people affected\nLoss of productivityPossible revenue loss of <$20,000Cumulative revenue loss of <$20,000\n5 \u2013 Minor Workforce experiencing a problem that is not affecting\nefficiencyWorkforce experiencing only minor loss of productivityNo revenue or cost impactCustomer request for information\n\nService Level Management 35Delays in the resolution of a problem can also result in an escalation in severity\nlevel. Whether an automatic notification to the next support tier is made depends onthe IT department\u2019s escalation method. The escalation path and events causing theescalation should be well-documented in the system runbook.\nThe following example is a guideline from a sample problem report sheet.\nSample Problem Report Sheet:\nProblems should be reported to the IT help desk by phone to (606) 555-5300, by\nemail to help@helpdesk, or by fax to (606) 555-5301. Faxed or emailed troublereports will receive a call from a help desk engineer within 30 minutes of thereceipt of the report during specified core business hours and within 2 hoursoutside of the specified core business hours.\nPlease provide the following information:\n\u25a0Name\n\u25a0Department\n\u25a0Location\n\u25a0Phone\n\u25a0Email\n\u25a0Problem Description (including your desktop environment and application with\nwhich you are experiencing problems)\n\u25a0Number of users affected\nThe help desk will assign a severity level based on the problem description (and\nimpact) and will contact the appropriate IT personnel.\nIf your problem is Level 1 (Critical) or Level 2 (High), the help desk will\nimmediately notify the IT manager for that service and the appropriate ITengineers. The IT manager for the service will be notified of the problem every 60minutes, until the problem is resolved.\nLevel 3 problems will be upgraded to Level 2 if not resolved in 3 business days.Level 4 problems will be escalated to Level 3 if not resolved in 5 business days.\nTrouble-Reporting Metrics\nThe service description should also include metrics relating to trouble reporting.\nThese metrics should include the availability of the help desk, as measured by howlong the customer should wait from the first ring when calling, and the effectivenessof the help desk, as defined by the percentage of calls answered while the customeris on the phone.\n\n36Chapter 3    Suggested ProcessesReporting and Review\nRegular reports should be made available to the end-users, business unit\nmanagement, and IT management to indicate how well the IT department is doingwith meeting the service description. Usually, these reports are weekly, withmonthly, quarterly, and annual summaries. Reporting times will depend on theneeds of the customer. The monthly, quarterly, and annual reports should alsoprovide running averages of preceding months or quarters.\nRegular reviews of the KPI documents and SLA should be performed. These reviews\nshould be between the IT managers of the organization supporting the service andthe end-user managers. The SLA is a tool for maintaining availability, so theimportance of these meetings is paramount. In relatively static environments,quarterly meetings may be fine (with more frequent reports). However, withindynamic environments, such as growing e-commerce environments and serviceproviders, these meetings should be performed monthly or weekly to determine anychanges that need to be made to the service definition or the system and applicationinfrastructure. In some extreme cases where changes to the planned load and servicedefinition occur on a less predictable or projectable schedule, even a weekly servicereview may need to be complemented with impromptu meetings to providemaximum advance notice of change to the parties providing the service.\nAt these meetings, discuss the performance of the IT department in meeting the\nservice description goals, the new requirements, and growth expectations of the end-user. Identify problems that arose during the previous week and possiblerecommendations for ways to improve those problems. This is also a good time toclarify any questions about the agreement. It is much better to resolve questionsearly on in the life of the SLA, rather than later through arbitration.\nA sample weekly report is described in \u201cManagement Viewpoint Implementation\u201d\non page 135.\nCosts, Chargebacks, and Consequences\nOne of the key benefits of creating SLAs is that they enable organizations to predictthe resources required to support a service. Though it is always a painful process tocomb through historical performance and trouble ticket data, the reward is theability to predict the actual costs incurred to support a service more accurately.\nFuture service requirements exposed as a result of these meetings may affect\npersonnel resources and computing resources, thereby justifying additional peopleor hardware.\n\nService Level Management 37In the original example, it was determined that the resources listed in TABLE 3-4 were\nneeded to support the order processing group for the first year.\n*Note that these numbers do not include taxes, benefits, additional training, etc.\nFor the end-user department to hire the talent needed, the department would needto spend approximately $480,000 a year. Moreover, these resources would bededicated to that department, have no support structure (other IT departmentmentors), and considerable idle time (unless there were lots of problems with theservice).\nBy maintaining support for the service in a centralized IT department, the end-user\nbusiness unit can attain coverage by the IT department at nearly 40% of the originalcosts ($193,500 \u00f7480,000) \u00d7100). This savings is enhanced by the following benefits:\n\u25a0IT staff are not necessarily dedicated to a single service; their time is often shared\namong multiple services.\n\u25a0Mentoring from other IT department engineers provides cross-training and skillenhancement throughout the support staff.\n\u25a0The datacenter environment, rather than the standalone environment, provideshigher quality, sharable, standardized resources for backup, restoration,monitoring, management, and reporting.\n\u25a0The IT department can provide career paths and training, so key talent won\u2019t beas likely to leave.\n\u25a0The engineers can standardize on datacenter disciplines, resulting in a move fromreactive to more proactive management practices.\nBy identifying the projects on which engineers are spending the most time, the IT\nmanager can identify recurring problems, find ways to rectify those problems, andlower the time spent by engineers. This increases the ability of the IT staff to addressproblem resolution. It also increases the leverage that the support organizations canwield over vendors, while lowering direct costs such as tool licensing and supportcontract costs.TABLE 3-4 Example Support Costs\nStaff HeadcountFractionalHeadcount\nRequired Salary* Total*\nHelp Desk Staff 1 1.00 $45,000 $45,000\nHelp Desk Staff 2 0.50 $45,000 $22,500UNIX Admin 0.45 $80,000 $36,000DBA 0.25 $80,000 $20,000Operator 0.50 $50,000 $25,000Network Admin 0.25 $80,000 $20,000Manager 0.25 $100,000 $25,000TOTAL $480,000 $193,500\n\n38Chapter 3    Suggested ProcessesFinally, there are the consequences of meeting or failing the service definitions. In\nsome IT departments, the IT department staff\u2019s bonuses are directly tied to meetingor exceeding stated service level goals. In other environments, failure to meet statedservice goals results in discounted or free services to the end-users. Theconsequences or rewards depend on your corporate support and culture.\nAn example of free service due to outage is as follows:\neBay:\n1\n\u201cFor any hard outage lasting two or more hours, eBay will automatically\nextend listings for 24 hours and automatically credit all associated feesfor affected listings. The following listings will be eligible for extensionand credit:\n\u201cAny listing scheduled to end during the hard outage\u201cAny listing scheduled to end in the hour after the end of the hard\noutage\u201d\nArbitration and Conflict Resolution\nEven with regular meetings and clear lines of communication, problems can arise.These issues can be the result of misinterpretation of the service definitions, failureto meet proposed definitions, or just a breakdown in communication between the ITdepartment and the end-users. During these times, it may be necessary to seekarbitration for a resolution.\nArbitration need not be an outside organization, though it very well can be. It can be\na person or a panel that can review the problems and reach a final, binding solution.Since the point of arbitration is resolution, the decision of the arbitration panel isfinal.\nQuite often, the arbitration panel will consist of the vice president of the IT\ndepartment, the vice president of the end-user department, and one other personoutside of the two organizations (maybe a CFO, HR director, or another vicepresident).\nWhen creating the SLA, identify the members of the arbitration panel and gain their\ncommitment from the onset. Once this approval is granted, the panel should beclearly identified in the SLA and its decisions adhered to by all involved.\n1. FromeBay,Inc.\u2019s\u201cOutagePolicy,\u201d http://pages.ebay .com/help/community/png-e xtn.html,February26,2000.\n\nService Level Management 39Reassessment and Updating\nThe managers and representatives of the service for both the IT department and the\nend-users should meet regularly to review the existing SLA and to propose changesto the agreement. Either party may propose changes, but both parties must approvethe changes.\nGenerally, changes are requested when there are significant deviations from the\noriginal service objectives, actual or expected changes to the IT environment, or newbusiness requirements.\nRegularly scheduled inspections of the agreement should also be held, and plans for\nend-user or IT department growth should be discussed at this time (if they have notalready been addressed and resolved in previous meetings). Since SLAs are expectedto be ongoing and of indefinite duration, these meetings provide a good time toreview the previous period\u2019s performance and to suggest changes for the upcomingperiod. The time periods and review schedules should be documented in the systemrunbook. The system runbook should also contain the point of contact in one of theinvolved organizations responsible for scheduling the meeting and contacting thenecessary parties for attendance.\nInventorying the Enterprise\nBefore you can project system requirements or specify hardware platforms, youmust first know what the enterprise actually contains. A baseline must be created,containing the current processing capacity and computing resources consumed. Inaddition, this baseline must include overall hardware infrastructure and the totalcosts incurred by the enterprise in supporting computing needs.\nTools such as Gartner Group\u2019s TCO tool can provide estimated and true cost analysis\nof a computing infrastructure. Total costs must include:\n\u25a0Real estate\n\u25a0Server and infrastructure assets\n\u25a0Depreciation\n\u25a0Lease and rental fees\n\u25a0Support\n\u25a0Maintenance\n\u25a0Professional services\n\u25a0Education\n\u25a0Training\n\u25a0Software costs\n\n40Chapter 3    Suggested ProcessesA complete TCO analysis is a long and involved process, but can be very\nenlightening; such an analysis can reveal hidden and forgotten costs in the ITbudget.\nThe computing needs for the business function must also be quantified in some\nmanner. This quantification can resolve to individual business functions such aspayroll and marketing data or can be further subdivided into more specific metrics\nsuch as paychecks processed ortrade show leads generated .\nOnce the computing needs for the business function have been identified, the\ncomputing resources dedicated to those business functions should be inventoried.Databases, server software, server platforms, application software, and client-sidedesktops should be categorized and summarized by business function andcomputing category. In some cases, especially in the case of desktop computerplatforms, resources will be shared among several unrelated business functions.These shared resources should be inventoried and characterized for an estimatedpercentage of use in each identified business function for which they are used.\nSome business functions are not so easily identified and are often overlooked. These\ncommonly include system and network management resources, as well asdevelopment, test, and prototyping environments, which may be very dynamic inconfiguration and structure.\nAll of these factors can be tallied against each other to define the current cost and\nperformance characteristics of the business and computing environment. Resultssuch as server ,maintenance , and support costs of the payroll system can be generated\nfrom the collected data for comparisons and projections against the business growthpotential and relative cost analysis of changes to the server platform or architecture.\nBaselining Business Services\nBaselining the performance of business services is the second step in understandingexactly what is happening in the datacenter. Performance baselines should measureeach business function in terms of transaction type, transactional volume, and wherepossible, average response times for the driving server systems, in both average and\npeak transactional periods.\nAs an example of a performance baseline summary statement, an analysis of an\nonline sales and distribution system might state that:\nShift average transactional volumes for the sample period of 10/1/99\nthrough 12/31/99 were measured as 12,000 catalog system lookups perhour. For shift hours measured at 11,500 to 12,500 transactions per hour,catalog system lookup response time averaged 3.2 seconds, with a worst-case query response time of 5.1 seconds.\n\nCapacity Estimation and Consolidation Processes 41A peak hourly rate of 22,000 catalog system lookups was measured for\nthe hour of 19:00 PST on 12/23/99. During this peak-measured hour,catalog system lookup response time averaged 4.4 seconds, with a worst-case query response time of 11.4 seconds.\nBy measuring current business functions over a representative period of business\ntraffic, we can establish quantitative as well as qualitative metrics. We can then usethe collected metrics to project future requirements and system capabilities, and toset user and management expectations for service qualities relative to servicevolumes.\nBy creating a scalar representation of the performance characteristics of a particular\nbusiness service, we accomplish two goals:\n\u25a0Being able to predict application impacts of service load to help applicationdevelopers and database administrators identify bottlenecks and inefficiencies inserver and application software\n\u25a0Plotting the linear scalability (or lack thereof) of a particular service to helppredict the future resource needs for business function growth\n1\nCapacity Estimation and Consolidation\nProcesses\nThis section describes these processes in general, and Chapter 5, \u201dCapacity\nEstimation,\u201d presents a detailed example of the implementation of these processes.\nQuantifying Capacity\nOne of the most difficult tasks in workload performance analysis andcharacterization is predicting the quantity of work that disparate server platformscan support. This challenge is slightly easier when the servers being compared are ofthe same base architecture (UltraSPARC\u2122) or, at the very least, from the samevendor (Sun Microsystems).\nIf there is an industry-standard benchmark that closely imitates your workload\ncharacteristics, then use that benchmark as a guideline for platform comparisons.Unfortunately, very few business implementations come at all close to the popularindustry-standard benchmarks; or more appropriately stated, very few industrybenchmark standards have any real meaning in actual business applicationcomputing.\n1. ThepredictionisaccomplishedthroughtheapplicationofAmdahl\u2019sLawforparallelscalabilityofsoftwaresystems.\n\n42Chapter 3    Suggested ProcessesSeveral commercial packages for performance monitoring and capacity planning\ninclude relative performance metrics for a variety of hardware and operating systemenvironments. Two examples that we can use as samples of market offerings areBMC (formerly BGS) Best/1 and TeamQuest. The Best/1 and TeamQuestperformance monitoring capabilities implement slightly different methods forcapacity metrics, but both packages supply the same basic function.\nBest/1 Performance Console for UNIX has an internal database of hardware and\noperating capabilities and assigns relative metrics to the processing and I/Ocapabilities of a given platform. Performance of a given system is quantified and canbe used as input into the BEST/1 modeller for system sizing.\nTeamQuest uses an external table to assign relative system capacity metrics. System\nperformance can be recorded and applied against a chosen platform\u2019s capacity. Thismanual process can be a bit cumbersome in a large enterprise datacenter, but itallows the user to create or expand on the default capabilities datasets provided inthe product.\nConsolidating Workloads\nThe actual process of consolidating workloads is fairly straightforward, once theworkload baseline has been established. To combine workloads in a single-serverplatform, the workload baseline metrics are summed and the supporting hardwareis sized to meet the projected computing needs. Target system peak shift and peakhours are chosen for sizing, and target system utilization is established for thosetime periods. Without strict resource management implementations, the rule ofthumb for target utilization is 70%. With resource management software and policiesimplemented, target utilizations of 90% or more can be safely projected.\nIf the measured workloads exhibit unpredictable behavior (such as intermittent\nspikes in resource utilization or noncyclic resource consumption trends), the targetutilization point might be lowered to accommodate the worst-case periods of\nconsumption. The analysis of workload performance metrics, combined with aformal SLA, describe the transactional and service level expectations for the businesscomputing function.\nDirectly relating business and service goals to server sizing and management\nconsiderations is a key byproduct benefit of a server consolidation project. Businessprojections for growth and service level expectations can be associated with systemplatform hardware, software, and licensing requirements for budgeting and ITresource planning of the project life cycle.\n\nCapacity Estimation and Consolidation Processes 43Resource Management\nWhen multiple business functions are run within a single-server platform, resource\nconflicts almost always surface. Some applications will parallelize very well,running their workload more efficiently at the expense of the other workloads on theshared platform. To reduce the impact of these resource conflicts, formal resourcemanagement policies must be established and enforced according to SLA guidelines.\nThe Sun Enterprise 10000 platform supports dynamic system domains (DSDs),\nallowing a single-server platform to act as several distinct servers, with independentinstances of the operating system and independent application environments.System resources can be migrated manually between system domains, allowingadministrators to reallocate idle resources from an underused domain to a domainstarving for resources. Resources can only be migrated at a system board level; inaddition, no automated or performance policy-based dynamic reconfiguration (DR)is supported at this time.\nSolaris OE processor sets allow the administrator to bind applications to a set of\ndefined processors. Processor sets create hard walls of CPU resource allotment foreach designated workload. Idle CPU resources within a processor set cannot be usedby other workloads outside that processor set; in addition, idle CPU resourcesoutside a processor set cannot be used by a workload starving within its processorset. The allocated CPU resources defined by processor sets can be dynamicallyadjusted; however, no automated process exists for policy-based CPU resourcemanagement.\nSolaris Resource Manager (SRM) provides a policy-based resource framework for\nserver resources that overcomes some of the limitations of processor sets and systemdomains. SRM is a fair share resource-scheduling system, capable of managing\nresource categories such as CPU, virtual memory, connect time, logons, and diskquotas.\nResources are defined as either static orrenewable , depending on their particular\ncharacteristics. A CPU is one example of a renewable resource because CPUresources are dynamic for any given point in time. There is no finite limit of CPUpower; as a machine runs, new processing power is always available. Staticresources include items like number of simultaneous logons permitted and connect time\nper session .\nWith SRM, renewable resources that a workload does not consume within its share\nboundaries are free for use by other workloads. Resource consumption is governedby the relative shares allocated to the cohabiting workloads. Workloads can consumeany idle renewable resources on demand. When renewable resources are notavailable, SRM will enforce throttles against any process currently consuming morethan its defined relative shares.\n\n44Chapter 3    Suggested ProcessesResource management can include combinations of SRM, processor sets, and\ndynamic system domains. These tools can complement each other, and they offerexcellent provisions for resource management architecture. In addition, these toolscan safely control resource utilization within the guidelines of the SLA and KPIdocuments. Refer to the Sun BluePrints book Resource Management for more detailed\ninformation on SRM.\nThe Never-Ending Cycle\nLike system and network management, performance and workload managementdefine a never-ending cycle. System platforms, operating systems, applicationsoftware, and the business functions that use them are in a continuous evolutionaryprogression. To properly consolidate server functions and satisfy businessrequirements, a complete understanding of the enterprise is necessary. Thisunderstanding must range from the business view of a service to the hardwareimpact of a running application.\nWhen a successful implementation of a business-computing component is complete,\nit is time for celebration. However, the celebration is short-lived because the close ofone system implementation signifies the start of ongoing management of theunderlying architecture. A well-monitored and managed computing environmentwill directly influence the modelling and prediction of the next incarnation of thearchitecture evolution. In addition, this environment will ensure that the computingresources provided for the business application will meet the requirements necessaryto honor the contracted service levels.\nWhen service levels exceed the limits defined or key performance indicators exceed\ndefined limits, the platform must be reviewed and possibly upgraded to meet thedefined business goals. In addition, the platform may need to have workloadsbalanced in the computing enterprise to allocate more resources to theunderperforming workload.\nEven the hardware and software procurement process is circular. Business groups\ncontract service levels with capacity planners. Capacity planners define a workloadmetric for the service levels contracted. Systems engineers specify a hardwareplatform that can satisfy the workload metric and place a procurement request forthe business group to budget.\n\nSummary 45Summary\nThis chapter highlighted the significance of establishing effective service level\nmanagement and also demonstrated the importance of defining and maintaining aservice level agreement between service providers and users. In addition, bestpractices were offered for inventorying the enterprise and establishing a baseline forbusiness services. Finally, this chapter suggested processes for quantifying capacityand consolidating workloads and described the impact of capacity estimation usingindustry benchmarks across multiple platforms.\nThe next chapter provides a recipe for successful scenario planning and presents a\nmethodology for modelling capacity and load.\n\n46Chapter 3    Suggested Processes\n\n47CHAPTER4\nScenario Planning\nUsing management inputs and modelling alternative scenarios, you can predict the\neffects of workload mix changes, marketing-driven load changes, performancetuning, and hardware upgrades. Start by determining trends of recent history, thenadd forward prediction using business inputs to predict what the future workloadshould look like.\nTrending techniques use a mixture of step functions that track changes in your\nconfiguration and a technique sometimes known as multivariate adaptive statisticalfiltering (MASF). This process extracts cyclic variations from your data so that youcan see the underlying trends. With this technique, you use analytical models topredict the behavior of complex workloads that consist of several classes of user andtransaction types.\nYou should not attempt to build a detailed model of every part of your system. Such\nmodels are too complex, too hard to calibrate, and would never be finished in timeto be useful. Model the primary workloads; make sure you include the ones likeorder entry, which make money for your business.\nThis chapter provides a guide to successful scenario planning and provides best\npractices for using spreadsheets to create a model for capacity and load. In addition,this chapter presents methods for tweaking the model to enhance accuracy.\nA Recipe for Successful Scenario\nPlanning\nSuccessful planning must be based on firm foundations. It is quite common for\nplanning to be attempted and then abandoned as a technique because the effort putin was misdirected and the return on investment (i.e., useful planning resultsobtained) was too low. This recipe provides a step-by-step guide to the process andgives examples of the kind of information that needs to be recorded in each step.\n\n48Chapter 4  Scenario Planning1. Sketch a simplified system architecture.\nYou probably have an overall system diagram showing every machine, every\nnetwork, and all kinds of other details. You need to reduce the complexity bylooking for the primary flows through the system. These are usually the flows thateither are subject to SLAs or are money-makers for the business. The initial sketchshould be based on the physical systems in place, since that is the simplest place tostart. When multiple systems are employed to do the same job, show them as asimple replicated set, even if they are not exactly the same. Be ruthless about paringdown the complexity to a minimum to start with. It is better to go too far in thisdirection and add systems back in later than to start off with a too-complexarchitecture and get bogged down trying to model it.\nFIGURE 4-1 shows an example\nof a physical architecture sketch.\nFIGURE 4-1 Example Physical Architecture Sketch\nThe physical sketch usually needs to be supplemented by a dataflow sketch(\nFIGURE 4-2 ) that shows the way the main classes of users and applications are\nconnected.Internet\n45 MbRouterhttp Front-end\nSystems (10)Midtier\nApplication\nServers (4)\n100 Mb Switch\nBack-end\nDatabase\nServerSearch\nEngines (3)\n\nA Recipe for Successful Scenario Planning 49FIGURE 4-2 Example Dataflow Architecture Sketch\nThese sketches provide a framework to which information can be attached. Later\nsteps in this recipe refer to the systems and flows.\n2. Determine primary bottlenecks.\nThe model should concentrate on the primary bottlenecks. It may be sufficient to\nlook at just the CPU load on a central database server, or you may also need towatch network capacity or a wider range of systems, depending on your situation.The bottlenecks will change over time, so they must be listed explicitly each time ascenario is modelled.\nFor each of the systems and network interfaces that are identified in the physical\nsketch, record an average utilization during the peak usage period. Record theutilization of each network interface as a percentage. For shared networkcomponents, measure the combined utilization of the network backbone or switch.For each system, you need to know the overall CPU utilization and the utilization ofthe busiest disk on that system. The highest utilizations you find are the componentsmost likely to be the primary bottlenecks.\nFIGURE 4-3 shows an example of a physical\narchitecture sketch with utilizations.Normal\nRequests\nSearch\nRequestscgi-bin\nPeriodic\nUpdatesqlnetMidtier\nApplication\nServers (4)\nBack-end\nDatabase\nServerhttp Front-end\nSystems (10)\nSearch\nEngines (3)\n\n50Chapter 4  Scenario PlanningFIGURE 4-3 Example Physical Architecture Sketch With Utilizations\nThe example shown in FIGURE 4-3 illustrates a system where the primary bottlenecks\nappear to be the CPU and disk on the back-end database and the CPU on the searchengines. Disk utilizations were found to be very low on the other systems, whichoperate mainly from memory caches.\n3. Measure service levels and workload intensities.\nIf you have an SLA or some estimates for response times and throughput, then\nannotate the data flow architecture sketch with both the latencies and frequency oftransactions wherever this can be determined or estimated (see\nFIGURE 4-4 ).Internet\n45 MbRouterhttp Front-end\nSystems (10)Midtier\nApplication\nServers (4)\n100 Mb Switch\nBack-end\nDatabase\nServerSearch\nEngines (3)70% CPU\n35%\n45%\n74% CPU\n85% disk98% CPU1%30%67% CPU\n73%\n30%20%\n\nA Recipe for Successful Scenario Planning 51FIGURE 4-4 Dataflow Architecture With Latencies and Intensities\n4. Determine operational modes.\nPerformance modelling assumes that the workload is essentially constant. Therefore,\nif the system has periodic changes in behavior, it is best to pick out the main modesand measure and model them separately. One of the main modes occurs whenbackups are taking place. In many cases, backups must take place online duringquiet periods, because when you are supporting Internet-based users around theworld, there is no convenient time for offline backups. Modelling backup behavior isuseful, as fast-growing environments will find that the amount of data to back upincreases rapidly. As a result, the backup duration increases and often overspillsfrom the quiet period, affecting performance. The disk subsystem will probably be atits busiest during backup.\nIn the example shown in the figures, there is a periodic search update, and the load\nis different while the update is occurring. It is useful to model behavior in thenormal state and also during an update. It seems possible that the back-end server\u2019snetwork interface would be a bottleneck during the update. The utilization levels aredifferent in each mode, so they need to be recorded separately.Normal\nRequests\nSearch\nRequestscgi-bin\nPeriodic\nUpdatesqlnetMidtier\nApplication\nServers (4)\nBack-end\nDatabase\nServerhttp Front-end\nSystems (10)\nSearch\nEngines (3)Requests 1000/sec\nResponses 8000/sec\nSearch Latency 10 sec98%500/sec\n100ms\n60 sec every\n1000 sec2%\n\n52Chapter 4  Scenario Planning5. Choose one bottleneck.\nIt is quite simple to model a single function on a single system, and extremely\ncomplicated to model an entire network of systems. However, the performance of anetwork of systems is limited by its bottleneck, so you get useful results simply andquickly by focusing on one core bottleneck and assuming that bottlenecks elsewherein the system can be removed fairly easily.\nOne way of looking at this issue is that the biggest and most expensive system you\nhave should be run at the highest utilization, so that you are not wasting anexpensive resource by leaving it idle. This system then tends to be the bottleneckand is the best candidate for modelling. You should find it more cost-effective toupgrade surrounding systems so that they are no longer bottlenecks.\nIn the example, the back-end database server is a single system (or perhaps a highly\navailable pair of systems); it is common for this system to be both the mostexpensive resource and also the primary bottleneck. It is much easier to replicatesmaller systems to take care of front-end services than to decompose a singledatabase instance into multiple systems.\n6. Choose service and utilization indicators.\nThe initial model should be a crude and oversimplified base for planning future\nscenarios. The hardest thing to learn for successful modelling is to let go of thedetails. You can build a very useful model with incomplete and inaccurate data aslong as you are smart in what you include and what you leave out. For the back-endserver, a mixture of transactions is taken from the application servers. However, aslong as that mixture remains fairly constant, you can average all transactions into ageneric transaction rate. If the back-end server load level fluctuates a lot because ofseveral competing applications, you need to perform a workload breakdown toobtain just the CPU and disk utilization that is involved in servicing the incomingtransactions. If this is the primary load on the system, then you can probably getaway with using the overall CPU utilization. Assume that there is a direct linearrelationship between the total number of transactions processed and the CPUutilization. The average CPU time per transaction may change when the systemconfiguration or application changes. However, the characteristics of the systemremain constant between changes and provide a useful basis for a simple growthmodel.\n7. Pick a large time scale.\nFor scenario planning, we are interested in what will happen over the coming days,\nmonths, and years, not the next few seconds\u2014this is why we can ignore so muchfine detail. Also, to quote the mainframe capacity-planning guru Pat Artis:\nTo predict the future, you must know the past.\nYou need to collect historical data that spans a time period similar to the period you\nare trying to predict. So, a week\u2019s collected data gives you a good prediction of nextweek. To predict the next year, you need collected data for the whole of the previousyear. If you want to separate time-based or seasonal variations from one-off events,\n\nModelling Capacity and Load 53you need to see the same season twice. Lack of data is no excuse; however, you\nshould start to plan immediately and as time goes by, the extra historical data willimprove the quality of your predictions.\n8. Work through the planning process.\nThe rest of the scenario modelling and planning process is illustrated with a simple\nparameterized spreadsheet in the next section.\n9. Compare the prediction with reality.\nOnce you have predicted the transaction rate and system utilization for the coming\nmonths, you need to measure and compare with reality. This way, you learn whichestimates in the model were optimistic or pessimistic and how to build bettermodels.\n10. Recalculate the predictions regularly.\nAfter each time period you predict (e.g., monthly), you have a new baseline of real\ndata and you must recalculate the model needs.\nIn the next section, a simple spreadsheet-based scenario planning model is\nexplained. It should be quick and easy to implement, and if nothing else, it shouldhelp you think more clearly about the assumptions you are making over the comingmonths and years.\nModelling Capacity and Load\nThis section explains how a spreadsheet can be used to quickly generate estimates ofcapacity and load. By combining projected effects of various influences on load atany given time (such as growth of the user base and seasonal variation), you cangenerate a model that projects the total future load on the system. A model thatconsiders the current capacity a baseline and then estimates performanceimprovements from various effects can be used in conjunction with the load modelto determine if the system will have the capacity to support the required load in thefuture.\nModelling Load\nSeveral influences on load are introduced in the subsequent sections. For each suchinfluence, a mathematical model appropriate for predicting future behavior ispresented. These models will be appropriate in a wide variety of circumstances, butthey can also be easily modified to better model a specific environment. For each ofthe influences presented, the formulas presented in the descriptions can be used to\n\n54Chapter 4  Scenario Planningcreate a spreadsheet that estimates the \u201cgrowth factor\u201d from a baseline. If additional\ninfluences have an impact on the load, they can be computed in a manner similar tothe influences described below and can be combined with the other influences to geta total load. Additionally, different models can be used for the influences describedbelow if the models provided aren\u2019t sufficient.\nThe basic idea of the spreadsheet modelling aid presented here is that the effect of\nany influence at a given time can be represented as an independent growth factorthat represents how that influence affects the load. The growth factor is simply thefactor that the original (baseline) load must be multiplied by to get the estimatedload supported at the time in question. If the baseline load was measured in Januaryand the estimated growth factor for June is 2.0, this means that the load can beexpected to increase two-fold from January to June.\nBecause the growth factors are represented as multipliers, the units are immaterial.\nLoad is plotted, but all the values are normalized. This means that they are dividedby a normalization value. For repetitive influences like daily, weekly, or seasonalvariation, the normalization value is the highest value present. For absolute(nonrepetitive) influences, the normalization value is the original (baseline) load. Ineither case, all the values are divided by the normalization factor to obtain thegrowth factors. When this occurs, the units of measurement cancel each other out.However, to obtain meaningful results, the same units should be used for allmeasurements. The units in a particular installation should be a meaningfulmeasurement that represents the bottleneck\u2014for instance: transactions per second,reads per second, network throughput, or CPU usage. In the example below, themeasurements are of CPU load, because the CPU is the bottleneck. Determining theappropriate bottleneck is described in \u201cA Recipe for Successful Scenario Planning\u201don page 47. The spreadsheet method outlined here is designed for systems with onlyone significant bottleneck. Systems with multiple bottlenecks need to use morecomplex methods.\nOnce all influences are computed in a spreadsheet, you can combine them by\nmultiplying the growth factors for each influence together. An example of this isshown in\nTABLE 4-1 and in FIGURE 4-5 .\nIt is also important to do a sanity check of the models. For instance, even if capacity\nplanning shows a growth rate of 2 times month over month for several monthswhile data is being gathered, projecting the trend may not be appropriate. Projectingthis trend would lead to an estimate of load at the end of the year being 4096 timesthe baseline. Because many unmeasurable factors affect growth, this approach maynot be appropriate. However, the spreadsheet model is flexible enough to allow fortailoring the models. More complex formulas can be used, or different formulas canbe used for different time spans. For instance, the assumption could be made thatthe geometric exponential growth rate tapers off after a few months as a higherpercentage of the limited target population becomes users. This could beimplemented in the spreadsheet by using a growth factor of 2 for the first 4 months,\n\nModelling Capacity and Load 55followed by 1.5 for the subsequent months. Modifying the spreadsheet so that it\nbetter estimates the load and capacity of the systems in question is discussed in\u201cTweaking the Model\u201d on page 74.\nA final important consideration is that the time scales of each spreadsheet must\ncoincide. For instance, if one spreadsheet measures growth as the result of amarketing campaign on a day-by-day basis and another spreadsheet measuresgrowth as a result of seasonal effects on a monthly basis, they cannot be combined.The number of data points (spreadsheet cells) should also be the same for eachinfluence. Additionally, it is important that the seasonal variations begin with thebaseline in the correct season. For instance, if the baseline was taken in April, it isimportant that the seasonal cycle in the spreadsheet starts in April.\nDaily and Weekly Variations\nNote \u2013 Although daily and weekly variations are discussed below, they are not\nactually used in the example because other influences in the example are measured\nmonth-by-month.\n\n56Chapter 4  Scenario PlanningThe chart in FIGURE 4-5 shows most of a week from Tuesday to Sunday on a busy\nback-end server system in terms of transactions per second. Each transaction queriesor updates the database and is a good indicator of the business activity level for thissystem. You can see that the system never sleeps, but there is a predictable patternthat peaks in the evening during weekdays and falls off a little during the weekend.\nFIGURE 4-5 Daily Workload Variations Example\nThe important data is the prime time daily peak level. If the prime time is plotted day\nafter day, then a pattern like the factors shown in TABLE 4-1 will emerge.\nFrom the detailed information shown in FIGURE 4-5 , all you really need is the typical\ndaily peak level. The peak is most important, because good performance duringpeak loads is required to keep the users happy and make them more likely to visitthe site in the future. The actual data from several weeks can be analyzed to extractthe typical day-to-day variation or enable it to be estimated.\n\n\nModelling Capacity and Load 57TABLE 4-1 is part of the spreadsheet; you enter your own score levels in the rightmost\ncolumn to rate each day. These scores are normalized by the busiest day (in this case,Monday or Tuesday). This data is then charted so you can see the daily variationrepeated for four weeks.\nThe chart in\nFIGURE 4-6 shows the peak level day by day. In this case, Monday is\nshown as the busiest day, with Friday a little lower, and a quieter weekend. Sundaysare often busier than Saturdays, partly because systems located in California arenear the end of the international dateline; hence, when it is Sunday in California, itis already Monday in the Asia-Pacific region.\nFIGURE 4-6 Chart of Daily Workload Variation FactorTABLE 4-1 CPU Peak Load Factor by Weekday\nDayNormalized Peak Load\n(Peak Load Divided byMax Peak Load) Peak Load\nSunday 0.64 0.70\nMonday 1.00 1.10Tuesday 1.00 1.10Wednesday 0.91 1.00Thursday 0.91 1.00Friday 0.91 1.00Saturday 0.55 0.60\nTime in DaysNormalized CPU Load Factor\n\n\n58Chapter 4  Scenario PlanningThere are three ways to obtain the weekday variation coefficients. The first way is to\nsimply guess at something that \u201clooks about right.\u201d Guesses are better if they aremade by consensus with a group of people. The second way is to take measureddata for several weeks, average all the data in a spreadsheet, and measure the dailypeaks. The third way is to use statistical analysis, specifically analysis of variance(usually known as ANOVA), to fit a model to the data. This technique is described ina paper by Joe Hellerstein et al., published at the 1998 Computer MeasurementGroup conference. The paper uses several months\u2019 worth of Web site hit rate datacollected in 1996 on\nwww .sun.com by Adrian Cockcroft as its raw measurements,\nand fits an increasingly sophisticated model to the daily, weekly, and monthlyvariations.\nSeasonal Effects\nIn addition to the variations from day to day, there are seasonal effects from monthto month throughout the year. In the rest of this scenario planning exercise, monthlydata is the basis of the plan. Within each month there will be daily variations, but thetime scale over which changes and upgrades can be made to a production system ismeasured in months. Therefore, the peak load needs to be sustainable in any givenmonth.\nIf you have at least a years\u2019 worth of collected data, you can use it to generate your\nestimates for the monthly variations. Keep in mind that if you have limited historicaldata, it is likely that the perceived seasonal effects will be a combination of theactual seasonal effect and other (potentially large) influences such as geometricexponential growth. \u201cTweaking the Model\u201d on page 74 offers guidelines on how tomake the model fit the data by use of a simple iterative tweaking method.\n\nModelling Capacity and Load 59Alternatively, you should come up with estimates by consensus with your\ncolleagues. You can enter these estimates into the spreadsheet as scores in the formshown in\nTABLE 4-2 . As before, the scores are normalized to a maximum of 1.0.\nFIGURE 4-7 shows a graph of seasonal load variations by month.\nFIGURE 4-7 Chart of Seasonal Load Variations by MonthTABLE 4-2 Seasonal Load Variations by Month\nMonthNormalized Monthly Peak\nLoad (Monthly Peak LoadDivided by Max Peak Load) Peak Load\nJanuary 0.89 8.0\nFebruary 0.72 6.5March 0.61 5.5April 0.56 5.0May 0.44 4.0June 0.39 3.5July 0.33 3.0August 0.33 3.0September 0.78 7.0October 0.89 8.0November 1.00 9.0December 1.00 9.0\nNormalized CPU Load Factor\nTime in Months\n\n\n60Chapter 4  Scenario PlanningTwo years of example data are shown in FIGURE 4-7 , starting with January. This\npattern depends on external events as well as the nature of the business. It must bedetermined and calibrated against business metrics, but other factors must first betaken into account. This is shown in the next few charts, which together will form amonth-by-month predicted load level.\nGeometric Exponential Growth\nThe growth of the Internet combined with a growth in awareness of your site maycause a geometric exponential growth in load levels. This can be easily expressed intwo numbers as a growth factor over a time period. For example, growth might beexpressed as 50% per quarter, or doubling in a year.\nNote \u2013 The type of growth described here is mathematically referred to as geometric\ngrowth. However, because it is usually (somewhat incorrectly) referred to asexponential growth, it is referred to as geometric exponential growth here to avoid\nconfusion.\nA spreadsheet computes the growth factor and monthly factors for the coming\nmonths with a growth rate and duration provided by a user. The computation isbased on the following formula:\nL\nt=L0( G+1 )t-t0\nIn this case, tis the time (in whatever units you are using); Ltis the load at time t;L0\nis the baseline load value at time t0; and Gis the growth per time unit. If capacity\nplanners have measured the growth over a given time period (and can safely assumethe growth is geometric), they can extrapolate the growth rate ( G) at a given time.\nFor instance,\nTABLE 4-3 shows a month-by-month growth that extrapolates from a\ngrowth of 1.5 times in 3 months. Using the known growth of 1.5 times in 3 months,theGcan be solved for and substituted into the original formula. Since, the values\nforGand L\n0are known and can be substituted into the formula, the load at a given\ntime ( Lt) is computed based only on the time ( t-t0). The value t0is zero if tis\nmeasured in relation to it. In the following table, the values used are G= .22 and\nL0= 1, which makes the following formula:\nLt= 1.14t\nThis formula is provided in a capacity planning spreadsheet to generate the values\npresented in the following sections. This spreadsheet is available at\nhttp://www .sun.com/blueprints/tools and has all of the appropriate formulas in\nplace\u2014you only have to enter the growth over a given period.\n\nModelling Capacity and Load 61TABLE 4-3 shows an example of geometric exponential growth in user activity.\nTABLE 4-3 Geometric Exponential Growth in User Activity\nMonth Growth Factor\n01\n1 1.12 1.33 1.54 1.75 2.06 2.37 2.68 2.99 3.410 3.911 4.412 5.113 5.814 6.615 7.616 8.717 10.018 11.419 13.020 14.921 17.122 19.623 22.424 25.6\n\n62Chapter 4  Scenario PlanningWhen plotted, the growth rate in TABLE 4-3 looks like the graph shown in FIGURE 4-8 .\nFIGURE 4-8 Chart of Geometric Exponential Growth Rate\nThere is an underlying growth factor involved in the size of the total Internet\nmarketplace. At one point this was doubling about every 100 days, but recentgrowth rates suggest a doubling period of a year or more. The average activity levelof Internet users also changes over time, increasing with the roll-out of broadbandDSL and cable connections. Several large Internet sites are seeing business grow at arate of about 50% every three months, or a doubling period of just under six months.This translates into the geometric exponential growth shown in\nFIGURE 4-8 . After two\nyears, the monthly load level is about 25 times higher than at the start. Your growthrate will vary and may not continue at this rate for this long, but some kind ofgrowth model needs to be estimated and then calibrated against experience overtime. For systems in which growth is not a geometric exponential, other formulascan be used to describe growth more closely.\nNote that you should tune the growth rate as you obtain more data. Computing the\ngrowth rate over several different time ranges (for instance, January to March,March to May, and May to July) and averaging the results may yield a much betterestimate of the growth. Also, continually estimating the growth rate will tell you ifthe geometric exponential growth rate is speeding up or slowing down, either ofwhich could limit the effectiveness of the model.\nMarketing Events\nThere have been many examples of companies that run a marketing campaign that istoo successful in getting new users and thus overloads their systems. To add thiseffect into the future growth plan, we developed a spreadsheet-based model that\nGrowth Factor\nTime in Months\n\n\nModelling Capacity and Load 63parameterizes the expected increase in load. The parameters should be expressed in\na form similar to the justification for the campaign generated by the marketingpeople. These parameters include the duration of the campaign ( L); a scale factor\nrelated to the impact or reach ( S); a residual level which is the long-term increase in\nuse load resulting from this campaign ( r); and a delay that sets the starting point for\nthe campaign in the model ( d).\nThe spreadsheet available at\nhttp://www .sun.com/blueprints/tools uses a formula\nthat estimates the marketing reach of a campaign. This formula is appropriate insome circumstances; however, other formulas may better represent a marketingimpact depending on your environment. Most formulas can be easily implementedin a spreadsheet to work with the rest of this procedure. The following exampleprovides a brief description of the formula the spreadsheet uses for a marketingcampaign.\nBecause it may be difficult to understand the mathematics implemented by the\nspreadsheet, the following formula may be helpful in understanding how the spreadsheetwas derived. However, it is not necessary to understand this formula to use thespreadsheet model.\nThe load change (L\nt) caused by the marketing effects at time t is :\nLtL0SeL\u2013Ltd\u2013\ntd\u2013()!--------------------\uf8ed\uf8f8\uf8eb\uf8f6r+ =\n\n64Chapter 4  Scenario PlanningTABLE 4-4 shows an example of the results of the spreadsheet implementation using\nthe following values:\n\u25a0Duration ( L) = 3.4 months\n\u25a0Scale ( S) = 3.0 times\n\u25a0Residual level ( r) = 1.2 times\n\u25a0Delay ( d) = 4.0 months\nTABLE 4-4 shows an example of marketing campaign load boost factors.\nTABLE 4-4 Marketing Campaign Load Boost Factors\nMonth Growth Factor\n1 1.00\n2 1.003 1.004 1.305 1.546 1.787 1.868 1.769 1.5810 1.4111 1.3012 1.2413 1.2214 1.2115 1.20\n\nModelling Capacity and Load 65FIGURE 4-9 graphically represents the marketing campaign load boost.\nFIGURE 4-9 Graph of Marketing Campaign Boost\nInFIGURE 4-9 , a marketing campaign starts in the fourth month to counteract the\nexpected seasonal drop in load level. It continues for four months, then thememories fade away and a residual increased level of activity is seen. The short-termand residual gain should be something that the marketing department uses to justifythe campaign in the first place, so its impact can be modelled in the same way as itsimpact on the business. The campaign is not repeated in the second year; the chart in\nFIGURE 4-9 shows why it was not needed. In practice, many smaller marketing boosts\nmay be modelled rather than one large boost.\nGrowth Factor\nTime in Months\n\n\n66Chapter 4  Scenario PlanningCombined Load Estimate\nWhen all the load increasing factors are multiplied together (i.e., the seasonal effect,\ngeometric exponential workload growth, and marketing boost), a combined growthfactor will result. In this case (see\nTABLE 4-5 ), you can see that the final end point is\nabout 30 times the start point after two years, and that this boost mostly occurs atthe end, where seasonal and geometric exponential growth combine.\nTABLE 4-5 Combined Load Calculation\nMonth Workload Marketing Seasonal Total Load\n1 1.14 1.00 0.89 1.02\n2 1.31 1.00 1.00 1.313 1.50 1.00 0.61 0.924 1.72 1.30 0.56 1.245 1.97 1.54 0.44 1.356 2.25 1.78 0.39 1.567 2.58 1.86 0.33 1.598 2.95 1.76 0.33 1.739 3.38 1.58 0.78 4.1510 3.86 1.41 0.89 4.8611 4.42 1.30 1.00 5.7712 5.06 1.24 1.00 6.3013 5.80 1.22 0.89 6.2714 6.63 1.21 0.72 5.7815 7.59 1.20 0.61 5.5816 8.69 1.20 0.56 5.8017 9.95 1.20 0.44 5.3118 11.39 1.20 0.39 5.3219 13.04 1.20 0.33 5.2220 14.93 1.20 0.33 5.9721 17.09 1.20 0.78 15.9522 19.56 1.20 0.89 20.8623 22.39 1.20 1.00 26.8724 25.63 1.20 1.00 30.75\n\nModelling Capacity and Load 67FIGURE 4-10 graphically shows the combined load.\nFIGURE 4-10 Chart of Combined Load\nModelling Capacity\nAs with load, the factors that affect capacity can be separately computed in a\nspreadsheet and then combined to get a combined capacity estimation. Manycapacity measurements can be modelled. The two influences on capacity shownbelow are efficiency variations and capacity increases.\n\u25a0Efficiency variations cover all other factors that increase effective capacity. Thisincludes application and system tuning, as well as the removal of load from thebottleneck by parallelizing or offloading functionality. Keep in mind that anyload-reducing effects reflected in the load total should not be reflected by theefficiency variations.\n\u25a0Capacity increases include anything that allows increased capacity because ofupgrades to the bottleneck. Since the bottleneck in this example is the CPU,anything that affects the total CPU power of the machine is considered a capacityincrease. These include processor upgrades, as well as OS upgrades and databaseupgrades (which may use the existing processor power more or less effectively).\nGrowth Factor\nTime in Months\n\n68Chapter 4  Scenario PlanningEfficiency Variations\nThe model for load is based on a relatively constant resource usage per transaction\nof the resource causing the bottleneck. In the example above, CPU usage is thebottleneck, and the load model is based on constant average CPU usage pertransaction. Large changes in the application or transaction mix need to beaccounted for in the scenario plan. Ideally, performance tuning the application oroffloading some functionality to other systems will reduce the amount of bottle-necked resource used per transaction every few months, so that a schedule forexpected (or desired) changes in the application efficiency can be laid out as part ofthe model and recorded in the spreadsheet.\nTABLE 4-6 shows the effects of heavy application tuning on the CPU usage per\ntransaction in the above example. Although these results may be unrepresentative ofwhat can be achieved in many situations, it is sometimes possible to achieve a four-or five-fold performance improvement (like the one shown here) by tuning customerapplications or databases. When planning for future capacity, it is important to talkto the programmers, consultants, or DBAs tuning the system in question to get anaccurate expectation of the amount of tuning that can be achieved.\nThe second column of\nTABLE 4-6 shows the amount of relative CPU usage per\ntransaction (baselining the initial value at 1). The third column shows the inverse ofthis, which represents the application\u2019s efficiency in terms of how much moreefficient the tuned application is compared to the untuned one.\nTABLE 4-6 Application CPU Usage per Transaction\nMonthApplicationCPUUsage\nper TransactionApplicationEfficiencyFactor\n(Transactions per CPU)\n1 1.00 1\n2 1.00 13 0.80 1.254 0.80 1.255 0.90 1.116 0.90 1.117 0.90 1.118 0.70 1.439 0.70 1.4310 0.55 1.8211 0.55 1.8212 0.55 1.82\n\nModelling Capacity and Load 69The factors in TABLE 4-6 are plotted in FIGURE 4-11 .\nFIGURE 4-11 Chart of Application CPU Usage per Transaction\nTo counteract the increased load levels, the application is tuned and the hardware is\nupgraded. If your initial application is untuned, it may be possible to incrementallyachieve a four- or five-fold improvement by tuning and reimplementing custom13 0.55 1.82\n14 0.50 215 0.50 216 0.50 217 0.30 3.3318 0.30 3.3319 0.30 3.3320 0.20 521 0.20 522 0.20 523 0.15 6.6724 0.15 6.67TABLE 4-6 Application CPU Usage per Transaction (Continued)\nMonthApplicationCPUUsage\nper TransactionApplicationEfficiencyFactor\n(Transactions per CPU)Growth Factor\nTime in Months\n\n\n70Chapter 4  Scenario Planningapplications and databases over several years. In the chart shown in FIGURE 4-11 ,\ndatabase tuning or application software upgrades give the first two gains; then thereis a reversal as a new, more complex user interface is introduced. This makes theusers happier but adds to the load. After a while, several more tuning improvementsare made, until after two years, the computer resources used per transaction isreduced to 15% of that at the start.\nCapacity Increases\nHardware vendors can supply capacity increases in the form of more efficientoperating systems, more CPUs, faster CPUs, or new generations of systems. Systemsthat depend on a database can include the database version here as part of theplatform efficiency. This causes a series of steps (see\nTABLE 4-7 ) that are captured in\nthe spreadsheet and shown graphically in FIGURE 4-12 . The performance factors must\nbe estimated from experience with the application in use and standard industrybenchmarks; the factors shown here were made up for illustrative purposes.Appendix A, \u201dSun Constant Performance Metrics,\u201d shows some performancenumbers for various machines that may be appropriate for projecting the capacity ofnew hardware.\nTABLE 4-7 Hardware Upgrade Capacity Increase Factors\nMonth CommentPerformance\nAssuming 70%UtilizationFactor\nAssuming70%UtilizationFactor\nAssuming100%Utilization\n1 CPU E6K 12 x 333 MHz + 4 MB 1000 1.0 1.43\n2 1000 1.0 1.433 1000 1.0 1.434 1000 1.0 1.435 CPU E6K 16 x 333 MHz + 4 MB 1200 1.2 1.716 1200 1.2 1.717 1200 1.2 1.718 Oracle 8 Upgrade 1400 1.4 2.009 1400 1.4 2.0010 CPU E6K 20 x 333 MHz + 4MB 1600 1.6 2.2911 1600 1.6 2.2912 CPU E6K 24 x 333 MHz + 4MB 1800 1.8 2.5713 1800 1.8 2.57\n\nModelling Capacity and Load 71A notional level of 70% capacity utilization was also factored in, assuming that the\nsystem was 70% busy before the start.\nFor the graph in FIGURE 4-12 , some system configurations and performance factors\nwere invented.\nFIGURE 4-12 Chart of Hardware Upgrade Capacity Increase Factors14 Solaris 7 OE upgrade 2000 2.0 2.86\n15 2000 2.0 2.8616 2000 2.0 2.8617 2000 2.0 2.8618 2000 2.0 2.8619 2000 2.0 2.8620 CPU E10K 40 x 400 MHz + 8 MB 3000 3.0 4.2921 3000 3.0 4.2922 Solaris OE/Oracle 64-bit upgrade 3500 3.5 5.0023 3500 3.5 5.0024 3500 3.5 5.00TABLE 4-7 Hardware Upgrade Capacity Increase Factors (Continued)\nMonth CommentPerformance\nAssuming 70%UtilizationFactor\nAssuming70%UtilizationFactor\nAssuming100%UtilizationGrowth Factor\nTime in Months\n\n\n72Chapter 4  Scenario PlanningInTABLE 4-7 , the third column shows the CPU utilization in each month. This\nassumes a start point, which for this case is entered into the spreadsheet as 70% busyat month zero. The utilization is calculated by taking into account all the differentgrowth factors explained in the next section.\nCombined Capacity Estimate\nThe combined effects of the capacity increases can be computed in a manner similarto combining the load increases as shown in\nTABLE 4-8 . This computation can easily\nbe done with the spreadsheet. The final column shows the utilization each month,which consists of the total load (from\nTABLE 4-5 ), divided by the capability computed\nbelow.\nTABLE 4-8 Combined Capability Calculation\nMonthHardware Capacity\nFactor Assuming100% UtilizationApplication\nEfficiencyCapability (Hardware\nCapacity TimesApplication EfficiencyUtilization\n(Load/Capability)\n1 1.43 1 1.43 71%\n2 1.43 1 1.43 92%3 1.43 1.25 1.79 51%4 1.43 1.25 1.79 69%5 1.71 1.11 1.90 71%6 1.71 1.11 1.90 82%7 1.71 1.11 1.90 84%8 2.00 1.43 2.86 60%9 2.00 1.43 2.86 145%10 2.29 1.82 4.16 117%11 2.29 1.82 4.16 139%12 2.57 1.82 4.68 135%13 2.57 1.82 4.68 134%14 2.86 2 5.71 101%15 2.86 2 5.71 98%16 2.86 2 5.71 101%17 2.86 3.33 9.52 56%18 2.86 3.33 9.52 56%19 2.86 3.33 9.52 55%\n\nModelling Capacity and Load 73The combination of efficiency and hardware capacity upgrades can be plotted on the\nsame axis ( FIGURE 4-13 ) to see if the system is capable of sustaining the load.\nFIGURE 4-13 Chart of Combined Load vs. Capability\nThe plot in FIGURE 4-13 shows that the combination of a marketing campaign and the\nseasonal boost overloads the server at peak times through the first Christmas period.If this model is correct, something else needs to be done to sustain this futurebusiness scenario. Either a faster system could be installed earlier, or more efficiencycould be squeezed out of the system. However, another option is that the work itselfcould be split in some way, possibly by functional or geographical aspects. If youwere modelling the front-end http servers, search engines, or application servers,you could increase the number of servers. It is much harder to split up a back-enddatabase.20 4.29 5 21.43 28%\n21 4.29 5 21.43 74%22 5.00 5 25.00 83%23 5.00 6.67 33.33 81%24 5.00 6.67 33.33 92%TABLE 4-8 Combined Capability Calculation (Continued)\nMonthHardware Capacity\nFactor Assuming100% UtilizationApplication\nEfficiencyCapability (Hardware\nCapacity TimesApplication EfficiencyUtilization\n(Load/Capability)Growth Factor\nTime in Months\n\n\n74Chapter 4  Scenario PlanningTweaking the Model\nThe example spreadsheet used previously has been carefully tweaked to give a\ncapability that tracks the load level reasonably well. When you first enter your owndata and estimates into the spreadsheet, you will probably find that you have a verypoor fit after the first few months. You will need to spend some time tweaking theparameters in the model, trying out alternative what-if scenarios until you are happy\nthat your assumptions make sense. When manipulating the data, bear in mind thatthe analysis is based on peak loads, not average loads. If you take measurements atintervals of a few minutes, they can be averaged; but as soon as you zoom out toconsidering intervals of an hour or more, you want to know the peak level duringthat hour and the peak hour during the day.\nPredicting Tomorrow\nWith all this data and some idea of the typical daily profile, you can predict the nextfew days in detail, then monitor the real load level to see how close you get. You cansimply combine the weekly and monthly predictions to get an estimate of the loadlevel over the next week. The monthly data is based on the peak for the wholemonth, and it could occur at any time during the month; so, you could simplyassume that the monthly peak could occur on any of the days that coincide with theweekly peak. For example, if the weekly peak is a Monday, then the monthly peakcould occur on any Monday in the month. This gives you a slightly conservativeestimate.\nFitting the Model to the Data by Tweaking\nFor a large Internet site, the daily Web traffic was analyzed to pick out the cgi-bintransactions that caused the back-end database to be invoked. These counts werefurther reduced to obtain the busiest day in each month for an 18-month period.This set of numbers was then fitted into the spreadsheet-based model, taking intoaccount geometric exponential growth, seasonality, and marketing-relatedvariations.\n1. Add the measured data to the planning spreadsheet.\nThe first step is to make a new graph that shows the measured data and the\npredicted data, including a table showing the proportional difference (the error inthe fit) between the two.\n\nSummary 752. Tweak the geometric exponential growth.\nSet the workload growth parameters shown in TABLE 4-3 to get a first approximation\nof the overall long-term growth rate.\n3. Tweak the seasonality.\nLook at the difference factors calculated in the first step. Where two months a year\napart are out by a similar factor, add that factor to the seasonal load variationestimates in\nTABLE 4-2 .\n4. Carry on tweaking.\nGo back and vary the exponential growth factor to try and get the difference to\nmatch better year after year, and tweak the seasonality factors until as many monthsas possible have a good match. Go around this loop several times.\n5. Set up marketing factors.\nAnything that is not explained by growth and does not repeat from year to year is\ntaken care of as if it is due to marketing awareness bias. The marketing factors canbe set up to zero out the difference factors.\n6. Keep tinkering.\nMake sure that you don\u2019t have marketing factors that repeat year after year; these\nvalues should be moved to be seasonal.\n7. Predict the next few months.\nCome up with estimates for the next few months, compare the estimates with reality\nas they occur, and refine the model to take new data into account.\nSummary\nSpreadsheet-based models are a good starting point, but are extremely limited andcan be clumsy to use and extend. As soon as you want to model more than onesystem or a mixture of several kinds of workloads, the mathematics become toocomplex for a spreadsheet. There are several tools that can handle multiclass andmultisystem models.\nIt is easier to model CPU utilization than disk capacity and performance, but large\ndisk subsystems are expensive and may be the primary bottleneck of yourapplication. In the next chapter, we introduce a simplified model of diskperformance that can cope with complex disk subsystems.\n\n76Chapter 4  Scenario Planning\n\n77CHAPTER5\nCapacity Estimation\nThe four key components of any system can be grouped into CPU, memory, disks,\nand networks. Each of these need to be examined to determine the capacity availableand the utilization of that capacity. By identifying the primary bottleneck, theexamination shows where the system can be expected to run out of headroom first.\nEarlier in this book, we assumed that capacity and utilization were easy to obtain for\nbottleneck estimation. In practice, getting those values may actually be morecomplex, so this chapter examines the problems in more detail.\nOverview\nCPU capacity is quite easy to estimate from numbers obtained with vmstat and\nsar. Sum the user and system time to calculate CPU utilization, leaving the idle and\nwait for I/O time as the remaining headroom. The underlying data is inaccurate atlow utilization levels. As long as the utilization is above 10% busy, the reportedutilization is good enough for the purposes of capacity estimation. The SunBluePrints book Resource Management discusses the sources of error in more detail in\nthe Measurements and Workload Analysis chapters.\nYou measure network capacity by using the byte level throughput counters and\napplying some configuration knowledge of what bit rate the interface is running at.These counters are not printed by standard system utilities because they are device-dependent. The SE toolkit\n1nx.se script formats them nicely, or the command\nnetstat -k displays the raw data for the hme,qfe, andgeinterfaces. The Solaris\n8 OE adds the kstat command, which obtains and processes these counters more\nconveniently.\nMemory capacity is difficult to estimate in older Solaris OE releases. The\npriority_paging option improves memory usage characteristics and has been\nbackported to the Solaris 2.5.1 and 2.6 OEs. The priority_paging option is\navailable in the appropriate kernel update patch. Solaris 7 OE includes this code as\n1. The latest SE toolkit is available at http://www .setoolkit.com.\n\n78Chapter 5  Capacity Estimationstandard, but it must be enabled explicitly in /etc/system . Solaris 7 OE also adds\na new option to vmstat to break down the reasons for paging. If executable code is\nbeing paged out, you definitely need more memory; however, if just file cache isbeing paged out and freed, you have no problem. Solaris 8 OE changes the memorymanagement subsystem in several ways to make paging much simpler tounderstand. There is no need for priority paging in Solaris 8 OE because a moresophisticated scheme is used by default. You should see no scanning, a lot morepage reclaims, and a much higher free memory value reported by vmstat . With\nSolaris 8 OE, the free value in vmstat can be used reliably as a measure of how\nmuch memory is free for new applications to use on a system, so it solves thecapacity estimation problem.\nDisk capacity estimation has become very complex because of too many different\ntypes of CPUs with varying scalability characteristics. Consequently, these varyingcharacteristics create complicated disk subsystems. The following sections giveexamples of system resource modelling and look at the problem of capacity planningfor CPU and disk subsystems in detail.\nSystem Measurement Frames\nThe time period measured for the utilization of system resources is key to accuratesystem resource modelling. By choosing a representative period that accuratelyreflects system usage, you can use the model to project system utilization and loadplanning impacts through to the applied system changes.\nA system can be modelled as a whole or broken down into individual services to\nbuild separate models for different functions. One primary example of this is anonline transaction processing (OLTP) order processing system that runs backupsnightly and has an integrated database report generation system. Separate SLAsmight exist for system performance during system maintenance tasks, systemperformance during business hours, and the length of time allowed for systemmaintenance tasks and database batch report runs. In this scenario, models shouldbe created showing interactive performance inside and outside the allowedmaintenance window, as well as the \u201cbatch load\u201d performance of the backup systemand reporting system.\nResource management schemes, such as processor sets and SRM software, can be\nused to control the backup and reporting systems to reduce impact on OLTP . Inaddition, these schemes can be used to reduce the impact of transaction processingto assure that backups and reports can be completed in the assigned processingwindows. At this point in the capacity and performance management cycle, intimateknowledge of the business system being modelled is required to prioritize theworkload that might be competing for system resources. In this simplified example\n\nSystem Measurement Frames 79of system utilization ( FIGURE 5-1 ), a basic \u201cconsumer shift\u201d of customer interaction\nbetween 8:00 a.m. and 6:00 p.m. and a maintenance task shift between 1:00 a.m. and4:00 a.m. are easily recognized.\nFIGURE 5-1 Graph of CPU Utilization Over a 24-Hour Sample\nThe first step in building a model is to choose an appropriate sample period. Forconsumer shift processing, SLAs are most likely defined for transactional volumesand system response time. The sample period should reflect the maximum period oftime for which degraded system performance is to be tolerated, with a criticalthreshold of maximum degradation to be allowed. In an online processing systemthat has direct interaction with the paying customer of the business, the sampleperiod could be as low as one minute. These samples are used for analysis andtracking and are averaged into longer time frames for summarized reporting.\nIn a batch processing environment such as scheduled report generation and system\nbackups, the high system utilization and resulting system OLTP performancedegradation are not exposed to the end-user and they are actually expected andplanned for. System batch processing that occurs outside the consumer-exposedprocessing window should fully utilize system resources to shorten the runtime ofthe batch processes. For this reason, SLA-defined thresholds for system utilizationand transactional performance are not in effect for batch processing windows.\nOverlapping batch processing windows and transactional processing should be\navoided where possible. Batch reports, database record purging, system backups,and other batch system maintenance should be scheduled into time windows\n%busy\nTimeUtilization23:00\n22:00\n21:00\n20:00\n19:00\n18:00\n17:00\n16:00\n15:00\n14:00\n13:00\n12:00\n11:00\n10:00\n09:00\n08:00\n07:00\n06:00\n05:00\n04:00\n03:00\n02:0001:00\n00:00100\n90\n80\n70\n60\n50\n40\n302010\n0\n\n80Chapter 5  Capacity Estimationisolated from periods of customer interaction. In systems where customer interaction\nis necessary 24 hours a day, 7 days a week, strict and balanced system resourcemanagement is required.\nFor systems where transaction processing overlaps the maintenance window, more\ncomplex modelling is required. Capacity management suites such as TeamQuest andBMC\u2019s Best/1 are capable of accurately separating the workload processing andmodelling the resource conflicts of multiple workloads competing for systemresources. With basic scripting, simple models using system utilities such as mpstat\ncan be used in conjunction with processor sets to monitor shared workloadenvironments to collect and present the system resource data. In this example(\nFIGURE 5-2 ), the batch report generation system is operating around the clock, the\nOLTP system is operating on a peak shift of 8:00 a.m. to 6:00 p.m., and systemmaintenance tasks are competing for system resources during the systemmaintenance window of 1:00 a.m. to 4:00 a.m.\nFIGURE 5-2 Graph of CPU Utilization by Workload\nOne immediate change to workload scheduling can be derived from this graph.There appear to be batch reports running during the system maintenance window.By moving these batch reports outside the defined maintenance window andcreating a defined shift for reporting tasks (see\nFIGURE 5-3 ), we can enforce resource\nmanagement through a published scheduling policy. System utilization is thencontrolled through policy, rather than system enforcement. This simple approach isoften the most cost-effective (free) at the onset, but can result in underutilizedsystem resources when workloads could otherwise coexist without contention orwith contention minimized through the proper use of resource management tools.\nReports\nBackup\nOLTP\nTimeUtilization23:00\n22:00\n21:00\n20:00\n19:00\n18:00\n17:00\n16:00\n15:0014:00\n13:00\n12:00\n11:00\n10:00\n09:0008:00\n07:00\n06:00\n05:0004:00\n03:00\n02:0001:00\n00:00100\n8060\n40\n20\n0\n\nSystem Measurement Frames 81To illustrate this case, midday reports are allowed between 12:00 p.m. and 1:00 p.m.\nfor hourly summary tracking of morning transactions; all other report generation isassigned to the 6:00 p.m. to 0:00 a.m. window.\nFIGURE 5-3 Graph of CPU Utilization After Load Balancing\nIsolating the different system processing tasks greatly simplifies the implementationof monitoring tools and system resource data collection.\nSun Constant Performance Metric (SCPM) Value\nCapacity Planning\nAt Sun, we have been using our own capacity estimation method (developed by\nBrian Wong), which is based on historical mainframe capacity planning methodsand has been refined through many years of internal and customer engagements.Brian\u2019s method is commonly referred to as the \u201cM-value process,\u201d after thetraditional mainframe performance metric. By creating a series of generic terms todefine work on a computer and creating metrics for a volume of work, it is possible\nto quantify several statistics. These statistics describe the capacity of the machineplatform, as well as the performance characteristics of a given workload on amachine.\nTimeUtilizationReports\nBackupOLTP\n22:00\n22:00\n22:00\n22:00\n22:00\n22:00\n22:0022:0022:00\n22:00\n22:00\n22:0080\n70\n60\n50\n40\n30\n20\n10\n0\n\n82Chapter 5  Capacity EstimationThe basis of this method is a performance metric describing the quantity of work\nthat a particular system configuration is capable of processing. This number, calledthe Sun constant performance metric (SCPM), represents the relative performance ofa given server. The metric by which an SCPM is scaled is called the quanta (Q).SCPMs are adjusted based on measured performance values using predictablebenchmarks. In addition, SCPMs are a real-world estimation of Amdahl\u2019s Lawapplied to system architectures and operating system versions for cases of regular,predictable, and normal workloads. Occasionally, significant patches may influence\nthe scalability of an operating system; in that case, SCPM values must be adjustedfor the relevant platforms. The trends in SCPM values across the operating systemand patch versions can be examined and used when upgrading. Examining thesetrends can provide guidance for predicting the impact that upgrade modificationsmight have on system performance. See Appendix A, \u201cSun Constant PerformanceMetrics,\u201d for a complete list of comparative system capacity estimations for SolarisOE versions 2.5, 2.5.1, 2.6, 7, and 8.\nAny metric that relates to your particular application\u2019s performance characteristics\non a given hardware platform can be used. Examples of possible alternatives includeSPECrate, SPECint, TPC benchmark numbers, and benchmarks from softwarevendors such as SAP , Baan, and Peoplesoft. This method has proven valid, withaccuracy dependent on the applicability of the chosen comparative performancemetric to your particular workload.\nRather than using the supplied, somewhat conservative estimations of\nmultiprocessor scalability, you can measure the workload being projected forparallelization with the SCPM values, providing relative CPU performance metricsfor CPU and architecture changes. Measuring the potential parallelization of yourparticular workload provides a more accurate model of the performancecharacteristics of your workload, which results in more accurate projections ofsystem resource utilization.\nAmdahl\u2019s Law\nAmdahl\u2019s Law provides the calculations for predicting the benefit of running aworkload on a scalable multiprocessor environment. We can apply a simplified set ofcalculations based on Amdahl\u2019s Law to project the scalability of our workload.\nThe first result of interest is the speed-up factor of your measured workload. The\nspeed-up factor is a calculated variable for your workload; it describes the capabilityof your application to do work in parallel while taking advantage of additionalprocessors. The parallelization is limited by finite resource contention and \u201cserialoperations\u201d that cannot take advantage of additional processors and could actuallydegrade workload throughput. Application of Amdahl\u2019s Law measures CPUarchitectures and presumes that there are no bottlenecks in the disk or networksubsystems and that memory is adequate to process the workload withoutstarvation.\n\nSystem Measurement Frames 83The speed-up of a workload is calculated from measurements taken with a chosen\nworkload running on one CPU and that same workload running on several CPUs. Itis best to use a series of measurements of speed-up, using a logical progression ofCPU counts reflecting the likely upgrade path to your system architecture (1, 2, 4, 8,12, 16, \u2026).Speed-up is calculated using the time taken to accomplish a defined set ofrepresentative transactions. The following formula is used for a measured time ( T)t o\ndo the same transactions with one CPU and nCPUs:\nBy applying some simple algebra to this formula and using a fixed time frame for\nmeasurement, we can calculate speed-up from the throughput of transactions withvarying quantities of CPUs. The following formula is used for throughput in numberof transactions ( N) of a given workload run over a fixed period of time on one CPU\nand nCPUs:\nGeometric Scalability\nThe speed-up factor that is calculated with a single CPU and two CPUs can be used\nto calculate the scalability factor ( S) of a given workload. The scalability factor is a\nconstant (associated with the measured workload), describing how well theworkload will operate on a machine containing more of the same type of CPU. Thefollowing formula demonstrates that the scalability factor represents the percentageof the workload that is parallelized, and not the finite resource constrained:Speed upn()\u2013T1()\nTn()------------=\nSpeed upn()\u2013Nn()\nN1()------------=\nS Speed up2()\u20131\u2013 =\n\n84Chapter 5  Capacity EstimationThe scalability of a given machine for a given workload can be predicted with the\nscalability factor, and by compounding the scalability for a given number of CPUs.The scalability factor may be reduced if there is resource contention. Adding CPUswill increase the contention, thereby reducing the percentage gains. The followingformula is used for a machine with nCPUs, a scalability factor of S, and a measured\nthroughput of T\n(1)with a single CPU:\nThe graph in FIGURE 5-4 shows scalability factors of 100% (linear scalability), 99.5%,\n98%, and 70% for an eight-CPU architecture; in addition, the graph shows therelative performance increase of adding CPUs. Lower scalability factors caninfluence system architecture decisions toward wider server architectures, with more\nsmaller servers combined to provide the end-user service. Higher scalability factorscan help influence architecture decisions to deeper server architectures, with fewer\nlarger machines providing the service.\nFIGURE 5-4 Eight-CPU Geometric ScalabilityTn()T1()ST1()\u00d7() S2T1()\u00d7() \u2026 Sn1\u2013T1()\u00d7() ++ + +=\nNumber of CPUsRelative Performance8\n7\n6\n5\n4\n3\n2\n1\n0\n12345678100.00%\n99.50%98.00%\n70.00%\n\nSystem Measurement Frames 85Deeper architectures are generally easier to manage and can lower infrastructure\nand support costs. With deeper architectures, a direct relationship is establishedbetween the scalability of an application architecture and the potential total cost ofownership (TCO). With lower scalability factors, the progressively lower increase inperformance benefit of adding additional CPUs (see\nFIGURE 5-5 ) can outweigh the\npotential savings in TCO of managing fewer systems. Larger system architecturesgenerally provide more reliability and serviceability features. For example, the\u201cN+1\u201d power and cooling subsystems supply enough power and cooling capabilities\nto survive a single component failure, as well as other redundant components,redundant data paths, and redundant peripheral adapters. Consequently, the N+1\npower and cooling subsystems increase the potential system availability.\nFIGURE 5-5 64-CPU Geometric Scalability\nThe scalability of any particular software on a particular server might displaynongeometric scalability characteristics. For most normal workloads that we have\nmeasured, the variances from the geometric scalability curves are minimal. Factorsthat might cause a significant variance include:\n\u25a0CPU cache/code efficiency \u2013 If the whole of, or a significant portion of, the server\napplication\u2019s execution can occur from within the CPU cache without preemptionor flushing of the CPU cache, the scalability of that application will \u201cbump up\u201dwhen the most frequently accessed pieces of an application can fit within thecombined CPU caches and across a number of CPUs.\nNumber of CPUsRelative Performance70\n60\n504030\n20\n10\n01471013161922100.00%\n99.50%98.00%\n70.00%\n2528313437404346495255586164\n\n86Chapter 5  Capacity Estimation\u25a0Limits to application software threading \u2013 If the application software is incapable of\nthreading, or running as multiple instances, or has some static limit to the numberof server threads utilized to provide the service, additional CPUs will notgenerally add significant increases in throughput or improved response times.\n\u25a0Finite resource consumption \u2013 If the software consumes a finite set of one or more\nresources, at some point, the contention for resources could actually degrade theoverall throughput and increase application response times. The reason is that toomany threads or instances of a server application could be competing with eachother. Examples of resource contention areas include locking mechanisms forexclusive access to resources, single pipelines for data I/O, and even hardwaredevice and device driver limitations.\nMeasuring Utilization\nThe SCPM capacity planning process applies the basis of the Utilization Law tomeasure how much of a server\u2019s resources are being consumed during a samplemeasured period. The Utilization Law\n1is defined as:\nDuring a period of time ( T), tasks are submitted to the server. The server\nprocesses the tasks, and the tasks have a finite conclusion. Componentresources (CPU time, disk I/Os per second) within the server have afinite capacity. Processing consumes measurable resources in the serverplatform. An observer of the server during the observed time frame canidentify the:\n\u25a0Amount of time during the observed time frame that the server resourcecomponent is busy processing tasks ( B)\n\u25a0Number of tasks completed during the time frame ( C)\nThese measurements result in the following calculations:\n\u25a0Output rate or throughput of the server measured in tasks per unit of time:\nThroughput =C\u00f7T\n\u25a0Average service time to complete a computing task:\nService Time =B\u00f7C\n\u25a0Percentage of the observation period that the server resource component is\nprocessing tasks, called the utilization of the server:\nPercent Utilization =B\u00f7T\nFrom this application of the Utilization Law, we can state empirical measured\nmetrics to characterize the measured workload such as:\n1. \u201cQueueing in Networks of Computers,\u201c American Scientist 79, 3 (May-June 1991).\n\nSystem Measurement Frames 87For a sustained level of 12,000 catalog lookup queries per hour, CPU utilization\nwas measured as 62% busy and disk utilization was measured as 22% busy.\nFortunately, common system reporting utilities report percent utilization, but it is\nstill useful to understand the underlying computations.\nSCPM Measurement\nIn the SCPM process, a unit of work is called a quanta (Q). Systems that containpotential for doing work are represented by (M), which is a multiple of a quanta.The amount of work being done on a machine is the quanta consumed (QC), ormeasured system utilization multiplied by (M).\nFIGURE 5-6 is an example of the\nSCPM process.\nFIGURE 5-6 Sample SCPM Process\nThis description of the SCPM process has been simplified for this exercise. There aremany other considerations in performance baselining with the SCPM process, butthe basic concept behind the process is fairly easy to understand. By measuring realworkloads and their relative disk I/O, CPU, memory, and network consumption,you can predict the effective resource consumption of that workload. You can alsoproject resource consumption into other SCPM-rated platforms or modify thevolumes of work being performed within the model.heisenberg%sar -s 08:00 -e 16:00 -i 3600 -f /var/adm/sa/sa27\nSunOS heisenberg 5.8 sun4u    03/27/0008:00:00    %usr    %sys    %wio   %idle\n09:00:00      24       9       0      6610:00:00      34      13       0      5311:00:00      34      12       0      5412:00:00      21       9       0      7013:00:00      52      21       0      2714:00:00      61      27       0      1215:00:00      58      24       0      1816:00:00      54      23       0      23\nAverage       42      17       0      41\n\n88Chapter 5  Capacity EstimationIn this example, hourly samples of CPU utilization were extracted from the system\nsar data to represent a given shift. The start time is 8:00 a.m., and the end time is\n5:00 p.m., or 16:00 plus the hour of the sample. The sample interval is 3600 seconds,or one hour. The output of sar can easily be imported into a spreadsheet for further\ncalculations (see\nFIGURE 5-7 ).\nFIGURE 5-7 Sample Spreadsheet of SCPM Process\nBy adding the %usr and%sys columns from the sar-generated data, we now have\nthe%busy for each sample period and a shift average. The %wio column is not\nconsidered for this exercise, because %wio indicates the time that the CPUs were\nwaiting, and not processing. This process measures workload processing or \u201cthinktime\u201d; thus, waiting on disk activity would be considered \u201cidle.\u201d The %wio is always\na function of the disk subsystem and will be minimized elsewhere in the capacityplanning process.\n\n\nSystem Measurement Frames 89If our target peak hour utilization has been defined in the KPI document to be 70%,\nwe are showing four hours of utilization over the target. This can be presentedgraphically (\nFIGURE 5-8 ) for the executive summary of the SLA and KPI reports.\nFIGURE 5-8 CPU Utilization Graph\nTimeUtilizationTarget\nPeak\n%sys\n%usr\nAverage\n16:00:00\n15:00:00\n14:00:00\n13:00:00\n12:00:00\n11:00:00\n10:00:00\n09:00:00100\n60\n40\n20\n080\n\n90Chapter 5  Capacity EstimationWe can also project the measured system utilization into the system SCPM value to\nproduce the quanta consumed (QC) over time (see FIGURE 5-9 ). This representation\ncan sometimes be easier to understand than percentage utilization graphs because itfocuses more on the goal of system resource consumption rather than on the used/unused resources of a system. The utilization-based graph can sometimes beinterpreted as showing that there is plenty of system capacity, despite thepresentation of utilization exceeding the stated goals of the KPI. These graphicrepresentations can be included in an executive overview document for SLA andKPI review.\nFIGURE 5-9 System Quanta Consumed Graph\nQC\nTarget QC\nAverage\n16:00\n15:00\n14:00\n13:00\n12:00\n11:00\n10:00\n09:002967726709\n23742\n20774\n1483917806\n11871\n8903\n5935\n0\nTimeQuanta Consumed (QC)\n\nSystem Measurement Frames 91We can now project the measured workload into a different platform, projecting the\nsystem utilization of that system. In this example ( FIGURE 5-10 ), we upgrade from our\nmeasured platform (an E4500 with 12 \u00d7250 MHz, 4MB cache CPUs) to a Sun\nEnterprise 4500 server with 12 \u00d7400 MHz, 8 MB cache CPUs. The upgraded system\nwill have a predicted utilization of (QC \u00f7M).\nFIGURE 5-10 Upgraded CPU Utilization Graph\nIf we divide the QC on a given platform by the number of users accomplishing that\nwork, we now have the quanta consumed per user (QC \u00f7User). You can then\ncompute the number of users that a given machine will support:\nTotal Users Supported =(SCPM\u00d7Target Utilization) \u00f7(QC\u00f7User)\nAs part of load planning or server consolidation exercises, the (QC \u00f7User) can help\nus plan for future growth and load changes within our model and will projectsystem utilization for a variety of possible scenarios.\n100\n80\n60\n40\n20\n0\nTimeUtilizationAverage\n16:00:00\n15:00:00\n14:00:00\n13:00:00\n12:00:00\n11:00:00\n10:00:00\n09:00:00Upgrade\n%busy\nTarget\nPeakQC Upgrade %busy\n9793.41\n13948.19\n13651.42\n21664.21\n26115.76\n24335.14\n22851.29\n17509.438903.1\n3821\n30\n29\n19\n46\n56\n52\n49\n\n92Chapter 5  Capacity EstimationMeasuring Disk Storage Capacity\nCapacity planning with SCPM values provides a simple method for measuring,\nreporting, and predicting consumption of CPU resources. This same method can bemodified slightly and applied to disk subsystems. By measuring the transactionsand utilization of the disk subsystems and projecting against the capabilities of thesubsystem being consumed, we can create a model for storage capacity planning.\nThe first measurable metric used in this method is the total number of disk I/O\nactivities per unit of time (generally averaged out to one second and referred to asI/Os per second, or IOPS) generated by the measured workload. This metric can bemeasured and reported with iostat ,sar, SunMC, or other system monitoring\ntools. It is critical to the validity and accuracy of these metrics that themeasurements be taken during a steady state, or a time frame when the flow oftransactions is relatively constant and indicative of a period of normal user activity.\nThe activity being measured is only related to disk activity, so data generated by\ntools that report tape drive, Network File System (NFS), or CD-ROM activity mustbe \u201ccleaned\u201d before being used for capacity planning. The number of disk activitiesis measured and reported for total disk activity, including reads and writes ( S), as\nwell as individual measurements for disk read activities ( S\nR) and disk write activities\n(SW).\nFor the measured period shown in TABLE 5-1 , the total number of disk activities ( S)\nwas 182, which is the sum of the r+w/s column, excluding the nfs1 andst11\ndevices (NFS and tape drive utilization).\nTABLE 5-1 sar Disk Output\n00:00:01 device %busy avque r+w/s blks/s avwait avserv\nAvg nfs1 0 0.0 0 0 0.0 0.0\nsd0 1 0.0 12 19 0.3 79.9sd1 0 0.0 14 22 0.0 30.6sd2 1 0.0 8 13 0.3 79.9sd3 0 0.0 9 15 0.0 30.6sd4 1 0.0 11 18 0.3 79.9sd5 0 0.0 6 8 0.0 30.6sd41 31 0.4 34 41 0.5 22.2sd42 14 0.2 16 19 0.2 24.1sd68 19 0.2 21 25 0.2 23.8\n\nMeasuring Disk Storage Capacity 93The number of disk activities during the measured period can be referenced in\nrelation to the service levels provided in that sample time. By dividing the diskactivities per second by the transactional rate in transactions per second (TPS), wecan compute a metric indicating the disk activity per transaction, which is a vitalindication of the relative disk resource consumption of the application:\nReads per Transaction =S\nR/ TPS\nWrites per Transaction =SW/ TPS\nDisk Activities per Transaction = S / TPS\nThe disk activity can also be computed in relation to the SCPM QC to acquire the\nrelative disk I/O content ( R) value of the measured workload. This metric describes\nthe relationship of disk activity to CPU resources consumed, independent of theserver platform on which the workload was measured. For a given amount of workdone by the CPUs, the following formula shows that we can now associate a relativequantity of work done by the disk subsystem to support that processing:\nR=S\u00f7QC\nThe relative disk I/O content for a given workload should remain almost constant.\nChanges to a workload (new features, new data structures) can change the R valuefor a workload, and should trigger a new measurement cycle to establish a newbaseline. The historic measurements should be maintained relative to the establishedsoftware change management process to document the performance characteristicchanges induced by changes in the service application and server architecture.\nTotal disk throughput ( T\nDisk) can be computed by multiplying the disk I/O size of\nthe given storage subsystem (raw disk access size, or file system block size) timesthe number of disk activities ( S) for the system. This number does not take into\naccount the metadata activity (ACL, inode, directory lookup) and will be slightlyhigher than the measured activity. The size of the datasets generally increases fasterthan the volume of metadata activity (files grow faster than the number ofindividual files grows), so in this case, we choose to err on the side of safety andoverestimate the maximum required disk throughput. As the system grows, thisvalue should approach reality and become more accurate as we calibrate themeasured average I/O sizes against the data access I/O size.sd69 19 0.2 21 26 0.2 22.9\nsd70 6 0.1 14 17 0.2 18.1sd71 13 4.3 16 19 0.2 24.1st11 2 4.3 122 484 2.3 332.2TABLE 5-1 sar Disk Output (Continued)\n00:00:01 device %busy avque r+w/s blks/s avwait avserv\n\n94Chapter 5  Capacity EstimationIncreased cache efficiencies provided by volume management, file systems,\napplications, database software improvements, and database query optimizationscan significantly reduce the disk I/O required to complete a transaction. Newfeatures and functionality in the software architecture can increase the processingpower necessary to complete a transaction. The combination of these two trendswould cause the R value to reduce over time for a given application. Softwareapplication efficiency improvements, compiler optimization improvements, andincreasing sizes of datasets (in relation to the size of available cache layers used inaccessing the data) can increase the R value associated with a given workload.\nSCPM Load Planning\nFor a simplified example of a server with an SCPM value of 40,000, the followingmeasurements were taken for a steady state of 300 users and 50 transactions persecond during the peak measured shift:\nUsing these measurements and assuming that transactions are flowing at a relatively\nconstant rate, we can characterize the workload in many interesting ways, as shownin the following projection examples:Average CPU Utilization 75%\nAverage Disk Reads per Second 3000Average Disk Writes per Second 1500Total Disk Avtivities per Second 4500Filesystem I/O Size 8KB\nQC (40,000 (Q))  *  (.75) 30,000 Q\nQC / User (30,000 (Q))  /  (300 (Users)) 100 QQC / Transaction (30,000 (Q))  /  (50 (Transactions)) 600 Q*secS\nW  /  User (1500 (writes))  /  (300 (Users)) 5 IOPS\nSR  /  User (3000 (reads))  /  (300 (Users)) 10 IOPS\nS  /  User (4500 (I/Os))  /  (300 (Users)) 15 IOPSS\nW /\nTransaction(1500 (writes)) / (50 (Transactions)) 30 IOPS\nSR /\nTransaction(3000 (reads))  /  (50 (Transactions)) 60 IOPS\nS / Transaction (4500 (I/Os))  /  (50 (Transactions)) 90 IOPS\nTDisk (4500 (I/Os))  *  (8 (KB)) 36 MB/sec\n\nMeasuring Disk Storage Capacity 95During our monthly load planning meeting, the marketing department presented a\ngrowth projection of 50% for the next fiscal quarter, causing the business unitconsuming the service to increase their transactional and user community volumesby 50% to meet the demands.\nWe can now apply these projections against our system\u2019s capabilities to predict the\nsystem utilization while running the new workload volumes. The server running theworkload has an SCPM value of 40,000 Q, and the load projection shows aprocessing requirement of 45,000 Q. From these values, it is obvious that the currentserver will not be able to process the projected workload. If we upgrade the serverthrough module count increases, CPU module upgrades, or a \u201cforklift upgrade\u201d toan entirely new machine with an SCPM value of 72,000 Q, we can predict theutilization of the server running the projected workload. The optimal case wouldallow us to run the current measured workload of user and transactional volumes onthe new server system to validate our workload model, but this is rarely the case ina rapid growth environment. The following formula predicts the system utilizationwhile the new workload volumes are running:\nSystem Utilization =QC\u00f7SCPM (Q)\n45,000\u00f772,000 =62.5%\nSimilarly, the projected disk utilization can be predicted based on the capabilities of\nthe disk subsystem (in throughput and IOPS), and the projected throughputconsumption and generated IOPS of the workload after applying the growthintroduced in the load planning process.\nWorkload Characterization\nIn cases where the workload is defined but not measurable (for example, for a newsystem under development), historical data can be used from past measured systemsto characterize a likely resource consumption with a reasonable rate of accuracy. Inour applications of this process and from studies of both internal and externalcustomer data, we were able to identify and characterize very predictable systemperformance characteristics based on generic definitions of workload type.Regardless of the actual application, OLTP servers generally consume disk, CPU,and memory resources at a similar rate.QC (30,000 (QC))  *  (1.5) 45,000 Q\nSW (1500 (writes))  *  (1.5) 2250 IOPS\nSR (3000 (reads))  *  (1.5) 4500 IOPS\nS (4500 (I/Os))  *  (1.5) 6750 IOPST\nDisk 3.6 MB/sec  *  (1.5) 5.4 MB/sec\n\n96Chapter 5  Capacity EstimationBy applying the SCPM process, some historical rules of thumb for hardware\nperipheral behavior, and basic measurement and application of Amdahl\u2019s law for themeasured workload, we can generate reasonably accurate system capacity modelsfor a known workload. The SCPM process has proved very effective in modellingand predicting workloads and resource consumption for a large percentage of themachines sampled. Some workloads do break the mold, and are not as predictable asthe vast majority.\nIn some cases, applications will consume some critical resources or cause contention\nfor which the SCPM process cannot predict. These fringe cases must be accountedfor using more accurate and intensive (and thus more expensive) methods inaddition to the SCPM process, where very accurate models are critical. Theseintensive capacity, performance measurement, and prediction methods include:\n\u25a0Discrete event modelling\n\u25a0Benchmarks\n\u25a0Simulation\n\u25a0Queuing theory application\n\u25a0Finite resource consumption studies\nThe SCPM process is very useful in a first pass of a TCO analysis as part of a server\nconsolidation effort. Estimated machine and workload characteristics can becomputed and predictions can be made against a computing environment to supportthe current and future computing needs of the enterprise.\nCapacity Planning for Complex Disk\nSubsystems\nThe details of how disk measurements are derived are covered in-depth by Adrian\nCockcroft in Sun Performance and Tuning and the Sun BluePrints book Resource\nManagement . Brian Wong, in his book Configuration and Capacity Planning for Solaris\nServers, describes the architecture and performance characteristics of disk\nsubsystems based on SCSI and Fibre Channel. Here we focus on extending the use ofconventional iostat -based data to take into account the configuration of a more\ncomplex disk subsystem.\nDisk response time is slow at nearly 100% busy, but a striped volume or hardware\nRAID unit still seems to respond quickly at high utilization levels. This occursbecause more complex systems do not follow the same rules as simple systems whenit comes to response time, throughput, and utilization. Even the simple systems are\n\nCapacity Planning for Complex Disk Subsystems 97not so simple, so we start by looking at measurements available for a single disk,\nthen observe combinations, and finally show you how to combine the availablemeasurements into capacity estimates.\nCapacity Measurements for Single Disks\nSingle disks are quite simple to plan for, and their performance characteristics canusually be obtained from the disk vendor\u2019s Web site. The essential characteristics foreach disk are its average seek time (which may be different for reads and writes), itsaverage formatted transfer rate, and its spindle speed in RPM. For example, a high-end server 18.2 GB FC-AL drive is the Seagate ST318203FC, with 5.2 ms read seek,6.0 ms write seek, 24.5 MB/s transfer rate, and 10,000 RPM.\nThe most important measurement to take on each disk is the average transfer size.\nThis may be very stable or it may fluctuate depending on the workload. In somecases, you may need to break down the day into shifts or modes (e.g., online mode,backup mode) and analyze capacity for each mode separately.\nMeasurements on a Single Disk\nThe device driver maintains a queue of waiting requests, which are serviced one at atime by the disk. The terms utilization ,service time ,wait time ,throughput , and wait\nqueue length have well-defined meanings in this scenario, and for a single disk, this\nsetup (\nFIGURE 5-11 ) is so simple that a very basic queuing model fits it well.\nFIGURE 5-11 Simple Disk Model\nOver time, disk technology has moved on. These days, a standard disk is SCSI-basedand has an embedded controller. The disk drive contains a small microprocessor andabout 1 MB of RAM. It can typically handle up to 64 outstanding requests via SCSItagged-command queuing. The system uses a SCSI host bus adapter (HBA) to talk tothe disk. Large systems have another level of intelligence and buffering in ahardware RAID controller. The simple model of a disk used by iostat and its\nterminology have become confused. In addition, the same reporting mechanism isused for client-side NFS mount points and complex disk volumes set up by SolsticeDiskSuite\u2122 software or Veritas Volume Manager (VxVM).I/OI/OI/O\n\n98Chapter 5  Capacity EstimationIn the old days, if the device driver sent a request to the disk, the disk would do\nnothing else until it completed the request. The time it took was the service time,and the average service time was a property of the disk itself. Disks that spin fasterand seek faster have lower (better) service times. With today\u2019s systems, if the devicedriver issues a request, that request is queued internally by the RAID controller andthe disk drive and several more requests can be sent before the first one comes back.The service time, as measured by the device driver, varies according to the load leveland queue length, and is not directly comparable to the old-style service time of asimple disk drive. The response time is defined as the total waiting time in thequeue plus the service time. Unfortunately, iostat reports response time but labels\nitsvc_t . We\u2019ll see later how to calculate the actual service time for a simple disk.\nConsider the utilization of the device. As soon as the device has one request in its\ninternal queue, it becomes busy, and the proportion of the time that it is busy is theutilization. If there is always a request waiting, the device is 100% busy. Since asingle disk only completes one I/O request at a time, it saturates at 100% busy. If thedevice has a large number of requests and is intelligent enough to reorder them, itmay reduce the average service time and increase the throughput as more load isapplied, even though it is already at 100% utilization.\n\nCapacity Planning for Complex Disk Subsystems 99FIGURE 5-12 shows how a busy disk can operate more efficiently than a lightly loaded\ndisk. The difference is that the service time is lower for the busy disk, but theresponse time is longer. This difference is because all the requests are present in thequeue at the start, so the response time for the last request includes waiting forevery other request to complete. In the lightly loaded case, each request is servicedas it is made, so there is no waiting and response time is the same as the servicetime. When you hear your disk rattling on a desktop system as you start anapplication, it is because the head is seeking back and forth, as shown in the firstexample below of disk activity not taking advantage of I/O request queuing.Unfortunately, starting an application tends to generate a single thread of page-indisk reads and until one is completed, the next one is not issued; so, you end upwith a fairly busy disk with only one request in the queue and it cannot beoptimized. If the disk is on a busy server instead, lots of accesses are coming inparallel from different transactions and different users, so you will get a full queueand more efficient disk usage overall.\nFIGURE 5-12 Disk Head Movements for a Request Sequence\nSolaris OE Disk Instrumentation\nThe instrumentation provided in the Solaris OE accounts for this change byexplicitly measuring a two-stage queue: one queue, called the wait queue, in thedevice driver, and another queue, called the active queue, in the device itself. A reador write command is issued to the device driver and sits in the wait queue until theSCSI bus and disk are both ready. When the command is sent to the disk device, itSector 0       1000      2000      3000      4000      5000      6000      7000      8000      9000     10000Disk Head\nResponse order        1       3     6   8                                 9    4   7   5                                                            2\nResponse order        1       2     3   4                                 5    6   7   8                                                            9with no queue\nwith full queue\n\n100Chapter 5  Capacity Estimationmoves to the active queue until the disk sends its response. FIGURE 5-13 shows\nexample iostat -x output for a single disk in the common format supported in all\nSolaris 2 OE releases.\nFIGURE 5-14 shows the output of iostat -xn , which separates svc_t into its two\nsubcomponents and gives a more useful name to the device. This format wasintroduced in Solaris 2.6 OE, and at the same time, disk slices, tapes, and NFS mountpoints were added to the underlying kernel data source.\nThe problem with iostat is that it tries to report the new measurements in some of\nthe original terminology. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case, and theword wait is being used to mean two different things (see\nFIGURE 5-15 ).\nFIGURE 5-15 Two-Stage Disk Model Used by Solaris 2 OE%iostat -x\n                               extended device statisticsdevice    r/s  w/s   kr/s   kw/s wait actv  svc_t  %w  %bsd106     0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0\nFIGURE 5-13 Example iostat -x Output\n%iostat -xn\n                              extended device statistics  r/s  w/s   kr/s   kw/s wait actv wsvc_t asvc_t  %w  %b device\n21.9 63.5 1159.1 2662.9 0.0 2.7 0.0 31.8 0 93 c3t15d0\nFIGURE 5-14 Example iostat -xn Output\nI/OI/OI/OI/OI/OI/OSCSI\nBus\nI/Os active in\ndevice actv ,\nasv_tI/Os being\nserviced by\ndevice %bI/Os being\nsent to\ndevice %wI/Os waiting in\ndevice driverwait ,wsvc_t\n\nCapacity Planning for Complex Disk Subsystems 101Utilization ( U) is defined to be the busy time ( B) as a percentage of the total time ( T),\nas shown in the following examples:\nNow, we get to something called service time ( S), but it is notwhatiostat prints\nout and calls service time. This is the real thing! The following examplesdemonstrate that it can be calculated as the busy time ( B) divided by the number of\naccesses that completed, or alternately, as the utilization ( U) divided by the\nthroughput ( X):\nThe meaning of S\nrunis as close as you can get to the old-style disk service time.\nRemember that modern disks can queue more than one command at a time and canreturn them in a different order than they were issued, and it becomes clear that itcannot be the same thing. To calculate it from iostat output, divide the utilization\nby the total number of reads and writes as shown from the values in the followingexample:\nIn this case, U= 93% (0.93) and throughput X=r/s +w/s = 21.9 + 63.5 = 85.4. So,\nservice time S=U/X = 0.011 = 11 ms, and the reported response time R= 31.8 ms.\nThe queue length is reported as 2.7; this makes sense, since each request has to waitin the queue for several other requests to be serviced.%iostat -xn\n  r/s  w/s   kr/s   kw/s wait actv wsvc_t asvc_t  %w  %b device\n21.9 63.5 1159.1 2662.9 0.0 2.7 0.0 31.8 0 93 c3t15d0UwaitWait Queue Utilization iostat %w100Bwait\u00d7\nThires------------------------------ == =\nUrunRun Queue Utilization iostat %b100Brun\u00d7\nThires--------------------------- - == =\nSwaitAverage Wait Queue Service Time in MillisecondsBwait\nC100000\u00d7--------------------------- -Uwait\nX---------------- == =\nSrunAverage Run Queue Service Time in MillisecondsBrun\nC100000\u00d7--------------------------- -Urun\nX------------- == =\n\n102Chapter 5  Capacity EstimationWith the SE Toolkit, a modified version of iostat written in SE prints out the\nresponse time and the service time data using the format shown in FIGURE 5-16 .\nWe can get the number that iostat calls service time. It is defined as the queue\nlength (Q, shown by iostat with the headings \u201c wait \u201d and \u201c actv \u201d) divided by the\nthroughput, but it is actually the residence or response time and includes all queuingeffects, as shown in the following examples:\nTaking the values from our iostat example, R=Q\u00f7X= 2.7\u00f785.4 = 0.0316 = 31.6\nms, which is nearly the same as what iostat reports. The difference is 31.6 vs. 31.8\nand is due to rounding errors in the reported values, 2.7 and 85.4. With fullprecision, the result is identical, since this is how iostat calculates the response\ntime.%se siostat.se 10\n03:42:50 ------throughput------ -----wait queue----- ----active queue----disk r/s w/s Kr/s Kw/s qlen res_t svc_t %ut qlen res_t svc_t %utc0t2d0s0 0.0 0.2 0.0 1.2 0.00 0.02 0.02 0 0.00 22.87 22.87 003:43:00 ------throughput------ -----wait queue----- ----active queue----disk r/s w/s Kr/s Kw/s qlen res_t svc_t %ut qlen res_t svc_t %utc0t2d0s0 0.0 3.2 0.0 23.1 0.00 0.01 0.01 0 0.72 225.45 16.20 5\nFIGURE 5-16 SE-Based Rewrite of iostat to Show Service Time Correctly\nRwaitAverage Wait Queue Response Time iostat  wsvc_tQwait\nX---------------- == =\nRrunAverage Run Queue Response Time iostat asvc_tQrun\nX------------- == =\n\nCapacity Planning for Complex Disk Subsystems 103Another way to express response time is in terms of service time and utilization.\nThis method uses a theoretical model of response time, which assumes that as youapproach 100% utilization with a constant service time, the response time increasesto infinity, as shown below:\nTake our example again: R=S\u00f7(1 \u2013 U) = 0.011 \u00f7(1 \u2013 0.93) = 0.157 = 157 ms. This is\na lot more than the measured response time of 31.8 ms, so the disk is operatingbetter than the simple model predicts for high utilizations. This occurs because thedisk is much more complex than the model and is actively trying to optimize itself;the service time is not constant, and the incoming data is not as random as themodel. However, the model does provide the right characteristic and can be used asa simple way to do a worst-case analysis.\nComplex Resource Utilization Characteristics\nOne important characteristic of complex I/O subsystems is that the utilizationmeasurement can be confusing. When a simple system reaches 100% busy, it has alsoreached its maximum throughput because only one thing is being processed at atime in the I/O device. When the device being monitored is an NFS server, hardwareRAID disk subsystem, or striped volume, the situation is clearly much morecomplex. All of these can process many requests in parallel. A simple view of thiscomplexity that approximates the results reported by iostat for a Solstice\nDiskSuite MetaDisk stripe is shown in\nFIGURE 5-17 . The VxVM acts the same way, but\ndoes not report any data via iostat .1\n1. Veritas Corp. does not support the Solaris OE disk kstat information, but Veritas Corp. does report a subset\nof this information via the vxstat utility. Veritas Corp. does not publish a programming interface that is\nreadable by iostat -like tools.RwaitAverage Wait Queue Response Time Prediction iostat  wsvc_tSwait\n1Uwait\u2013----------------------- == =\nRrunAverage Run Queue Response Time Prediction iostat asvc_tSrun\n1Urun\u2013--------------------- == =\n\n104Chapter 5  Capacity EstimationFIGURE 5-17 Complex I/O Device Queue Model\nAs long as a single I/O is being serviced at all times, the utilization is reported by\niostat as 100%, which makes sense because it means that the pool of devices is\nalways busy doing something. However, there is enough capacity for additionalI/Os to be serviced in parallel. Compared to a simple device, the service time foreach I/O is the same, but the queue is being drained more quickly, so the averagequeue length and response time is less and the peak throughput is more. In effect,the load on each disk is divided by the number of disks, so the true utilization of thestriped disk volume is actually below 100%. The true utilization must be dividedacross the number of devices, M. The average utilization of the underlying drives is\nmuch lower than iostat reports. The combined throughput and service time is\nnormally called the service intensity rho, and it is like a utilization that can be over100%.\nAn alternative approach to this issue argues that if several requests can be serviced\nat once, then when a queuing model is used, the model works as if the averageservice time was reduced by the same factor, and utilization is still only up to 100%.\nSo, there is some confusion here over how to handle this case. Either the service time\nor the utilization must be divided by the number of underlying disks. Each of thesetwo cases is, in fact, valid in different situations.\nFor the first case, we assume that each disk I/O request is small, that it cannot be\nfurther subdivided, and that it will access a single disk. This is normally the casewhen the request size is smaller than the stripe interlace. Each request picks a disk atrandom, then provides the usual service time, so overall, the utilization of all thedisks is reduced by the factor M.\nFor the second case, we assume that each disk request is large\u2014ideally, a full-stripe\naccess, which is subdivided into many smaller requests that hit the entire set ofdisks. In this case, all the disks are kept busy, so the utilization of each individualI/OI/OI/O I/OI/OSCSI\nBus\nI/Os active in\ndevice\nI/Os being\nserviced byM devicesI/Os being\nsent to\ndeviceI/Os waiting in\ndevice driver\nS\nrunI/OI/O\nrun run runQ RU, ,SwaitQwait ,Rwait ,Uwait\n\nCapacity Planning for Complex Disk Subsystems 105disk is the same as the utilization of the entire set. However, the service time for\neach of the small requests is less than the service time of the big one, so we see areduced service time.\nCached Disk Subsystem Optimizations\nWhen a cache is introduced, a certain proportion of the accesses will hit the cache,but it is not often possible to obtain a measurement of this proportion. The cachealso allows many optimizations that combine small accesses into fewer largeraccesses to take place. Some of the common optimizations are:\n\u25a0Read prefetch clustering \u2013 A small read triggers a larger read into cache, then\nsubsequent nearby reads and writes are satisfied by the cache. Modified cacheblocks are written back asynchronously. The service time of the first read will beincreased somewhat, and the service time of subsequent read and write hits willbe very quick. A write-back may cause a delay to subsequent accesses.\n\u25a0Asynchronous write caching \u2013 If the cache is nonvolatile, it is safe to acknowledge\nthe write immediately and to write the data to disk later on. The service time ofthe write will always be fast as long as a cache block is available. Write-backs canbecome a bottleneck and cause cache block allocation delays that slow downwrite service times.\n\u25a0Write cancellation \u2013 Multiple writes to the same block often occur when file system\nmetadata is updating. When the block is cached, subsequent writes cancel out thefirst one and many updates can occur for each write-back to disk.\n\u25a0Write coalescing \u2013 Multiple writes to adjacent data are coalesced into a single\nlarger write. If the larger write exceeds the underlying disk block size, then thereis no need to read a large block because it modifies it by incorporating the smallwrite and writes the large block back.\nAll of these optimizations can be happening at the same time, and there is no\nstandard way for the disk subsystem to report what is going on in its cache. In mostcases, the disk and cache do not collect this kind of information at all. In the nextsection, to make sense of the data reported by iostat , we need some configuration\ninformation, a simplified model of the cached disk subsystem, and someassumptions about the workload mix.\nCached Disk Subsystems\nThere is a lot of read caching being done by the file system in main memory, but thefile system cannot do write caching safely for logs, metadata, or databasetransactions, and this is where performance bottlenecks often occur.\n\n106Chapter 5  Capacity EstimationThe disk cache can be inserted into the disk subsystem in two places: in the host\ncomputer or in the disk array. The Sun Prestoserve and Sun StorEdge\u2122 Fast WriteCache products add nonvolatile memory to the host computer and use it primarilyas a write cache. A very small amount of read-back from the cache may occur, butusually the source data is still available to be read from main memory longer than itis stored in cache. To simplify the model, we can assume that reads always go to theunderlying disks and writes always go to the write cache first. The capacity modelfor reads is the same as that of a simple striped disk setup; read prefetchoptimizations are done by the file system layer in both cases, so for raw disk, there isno prefetch or read cache in either case.\nThe model for writes is that writes are extremely fast until the write cache is\noverwhelmed. This overwhelming occurs when the underlying disks cannot copewith the rate at which data is being flushed from the cache. The slight CPU overheadfrom the extra copying of the data can be estimated from the maximum access rateof the NVRAM. The worst-case performance for writes is limited by the write-backrate and is about the same as for an uncached system. There can be a choice ofpolicies for the cache: it might cache only synchronous writes, or it could cache allwrites. It is best to cache all writes to start with; also, if the write-backs saturate,drop back to only caching synchronous writes.\nHost-Based Write Cache Model Interconnect\nParameters\nUltraSCSI runs at close to 40 MB/s for large transfers, but reads and writes compete\non the same bus. This interconnect is used for simple disk packs and the Sun D1000and A1000. The A3500 uses dual load-balanced UltraSCSI buses for a total of almost80 MB/s. The transfer latency for small transfers over UltraSCSI is around 0.6 ms.\nFibre Channel has the advantage that reads and writes use separate paths. The Fibre\nChannel Arbitrated Loop (FC-AL) standard can suffer from arbitration delays whenvery long loops are used, but small configurations are much faster than UltraSCSI.The older SPARCStorage\u2122 Array (SSA) runs at 25 MB/s in each direction, and atabout 1.0 ms minimum transfer latency. The current FC-AL standard runs at up to100 MB/s each way, with latency of about 0.4 ms.\nDual FC-AL loops have been measured at up to 180 MB/s, again with a latency of\nabout 0.4 ms. If both loops run over the same SBus interface, the SBus itself maybecome a bottleneck.\n\nEstimating Capacity for Complex Disk Subsystems 107Estimating Capacity for Complex Disk\nSubsystems\nThis section provides an overview of performance factors and capacity planning\nmodels for complex disk subsystems. It includes RAID5 and cached stripe examples.\nSingle Disks\nThe capacity model for a single disk is simple and has already been described. Theschematic in\nFIGURE 5-18 shows a single disk with incoming and outgoing requests.\nIn the following sections, we extend the schematic to show more complex diskconfigurations.\nFIGURE 5-18 Single Disk Schematic\nMirrored Disks\nFIGURE 5-19 is an example of mirrored disks.\nFIGURE 5-19 Mirrored Disks Schematic\nAll writes go to both disks when they are mirrored. However, there are several readpolicy alternatives, such as:\n\u25a0All reads from one side (referred to as \u201cpreferred\u201d)\n\n\n108Chapter 5  Capacity Estimation\u25a0Alternate from side to side (referred to as \u201cround-robin\u201d)\n\u25a0Split by block number to reduce seek\n\u25a0Read both, and use first to respond\nWe can make a simple capacity assumption if we assume that duplicated\ninterconnects are configured. Overall, the capacity of a mirrored pair of disks orvolumes is the same as the capacity unmirrored. In practice, writes take a small hitand reads a small benefit, but these are second-order effects that can be ignored forthe sake of simplicity.\nConcatenated and Fat Stripe Disks\nFIGURE 5-20 illustrates concatenated and fat stripe disks.\nFIGURE 5-20 Concatenated and Fat Stripe Disks Schematic\nWhen disks are concatenated or the request size is less than the interlace, individualrequests will only go to one disk. If the workload consists of single-threadedrequests, then you will only have the same capacity as a single disk, regardless ofhow many disks there are in the stripe. Single-threaded accesses are typical for logdisks.\nMultithreaded requests can make good use of this disk configuration, as long as the\nrequests are evenly distributed over concatenated disks or the stripe size is not toolarge. This is a common situation for database tables where small requests of 2 to 8kilobytes are used and it is impractical to attempt to make a stripe interlace thissmall.\nThis configuration has the same service time as one disk, since any one request\nalways hits a single disk. It has the throughput of Ndisks if more than Nthreads are\nevenly distributed.\nStriped Disk Accesses\nFIGURE 5-21 illustrates striped disk accesses.\n\n\nEstimating Capacity for Complex Disk Subsystems 109FIGURE 5-21 Striped Disk Accesses Schematic\nWhen the request size is more than the interlace size, it will be split over Ndisks for\nboth single- and multithreaded requests. Nis set by the request size divided by the\ninterlace. The throughput of Ndisks is obtained.\nThe reduced size of the request on each underlying stripe reduces service time for\nlarge transfers, but you need to wait for all disks to complete, and the slowestdominates. This kind of operation works efficiently when one-megabyte requests aresplit into a 128 K interlace. As the request size and interlace are reduced, the overallefficiency also reduces.\nRAID5 for Small Requests\nFIGURE 5-22 illustrates RAID5 for small requests.\nFIGURE 5-22 RAID5 for Small Requests Schematic\n\n\n110Chapter 5  Capacity EstimationThe operation of RAID5 is described in detail in Configuration and Capacity Planning\nfor Sun Servers by Brian Wong. In essence, a parity disk is maintained alongside a\nstripe to provide storage that is protected against failure, without the fullduplication overhead of mirroring. Writes must calculate parity that is rotatedthrough the disks. A write must do the following:\n1. Read the parity and old data blocks.2. Insert the small write into the block.3. Calculate new parity data.4. Write to a separate log that ensures consistent data integrity.5. Write data and parity back to the disk.The problem is that this sequence takes approximately triple the service time of a\nnormal write and one-third the throughput of a single disk. This situation may beencountered with software RAID5 implementations of DiskSuite and VxVM whenthe workload consists of many small writes.\nReading the RAID5 combination performs like a normal stripe, with a throughput of\nN-1 disks and service time of one.\nIf a disk has failed, the read must reconstruct the missing data by performing extra\nreads; overall, it is best to assume that degraded mode throughput is about the sameas one disk.\nRAID5 for Large Requests\nFIGURE 5-23 illustrates RAID5 for large requests.\nFIGURE 5-23 RAID5 for Large Requests Schematic\n\n\nEstimating Capacity for Complex Disk Subsystems 111When a write is large enough to include the full stripe width, there is no need to\nread any of the data, so all the writes and the parity can be written immediately.Capacity is similar to a stripe; it has similar read and write performance and thethroughput of N-1 disks. The service time for the transfer is reduced as the size of\neach write is reduced by the factor N-1.\nOverall, there is less load on the disk interconnect than it takes to keep a mirror\nsynchronized.\nIf a disk fails in degraded mode, throughput and service are similar since the entire\nstripe is read and written as a unit. Extra CPU time is used to regenerate the missingdata.\nCached RAID5\nFIGURE 5-24 illustrates cached RAID5.\nFIGURE 5-24 Cached RAID5 Schematic\nWhen a nonvolatile RAM cache is added to a RAID5 setup, it works much moreefficiently. There is no need for a recovery log disk, since the log that keeps track ofpartially complete operations can be kept in nonvolatile memory.\nThe cache provides fast service time for all writes. Interconnect transfer time is the\nonly component. The cache optimizes RAID5 operations because it allows all writes,whether large or small, to be converted to the optimal full-stripe operations.\nThe RAM cache is not normally large enough (in comparison to the size of the disks)\nto provide any benefit to read performance.\n\n\n112Chapter 5  Capacity EstimationCached Stripe\nFIGURE 5-25 illustrates cached stripe.\nFIGURE 5-25 Cached Stripe Schematic\nWrite-caching for stripes provides greatly reduced service time. It is worthwhile for\nsmall transfers, but large transfers should not be cached.\nFor the A3500, some tests confirm that 128 K is the crossover point at which the\nsame performance is obtained with and without the cache. For consistently largertransfers, better performance is obtained without a write cache. For smallertransfers, much better performance is obtained with the write cache.\nWrite caches also perform a few useful optimizations. Rewriting the same block over\nand over again cancels in the cache so that only one write needs to be made. Smallsequential writes also coalesce to form fewer larger writes.\nCapacity Model Measurements\nMeasurements are derived from iostat output and knowledge of the configuration\nof the disk subsystem.\nSome example output from iostat -x is shown below, with measurements derived\nfrom it:\n\u25a0Utilization U=%b\u00f7100 = 0.27\n\u25a0Throughput X=r\u00f7s+w\u00f7s= 41.8\n\u25a0Size K=Kr\u00f7s+Kw\u00f7s\u00f7X= 8.2K\n\u25a0Concurrency N=actv = 2.3 extended disk statistics\ndisk      r/s  w/s   Kr/s   Kw/s wait actv  svc_t  %w  %bsd9      33.1  8.7  271.4   71.3  0.0  2.3   15.8   0  27\n\n\nEstimating Capacity for Complex Disk Subsystems 113\u25a0Service time S=U\u00f7X= 6.5 ms\n\u25a0Response time R=svc_t = 15.8 ms\nThe configuration parameters for this system are:\n\u25a0Number of data disks M= 4 (ignore mirrors)\n\u25a0Do not include RAID5 parity disks\n\u25a0Stripe interlace I= 64K (use whole disk size if unstriped)\n\u25a0Max SCSI rate Brw = 40000K/s or FC-AL Br=Bw= 100000K/s\n\u25a0Max disk data rate D= 24500K/s\nDisk and Controller Capacity\nHere we check for a throughput-based bottleneck; we sum kr/s andkw/s by\ncontroller for all the disks on that controller.\n\u25a0Urw =(\u2211kr+\u2211kw)\u00f7D\n\u25a0Ur=\u2211kr\u00f7Br\n\u25a0Uw=\u2211kw\u00f7BworUrw =(\u2211kr+\u2211kw)\u00f7Brw\nPerformance Factor P\nTo simplify the comparison of different disk configurations for a specific workload, a\nworkload-specific performance factor can be calculated. The number of underlyingdisks that contribute to performance varies from 1 to stripe width M, depending\nupon configuration and workload.\nThe performance factor Pcan vary for read and write, but it does not include cache\neffects. Here is an example of calculating the performance factor P.\nFor small IOPS K\u2264I, that is, request ( K) is less than interlace ( I).\nIf we assume that requests are aligned with the stripe interlace, then for any request\nsmaller than the interlace, the request will hit only a single disk.\nWe use the notation of Mdisks, with a workload concurrency of N.% iostat -xn\n                              extended device statistics  r/s  w/s   kr/s   kw/s wait actv wsvc_t asvc_t   %w   %b device\n21.9 63.5 1159.1 2662.9 0.0 2.7 0.0 31.8 0 93 c3t15d0\n\n114Chapter 5  Capacity EstimationP=max(1,min(M,N)), that is, at least 1, and no more than MorN.\nFor large IOPS, K>I, that is, request ( K) is more than interlace ( I).\nThe number of disks involved in each request should always be rounded up to the\nnext integer. If we divide the size by the interlace, as soon as the result is over 1.0,we have to round it up to 2.0 because both disks will be involved. In general, wedivide KbyIand round the result up to the next whole number. We can use the\nnotation roundup (K\u00f7I) to show this.\nThere is an effective increase in concurrency at the disk level because each thread of\naccess in the workload is being subdivided by the disk interlace to cause morethreads of disk accesses. This division allows more performance to be obtained froma disk subsystem, up to the limit of the number of disks in the stripe.\nP=max(1,min(M,N\u00d7roundup (K\u00f7I)))\nExample 1\nYou can perform a capacity estimation from iostat with the example data shown\nbelow and for M= 6 disks, I= 128K interlace:\nkr= 53K read size, kw= 42K write size, N= 2.7 threads, and S= 0.93/(21.9 + 63.5) =\n10.9 ms service time.\nPerformance is limited by the number of threads to P= 2.7. Utilization reported by\niostat isUiostat = 0.93, but the effective utilization of the stripe is divided by P,s o\nUstripe =Uiostat\u00f7P= 0.34. Therefore, the stripe is 34% busy and has 66% spare\ncapacity for this workload.\nExample 2\nThis example is based on the same iostat data shown above, but for M= 6 and\nI= 16K interlace. The change in the interlace would change the data reported by\niostat in practice, so the results are not comparable with Example 1.\nSince the interlace is smaller than the read and write sizes, you can calculate that\nroundup (kr\u00f7I)=4a n d roundup (kw\u00f7I) = 3. This is then multiplied by N, the number\nof threads, and however you calculate it, the full stripe width of six disks will beutilized by this workload, so P= 6. The stripe utilization is divided down fromr/s w/s kr/s kw/s wait actv wsvc_t asvc_t %w %b device\n21.9 63.5 1159.1 2662.9 0.0 2.7 0.0 31.8 0 93 c3t15d0\n\nEstimating Capacity for Complex Disk Subsystems 115Uiostat\u00f7P= 0.93\u00f76t o Ustripe = 0.16. The stripe is 16% busy and has 84% spare\ncapacity for this workload. The average service time is effectivelyS= 0.16\u00f7(21.9 + 63.5) = 1.9 ms.\nCache Performance Impact Factors\nThe effect of cache on throughput is hard to model because there is no\ninstrumentation to indicate the occurrence of clustering and write cancellationimprovements. There is also a small overhead from copying data to and from thecache on its way to the disk. You should make the pessimistic assumption thatthroughput is unchanged by the cache.\nThe primary benefit of cache is fast response time. The benefit can be summarized\nby two cache efficiency factors: Eand H.\nEis defined as the service time speed-up due to cache for reads and writes. E= 1.0 is\nthe same as uncached. E=S\ndisk\u00f7Scache, and we can also assume that Eread=Ewrite is\na close enough approximation to prevent the need to calculate reads and writesseparately.\nHis defined as the cache hit rate. H= 0.0 for uncached requests, and H= 1.0 (i.e.,\n100%) for fully cached requests. Hvaries greatly with workload mix and intensity\nchanges. It must be calculated separately for reads and writes, since H\nreadand Hwrite\nhave very different values.\nService Time and Cache Hit Rate\nYou can infer the cache hit rate from knowing disk and cache service times.\nThe measured service time calculated from iostat data is\nSmeasured =Sdisk(1 \u2013 H)+H\u00d7Sdisk\u00f7E; that is, the slow proportion of cache misses\nplus the fast proportion of cache hits. We can turn this calculation around, usingmeasurements or estimates of the values of Eand S\ndiskto get an estimate of the cache\nhit rate:\nHestimated =( 1\u2013 Smeasured /Sdisk)\u00f7(1 \u2013 1/ E)\nThis calculation is sensitive to errors and variations in the values of Sdiskand E. If the\nresults of the estimate are outside the permissible range of H (0.0 to 1.0), then youneed to make sure that S\ndiskis in particular big enough.\nSdiskmust be the worst-possible-case disk service time.\nSdisk\u00f7Emust be the best-possible-case cache hit service time.\n\n116Chapter 5  Capacity EstimationSolid-state disks have H= 1.0 for both reads and writes as data is engulfed by the\ncache. Other disk types have varying values for H, but characteristic values for E.\nSome example cache values are:\n\u25a0Cached disk array or solid-state disk with UltraSCSI interface, E\u224810; with\nFC-AL interface, E\u224815\n\u25a0Sun StorEdge Fast Write Cache product uses mirrored 32 MB NVRAM cards, no\nread cache, E\u2248500\n\u25a0File system read cache, that is, access to a data file already in memory, no device\ndriver involved, E\u22485000\nMain memory provides the best cache for reads, and NVRAM provides the best\ncache for writes. The only disadvantage of the NVRAM cache is that in clustered,highly available disk subsystems, it cannot be used because it is internal to eachsystem.\nCaveats\nThis discussion has concentrated on first-order effects. Many simplifyingassumptions have been made, so do not depend on this model for fine accuracy. Youshould use this model to get into the right \u201cballpark,\u201d so you can configure yourdisk subsystem appropriately for your workload.\nSummary\nThis chapter focused on CPU, memory, disk capacity estimation, and the varyingscalability characteristics that create complicated disk subsystems. In addition, thischapter gave examples of system resource modelling and also looked at some of theproblems of capacity planning for CPU and disk subsystems.\nThe next chapter concentrates on the importance of observability requirements for\ninformation collection and presentation to three different audiences.\n\n117CHAPTER6\nObservability\nObservability is the first requirement for performance management and capacity\nplanning. The performance characteristics need to be presented in a way that makessense to those who are looking at them, so they can observe the behavior of thesystems and understand what is happening. Three different viewpoints need to beprovided for different audiences:\n\u25a0Operations\n\u25a0Management\n\u25a0Engineering\nEach audience requires significantly different information. This information can be\nobtained from disparate output of the same performance database or withcompletely separate tools. This chapter looks in detail at gathering, organizing, andpresenting system performance data to these three audiences.\nOperations Viewpoint\nThe operations room works in the immediate time frame, using a real-time display,updated every few seconds. Alert-based monitoring should be combined withautomatic high-level problem diagnosis if possible. However, this level ofsophistication is rare. Simple high-level graphs and views of current data will beneeded.\nManagement Viewpoint\nManagement will never have time to read a voluminous report. A single-page dailysummary of status and problems is more appropriate. The summary should contain:\n\u25a0Business-oriented metrics\n\u25a0Future scenario planning\n\u25a0Concise report with dashboard-style status indicators\n\n118Chapter 6  ObservabilityMarketing and management input is needed to define the scenarios. These form a\ngrid, with things that change the load level down the side and things that change thecapacity across the top. It is then easy to see which tuning and upgrade projectsmust be implemented to cope with the increased load of the next marketingcampaign.\nEngineering Viewpoint\nEngineers need to see large volumes of detailed data at several different time scales.The data is used as input to tuning, reconfiguring, and developing future products,and aids in low-level problem diagnosis.\nYou only need to generate and store large volumes of raw data on systems that are\nbeing tuned. Detailed reports with drill-down and correlation analysis are needed,and many tools implement this kind of functionality very well. Engineers know howto build tools, and they tend to build the tools that they need themselves, so goodengineering-oriented features are common.\nExample Scenarios\nIn the following sections, techniques and examples for implementing theseviewpoints are presented. The operations viewpoint is demonstrated with the SunManagement Center (SunMC, a.k.a. Sun Enterprise SyMON\u2122 2.1). The engineeringviewpoint is implemented with SunMC and the SE toolkit to collect additionaldetailed data. The management viewpoint is implemented as a spreadsheet.\nOperations Viewpoint Implementation\nThis section describes an implementation of operations management that is genericin the techniques being used, but specific in the tools used to implement it in aprimarily Sun based environment.\nThe steps covered in this section illustrate how to use a system monitor to automate\nthe process of tracking multiple systems for unusual or problematic conditions.\n1. Enable rules.\nMany monitoring tools are shipped with their monitoring capability unconfigured or\npartially enabled, so the first step is to enable all relevant rules. If you have BMCPatrol (\nwww .bmc.com), load Knowledge Modules. With SunMC, basic rules are\nenabled, but you can load additional health monitoring tools.\n\nOperations Viewpoint Implementation 1192. Set up rule thresholds.\nThe default rules may be set up for a small system, in which case, they can generate\nfalse alarms on a much larger server. They may also be set too high, so a problemcould occur without firing the alarm. For all the rules that you have, you shouldmake a list of each rule, listing its threshold values and the typical and maximummetric values you see in normal operation. If you can identify the values for thesemetrics seen during problem situations, then you can set alert thresholds correctly toindicate transitions from normal (green) to high (yellow) to problematic (red) levels.If you have never seen a problem with a particular metric, then set its threshold alittle higher than the highest value you have ever seen. This way, you \u201csurround\u201dthe systems being monitored with a multidimensional behavior envelope. If thebehavior goes beyond that envelope in any dimension, then an alarm tells you thatsomething abnormal is happening. Document and save the customized rules andthresholds so they can be replicated on similar systems and are not lost duringsystem upgrades.\n3. Monitor and reset alarms.\nWhen an alarm condition occurs, the alarm is useless if no one notices or responds.\nUsually, the monitoring software will signal that a problem state has occurred, andyou can acknowledge the alarm and investigate the source of the problem. Alarmsthat don\u2019t appear to be useful can be disabled, and new alarms can be created towatch metrics that are more relevant to your workload. The alarm log should bearchived. The number of alarms that occur at each of the problematic levels (e.g.,yellow and red) should be collected daily so it can be reported to management andplotted as a trend over time.\n4. Classify problems.\nClassify downtime, alarms, and problems that occur which do not cause an alarm in\none of the following categories, and report the number of occurrences per day:\n\u25a0Problem with no corresponding alarm\n\u25a0Planned downtime\n\u25a0Planned downtime overrun\n\u25a0System failure downtime\n\u25a0Alarm\n\u25a0Warning\n\u25a0Persistent known problem carried over from previous day\n5. Perform root cause analysis.\nFor each problem that was counted in Step 4, try to track down the cause and record\nwhat you find. If the number of problems analyzed each day matches the number ofproblems reported, then you are keeping on top of the situation very well. Try tofind time to modify the alarm rules to reduce the number of false alarms, warnings,and problems that did not cause a corresponding alarm, such as:\n\n120Chapter 6  Observability\u25a0False alarms\n\u25a0False warnings\n\u25a0Alarm rule modifications, for example, to prevent false alarms\n\u25a0Root cause identifications\n\u25a0Root cause fixed notifications\n\u25a0No trouble found notifications\nSummarize the problem and analysis counts and types into a daily management\nreport.\nImplementing with Sun Management Center\nSunMC is used in this example because it is becoming increasingly important as aprimary platform for the management and monitoring of Sun systems. Many third-party tools could be used in this role. The primary advantage of these third-partytools is also their primary disadvantage. They are not Sun specific and can managemany kinds of systems from several vendors. But because they are not Sun specific,they do not provide anything comparable to the detailed, platform-specific, activemanagement and diagnosis capabilities of SunMC. The following list summarizesthe options and capabilities of SunMC:\n\u25a0SunMC has third-party vendor support for managing Windows NT-basedsystems, and relational databases for performing generic system administrationfunctions. Contact Halcyon Inc. (\nwww .halcyon.com) for more details.\n\u25a0Interoperation with CA Unicenter, BMC Patrol, and HP IT Operations (amongothers) allows SunMC to integrate into existing multivendor environments.\n\u25a0The recent addition of a software development kit (SDK) allows anyone todevelop extended agent modules and user interface components.\n\u25a0SunMC is secure, using public key authentication by default, and allows fullencryption as a higher overhead option. This feature allows a remoteadministrator to modify the system configuration safely. Administrative roledefinitions and access control lists provide flexible security control.\n\u25a0Full system configuration information is available through SunMC. Thiscapability is not at all trivial to implement, but no other tool can provide anythingcomparable. Both logical and physical views using pictures of components areavailable. A recent addition is support for display and management of the SunEnterprise 10000 servers System Service Processor (SSP) and Dynamic SystemDomains.\n\u25a0Hardware products and software to manage the hardware are developed inparallel at Sun; many new hardware and software products will use SunMC astheir primary management console or be launched from SunMC.\n\nOperations Viewpoint Implementation 121\u25a0Diagnostic tools such as Sun\u2019s Configuration Service Tracker (CST) use SunMC as\ntheir user interface console.\n\u25a0Since SunMC is based on the SNMP protocol, it can integrate closely withnetwork management products and tools as a peer, rather than just forwardingSNMP traps. SunMC\u2019s SNMP version 2 with user security implementation ismore efficient at bulk transfer of data and adds security to the common SNMPstandard.\nAlert Monitoring with SunMC\nSunMC can be used to monitor systems using complex rule-based alerts to diagnoseproblems. In this section, screenshots illustrate how to configure and use SunMC toperform basic monitoring operations that are useful for operations management. Thescreenshots in these examples are taken from SunMC 2.1.1, which is a minor updateto Sun Enterprise SyMON 2.0 software with a new name and more features.\nThe SunMC Health Monitor\nSunMC includes a system health monitoring module that can be used by operationsto simply see if a system has enough resources to run comfortably. For example, ifCPU state is reported as red,then either less work or more CPU power may be\nneeded on that system. Similarly, if a memory rule reports red,then the system may\nneed more memory.\nThe SunMC health monitor is based on a set of complex rule objects. The health\nmonitor is not enabled by default when SunMC is first installed because only thebasic modules are loaded into the agent. The health monitor rules are based on thoseimplemented by Adrian Cockcroft in the SE Toolkit script virtual_adrian.se\nand described by him in Sun Performance and Tuning: Java and the Internet .\nA simple rule can be placed on any single SunMC metric to monitor its value.\nAdditional rule objects implement more complex rules that refer to several metricsor perform advanced processing on the data.\n\n122Chapter 6  ObservabilityTo load the health monitor module, start SunMC with the default administrative\ndomain, select the system, pop up a menu, then select the Load Module option fromthe menu, as shown in\nFIGURE 6-1 .\nFIGURE 6-1 SunMC Console\n\n\nOperations Viewpoint Implementation 123Next, scroll down and choose the Health Monitor module; it may already be loaded\nif SunMC has been preconfigured. Otherwise, select it and click the OK button (see\nFIGURE 6-2 ).\nFIGURE 6-2 Load Health Monitor Module\n\n\n124Chapter 6  ObservabilityNow, any health monitor alerts will be logged for this system. You could drill down\nto the subsystem that caused an alert, but we don\u2019t expect any health monitor alertsyet, and there is already an unrelated disk space alert on this system. So instead,select the system and open the detailed view. The Details menu option was seenpreviously when we opened the Load Module window. When it is selected, a secondwindow opens that is specific to the system being monitored (see\nFIGURE 6-3 ).\nFIGURE 6-3 Host Details Window\nThe Browser tab of the host Details window shows the modules that are loaded.Under Local Applications (which opens if you click the bullet next to it), you willfind the Health Monitor module, and inside that you find the eight rules that areimplemented to monitor several system components. Each rule shows a fewvariables; the RAM rule that is displayed in\nFIGURE 6-3 shows that the current scan\n\n\nOperations Viewpoint Implementation 125rate is zero, so the rule value is a white box. This box would go red, and the red state\nwould propagate up the hierarchy if the ratio of scan rate to handspread went toohigh and the page residence time dropped below the preset threshold.\nYou can view and edit rule attributes and thresholds by popping up a menu over a\nrule value. The way to use these rules is to increase the thresholds until there are nowarnings in normal use on a system that is performing well; then, as the loadincreases over time, you will start to get warnings that report which subsystem islikely to be the bottleneck. If you have a system that is not performing well to startwith, then these rules should help you eliminate some problem areas and suggestwhich subsystems to concentrate on.\nThe browser mode can be used to explore all the operating system measurements\nsupplied by the kernel reader for this system, including CPU usage, paging rates,and disk utilization.\nHandling Alarms in SunMC\nSunMC can monitor hundreds of systems from a single console. Systems can begrouped into various hierarchies; for example, by subnet, application, ordepartment. SunMC gives each group a \u201ccloud\u201d icon; or, you can load a backgroundimage to form a map or building plan and then position the icons on the map. Whena problem occurs, the status propagates up the hierarchy, causing the clouds tochange to the appropriate color, so you can drill down to an individual system. TheSunMC console supports multiple administrative domains that can be used bydifferent types of users to see different views of the systems being monitored.\nWhen a simple rule or one of the health monitoring rules generates an alarm, it is\nlogged by SunMC. At the domain-level console, the worst alarm state for eachsystem being monitored is counted. This means that with only one system beingmonitored, only one alarm will be indicated. In our example, it is in the yellow state.\n\n126Chapter 6  ObservabilityIf you click the yellow indicator, shown in FIGURE 6-4 with a \u20181\u2019 next to it, a new\nwindow opens that shows all the alarms for all the systems in this domain.\nFIGURE 6-4 SunMC Console\nClick Here\n\nOperations Viewpoint Implementation 127FIGURE 6-5 displays only the systems or other network components that are in the\nyellow (warning) state. In this case, we can see that available swap space isbeginning to get low. This very useful form of filtering allows you to handle a floodof alarms from a large number of systems by selecting a domain and dealing withthe critical problems for all systems in that domain first.\nFIGURE 6-5 Domain Status Details Window\n\n\n128Chapter 6  ObservabilityIf you now either double-click the alarm or select the alarm and click the Details...\nbutton, the Details window for that system opens with its Alarms tab selected, asshown in\nFIGURE 6-6 .\nFIGURE 6-6 Alarms Details Window\n\n\nOperations Viewpoint Implementation 129The next step is to select one or all of the alarms and acknowledge them by clicking\nthe Acknowledge button. It is best to select them all and acknowledge them all atonce. It takes some time to perform the acknowledgment, since it involvescommunicating all the way back to the agent on the server being monitored. Once analarm is acknowledged, a check mark appears next to it, as shown in\nFIGURE 6-7 .\nFIGURE 6-7 Acknowledged Alarms\nIf the Alarms Details window is now closed and you return to the Domain StatusDetails window, the latter may not have changed. Click the Refresh Now button andthe alarm entry will go away. You can now close this window. Look back at theDomain Console and notice that the server no longer has a yellow marker on it andthe Domain Status Summary is all zeroes.\n\n\n130Chapter 6  ObservabilityThe Sun BluePrints book Resource Management shows how to separate and monitor\nmultiple workloads with SunMC. Use this capability to track application processesand define alarms that will tell you when something goes wrong\u2014for example,when a process goes away or a workload starts to use too much CPU.\nKey Performance Indicator Plots\nOperations must have access to displays that show daily data for the maincomponents of key systems. These are a great help with problem diagnosis. A simpleapproach is to log data to a table, import it into a spreadsheet, and plot the keyindicators. It is also possible to automatically generate plots on a Web page by usingfree tools such as MRTG (Multi Router Traffic Grapher,\nwww .mrtg.or g) and Orca\n(www .orca.net). The disadvantage of these methods is that they do not operate in\nreal time and need to be refreshed manually. Almost all commercial performancetools supply data in real time for display in ways that are suitable for an operationsmonitoring display.\nAn example of a useful display is a spreadsheet that reads in data logged to a file\nusing the SE toolkit. This spreadsheet is available for download at\nhttp://www .sun.com/blueprints/tools. In this spreadsheet, plots cover the main\ncomponents of a single system, such as CPU, disk, network, and memory. Thefollowing plots are displayed in the next five figures:\n\u25a0FIGURE 6-8 and FIGURE 6-9 show CPU utilization, the balance of user CPU time to\nsystem CPU time, and the load average (number of runnable jobs) relative to thenumber of CPUs configured and online.\n\u25a0FIGURE 6-10 shows disk utilization for the busiest disk in the whole system and the\naverage utilization over all the disks in the system. This graph shows howskewed the load is, and highlights the most likely disk bottleneck. Read and writethroughput in KB/s shows the data transfer characteristics.\n\u25a0FIGURE 6-11 displays network throughput in KB/s. This number can be obtained\neither by monitoring the TCP stack or by monitoring the network interfaces. Mostinterfaces report byte-level counters these days, but many tools just read packet-level information, which is far less useful, given the wide range of possible packetsizes.\n\u25a0FIGURE 6-12 displays memory and swap space; they can be plotted together, but\nneed to be interpreted differently. Swap space must always be available so thatprocesses can start and grow, because if a process runs out, the system will haveproblems. Main memory is managed by reclaiming itself once it reaches a lowthreshold, so it will tend to hover around that threshold. The page residence timeis another useful measure, as explained here.\n\nOperations Viewpoint Implementation 131FIGURE 6-8 CPU User and System Time for a Day\nFIGURE 6-9 Five-Minute Load Average and Number of CPUs Online0102030405060708090100\n\n132Chapter 6  ObservabilityFIGURE 6-10 Disk Utilization \u2013 Busiest Disk and Average Over All Disks\nFIGURE 6-11 Disk Throughput Read and Write KB/s010203040506070809000\n\nOperations Viewpoint Implementation 133FIGURE 6-12 Network Throughput Over a Day\nThe network plot shows a few spikes of high throughput because of file transfers\ntaking place on a local fast network, but its overall shape follows the load averagefairly closely in this case. The characteristics of these plots show that this system isactive all the time, and it has a load shape that is typical of many Internet sites. Thesystem is quiet overnight, gets busy and plateaus during the day, then peaks in theearly evening. The CPU and disk plots also show that there is extra system CPU timeduring the night and a higher disk data rate associated with an online backup takingplace. The axis scales have been removed from some of these plots, partly to obscurethe source of the example data and because the absolute values are less importantthan the changes in shape from day to day.\nThe memory plots below show a regular batch job creating and deleting a large file,\nand memory and swap space being regularly consumed and freed (see\nFIGURE 6-13 ).\nThere are a few times when the page residence time drops to a low level during thebackup period. This value is calculated by dividing the page scan rate into a systemparameter called the handspread pages. Since the scan rate can be zero, a divide byzero is caught, and the residence time is clamped to 600 seconds (see\nFIGURE 6-14 ).\nSustained residence times of 30 seconds or less can be a problem with older SolarisOE releases. An optional setting for Solaris 7 OE (which is also included in thekernel patch for Solaris 2.6 and 2.5.1 OEs) turns on priority paging. When enabled,much higher page scan rates can be sustained without problems because the filecache pages are reclaimed before executable code, stack, and heap pages. The ruleshould be modified to have five seconds as the problem level for residence time\n\n134Chapter 6  Observabilitywhen \u201c priority_paging = 1 \u201d is set in the kernel. Solaris 8 OE uses a completely\ndifferent algorithm, which allows the file cache to be included in the free memoryvalue, and the absolute value of free memory to be used directly to see how muchmemory capacity is available for use. In Solaris 8 OE, scanning only occurs when thesystem is extremely short of memory.\nFIGURE 6-13 Memory and Swap Usage\nFIGURE 6-14 Memory Demand Viewed as Page Residence Time\n\nManagement Viewpoint Implementation 135Operations Viewpoint Implementations Summary\nOperations monitoring concentrates on real-time, alert-based problem diagnosis. It\ncan be implemented with a variety of tools and displays along the lines describedabove. Its requirements are quite different from those of capacity planners andmanagement reports, although it feeds information to both of them.\nManagement Viewpoint Implementation\nIn this section, we describe a weekly report for management. It is constrained to asingle sheet of paper and provides \u201cat a glance\u201d status information for futurescenarios, as well as an overview of the previous week. The report must becustomized to your own situation, so we describe the construction and reasoningbehind the report in detail.\nFor this discussion, managers are assumed to have the following characteristics:\n\u25a0They assign resources that can affect project schedules.\n\u25a0They approve capital equipment purchases.\n\u25a0They have a short attention span for technical issues, but need to be able to\nsummarize any externally visible problems to analysts and investors.\n\u25a0They like to be informed about what is happening, but it is more important to tellthem when they need to take action on and what options they have.\n\u25a0They are seemingly incapable of turning over a piece of paper or scrolling down aWeb page to look through a whole report.\n\u25a0They are quite likely to glance at a single sheet of colorful paper, especially if ithas lots of red on it.\n\n136Chapter 6  ObservabilityThe example report shown in FIGURE 6-15 is customized for use by a large Internet\ne-commerce site where the primary bottleneck is a large back-end database server.For many consumer-oriented sites, Monday has the highest load of the week, so thereport is designed to be delivered each Tuesday. It covers from Monday of theprevious week to the Monday before the report and predicts whether there isenough site capacity to survive the load expected for the following Monday. Thereport is in four sections:\n1. A short text summary of issues and noteworthy events2. A colored dashboard table showing the status of future scenarios3. A colored table showing the daily status over the last week and a chart showing\nthe level of activity over the last week\n4. An excerpt from an external site testing service, such as Keynote, that measures\nsite availability and performance\n\nManagement Viewpoint Implementation 137FIGURE 6-15 Example Management Status Report\nThe site ran fairly smoothly the past week. A new record bandwidth of 253.8 Mbit/s was seen on Monday\nnight. User-visible problems caused by nightly backup overrunning into daytime operation on 14, 17, and\n18th. Currently, no con\ufb01guration changes may go into the site without \ufb01rst being approved by the teamworking on this issue. The schedule for hardware upgrade on 12-May is at risk due to component deliveryWeekly Status Report: April 18, 2000\nLoad and Capacity Increase Scenarios and Primary Bottleneck Utilization\nLast Week Status, Problem Severity and Count Overview, and Graph\nA V AILABILITY  (Availibility Rating: * * * * *) and PERFORMANCE  (Performance Rating:  * * * * *)lead times\n\n138Chapter 6  ObservabilityWeekly Summary\nSummarize the week\u2019s activity into four or five lines of text (see FIGURE 6-16 ).\nFIGURE 6-16 Management Report: Weekly Summary Section\nThis summary indicates that user-visible problems over the past week were caused\nby the nightly online backup taking too long and spilling over into the high-loaddaytime period, where it adversely affected user response times. It also notes apossible schedule slip for the next site upgrade, so that senior managers can expectto be involved in contingency planning and supplier meetings to firm up theschedule.\nScenario Planning\nThe entire capacity planning process is described in more detail in Chapter 4,\u201cScenario Planning.\u201d The output from the scenario planning process includes aschedule of upgrade events that increase site capacity and a schedule of expectedincreases in load. The important messages to convey to management are whetherthere will be enough capacity to handle the load in the future, and how they shouldmanage resources and schedules to keep the site running.\nThe weekly report is delivered on Tuesday so that there is time to make and test any\nchanges needed to cope with the expected peak on the following Monday. The entirecapacity plan is recalculated once a week, based on updated information aboutschedules and expected performance improvements. Taking new current data as abaseline, the recalculation also updates the first column with the projected futuredates.\nThe capacity plan is based on a simple primary bottleneck scenario. The utilization is\nshown as a percentage for each scenario. Scenarios that are at risk due to scheduleslips are colored yellow; scenarios that will have enough capacity are colored green;and scenarios that are over 90% busy are colored red. The load increases rapidlywith time and takes into account seasonality effects and a marketing campaign thatis labelled \u201cMktg Push.\u201d The load starts at 71%, and actually decreases slightly to65% the following week due to a seasonal lull. In five weeks time on 29-May, theload will be 92% with the current site configuration. A hardware upgrade isscheduled for 12-May and the effect of this upgrade is shown in the second column.The site ran fairly smoothly the past week. A new record bandwidth of 253.8 Mbit/s was seen on Monday\nnight. User-visible problems caused by nightly backup overrunning into daytime operation on 14, 17, and18th. Currently, no con\ufb01guration changes may go into the site without \ufb01rst being approved by the teamworking on this issue. The schedule for hardware upgrade on 12-May is at risk due to component deliveryleadtimes.Weekly Status Report: April 18, 2000\n\nManagement Viewpoint Implementation 139The upgrade is estimated to give a 22% performance increase, which (if\nimplemented the day of the report [Tues., 18-Apr]) would reduce the 71% utilizationto 58%. If the hardware upgrade was implemented the following week, it wouldreduce 65% to 53%, and in five weeks, it would reduce the 92% utilization to 75%.However, the schedule for this upgrade is at risk, and if it slips from 12-May back tonear 29-May, there will not be enough capacity to cope with the load, hence theyellow status indicator. Similarly, an application code for an upgrade to Version 2.6,(\u201cApp V2.6\u201d) scheduled on 20-May is expected to give a 36% performance boost.This must be scheduled before the \u201cMktg Push\u201d on 16-Jun, and it is currently ontrack.\nTo give a target for the long term, the last row estimates the capacity requirements in\na year\u2019s time. This helps encourage the formation of large projects that have a longlead time but a significant impact, such as the \u201cApp V3.0\u201d improvement of 80% inthe last column (see\nFIGURE 6-17 ). This project might involve splitting functionality\nover several systems to reduce the amount of work being routed to the primarybottleneck.\nFIGURE 6-17 Management Report: Scenario Planning Summary\n\n\n140Chapter 6  ObservabilityWeekly Problem Summary\nThis section gives a view of events over the previous week or so (see FIGURE 6-18 ). It\nincludes both problem counts and a graph of activity from Monday of the previousweek through to about midday the next Tuesday.\nFIGURE 6-18 Management Report: Weekly Problem Summary\nThe first row gives the total number of problems that were unresolved at the end ofeach day. It also color-codes the overall status of the day according to the severity ofproblems that occurred at any time during the day. In this case, three days areshown yellow because backups overran and affected daytime operation.\nThe second row shows how many problems were resolved during a day and how\nmany new problems were registered. If staff are being overwhelmed with so manyproblems that the problems are not being analyzed and resolved, then the statuswould turn yellow or red in the second row, and the problem count would keepincreasing in the first row. Problems can occur at a relatively constant ratethroughout the week on average, but analysis and resolution tend to take placeduring the week when all the staff are available. This means that the total problemcount may tend to accumulate over the weekend and drop back quickly at the startof the week.\nThe graph is aligned to the daily problem summary. This particular graph was\nobtained from a freely available tool called MRTG, which is monitoring the total\nInternet-connected bandwidth of this site. The bandwidth drops to near zero justafter midnight on Sunday; this drop corresponds to a scheduled maintenancedowntime. Looking at the load pattern, we see this is not a good time to take the sitedown. It might be better to schedule downtime at a different time, for example,\n\n\nManagement Viewpoint Implementation 141Thursday night/Friday morning. For this site, that would correspond to the lightest\nload on a Friday and would have the least business impact if problems occurredduring or after the maintenance period. There is also a good system administrationprinciple of not changing anything during Friday, so that you don\u2019t have to work theweekend sorting out any problems!\nExternal Monitoring Summary\nSeveral companies provide external monitoring services for Web sites; the bestknown is Keynote Corp (\nwww .keynote.com). Keynote uses connections from many\nparts of the Internet so that they cover a sample of the end-user population. Theycontact the site being tested at regular intervals, count how many times they get aresponse, and measure the response time. Two kinds of Web pages on a site can betested: a standardized page that allows comparisons between Web sites, and a site-specific page that tests the functionality of the actual Web site and any back-endsystems that may be involved. Other vendors in this market are Mercury Interactive(\nwww .mer cury .com) and Envive Corp ( www .envive.com). In addition to measuring\navailability and performance in normal operation, Mercury and Envive also providea load testing service that can be used to stress-test sites before they go intoproduction or to apply a stress test during low-load periods.\nA full Keynote report covers the performance of many components of the Web site,\nwhich is measured from many parts of the Internet. For the management report, asingle measurement is sufficient, as shown in\nFIGURE 6-19 .\nFIGURE 6-19 Management Report: Site Availability and Performance Summary\nThis report was constructed to illustrate the principles involved in this example.\nManagement reports need to be customized to the exact situation at hand.\n\n\n142Chapter 6  ObservabilityEngineering Viewpoint Implementation\nEngineering groups include capacity planning, application development, system\nadministration, database administration, and vendor service and presales systemsengineers. They are responsible for recommending upgrades, system performancetuning, application tuning, and problem diagnosis. Hence, large amounts of detailedperformance data need to be collected and analyzed.\nMost performance tools collect a large number of system statistics, but there are\nsome common areas of weakness. Commercial tools try to support several versionsof UNIX systems in a generic manner and tend to skip anything that is specific toone particular implementation. For a full and detailed picture of what is happeningon a Solaris OE system, additional information generally needs to be collected. TheSunMC product provides data on some of the Sun specific capabilities; in particular,it has very detailed configuration information. However, it does not include allpossible data sources and currently does not include a performance database forlong-term historical data storage. Some of the additional data that should becollected is listed below:\n\u25a0sar data\nThe standard system utility sar is already set up through cron to collect data\nat regular intervals; it just needs to be enabled. The sar data files are collected\nin the directory /var/adm/sa and should be archived to a long-term storage\nlocation because the default collection scripts overwrite data after one month.Thesar data file contains disk information as well as many of the system and\nvirtual memory counters accumulated over all CPUs.\n\u25a0TCP network statistics\nThis measurement should include connection rates, TCP byte-level throughput,\nand retransmission rates. The raw counters can be obtained from netstat -s\noutput or through SNMP . The counters must be read and compared frequentlyenough to avoid counter values wrapping around.\n\u25a0Detailed network interface statistics\nThe default statistics for each network interface include only packet and error\ncounters and a collision rate. Since most modern networks are switched,collisions rarely occur. Additionally, interfaces such as the Sun 100 Mb Ethernethme andqfe and the Gigabit Ethernet gecollect many detailed metrics that\nare not reported by the netstat utility. It is important to obtain byte count\nthroughput metrics for capacity planning; but in addition, there are bufferoverflow counters that show when packets are being dropped because of aninput overrun.\n\nEngineering Viewpoint Implementation 143\u25a0Volume manager information\nThis information should show the throughput for each volume. The Sun\nSolstice DiskSuite product provides these volumes with the normal iostat\ntool and kstat mechanisms, but some tools do not understand or report the\nmetadevice (md) entries correctly. The VxVM requires that the vxstat\ncommand be run separately for each disk group that is configured on thesystem.\n\u25a0Web server access logs\nThese logs provide a useful source of performance information. Sometimes the\nlog can be configured to measure connection times so that response time ismeasured directly. Operation counts for the HTTP protocol and counts of thenumber of times particular cgi-bin scripts are run can usefully characterize theworkload mix for a site.\n\u25a0Detailed CPU activity metrics\nThese metrics are only reported by the per-CPU mpstat tool. Particularly,\ninterrupt counts ( intr ), cross-calls ( xcal ), and mutex stalls ( smtx ) should be\ncollected.\nThe overhead of collecting data from the system utilities such as sar,iostat ,\nnetstat ,mpstat , andvxstat is low as long as the collection interval is set to a\nreasonable level, such as 30 seconds, and data is logged directly to disk files. Thebest organization on disk is to build a collection script that maintains a dailydirectory that contains data files for each tool for each hour, for example:\nIt is easy for this data to add up to many megabytes per day, so make sure there is\nenough space, and archive at regular intervals.\nSE Orcollator Logs\nOne approach to data collection is to use the SE toolkit to build a custom datacollector that includes whatever data you like and stores it in any format that youfind convenient. The best starting point is the orcollator.se script, which is in\nuse at many Sun based Internet sites. This script was developed from AdrianCockcroft\u2019s original percollator.se script by Blair Zajac while he was working at\nthe GeoCities Web site. Blair also developed the Orca Web-based data plotting tool.The disadvantage of using the SE toolkit is that it is unsupported free code. Youmust devote some engineering resources to maintaining the SE scripts yourself, butsince you probably already have shell scripts that collect performance data, someadditional scripts may not be a problem./var/adm/data/16-04-2000/10:00_iostat\n\n144Chapter 6  ObservabilitySAS Data Import and Analysis\nSAS ( www .sas.com) is a general-purpose statistics package that is widely used to\nanalyze and report computer performance data in the mainframe world. It isparticularly useful as a central place to consolidate data from many other tools. Twomain add-on packages customize SAS for use as a capacity planning tool: the SAS ITService Vision (SAS/ITSV) package, which was originally called SAS/CPE; andMXG (\nwww .mxg.com) by Barry Merrill. SAS/ITSV has very good graphical and\ndata analysis capabilities, but it is tricky to write the data import utilities needed toget arbitrary data into the system. If you purchase SAS, it is a good idea to alsopurchase some SAS consultancy to build data import scripts for all the data sourcesto get them into SAS/ITSV format. MXG primarily focuses on mainframe-orienteddata import and analysis; however, discussions have taken place between AdrianCockcroft and Barry Merrill, and some Sun/Solaris OE data can be imported fromorcollator data files to MXG for analysis.\nSummary\nThis chapter concentrated on the requirements for information collection andpresentation to three different audiences. Some tools were mentioned in passing; thenext chapter presents a more in-depth survey of useful tools for capacity planning.\n\n145CHAPTER7\nTools Overview and Evaluations\nSun\u2019s own tools are currently focused on system management monitoring and\navailability. Third-party tools include performance databases and trending andmodelling capabilities. Over time, more of these functions are likely to be integratedinto Sun\u2019s product line.\nThis chapter describes several useful Sun and third-party tools for capacity\nplanning. It also offers several ways to balance peak load both within and acrosssystems to eliminate underutilization and maximize IT computing resources.\nTools and Products for Performance\nManagement\nBy combining service definition monitoring and the ongoing task of performance\nmonitoring, an IT department can easily begin identifying utilization trends andresource restrictions within its networks and servers. Analysis of this informationmay reveal that many systems are either underutilized or seriously strained duringpeak loads. Fortunately, there are a number of ways to smoothly balance the loadboth within and across systems to maximize IT computing resources.\nServer Consolidation\nServer consolidation has become a popular endeavor for IT departments to decreasethe datacenter space, simplify management, increase reliability, and decrease costs.By analyzing the resource requirements of multiple applications on distinct servers,you may be able to consolidate those services on fewer machines. By analyzing theservices that are currently running in the datacenter, you may be able to identifyservices that would be prime candidates for consolidation.\n\n146Chapter 7    Tools Overview and EvaluationsRemember to analyze the service requirements carefully and ensure that the new\nsystem can handle the load of the combined services during the peak hours. Also, beon the lookout for batch jobs and reports to make sure that the system can handlethe requests from multiple competing services. If a defined approval process orschedule for starting batch jobs and recording their duration and resource usage isnot in place, institute one so that service levels can be managed for batch and onlineactivity.\nDomains and Dynamic Reconfiguration\nThere are occasions when server consolidation is desired, but because of resource orapplication constraints, it may not be desirable to run multiple services on a singleinstance of Solaris OE. More than one version of Solaris OE will need to besupported as applications transition to support newer releases.\nIn these cases, a Sun Enterprise 10000 server with domains and DR may be the ideal\nsolution. By creating discrete domains within the Sun Enterprise 10000 server, eachwith its own version of Solaris OE, you may be able to consolidate many differentservices into the server. The added benefit of DR also enables you to move a systemboard containing memory and CPUs from one domain to another. This feature isideal when one domain needs additional computing power, say, for a large report,and another domain has an idle time period and does not need the extra resources.\nSolaris Resource Manager\nDR in the Sun Enterprise 10000 server provides discrete \u201chard\u201d partitioning ofdomains, whereas SRM software provides a way to distribute CPU and virtualmemory to specific applications or users within a single image of Solaris OE. Theseresource allocations act as a form of \u201csoft\u201d partitioning; that is, they carve upresources in a single instance of Solaris, whether it is on a single server or a SunEnterprise 10000 server domain.\nThis tool can be extremely useful when you combine mixed workload applications\non a single server. To illustrate how this works, let\u2019s look at the following examples:\nAssume a server is running a database that serves both interactive users\nand batch processing of reports. During the day, 300 users are online andthe system is at 70% utilization. At night, the department manager runsdaily reports and trending reports that consume about 85% of themachine\u2019s resources. For reasons unknown, the manager decides to runthe reports in the middle of the day. The result is invariably very poorresponse for the interactive users.\n\nTools and Products for Performance Management 147At the end of the year, the manager also needs to run the annual reports.\nUnfortunately, business was so good this year that the batch job does notfinish during the night; so when the users come into work the nextmorning, the response time is, again, very poor. At this point, there arereally only two possibilities for the administrator: 1) suspend the report,or 2) have the users suffer.\nThe SRM software enables the administrator to establish resource usage policies for\nthe CPU and virtual memory resources, so priorities can be established to preventthese problems. By the interactive users being alloted an 80% share and the batchprocesses a 10% share of the resources during working hours, the users will have anormal day. When they all go to lunch or the machine has an idle moment, the batchprocess is allowed to use those idle resources, since the CPU share constrains thesystem only when it is oversubscribed. When the users come back, the batch jobinstantly relinquishes the CPU for their use. At night, the administrator can set up adifferent rule that gives the batch process the lion\u2019s share of the resources andconstrains the interactive users, thereby giving the batch process priority in caseusers leave some CPU-intensive jobs of their own running overnight.\nNote that even though a database was used in this example, any application can be\ncontrolled by the SRM software. This capability is particularly useful when the userpopulation is \u201cout of control\u201d and can be used to constrain CPU-related denial-of-service problems on Web sites. In addition, it can be used to ensure that fee-payingsubscribers get better service levels than casual or guest users. When multiple Websites are hosted on a single server, a common problem is that poorly written cgi-binscripts and searches from a single Web site can saturate the server and affect theperformance of all the other sites. The SRM software can be used to give each site itsown equal share, so that no one site can dominate the system.\nSolaris Bandwidth Manager\nIn many ways, the Solaris Bandwidth Manager (SBM) software provides the samecapability for network bandwidth that the SRM software provides for CPUs andvirtual memory. It allows the administrator to establish controls for the amount ofbandwidth that applications, users, and departments are allowed to use. The SBMsoftware provides the framework for quality of service (QoS) guarantees and SLAsfor critical networked applications.\nOne example of using the SBM software to guarantee compliance with a QoS or SLA\ncould be a corporate intranet server that provides both Web and ftp data. Forinstance, in the middle of the day, a user decides to ftp every file found on theserver. The Web users will start experiencing greater latency as the ftp server startssending files. By establishing rules in the SBM software, the administrator canguarantee enough network bandwidth by throttling the ftp traffic. It is also possibleto manage according to the destination of the traffic. A server can be dedicated togive the best service to a certain class of users that have a specified network address.\n\n148Chapter 7    Tools Overview and EvaluationsThis can be useful in corporate intranets and can also be used to constrain denial-of-\nservice attacks on the network without shutting off access completely. For example,if a hacker uses a large ISP (Internet service provider) to attack your site by floodingit with packets, you could deny all access from that site fairly easily, but this wouldupset legitimate users. With the SBM software, you can limit the incomingbandwidth from a certain ISP to a data rate that you can handle, and then work tofilter out very specific types of packets that are coming from the hacker.\nLoad Sharing Facility and Codeine\nThe load sharing facility (LSF) from Platform Computing Corporation is a tool thattakes advantage of idle CPU cycles on a network to assist with batch processes. Thistool is most useful in environments that have intensive technical batch applications,like simulations. LSF may be the right tool to allow IT departments to meet batchrequirements in an SLA.\nLSF uses a cluster model for implementation. There are three roles to which hosts\ncan be assigned within a cluster, and hosts can have more than one role:\n\u25a0Master host \u2013 Maintains information about all other hosts in the cluster and thebatch queues.\n\u25a0Submission host \u2013 Accepts requests from users for a batch process.\n\u25a0Execution host \u2013 Processes the request (or a piece of the request).\nLSF enables users to submit batch jobs and gives the administrator the capability to\nset up rule sets to prioritize the requests.\nCodeine is a similar product developed by Gridware. In mid-2000, Sun acquired\nGridware and is integrating Codeine into the Sun product line.\nSun Management Center (SunMC)\nThis product is described in more detail in \u201cImplementing with Sun ManagementCenter\u201d on page 120. Earlier versions of the product were known as Sun EnterpriseSyMON software. To obtain more information and to download a version that workson a single system, go to\nhttp://www .sun.com/softwar e/sunmanagementcenter. An\nadditional license is required to manage multiple systems, and SunMC can be usedto manage hundreds of systems running Solaris 2.5.1, 2.6, 7, and 8 OEs from a singleconsole.\n\nTools and Products for Performance Management 149SunMC Hardware Diagnostic Suite 1.0\nSunMC Hardware Diagnostic Suite 1.0 is a comprehensive, network-aware\ndiagnostics tool that enhances overall availability by detecting hardware faultsbefore systems are affected and by reducing routine system maintenance throughscheduled testing. This module is free of charge to download from\nhttp://www .sun.com/softwar e/sunmanagementcenter/hwds/ and is an add-on\nsolution for SunMC.\nSunMC Hardware Diagnostic Suite enhances system availability by:\n\u25a0Finding hardware faults before the system is affected\n\u25a0Reducing (or eliminating) routine system maintenance time\nSunMC Hardware Diagnostic Suite achieves this goal through three key features:\n\u25a0Online testing to detect hardware faults\n\u25a0Test scheduling to enable latent fault detection\n\u25a0Integration with SunMC for unified system management\nConduct Online Testing While Applications Are Running\nWith online testing, system administrators can quickly diagnose, detect, and isolate\nfailing components, thereby reducing downtime caused by hardware failure.\nThe SunMC Hardware Diagnostic Suite enables you to run comprehensive, data-\nsafe, and non-resource-intensive hardware diagnostics testing in enterpriseenvironments while applications are still running. The testing does not corrupt dataon the system and uses minimal system resources. Types of online testing include:\n\u25a0Functional diagnostic testing \u2013 Covers device functional testing for processors,memory, network interfaces, disk drives, communication ports (serial andparallel), SSA enclosures, CD-ROM drives, and tape drives.\n\u25a0Quick check testing \u2013 Provides a quick test to see if devices are currentlyconnected to the system.\n\u25a0Test scheduling for routine system validation or for immediate hardware healthchecks \u2013 Schedules tests to run periodically in the background. Prepackaged testconfigurations are provided for scheduling to provide convenient systemcoverage. Scheduled routine hardware validation can replace other maintenancethat requires system downtime.\nSunMC Integration\nSunMC modules tie into the SunMC alarm framework so users can executecorrective action scripts or programs that eliminate faults.\n\n150Chapter 7    Tools Overview and EvaluationsUsers have remote access to diagnostic tests through the Java\u2122 technology-based\nSunMC console, which operates on the Solaris OE and Windows platforms. SunMC3.0 is the current release and it has a different license model. The base functionalitynow works across as many systems as you like, but extra functions such as healthmonitoring are now provided as add-on packages.\nSun Configuration & Service Tracker\nConfiguration & Service Tracker (CST) is an online tracking utility that continuouslytracks configuration changes of the system on which it is running. CST can beobtained for free from\nhttp://access1.sun.com/Pr oducts/solaris/cst/. The tracked\ninformation is presented in a user-friendly format through a browser-enabled Javaapplet.\nCST fills an important hole in the overall strategy of proactive system management\nat a customer site. It provides a macroscopic view of the system configurationchanges caused by failures or upgrades and service patterns over extended periods.CST performs these beneficial tasks:\n\u25a0Reviews configuration consistency at shipping installation and at other times\n\u25a0Detects incompatible upgrades\n\u25a0Gets field replaceable unit (FRU) information from a remote environment\n\u25a0Computes and tracks system availability statistics\nCST adopts a three-tier paradigm that includes middleware, a presentation client,\nand an agent that resides on each system being tracked. The data collected by theagent is managed by the middleware server, which enables clients to access it with aWeb browser. The client is implemented as a Java applet.\nThe agent automatically detects events that potentially involve configuration\nchanges. Examples of such events are:\n\u25a0Boot/reboot, system down, panic\n\u25a0Install/uninstall of software packages/patches\n\u25a0Dynamic attach/unattach (not available in CST1.0.)\n\u25a0Environmental changes detected through system interfaces (not available inCST1.0)\nThe agent also provides a facility for maintaining an electronic log of service events.\nFor each event, the CST agent creates a snapshot of the system configuration anddetermines the changes since the previous snapshot. It then catalogs the event andchanges in a change log file. It also maintains the current snapshot configuration ofthe system in a configuration file. Both these files\u2014the change log file andconfiguration file\u2014are maintained in a specific format to permit postprocessextraction of information. CST also notifies the user of each event it detects.\n\nTools and Products for Performance Management 151In an enterprise environment, a middleware server daemon receives updates from\nthe various CST agents in the enterprise. It stores them in a central repository. Thisfacilitates manipulation and viewing of data, even if the host is temporarilyinaccessible. The server supports an API (application programming interface) thatenables users to invoke a Java applet on a Web browser so they can view theconfiguration report and change log for any host on the network. The user interfacealso permits users to enter service comments to complement each service event. Inaddition to viewing the configuration or system changes, CST also permits users toprint the data or send it by email.\nBMC Patrol and Best/1\nBMC sold the Patrol application management framework for many years andrecently bought BGS and the Best/1 product line.\nBMC Best/1\nBGS was a well-established company that built only performance tools. It pioneeredthe use of performance modelling, starting off on mainframes over 20 years ago, andlaunching a UNIX-based product in 1993. There are two sides to the Best/1 product:a real-time performance monitor that covers the same ground as many otherproducts, and a performance analysis and prediction tool that is the market leader atthe high end. Best/1 is rapidly adding functionality to a range of tools for themainframe, minicomputer (VAX and AS/400), UNIX (most vendors), and PC (NTand OS/2) platforms. BMC\u2019s breadth of coverage puts their products into a marketcategory where large corporations standardize on a vendor and install tools rightacross the enterprise. The downside of this functionality is that the entry-level cost isat the high end of the scale, both financially and in the time and training required touse a modelling tool effectively.\nThis tool can be used in a production environment to monitor systems, but it also\ncan be used to set up and automate the entire process of generating regular reportsthat include the modelling of alternative future scenarios.\nBest/1 has a function-based GUI that works well, but is not easy to use. The initial\nconfiguration of the tool can be daunting. BMC is working to simplify theconfiguration; in large installations, training and consultancy support alleviate thisproblem.\nBest/1 for UNIX exports data into a PC-based Visualizer performance database. This\ndatabase also pulls in data from all other types of systems for which BMC has tools.For UNIX-oriented users, it is annoying to have to switch between systems to viewall the data. Visualizer can automatically generate reports that are viewable from aWeb browser.\n\n152Chapter 7    Tools Overview and EvaluationsVisualizer is one of the few tools that can analyze trend data in terms of cyclic\nfluctuations. It implements MASF, which is a sophisticated and automated approachto the problems discussed in Chapter 4, \u201cScenario Planning.\u201d\nIn summary, if you have experience in the way capacity planning is done in the\nmainframe world and want to merge this knowledge with UNIX-based systems anddatabase performance monitoring, this tool does the job. If you are coming from atypical UNIX system administrator background or are looking for low-pricedmonitoring tools, you may not be able to justify the investment needed to fullyutilize this product.\nBMC Patrol\nPatrol is a distributed application management system. Performance management is\nonly part of the problem to be solved; keeping applications up and running andcollecting application-specific measurements are also important roles for Patrol. Theproduct has a good underlying architecture that scales well in large, distributedinstallations with multiple data storage points and management consoles.\nLike SunMC or Best/1, BMC monitors by evaluating rules in a local agent on each\nnode. However, Patrol\u2019s rules are more complex, and its predefined knowledge\nmodules understand how to manage everything from an Oracle financials database to\na Netscape Web server.\nPatrol has been criticized for imposing a larger load on the system being managed\nthan do other tools. Their original collection strategy was to run a standard systemutility, such as vmstat , and parse the output\u2014a strategy that is less efficient than\nreading the kernel directly. More recently, Patrol and Best/1 have been merged touse the same highly efficient data collector. Patrol data can be viewed with a Webbrowser, and Patrol integrates closely with many network and system managementtools. Patrol does not include trending and prediction tools; however, future releasesof Patrol and Best/1 will become more integrated to provide the best of bothproducts.\nBMC is at\nwww .bmc.com.\nFoglight Software (RAPS)\nFoglight Software was initially known as Resolute Software and more recently has\nbecome a division of Quest Software. It competes in the application managementspace with BMC Patrol. RAPS uses very efficient data collectors and has anadvanced distributed storage architecture and a good GUI. Rich Pettit, who built thefirst SE toolkit while he was at Sun, is now the Chief Performance Architect forRAPS. The rule set that RAPS uses is based on a superset of thevirtual_adrian.se script, but the product is a full, distributed, multiplatform\n\nTools and Products for Performance Management 153tool. People sometimes ask if there are any plans for a fully supported version of the\nSE toolkit that runs on other platforms as well as on Sun machines, and this is asclose as you will get.\nFoglight Software is at\nwww .foglight.com.\nSAS IT Service Vision (CPE)\nSAS has a wide range of statistical analysis products. SAS/CPE has been used to\nprocess computer performance data for many years in both mainframe and UNIXenvironments. It has a much more sophisticated performance database than do otherproducts, with a very wide range of analysis, reporting, and modelling functions. Itis often used in conjunction with performance monitoring products from othervendors to provide capacity planning, performance prediction, and reportingcapability. The MXG Toolset is a notable alternative to the SAS/CPE functions thatalso runs on the basic SAS platform.\nMore information can be found at\nwww .sas.com and www .mxg.com.\nHyperformix/SES Workbench and Strategizer\nSES Workbench is a well-established performance simulation tool. The company has\nrecently changed its name to Hyperformix and has focused on adding a professionalservices capability to assist in capacity planning simulation development. Thesimulation used is very different from the analytical performance modelling done byBGS Predict and TeamQuest Model (which work by solving a set of equations). Witha simulator, you create a model of the components in your system, then drive it withinput load levels and wait for the simulation to stabilize over time. Running asimulation is far more resource-intensive than analytical modelling in both setuptime and CPU time. If you have a well-constructed simulation, you get much moreinformation than is available from an analytical model. SES Workbench runs onUNIX systems and NT.\nTo make it easier to construct models of computer systems and networks, SES\nStrategizer is preconfigured with high-level components that match common systemtypes in some detail. You still need a lot of experience to make good use of the tool,but it is a lot quicker to construct a model. SES Strategizer runs only on WindowsNT.\nSES is at\nwww .ses.com and www .hyperformix.com.\n\n154Chapter 7    Tools Overview and EvaluationsAurora Software SarCheck\nSarCheck is a relatively inexpensive tool that can be very useful to help with\ndiagnosis in an operations environment. It runs on many platforms, but on SolarisOE it reads data from sar andps, then writes a detailed explanation of what it sees,\nalmost like an essay. SarCheck uses rules that are based on those in AdrianCockcroft\u2019s virtual_adrian.se tool. The output is punctuated with cross\nreferences to specific pages in Adrian\u2019s Sun Performance and Tuning book for further\nexplanation.\nAurora Software Inc. is at\nhttp://www .sarcheck.com.\nCapacity Planning with TeamQuest\nModel\nTo show the capabilities of this class of tools, we have constructed an example using\na product from TeamQuest Corporation ( www .teamquest.com). This product has\nbeen used effectively by Sun Professional Services to provide a capacity planningstudy over a time scale of a month or so.\nThe TeamQuest suite of performance tools includes many useful tools for\nsimplifying the performance management process. Performance data is monitoredand recorded in real time, with both reporting and alert features. The data can beviewed live, showing system and workload performance measures, while the data isbeing recorded in the TeamQuest database. The performance data for a previouslyrecorded period can also be viewed in context, with drill-down capabilities fordebugging any performance problems.\nThe TeamQuest Alert tool can provide performance condition alarms to a central\nmanagement console system, which alerts performance analysts, systemadministrators, or capacity planners to retrieve the recorded performance data forthe critical period or analyze the real-time condition of the systems with theTeamQuest View tool.\nThe TeamQuest on the Web tool moves performance data from servers being\nmonitored to a central Web server host, where the performance data can be viewedby a standard Web browser with Java technology support.\nThe TeamQuest Model tool can take recorded performance data, categorized by a set\nof defined application sets, called workloads, and provide input for load andcapacity planning.\n\nCapacity Planning with TeamQuest Model 155Creating the Model\nFrom the recorded baseline and system inventory, we can create a model of system\nperformance, as shown in FIGURE 7-1 . The workloads defined during the baselining\nprocess can be refined and assigned attributes of batch, interactive, or transactionalloads. The transaction rates and response times can be adjusted to represent actualapplication measurements if they are known, or they can be estimated with relativecounts adjusted by the resource consumption of the workloads.\nFIGURE 7-1 TeamQuest Model\nThe system being modelled is a database server, with the database instance and twomiddleware applications running on the same system. The \u201cadmin\u201d workloadcontains administrative functions, including availability, performance, andapplication measurement and reporting tools. The \u201cOTHER\u201d workload includes allother work that is being measured, but is not assigned to a specific workloadcategory.\nThe model can now be adjusted and solved, as shown in\nFIGURE 7-2 , with the various\nsystem active and passive resources assigned to the workloads for each sampleperiod.\n\n\n156Chapter 7    Tools Overview and Evaluations.\nFIGURE 7-2 TeamQuest Model: Adjust and Solve\nWe will solve the model as accurately as possible and extract the results in a reports\nwindow, as shown in FIGURE 7-3 . The standard reports produced include several\ninteresting statistics for further analysis. The Principal Results report includesthroughput, response time, population, stretch factor, and passive resource (memoryor maximum multiprogramming level) statistics.\nThe Stretch Factor statistic represents the time spent waiting for a resource as\ncompared to the total time spent processing a transaction or unit of work. A higherstretch factor indicates a higher relative quantity of resource wait time. A stretchfactor of 1.0 indicates that the workload spent no time waiting for resources and wasrunning at the maximum possible efficiency. Stretch factors greater than 2.0 shouldbe analyzed\u2014they indicate a significant resource shortage with processing timeequal to wait time.\nThe Active Resource report includes resource consumption statistics such as\nthroughput, service time, wait time, average queue length, and number of server\ninstances for each active resource being monitored in the system. Active resources inthe example include CPUs, controllers, and disks.\n\n\nCapacity Planning with TeamQuest Model 157FIGURE 7-3 TeamQuest Model: Solved with Reports\nThe Workload by Active Resource Statistics report breaks down the active resources\nfor the system, as reported in the System Active Resource report, and correlatesthose resources consumed with the workloads configured in the model. Each activeresource utilization is reported as a percentage of the system total, simplifyingrelative process weight comparisons. The actual workloads can now be analyzed for\nindependent resource consumption characteristics, as well as for relative resourceconsumption and possible resource contention between the modelled workloads.\nWith the calibrated model, we can now add steps , defining the projected workload\nchange (see\nFIGURE 7-4 ). In this example, the workloads have all been assigned a 5%\nper month compounded growth factor, which is applied to transaction rates andactive resource visits. Active resource visits are counts of the number of discreteaccesses to a given resource such as a disk, CPU, controller, or other measuredqueue.\n\n\n158Chapter 7    Tools Overview and EvaluationsFIGURE 7-4 TeamQuest Model: Steps of Compound Growth Model\nThe calibrated model, including the new workload growth calculations, can now be\nexported to create graphs and tables of performance data in Microsoft Excel. Theperformance data and resource consumptions are then graphed for the system as awhole, and each workload is defined.\nThe first graph that we will examine is the Stretch Factor by Workload graph (see\nFIGURE 7-5 ). This graph shows the projected increase in workload wait time as a\nportion of total transaction time. The graph shows a column representing eachworkload for each step (each month has compound 5% growth in this example). Themodelled \"Database_App\" workloads exhibit a stretch factor of well over 2.0 in thefifth step, sometime during April. The stretch factor increases from just over 2.0 towell over 4.0 in the final step, representing the projected workload for June.\n\n\nCapacity Planning with TeamQuest Model 159FIGURE 7-5 TeamQuest Model: Workload Stretch Factor Graph\nThe \u201cDatabase_App2\u201d workload appears to have a response time increase, which\nindicates a resource contention problem. This is to be expected, with the increase inrelative queue wait time to resource service time represented by the stretch factor.\nWe can also look at the throughput data and graph for the \u201cDatabase_App2\u201d\nworkload. From the graph in\nFIGURE 7-6 and the table data provided from the\naccompanying spreadsheet, notice that the throughput of the measured workloadsincreases through the steps. In addition, notice that the \u201cDatabase_App2\u201d workloadhas not increased throughput as fast as the proposed workload steps have increased\u201cvisit count,\u201d or transaction volumes.\nMay Step: 5\nMarch Step: 3\nJanuary Step: 1OTHER\nDatabase_Server\nDatabase_App1\nDatabase_App2\nadmin4.5\n4\n3.5\n3\n2.5\n02\n1.5\n1\n0.5\n\n160Chapter 7    Tools Overview and EvaluationsFIGURE 7-6 TeamQuest Model: Workload Throughput Graph\nWe can further analyze this workload\u2019s behavior by using the Components of\nResponse Time graph for the \u201cDatabase_App2\u201d workload. The graph in FIGURE 7-7\nillustrates the time consumed by a transaction, or unit of work within the workload,with time spent in the workload. This is broken down by active resource service timeand active resource queue delay. The time between activity in a given workload(while the service is sitting idle), called Delay time, is removed from these graphs, sowe can better analyze the actual resource consumption and resource request delays.\nMay Step: 5\nMarch Step: 3\nJanuary Step: 1OTHER\nDatabase_Server\nDatabase_App1\nDatabase_App2\nadmin0.07\n0.06\n0.05\n0.04\n0.03\n0.02\n0.01\n\nCapacity Planning with TeamQuest Model 161FIGURE 7-7 TeamQuest Model: Components of Response Time for Database_App2\nBy examining this breakdown of the time spent per transaction in the workload, we\ncan see that the CPU Queue Delay is increasing at an alarming rate and that itaccounts for more than half of the per-transaction time by Step 6. This indicates thatthe system is on the verge of a serious performance degradation if the growth of theworkload matches the modelled steps.\nIn the Active Resource Utilization graph, shown in\nFIGURE 7-8 , we see the busiest\nsystem active resources, with the CPU utilization shown to be climbing rather high.These values are projected for CPUs in the growth period being modelled, as well asthe other busiest system active resources, in this case, disk resources.\nAR Queues Queue Delay\nAR Queues Servicec1t1d0 Queue Delay\nc1t1d0 Service\nc1t17d0 Queue Delay\nc1t17d0 Service\nCPU Queue Delay\nCPU Service\nJun Step:6 May Step:5 Apr Step:4 Mar Step:3 Feb Step:2 Jan Step:1700\n600\n500400\n300\n200\n100\n0\n\n162Chapter 7    Tools Overview and EvaluationsFIGURE 7-8 TeamQuest Model: System Active Resource Utilization Graph\nThe CPU resource utilization is projected over 95% average utilization by the time\nthe model reaches the sixth step (only five months away). Step 1 shows that we arecurrently running at approximately 70% utilization, with the active resourceutilization for the measured workloads showing very little resource queue delaytime (indicating contention for active resources). At Step 2, the workloads arebeginning to show some contention and CPU queue delay times, with CPUutilization just passing 75%. This trend would indicate that we should target 70% forthe KPI maximum CPU utilization threshold.\nTo solve this performance trend, we have several choices: we can upgrade the server\nwith more CPUs or faster CPUs, or we could move some of the work from thismachine to another server. This modelled system was a Sun Enterprise 10000 serverwith six 250 MHz UltraSPARC CPUs with 4 MB of e-cache. The chosen solution inthis case is to perform a one-to-one upgrade of the CPUs in the system to400 MHz UltraSPARC CPUs with 8 MB of e-cache using the \u201cChange CPU Name\u201dfunction of TeamQuest Model, shown in\nFIGURE 7-9 . With this solution, we can return\nto the TeamQuest Model window, create alternatives based on the current model,called \"what if\" frames, and modify the system resources to predict the projectedperformance of the growing workload into the new system configuration.\nCPU\nc1t17d0\nc1t17d0\nc1t17d0\nc1t17d0100\n90\n80\n70\n60\n5040\n30\n1020\n0\nJun Step:6 May Step:5 Apr Step:4 Mar Step:3 Feb Step:2 Jan Step:1\n\nCapacity Planning with TeamQuest Model 163FIGURE 7-9 TeamQuest Model: Change CPU Definition\nAfter recalculating the steps by using the new hardware configuration, we can use\nthe MVAP (Mean Value Approximation Package) Solver to predict the systemresource utilization, response times, and queue delays in the new system model byrunning the same workload steps. Examining the Stretch Factor graph (see\nFIGURE 7-10 ) for the new system configuration shows the relative queue delay time\nfor active resources in the new system model.\n\n\n164Chapter 7    Tools Overview and EvaluationsFIGURE 7-10 TeamQuest Model: Upgraded System Stretch Factor Graph\nThe Stretch Factor graph now shows stretch factors under 2.0 in all steps, for all\nworkloads modelled. In addition, the \u201cDatabase_App2\u201d workload displays anincremental increase in delay time per unit of work being done. We can delve deeperinto the characteristics of the \u201cDatabase_App2\u201d workload on the new server systemmodel by examining the Resource Component Response Time graph.\nThe graph in\nFIGURE 7-11 shows that the CPU wait queue time has been significantly\nreduced for the \u201cDatabase_App2\u201d workload as a result of the upgrade; it also showsthat the total response time for a unit of work has been greatly reduced in the lattersteps of the growth model.\nMay Step: 5\nMarch Step: 3\nJanuary Step: 1OTHER\nDatabase_Server\nDatabase_App1\nDatabase_App2\nadmin0.07\n1.8\n1.6\n1.4\n1.2\n1\n0.8\n0.4\n0.2\n00.6\n\nCapacity Planning with TeamQuest Model 165FIGURE 7-11 TeamQuest Model: Upgraded Components of Response Time Graph\nWe can also examine the total system CPU utilization by looking at the resulting\nspreadsheet table data or by viewing the Active Resource Utilization graph (see\nFIGURE 7-12 ). The CPU and disk active resources are graphed over the current,\nmeasured workload, as well as over the five steps of compound growth in the newserver platform model.\nAR Queues Queue Delay\nAR Queues Servicec1t1d0 Queue Delay\nc1t1d0 Service\nc1t17d0 Queue Delay\nc1t17d0 Service\nCPU Queue Delay\nCPU Service\nJun Step:6 May Step:5 Apr Step:4 Mar Step:3 Feb Step:2 Jan Step:1250\n200\n150\n100\n50\n0\n\n166Chapter 7    Tools Overview and EvaluationsFIGURE 7-12 TeamQuest Model: Upgraded System Active Resource Utilization\nThe modelled performance in the new server platform shows that CPU utilization is\napproaching 70% at the end of Step 5, five months into our compound growthmodel. By projecting this growth and using the previous model with degradingperformance in the measured workloads appearing at the 70% CPU utilizationthreshold, we can plan on another upgrade to the system in approximately fivemonths. We could project the model out even further with additional compoundgrowth steps and use other hardware upgrade scenarios. To maximize the accuracyof the model and account for possible software changes in the workloads, we willcontinue to monitor and report on the performance of the system. In addition, wewill begin a new model at some time in the future, before degradation sets in.\nThe Active Resource Utilization graph shows that disks are becoming rather busy\nthroughout the model and that disk resource queue delays are starting to appear inthe \u201cDatabase_Server,\u201d \u201cDatabase_App1,\u201d and \u201cDatabase_App2\u201d workloads. Wecould now return to the model and upgrade the disk subsystems to provide moreresource capabilities or move workload resources to balance the disk resources beingused.\nc1t17d0\nCPU\nc1t1d0\nc2t49d0\nc2t33d0\nJun Step:6 May Step:5 Apr Step:4 Mar Step:3 Feb Step:2 Jan Step:1100\n60\n5040\n30\n20\n10\n0\n\nSummary 167Summary\nThis chapter presented a detailed survey of useful tools for capacity planning and\ndiscussed the benefits of using both Sun and third-party tools. In addition, thischapter offered several ways to balance peak load both within and across systems toeliminate system strain and underutilization.\nAppendix A consists of several tables containing SCPM measurements that indicate\nthe processing potential of system configurations using Solaris OE versions 2.5, 2.5.1,2.6, 7, and 8.\n\n195Glossary\naccess control list (ACL) A file that specifies which users can access a particular resource, such as\na file system.\naccounting Keeping track of resource usage on a machine. The Solaris Resource Manager\n(SRM) software provides accounting features.\nACL Seeaccess control list .\nadministration tool A graphical user interface (GUI) tool for configuring the Solaris Bandwidth\nManager (SBM) software.\nadministrative\ndomain A collection of network elements under the same administrative control and\ngrouped together for administrative purposes.\nADOP Seeautomatic degree of parallelism .\nalarm The means by which notification is sent when an exception occurs.\nalternate pathing (AP) A software mechanism that works in conjunction with dynamic\nreconfiguration (DR) to provide redundant disk and network controllers andtheir respective physical links. The main purpose of AP is to sustaincontinuous network and disk I/O when system boards are detached from amachine or dynamic system domain, or DSD (in the case of the Starfire) that isrunning a live copy of the Solaris operating environment (OE).\nAP Seealternate pathing .\napplication resource\nmeasurement (ARM) A means of measuring the end-to-end response time of a system.\nARM Seeapplication resource measurement .\nASE Sybase Adaptive Server Enterprise.\nattaching SeeDR (dynamic reconfiguration) attach .\n\n196      Capacity Planning for Internet Servicesautomatic degree of\nparallelism (ADOP) A feature of the Oracle8 iDatabase Resource Manager that attempts to\noptimize system utilization by automatically adjusting the degree ofparallelism for parallel query operations.\nbackfilling The execution of a job that is short enough to fit into the time slot during\nwhich the processors are reserved, allowing for more efficient use of theavailable resources. Short jobs are said to backfill processors reserved for largejobs.\nBBSRAM Boot bus static random access memory.\nblacklist A file that enables you to specify components, such as system boards, that\nshould not be configured into the system. The blacklist file is read andprocessed at startup.\nBMC Best/1 BMC Software\u2019s Best/1 products provide tools to address performance\nmanagement requirements across OS/390, Parallel Sysplex, SAP R/3, UNIX,Windows NT, VM, and AS/400 environments.\nCBQ Seeclass based queuing .\nCCMS A tool that provides information to SAP R/3, allowing it to measure the\nperformance of key user transactions and the response time of the back-enddatabase for applications.\nCICS Seecustomer information control system .\nCIM Seecommon information model .\nclass-based queuing (CBQ) The underlying queuing technology used in the Solaris Bandwidth\nManager (SBM) software.\nclasses of service (CoS) A feature supported by the Solaris Bandwidth Manager (SBM) software\nthat allows network traffic to be organized so that urgent traffic gets higherpriority than less important traffic.\nclassifier A component of the Solaris Bandwidth Manager (SBM) software that allocates\npackets to a class queue. When a packet arrives, the classifier analyzes thepacket protocol, its type of service (ToS) value, URL information, sourceinformation, and destination information and allocates the packet to a classqueue where it waits to be processed.\nCLI command-line interface, as opposed to graphical user interface (GUI).\ncluster A collection of computers interconnected via a high-speed interface that allows\nthe environment to behave as a single unified computing resource.\nclustered cache A method of caching Web pages where multiple servers use the intercache\nprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if high\n\nGlossary 197availability is important, multiple servers are configured as siblings. Each\nsibling stores data in its cache, but also uses ICP to search the caches of othersiblings.\nCMIP A scalable OSI-based network management protocol that is used in situations\nwhere Simple Network Management Protocol (SNMP) is not powerful enough.\ncommon information\nmodel (CIM) A metamodel based on the unified modelling language (UML) that\nsupplies a set of classes with properties and associations. The CIM provides aconceptual framework within which it is possible to organize informationabout a managed environment.\ncontrol interval In control theory, the rate at which measurements are made and corrections are\napplied.\nCoS Seeclasses of service .\ncross-system coupling\nfacility (XCF) A Workload Manager (WLM) component that communicates policies,\nmetrics, and control data between Sysplex nodes.\ncustomer information\ncontrol system (CICS) An interactive transaction processing system from IBM.\nDDI_ATTACH A function, used by dynamic reconfiguration, or DR (called from dr_driver ),\nthat provides the ability to attach a particular instance of a driver withoutaffecting other instances that are servicing separate devices.\nDDI_DETACH A function, used by dynamic reconfiguration, or DR (called from dr_driver ),\nthat provides the ability to detach a particular instance of a driver withoutaffecting other instances that are servicing separate devices.\nDDI/DKI Device driver interface/device kernel interface. These are function call entry\npoints that device drivers should implement to fully support dynamicreconfiguration (DR). DDI/DKI is specified in the \u201cWriting Device Drivers\u201dsection of the Driver Developer Site 1.0 AnswerBook (\nhttp://docs.sun.com ).\nDDI_RESUME A function, used by dynamic reconfiguration, or DR (called from dr_driver ),\nthat provides the ability to detach a board that contains the kernel cage (OBPOpenBoot PROM), kernel, and non-pageable memory). The kernel cage canonly be relocated after all of the drivers throughout the entire dynamic systemdomain, or DSD (not just on the board being detached) are quiesced toguarantee the data integrity of the kernel cage relocation. DDI_RESUME\nresumes the drivers after the quiesce period.\nDDI_SUSPEND A function, used by dynamic reconfiguration, or DR (called from dr_driver ),\nthat provides the ability to detach a board that contains the kernel cage (OBP(OpenBoot PROM), kernel, and non-pageable memory). The kernel cage canonly be relocated after all of the drivers throughout the entire dynamic system\n\n198      Capacity Planning for Internet Servicesdomain, or DSD (not just on the board being detached) are quiesced to\nguarantee the data integrity of the kernel cage relocation. DDI_SUSPEND\nsuspends the drivers to begin the quiesce period.\ndecay The period by which historical usage is discounted.\nDEN The Directory Enabled Networks working group. The goal of this group is to\noffer a standard information model and directory schemas to tie together usersand applications with network elements, protocols, and services throughspecific relationships. By complying to this information model and the DENschemas, different network equipment and application vendors should be ableto build interoperable network elements around a central directory.\ndetaching SeeDR (dynamic reconfiguration) detach .\nDiff-Serv The Differentiated Services (Diff-Serv) working group of the Internet\nEngineering Task Force (IETF). Diff-Serv addresses network managementissues related to end-to-end quality of service (QoS) within diverse andcomplex networks.\nDIMM Dual in-line memory module. A memory module with a higher capacity and\nfaster performance than a SIMM (single in-line memory module). It is currentlyused as the memory source for all Sun Microsystems platforms.\ndirect control A means of control that operates on the resource you want to control. For\nexample, the Solaris Resource Manager (SRM) software controls CPU usageper user by implementing a scheduling class that decides who should get whatshare of the CPU.\nDISC Dynamic internal service class created by the Workload Manager (WLM).\nThese classes enable the WLM to manage transactions. Each DISC is associatedwith one or more normal service classes and a given server component. Thenumber of transactions using each route then allows the DISCs to be weighted.Thus, if the external or standard service class goal is not being met, theassociated DISCs can be managed (if that is where a bottleneck lies).\ndistributed queuing\nsystem A batch system product from Florida State University that is available in the\npublic domain. The set of system resources it understands is host (by name),system architecture, operating system type, amount of memory, and CPUusage.\nDMTF Desktop Management Task Force.\nDQS Seedistributed queuing system .\nDR Seedynamic reconfiguration .\nDR attach The process of bringing a system board under the control of the Solaris\noperating environment (OE) through use of the dynamic reconfiguration (DR)mechanism.\n\nGlossary 199DR detach The process of removing a system board from Solaris operating environment\n(OE) control through use of the dynamic reconfiguration (DR) mechanism.\nDSD Seedynamic system domains .\nDSS Decision support system.\nDSS/DW Decision support system/data warehousing.\ndynamic\nreconfiguration (DR) A Sun Microsystems technology supported on the Starfire and other Sun\nEnterprise servers which allows system boards to be added (attached) orremoved (detached) from a single server or domain.\ndynamic system\ndomain (DSD) A Starfire independent hardware entity formed by the logical\nassociation of its system boards. Each domain on the Starfire enjoys completehardware isolation from other domains, executes its own private version of theSolaris operating system, and is centrally managed by the system serviceprocessor (SSP).\nELIM Seeextended load information manager .\nEnterprise 10000 SeeSun Enterprise 10000 .\nERP Enterprise resource planning.\nerror event A discrete on/off event, as opposed to a continuous variable to be compared\nagainst a limit.\nexception A condition that represents a problem in processing a job. The load sharing\nfacility (LSF) can watch for several types of exception conditions during a job\u2019slife cycle.\nexclusive scheduling A type of scheduling used by the load sharing facility (LSF) that makes it\npossible to run exclusive jobs on a host. A job only runs exclusively if it issubmitted to an exclusive queue. An exclusive job runs by itself on a host. LSFdoes not send any other jobs to the host until the exclusive job completes.\nExtended Load\nInformation Manager (ELIM) The load sharing facility (LSF) uses the Load Information Manager\n(LIM) as its resource monitoring tool. To modify or add load indices, an ELIMcan be written.\nfairshare A form of scheduling used by the load sharing facility (LSF) to prevent a single\nuser from using up all the available job slots, thus locking out other users.Fairshare scheduling is an alternative to the default first come, first servedscheduling. Fairshare scheduling divides the processing power of the LSFcluster among users and groups to provide fair access to resources for all jobsin a queue. LSF allows fairshare policies to be defined at the queue level so thatdifferent queues can have different sharing policies.\n\n200      Capacity Planning for Internet ServicesFlowAnalyzer\n(NetFlow) An application that uses the output from NetFlow FlowCollector. It provides\nvery elaborate processing, graphing, and reporting options that can be used fornetwork analysis, planning, troubleshooting, and more\nFlowCollector\n(NetFlow) A NetFlow datagram consumer for one or more NetFlow devices. These\ndevices simply point to the host and port number on which the FlowCollectorsoftware is running. The FlowCollector aggregates this data, doespreprocessing and filtering, and provides several options to save this data todisk (such as flat files). Other applications such as network analyzing,planning, and billing can use these files as input.\nGigaplane-XB The interconnect on the Starfire that provides main memory access through a\npoint-to-point data router which isolates data traffic between system boardsand minimizes any performance degradation when memory interleaving isdisabled.\ngoal Goal-based policies are prescriptitive rather than reactive. A goal can be\ntranslated into a mixture of limits, priorities, and relative importance levels.Goals can include actions to be performed when the goal cannot be met.\nHealth Monitor SeeSyMON Health Monitor .\nheavily damped A system is heavily damped if you feed back a small proportion of an error\nover a longer control interval. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used.\nhierarchical fairshare A method of sharing resources, supported by the load sharing facility (LSF).\nHierarchical fairshare enables resources to be allocated to users in ahierarchical manner. Groups of users can collectively be allocated a share, andthat share can be further subdivided and given to subgroups, resulting in ashare tree.\nhost-based resources Resources that are not shared among hosts, but are tied to individual hosts. An\napplication must run on that host to access such resources. Examples are CPU,memory, and swap space. Using up these resources on one host does not affectthe operation of another host.\nHostview A graphical user interface (GUI) program that runs on the system service\nprocessor (SSP) machine (which is a component of an Enterprise 10000 system).Hostview enables you to monitor and control an Enterprise 10000. Forexample, Hostview can display continuous readouts of power and temperaturelevels at various locations within the Enterprise 10000 server.\nHPC High-Performance Computing.\nHP OpenView Computer-oriented local and wide area networks are normally managed using\nthe Simple Network Management Protocol (SNMP) with Solstice SunNetManager or HP OpenView products collecting and displaying the data. Bothproducts provide some visibility into what is happening in the computer\n\nGlossary 201systems on the network, but they are focused on network topology. Resource\nmanagement is done on a per-network basis, often by controlling the priorityof data flows through intelligent routers and switches.\nHTTP Hypertext transfer protocol. HTTP is used by Web servers to host content and\nrespond to HTTP requests from Web browsers.\nIBM Workload\nManager (WLM) A comprehensive toolset for MVS that provides an automated resource\nmanagement environment, driven by high-level business goals, and that, inmany cases, is self-tuning. Tools are provided to define the business goals orobjectives, to control system resources, and to feed metrics concerning theseresources back to the resource controller, which attempts to ensure that thegoals are met.\nIETF Internet Engineering Task Force.\nindirect control A means of control that works via resources that are dependent upon the\nresource that is being controlled. For example, to limit the I/O throughput of aprocess, it is sufficient to be able to measure the I/O throughput and limit theCPU resources for that process.\nintercache protocol (ICP) A protocol used to implement clustered caches. (See clustered cache .)\ninterleaving Seememory interleaving .\nintimate shared\nmemory (ISM) A way of allocating memory so that it can\u2019t be paged. The shared\nmemory area is often the largest component of a database\u2019s memoryrequirements, and is the easiest to insulate between database instances.Because intimate shared memory is wired down, the memory allocated to eachdatabase instance stays allocated and one instance cannot steal memory fromanother.\nInt-Serv The Integrated Services working group of the Internet Engineering Task Force\n(IETF).\nIP Internet Protocol. IP is the foundation of the TCP/IP architecture. It operates\non the network layer and supports addressing. IP enables data packets to berouted.\nISM Seeintimate shared memory .\nISP Internet service provider; a company that provides point-of-presence access to\nthe Internet.\nISPF Interactive system productivity facility; a generic MVS interface that can be\nused by the operator/administrator to define, activate, and deactivate policies.\n\n202      Capacity Planning for Internet ServicesJava Dynamic\nManagement Kit A JavaBeans based framework for developing and deploying dynamic\nmanagement based applications. Autonomous agents can be deployed in realtime to perform management tasks for devices on the network.\nJava Virtual Machine (JVM) The machine image, implemented in software, upon which Java code\nruns.\nJTAG Joint Test Action Group, IEEE Std. 1149.1. JTAG is an alternate communications\ninterface between the system service processer (SSP) machine and Enterprise10000 server, and is used when the standard network connection between theSSP and Enterprise 10000 is unavailable.\nJVM SeeJava Virtual Machine .\nkernel cage A special data structure (normally contained within a single system board) that\ncontrols the dynamic growth of all nonrelocatable memory, including theOpenBoot PROM (OBP) and kernel memory. When dynamic reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system to ensure that no I/O or kernelactivity occurs while the kernel cage is being relocated.\nkernel memory Memory that is used to run the operating system.\nkernel module A Solaris Bandwidth Manager (SBM) software module that contains the\nclassifier and scheduler .\nLAN Seelocal area network .\nlightly damped If you feed back a large proportion of an error with a short control interval, the\nsystem is said to be lightly damped. A lightly damped system is veryresponsive to sudden changes, but will probably oscillate back and forth.\nLIM SeeLoad Information Manager .\nlimit A simple rule with a single input measurement. It is also common to have\nseveral thresholds with a warning level action and a critical problem levelaction for the same measure.\nlnode Limit node; a node in a special resource tree used by the Solaris Resource\nManager (SRM) software. The SRM is built around lnodes, which are afundamental addition to the Solaris kernel. lnodes correspond to UNIX UIDs(user IDs), and may represent individual users, groups of users, applications,and special requirements. The lnodes are indexed by UID and are used torecord resource allocations policies and accrued resource usage data byprocesses at the user, group of users, and application levels.\n\nGlossary 203Load Information\nManager (LIM) The resource monitoring tool used by the load sharing facility (LSF). The\nLIM process running on each execution host is responsible for collecting loadinformation. The load indices that are collected include: host status, length ofrun queue, CPU utilization, paging activity, available swap space, availablememory, and I/O activity.\nload sharing facility (LSF) A software facility that provides the capability of executing batch and\ninteractive jobs on a pool of networked computers. The Sun MicrosystemsHigh-Performance Computing (HPC) package includes the LSF as a vehicle forlaunching parallel applications on an HPC cluster. In addition to starting batchjobs, the LSF also provides load balancing.\nlocal area network (LAN) A set of computer systems in relatively close proximity that can\ncommunicate by means of networking hardware and software.\nLPAR Logical Partitions; an IBM S/390\u2122 logical entity that runs its own operating\nsystem instance and allocated resources and is managed by ProcessorResource/Systems Manager (PR/SM\u2122).\nLSF Seeload sharing facility .\nLWP Lightweight process.\nmanagement\ninformation base (MIB) A database that contains network management variables and can be\naccessed via Simple Network Management Protocol (SNMP).\nmaster host The node where the load sharing facility (LSF) batch queues reside. When the\nLSF software initializes, one of the nodes in the cluster is elected to be themaster host. This election is based on the order of nodes listed in aconfiguration file. If the first node listed in the configuration file is inoperative,the next node is chosen, and so forth.\nmaximum bandwidth The amount of spare bandwidth allocated to a class by the Solaris Bandwidth\nManager (SBM) software. The maximum bandwidth is dependent on thepercentage of bandwidth the class can borrow.\nMDF Multiple Domain Facility\u2122; an Amdahl Corporation\u2122 technology that\nprovides logical partitioning for its mainframes. By integrating specialhardware for each logical partition or domain, Amdahl processor complexescould run multiple operating systems at close to native performance.\nmean value\napproximation\npackage (MVAP) A solver tool used with TeamQuest to predict system resource\nutilization, response times, and queue delays in new system models.\n\n204      Capacity Planning for Internet Servicesmemory interleaving A method of using computer memory that helps increase memory subsystem\nperformance by reducing the probability of hot spots or contention in a fewmemory banks. This is accomplished by spreading access to multiple memorybanks.\nMessage-passing\ninterface (MPI) An industry-standard interface used to parallelize applications.\nMIB Seemanagement information base .\nmicrostate accounting A method of accounting for resource usage where a high-resolution timestamp\nis taken on every state change, every system call, every page fault, and everyscheduler change. Microstate accounting provides much greater accuracy thansampled measurements.\nMPI Seemessage passing interface .\nMTM Seemulti-threaded mode .\nMulti-Threaded\nMode (MTM) A database topology where a single process serves many users.\nMV AP Seemean value approximation package.\nnegative feedback A method of applying feedback to a system where you take the error difference\nbetween what you wanted and what you got, and apply the inverse of theerror to the system to reduce the error in the future.\nNetFlow A product from Cisco that is supported by the Solaris Bandwidth Manager\n(SBM) software. NetFlow allows for detailed network measurements that canbe sent to other software packages, which can process and analyze the data.\nNetwork File System (NFS) An application that uses TCP/IP to provide distributed file services.\nnetwork queuing\nsystem (NQS) A public domain software product that has been enhanced by many\nhardware vendors. Sterling Software offers a distributed version of NQS calledNQS/Exec, which is geared toward a supercomputer environment. Limitedload balancing is provided as there is no concept of demand queues, since ituses traditional push queues instead. There is also no control over interactivebatch jobs.\nNFS SeeNetwork File System .\nNQS Seenetwork queuing system .\nNVRAM Nonvolatile random access memory.\nOBP OpenBoot PROM.\nODS Informix Online Dynamic Server.\n\nGlossary 205OLTP Online transaction processing.\noperational policy A policy that is implemented manually as part of operations management. For\nexample, an availability policy can include a goal for uptime and an automaticway to measure and report the uptime over a period. There is no direct controlin the system that affects uptime. It is handled by the operations staff.\nOracle8 iResource\nManager An Oracle facility that ensures system resources are applied to the most\nimportant tasks of the enterprise at the levels required to meet the enterprisegoals.\nPC NetLink A product from Sun Microsystems that is based on the AT&T Advanced Server\nfor UNIX. PC NetLink adds functionality that was not previously available onSolaris servers with products such as Samba and SunLink\u2122 PC\u2122 (a.k.a.Syntax TotalNET Advanced Server). PC NetLink adds file and print services,and enables Solaris servers to act as Microsoft Windows NT primary domaincontrollers (PDCs) or backup domain controllers (BDCs). For enterprises withmixed NT and Solaris Operating Environment (Solaris OE) servers anddesktops, Solaris PC NetLink 1.0 offers many new options for utilizinghardware resources and minimizing system administration overhead.\nPDP Seepolicy decision point .\nPEP Seepolicy enforcement point .\nperformance index The ratio of work completed vs. the amount of work that should have been\ncompleted to meet the goal.\nPIN Seepolicy ignorant node .\nplatform computing\nload share facility Seeload sharing facility .\npolicy agent A component of the Solaris Bandwidth Manager (SBM) software that\nimplements the configuration and handles communication with the kernel\nmodule .\npolicy control The application of rules to determine whether or not access to a particular\nresource should be granted.\npolicy decision point (PDP) In policy administration, the point where policy decisions are made.\npolicy element A subdivision of policy objects. A policy element contains single units of\ninformation necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. The policy elements themselves areexpected to be independent of which quality of service (QoS) signalingprotocol is used.\npolicy enforcement\npoint (PEP) In policy administration, the point where policy decisions are enforced.\n\n206      Capacity Planning for Internet Servicespolicy-ignorant node (PIN) A network element that does not explicitly support policy control using\nthe mechanisms defined in the applicable standard policy.\npolicy object An object that contains policy-related information, such as policy elements , and\nis carried in a request or response related to resource allocation decisions.\npolicy protocol A protocol for communication between the policy decision point (PDP) and\npolicy enforcement point (PEP). The policy protocol can be any combination ofCOPS, Simple Network Management Protocol (SNMP), and Telnet/CLI(command-line interface).\nPOST Power-ON self test; a suite of hardware diagnostic tests that ensure full\nfunctionality of a system board.\npreemptive\nscheduling A method of scheduling where a high-priority job can bump a lower priority\njob that is currently running. The load sharing facility (LSF) provides severalresource controls to prioritize the order in which batch jobs are run. Batch jobscan be scheduled to run on a first come, first served basis, fair-sharing betweenall batch jobs, and preemptive scheduling.\npriority A relative importance level that can be given to the work done by a system as\npart of a policy that prioritizes some activities over others.\npriority decay Seeprocess priority decay .\npriority paging A method of implementing a memory policy with different importance factors\nfor different memory types. Application memory is allocated at a higherpriority than file system memory, which prevents the file system from stealingmemory from other applications. Priority paging is implemented in the Solaris7 operating environment (OE).\nprocess\nmeasurements Measurements that show the activity of each user and each application.\nprocess memory Memory allocated to processes and applications.\nProcess Monitor An optional module within Sun Enterprise SyMON that can be used to view all\nthe processes on a system. The Process Monitor can also be configured topattern-match and accumulate all the processes that make up a workload.\nprocessor reservation A method that allows job slots to be reserved for a parallel job until enough\nslots are available to start the job. When a job slot is reserved for a job, it isunavailable to other jobs. Processor reservation helps to ensure that largeparallel jobs are able to run without underutilizing resources.\nprocessor set The set of processors available to a system.\nprocess priority decay A process decay method used by the Solaris Resource Manager (SRM)\nsoftware, where each process\u2019 priority is decayed according to a fixed decayfactor at regular intervals (each second).\n\nGlossary 207Project StoreX A technology being developed at Sun to address modern storage issues.\nStorage is now open for access in a heterogeneous multivendor environment,where multiple server and storage vendors can all be connected over thestorage area network (SAN). This is an emerging technology, and tools tomanage a SAN are still being developed. Project StoreX is based on adistributed pure Java framework that can run on servers from any vendor,interface to other storage management software, and manage any kind ofattached storage.\nprovider domain When relocating resources between dynamic system domains (DSDs), a\n\u201cprovider domain\u201d is the domain where a system board gets logically detachedfrom to then have it attached to a \u201creceptor domain.\u201d\nprovider DSD Dynamic reconfiguration (DR) on the Starfire allows the logical detachment of\na system board from a provider dynamic system domain, or DSD (the DSDfrom which resources are borrowed) and the logical attachment of the samesystem board to a receptor DSD (the DSD where loaned resources are applied).\nproxy cache A method of caching Web pages. A proxy caching Web server sits between a\nlarge number of users and the Internet, funneling all activity through thecache. Proxy caches are used in corporate intranets and at Internet serviceproviders (ISPs). When all the users are active at once, regardless of where theyare connecting to, the proxy cache server will get very busy\nPR/SM\u2122 Processor Resource/Systems Manager; an IBM S/390 hardware feature that\nallows customers to statically allocate processor and I/O resources to LogicalPartitions (LPARs) to concurrently run multiple operating system instances onthe same machine.\nQoS Seequality of service .\nquality of service (QoS) A measure of the speed and reliability of a service. The Solaris\nBandwidth Manager (SBM) software provides the means to manage yournetwork resources to provide QoS to network users. QoS is a network-wideissue;\nif congestion takes place anywhere on the network, it affects the\noverall QoS.\nRAS reliability, accessibility, and serviceability.\nreceptor domain When relocating resources between dynamic system domains (DSDs), a\n\u201creceptor domain\u201d is the domain that receives a system board after having itlogically detached from a \u201cprovider domain.\u201d\nreceptor DSD Dynamic reconfiguration (DR) on the Starfire allows the logical detachment of\na system board from a provider dynamic system domain, or DSD (the DSDfrom which resources are borrowed) and the logical attachment of the samesystem board to a receptor DSD (the DSD where loaned resources are applied).\nrepository access\nprotocol The protocol used to communicate between a policy repository and the\nrepository client. LDAP is one example of a repository access protocol.\n\n208      Capacity Planning for Internet ServicesResource Management\nFacility (RMF) A component of the workload manager (WLM) that tracks metrics\nincluding progress against goals.\nRMF SeeResource Management Facility .\nRSVP A protocol (part of the Int-Serv framework) that provides applications the\nability to have multiple levels of quality of service (QoS) when delivering dataacross the network. RSVP provides a way for an application to communicateits desired level of service to the network components. It requires each hopfrom end to end to be RSVP-enabled, including the application itself (throughan application programming interface, or API). Bandwidth is reserved at eachhop along the way before transmitting begins, guaranteeing that enoughresources will be available for the duration of the connection.\nSAN Seestorage area network .\nscheduler A component of the Solaris Resource Manager (SRM) software that schedules\nusers and applications.\nscheduler term The period of time during which the Solaris Resource Manager (SRM) software\nensures that a particular user or application receives its fair share of resources.\nsecurity policy A type of policy that aims at preventing access to certain resources or allowing\ndesignated users to manage subsystems. For example, Sun Enterprise SyMON2.0 software includes access control lists for operations that change the state ofa system, and multiple network domain views to give different administrativeroles their own view of the resources being managed.\nserver consolidation A current trend by data centers to reduce the cost of server ownership by\nreducing the physical footprint and reducing the number and managementcost of multivendor platforms. The basis of server consolidation is to combineapplications and data contained in several smaller servers into a single largerserver.\nservice class A class that defines a set of goals, together with periods, duration, and\nimportance. A number of individual processes and CICS/IMS (customerinformation control system/information management system) transactions canbe assigned membership to a service class. They will then become subject tospecified goals and constraints, including those imposed by any resourcegroup subscribed to by the class. In essence, this is analogous to the SolarisResource Manager (SRM) lnode, which effectively defines a resourcemanagement policy that can be subscribed to.\nService level\nagreement (SLA) A written agreement between system managers and end-users that\ncaptures the expectations and interactions between end-users, systemmanagers, vendors, and computer systems. Often, many additionalinteractions and assumptions are not captured formally.\n\nGlossary 209Service level\nmanagement (SLM) The process by which information technology (IT) infrastructure is\nplanned, designed, and implemented to provide the levels of functionality,performance, and availability required to meet business or organizationaldemands.\nservice provider In a network policy, the service provider controls the network infrastructure\nand may be responsible for the charging and accounting of services.\nservice time The time it takes for an I/O device to service a request. This can be complex to\nmeasure. For example, with today\u2019s disk storage systems, the device driverissues a request, that request is queued internally by the RAID controller andthe disk drive, and several more requests can be sent before the first one comesback. The service time, as measured by the device driver, varies according tothe load level and queue length and is not directly comparable to the old-styleservice time of a simple disk drive.\nSE Toolkit A toolkit that can be used to develop customized process monitors. The Solaris\nsoftware can provide a great deal of per-process information that is notcollected and displayed by the pscommand or Sun Enterprise SyMON 2.0\nsoftware. The data can be viewed and processed by a custom written processmonitor. You could write one from scratch or use the experimental scriptsprovided as part of the SE Toolkit. The SE Toolkit is a freely available butunsupported product for Solaris systems. It can be downloaded from\nhttp://www .sun.com/sun-on-net/performance/se3.\nSEVM Sun Enterprise Volume Manager; technically equivalent to the Veritas Volume\nManager (VxVM).\nShareII A resource management product from Softway. The Solaris Resource Manager\n(SRM) software is based on ShareII.\nshared resources A resource that is not tied to a specific host, but is associated with the entire\ncluster, or a specific subset of hosts within the cluster. Examples of sharedresources include: floating licenses for software packages, disk space on a fileserver which is mounted by several machines, and the physical networkconnecting the hosts.\nSHR Scheduler A component of the Solaris Resource Manager (SRM) that controls CPU\nresources. Users are dynamically allocated CPU time in proportion to thenumber of shares they possess (analogous to shares in a company), and ininverse proportion to their recent usage. The important feature of the SHRScheduler is that while it manages the scheduling of individual threads, it alsoportions CPU resources between users.\nSimple Network\nManagement Protocol (SNMP) An open network protocol used by network management systems that\nare based on TCP/IP .\nSLA Seeservice level agreement .\nSNIA Storage Network Industry Association.\n\n210      Capacity Planning for Internet ServicesSNMP SeeSimple Network Management Protocol .\nSolaris Bandwidth\nManager (SBM) A product from Sun that provides the means to manage your network\nresources to provide quality of service (QoS) to network users. It allowsnetwork traffic to be allocated to separate classes of service (CoS) so thaturgent traffic gets higher priority than less important traffic. Different CoS canbe guaranteed a portion of the network bandwidth, leading to morepredictable network loads and overall system behavior. Service levelagreements (SLAs) can be defined and translated into SBM software controlsand policies. Tools and application programming interfaces (APIs) provide aninterface for monitoring, billing, and accounting options.\nSolaris Management\nConsole (SMC) An application that provides a generic framework for gathering\ntogether operating system administration tools and interfacing to industry-standard initiatives such as the Web-based management initiative (WebM) andthe common information model (CIM).\nSolaris Resource\nManager (SRM) A software tool for enabling resource availability for users, groups, and\napplications. The SRM software provides the ability to allocate and controlmajor system resources such as CPU, virtual memory, and number ofprocesses. The SRM software is the key enabler for server consolidation andincreased system resource utilization.\nSolstice SunNet\nManager Computer-oriented local and wide area networks are normally managed using\nSimple Network Management Protocol (SNMP), with Solstice SunNet Manageror HP OpenView products collecting and displaying the data. Both productsprovide some visibility into what is happening in the computer systems on thenetwork, but they are focused on network topology. Resource management isdone on a per-network basis, often by controlling the priority of data flowsthrough intelligent routers and switches.\nSPARCcluster A highly integrated product line that is focused on improved availability in\ncommercial environments. Its management tools will eventually become anintegrated extension to the Sun Enterprise SyMON2.0 software. For High-Performance Computing (HPC), Sun HPC servers use the platform computingload sharing facility (LSF) to perform load balancing on much larger and moreloosely coupled clusters.\nSRM SeeSolaris Resource Manager .\nSRM(IBM) This is the\nSystem Resource Manager of the Workload Manager (WLM).\nThe term SRM(IBM) is used in this book to differentiate it from SolarisResource Manager. SRM(IBM) provides the algorithms for managingresources and caters to dynamic switching between compatibility and goalmodes.\n\nGlossary 211SSP System service processor; Starfire\u2019s system administrator and system\nmonitoring interface. The SSP configures the Starfire hardware through aprivate Ethernet link to create domains. The SSP collects hardware logs,provides boot functions, and produces consoles for each domain.\nStarfire SeeSun Enterprise 10000 .\nstatic resources Host information that does not change over time, such as the maximum\nrandom access memory (RAM) available to processes running on the host.\nStorage area network (SAN) A complex managed storage system, where networked storage using\nFibre Channel makes up an interconnection layer between multiple servers orclusters and multiple storage subsystems. A SAN can contain switches androuters just like local or wide area networks, but the protocol in common use isSCSI over Fibre Channel rather than IP over Ethernet. A SAN may also spanmultiple sites\u2014for example, where remote mirroring is being used for disasterrecovery.\nStoreX A technology developed at Sun that enables management of any storage\nresource in a heterogeneous distributed environment, from storage hardwarelike devices and switches, to storage software like backup solutions andvolume managers. For more information about StoreX, go to:\nhttp://www .sun.com/storage/stor ex/.\nsubmission host In a typical load sharing facility (LSF) workload configuration, the submission\nhost is the node where the user or operator submits the task to be performed.\nSun Enterprise 10000 A highly scalable 64-processor (UltraSparc II) SMP server with up to 64 GB of\nmemory and over 20 TB of disk space.\nSun Enterprise SyMON\n2.0 A product developed by Sun to act as a user interface to hardware features. It\nis a powerful and extensible system and network monitoring platform that isused to manage other products. Sun Enterprise SyMON 2.0 is a Java-basedmonitor with multiple user consoles that can monitor multiple systems usingthe secure extensions to Simple Network Management Protocol version 2(SNMPv2) to communicate over the network.\nSun Management\nCenter (SunMC) A product developed by Sun to act as a user interface to hardware\nfeatures (an upgrade from Sun Enterprise SyMON 2.0). It is a powerful andextensible system and network monitoring platform that can be used tomanage other products. SunMC is a Java-based monitor with multiple userconsoles that can monitor multiple systems using the secure extensions toSimple Network Management Protocol version 2 (SNMPv2) to communicateover the network.\nSunMC SeeSun Management Center.\nSunNet Manager SeeSolstice SunNet Manager .\nSyMON SeeSun Enterprise SyMON 2.0 .\n\n212      Capacity Planning for Internet ServicesSyMON Health\nMonitor A SyMON module that can be used in a resource management scenario to\ndetermine if a system has enough resources to run comfortably. For example, ifthe CPU state is reported as \u201cred,\u201d then either less work or more CPU powermay be needed on that system. Similarly, if the memory rule reports \u201cred,\u201dthen the system may need more memory.\nsystem level\nmeasurements A type of measurement. System level measurements show the basic activity\nand utilization of the memory system and CPUs. Some network measurementssuch as TCP/IP throughput are also available on a per-system basis. Per-process activity can be aggregated at a per-system level, then combined withnetwork measurements to measure distributed applications.\nTeamquest A workload analysis product. For more information, go to\nwww .teamquest.com.\ntime constant In control theory, the rate at which a system responds to changes.\nTNF Seetrace normal form .\nToS Seetype of service .\ntrace normal form (TNF) A format used to implement tracing (which makes it possible to trace\nthe execution steps of user and kernel processes). Trace normal form, which issupported by the Solaris Operating Environment (Solaris OE), provides a self-describing trace output format. TNF allows data structures to be embedded inthe trace file without the need for an external definition of their types andcontents.\ntype of service (ToS) A header field contained in IP packets. Its purpose is to convey\ninformation about how the packet should be routed. The Solaris BandwidthManager (SBM) software can use this information when classifying a packet. Itcan also change the information, to influence how the packet is routed.\nUDB DB2 Universal Database.\nusage decay A form of decay used by Solaris Resource Manager (SRM) software. The user\nscheduler is the most important and visible portion of the SRM software and itimplements usage decays which control long-term CPU allocationresponsiveness.\nvirtual memory A type of memory that is allocated from a central resource pool and is\nconsumed by an application when it requests memory from the operatingsystem. Virtual memory is not directly related to physical memory usage,because virtual memory is not always associated with physical memory. Forexample, if an application requests 16 MB from the operating system, theoperating system will create 16 MB of memory within that application\u2019saddress space, but will not allocate physical memory to it until that memory isread from or written to.\n\nGlossary 213virtual Web hosting A Web server configuration where a single server is configured to respond to\nhundreds or thousands of Internet addresses. Virtual Web hosting is often usedin situations where Web sites receive little or no activity most of the time. Inthese situations, it is usually too expensive to dedicate a single computersystem to each Web site.\nWAN Seewide area network .\nWebM Web-based management initiative.\nwide area network (WAN) A network that provides connectivity across a large geographical area.\nWLM SeeIBM Workload Manager .\nWorkload Manager SeeIBM Workload Manager .\nXCF Seecross-system coupling facility .",
        "summary": "Quick planning techniques for high growth rates. Success is measured by growth in the number of pages viewed,registered users, and in some cases, by the amount of business transacted. Growth rates for successful si. nies spend most of their investors\u2019 funds on advertising as they attempt to establish their name in the collective consciousness of consumers. Established companies are concerned about maintaining theirpreexisting brand image. Capacity planning is an optimization process. Service level requirements can be predicted and balanced against their costs. This Sun BluePrints book charts a course through the available techniques and tools. The topics covered in this book can be divided into the following sections:Principles and processes. Scenario planning techniques. The effective use of tools. This chapter presents methods for managing performance and establishing servicelevel agreements. It also examines the IT frameworks designed to providesolutions for business requirements. Additional ITframeworks from ISO FCAPS are presented, and tips are offered for implementation. performance management is the measurement, analysis, and optimization ofcomputer resources to provide an agreed-upon level of service to the end-user. Bydefining performance management and identifying the key components required tosafely and accurately implement performance management in the datacenter, youcan minimize the risks associated with high growth rates. Resourceutilization and resource utilization planning are the cornerstones of capacityplanning. Utilization is a measure of system resource impact, throughput defines thequantity of services, and latency defines the quality of the services being provided. Several layers of resources and resource consumption can be defined, tuned, and measured. The Business layer often provides the most significant opportunities for \u201ctuning\u2019 The Applicationlayer and Hardware layer can also provide a significant and obvious impact on the overall performance of the architecture. The Operating System and Infrastructure layer are often where administrators look for some magic cure. Performance management can be applied in an iterative, cyclic, four-phase approach. The output of each phase is used as input to the following phase. Each phase mustreach a steady state in which the results can be published and transferred to thefollowing phase. Once the four phases are locked in place and all results have beenpublished, those results are fed into the next generation of phases. Historical revisions should be retained for future examination and change analysis. Baselining creates a snapshot of a system as it currently exists and generates reports that describe the system performance. An accurate representation including hardware, software, and operating systemversions is critical to creating an accurate inventory. This configuration inventory isconsidered \u201clocked down\u201d for the life span of the baselining process. Load planning accounts for changes or new demands on the system. Any available data describing the changes and new demands is collected and analyzed. The old snapshot isretained as a historical record of configuration. Load Planning is a cyclical process that comprises three activities: load assessment, load data gathering, and load profiling. Each of these activities may need to berepeated multiple times before valid assumptions can be made about the datacollected. System capacity and resource consumption are modelled, calibrated, and validated against the current baseline data. The model compares the relative resourceconsumptions (measured during baselining activities) measured during the load-gathering phase. Capacity planning produces a new system and service configuration architecture. The relationship betweenresource consumption and system architecture capabilities creates the workloadcharacterization used to model and predict the effect of changes. The operating environment can be adjusted as necessary to meet the established performance requirements defined in the SLAand KPI. Resource management defines the controls, shares, quotas, and priorities necessary to manage the workload. System resource planning also provides additional input for creating the new baseline of systemperformance. Service Level Agreements (SLA) outline the level of availability of mission-critical services between the IT department and end-users. f load planning and capacity planning to improve model accuracy. The SLA is really just a list of expectations for a given service. It defines the transactional capabilities, networklatency projections, availability requirements, maintenance windows, escalationprocedures. ld take in theanalysis of the end-users\u2019 computing resource requirements, capabilities of the ITarchitecture, and formulation of the agreement. SLAs should be used as a basis of understanding between all parties involved. They should define key roles,responsibilities, and expectations that have been discussed and agreed upon by all parties. The SLA should be reviewed and modified regularly. Service Level Agreements (SLAs) can be difficult to work out. They can result in many unpleasantries, not the least of which are the perception of poor service. SLAs can improve the business process, affect the bottom line, andimprove availability. Long-term benefits of SLAs include increased end-user satisfaction, more accurate resource accounting, and more efficientprioritization. SLAs are not more widely used for a number of reasons. The following issues are the most common downfalls. A well-conveyed SLA can help develop higher serviceavailability and greater end-user satisfaction. With the SLA, anIT manager can assign personnel to requirements more effectively. preferable control over IT budgets and increased accuracy inlong-term budgeting can also be direct benefits of SLAs. Production Environment Engineering is the product of careful planning and design. This section addresses the needs of the datacenter production environment by using the ISO FCAPS model, the ITIL (information technology infrastructure library)framework, and the SunReady roadmap to production. This discussion provides an encompassing view of the infrastructure requirements and timelines necessary to successfully launch, manage, and maintain a new datacenter service or refresh an existing one. The dot-com startup Internet business has a related challenge. While their businessmodel relies on exponential growth for survival, they often lack the datacenter policies, procedures, andexperience. By learning from the cumulative experience of their more traditional predecessors, the Dot-com companies can implement best practices for the production environment. Some of the categories defined in the framework are company-wide, some are system-specific, and others are application-specific. Forexample, the security group could manage security policy and administration for theentire company, while a facilities management resource could be designated toseveral disparate datacenters. The FCAPS High-Level IT Framework. re defined five categories of.managed objects, covering the basic categories of enterprise support operations. These categories can be applied to network management systems, which is theintended purpose, or they can be expanded to include the skills, tools, and.capabilities in datacenter operations and management. An ITEF can be further divided to establish functional responsibilities and toprovide more detailed information about the services the ITEF offers. Several levels of criticality can be assigned to thecategories of IT services. FCAPS IT Extended Framework defines an ITEF based on FCAPS. This ITEF is bestrepresented in a tree structure that is viewable as a series of Web pages. The IT service coverage document should include a general statement describing the category of coverage. It should also include statements describing the individual servicesand areas of authority in the IT environment. The servi The change management system is the vehicle for recording ,authorizing ,monitoring , andreporting all changes to the open systems environment. By establishingcategories of coverage, the services provided by the IT department are well-defined, whichallows the establishment of enforcement and auditing policies. The IT service model (ITSM) introduces levels of criticality, levels of service, and \u201cbig rules\u201d to the ITEF. This multilevel approach extends the natural drill-downstructure for IT services. The ITSM can be implemented across platforms and used to create service leveldescriptions. The following is a sample of a service level description for user data backup. It should be modified to adhere to local datacenter policies. The Central Computing and Telecommunication Agency (CCTA) of the UK began publishing volumes of their established best practices for IT operations in 1978. The ITIL has evolved over the years, being studied, adopted, and adapted by many IT organizations. The ITIL establishes policies, not implementations. The ITEF can then beimplemented according to service levels, operating system, hardware, andapplication capabilities. The FCAPS framework was originally generated from the ITIL. The Sun Professional Services team has developed a project methodology for applying the FCAPSframework. This methodology creates a unified extended framework for IT operations. The implementation of supplemental service capabilities shadows the applicationdevelopment life cycle from the architecture phase to the pilot, development,integration, deployment, and sustaining phases. The IT service capabilities can thenbe tested and assessed simultaneously with the application\u2019s functional, stress,crash, and disaster recovery test cycles. Service level management is the key to maintaining the production environment. By establishing the rules of engagement, the capabilities necessary to support the service level objectives, the entire production infrastructure can be monitored and measured. The service desk controls information, brokers requests, audits request completion, andprovides service level reporting services. The service desk also brokers action items for the support organizations, bothinternal and external. With the dispatch of support activities controlled, theaffected end-user service levels can be monitored. Measurement Principles: Anything that is not measured is out of control. Measure the business as well as the computer systems and people. Measure and record trends in performance, cost, and availability. A system should be built to help the IT staff log arbitrary data into performancedatabases. These databases should have summariesand graph plotting that anyone can use. Parameterize each component separately and combine them into anavailability hierarchy. Production environment engineering and performance management may appear tobe obscure forms of art. By studying the collective experiences of the traditionaldatacenter community and applying proven best practices, the art can be converted into a well-understood science. The next chapter provides a methodology for service level management and looks at some problems of capacity estimation across multiple platforms. The next chapter looks at what actions are necessary to keep the modern open-system datacenter well-organized and well-managed as the best of the traditional mainframepredecessors. Measuring and reporting key performance criteria in an applicationdevelopment environment can help set release time expectations for performanceand capacity. These practices can also help you recognize features and identifychanges of performance implications of the application being developed. Use a strict change control process, and schedule batches of changes to manage risk. Arrange for the appropriate staff to be on site during change periods, to concentrate fire-fighting efforts. Service level management is a synthesis of traditional datacentermonitoring and the end-to-end availability of the service. The goal is to eventually be able to measure service performance from an end-user's perspective. Service Level Agreements (SLAs) are negotiated between consumers and providers of ITservices. There could be multiple SLAs between a single business unit and multipleservice providers. Some providers can also have SLAs in place with other providers. The SLA should not be mired in legalese; rather, it should be easily understood by anyone familiar with the service and the specified area of business. The SLAshould be implemented as part of the system runbook and maintained in an areaaccessible to both the business unit using the service. Service definition \u2013 Define the service being covered, location of the service (both servers and end-users), number of users, transaction volume expected,and service capacity. Determine coverage and schedules for the service. The SLA should be unique, based on the service being provided and the n. thod by which the KBIs are measured, and track compliance with the service definition. The process of defining these agreements requires tact and IT skills. An SLA is beneficial to both parties, providing resource consumption and service sizing information. Service Level Management is the process of defining a service. The goal is to include the lowest possible latency and maximum supported transaction rate. The service definition should be:Meaningful, Understandable, Cost-effective, Attainable. The SLA should be cost-effective for both the end-users and the IT department, while still meeting the demands of the business. The goals should be attainable, based on testing and historic performance. Service specifications should be based on the needs of the users and the performance data acquired from previous years (ifavailable) Business hours are defined as 0600 EST to 2000 EST, Monday through Sunday. Maintenance windows are defined from 2030 EST until 0530 EST. No transactions are permitted during these maintenance windows. In the event of a \u201cdisaster\u201d requiring the recovery of data or the restoration ofsignificant portions of the system, the help desk will notify the users of the event. Regular updates will be provided every 30 minutes until theservice is restored. The system shall maintain functional availability of 99.8% of business hours. Outages are periods during defined business hours that fall outside emergency maintenance windows. Planned or scheduled emergency maintenance is not included in outage calculations. The order processing system will sustain a transaction of 30,000 transactions per hour during business hours. 95% of all order transactions will be completed in less than 5 seconds (on average) with up to 300 users online. The system architecture, both hardware and software, corresponds directly to the impact on the service. Benchmarking of the application should be performed to identify the limits of the server\u2019s capabilities. Most IT departments choose not to include KPIs in the actual SLA. KPIs are intended for the IT department personneldirectly involved in the management of the server system. Commercial monitoring tools like SunMC, BMC Patrol, BMC Best/1, andTeamQuest can be configured to measure, monitor, and report on performance status. Most can then be integrated intoenterprise management tools like Solstice Enterprise Manager\u2122 and HP\u2019sOpenView. Table 3-2 lists example tool definitions that correspond to the sample thresholds in Table 3-1. Some metrics are vendor- or release-dependent, so multiple definitions need to be maintained. A KPI document can also include actions for immediate relief or resolution ofperformance events. These actions might include stopping, rescheduling, orprioritizing running batch jobs. The procedures should be tested and updated as necessary. Severity of problems can range anywhere from a single user having slow response to a global outage. To allocate resources appropriately, severity levels should be bedefined. A simple call sheet may suffice for the contact information. The contact information for different components of theproduction environment should be documented in the system runbook. This includes how to contact the help desk itself, owners of different categories of problems, and Vendors responsible for different parts of the environment. Problems should be reported to the IT help desk by phone to (606) 555-5300, by email to help@helpdesk, or by fax to (619) 515-5301. Delays in the resolution of a problem can also result in an escalation in severity. The escalation path and events causing theescalation should be well-documented in the system runbook. The help desk will assign a severity level based on the problem description and will contact the appropriate IT personnel. The IT manager for the service will be notified of the problem every 60minutes, until the problem is resolved. Regular reports should be made available to the end-users, business unitmanagement, and IT management. Reports should indicate how well the IT department is doing with meeting the service description. Meetings should be held monthly or weekly to determine any changes that need to be made. Identify problems that arose during the previous week and possiblerecommendations for ways to improve those problems. A sample weekly report is described in \u201cManagement Viewpoint Implementation\u2019 on page 135. It is much better to resolve questionsearly on in the SLA, rather than later through arbitration. For the end-user department to hire the talent needed, the department would need to spend approximately $480,000 a year. The datacenter environment, rather than the standalone environment, provides higher quality, sharable, standardized resources. By identifying the projects on which engineers are spending the most time, the IT manager can identify recurring problems, find ways to rectify those problems. This increases the ability of the IT staff to addressproblem resolution. It also increases the leverage that the support organizations canwield over vendors. In some IT departments, the IT department staff\u2019s bonuses are directly tied to meetingor exceeding stated service level goals. Failure to meet stated goals results in discounted or free services to the end-users. Arbitration need not be an outside organization, though it very well can be. It can be a person or a panel that can review the problems and reach a final, binding solution. The managers and representatives of the service for both the IT department and theend-users should meet regularly to review the existing SLA and to propose changesto the agreement. Either party may propose changes, but both parties must approve the changes. A complete TCO analysis is a long and involved process, but ca. n the systemrunbook. The system runbook should also contain the point of contact in one of theinvolved organizations. The computing needs for the business function must also be quantified in some way. This quantification can resolve to individual business functions such aspayroll and marketing data or can be further subdivided into more specific metrics. Baselining the performance of business services is the second step in understandingexactly what is happening in the datacenter. Performance baselines should measureeach business function in terms of transaction type, transactional volume, and wherepossible, average response times for the driving server systems. An analysis of an online sales and distribution system might state that:Shift average transactional volumes for the sample period of 10/1/99through 12/31/99 were measured as 12,000 catalog system lookups perhour. For shift hours measured at 11,500 to 12,500 transactions per hour,catalog system lookup response time averaged 3.2 seconds, with a worst-case query response time of 5.1 seconds. By creating a scalar representation of the performance characteristics of a particular business service, we accomplish two goals. We help applicationdevelopers and database administrators identify bottlenecks and inefficiencies inserver and application software. If there is an industry-standard benchmark that closely imitates your workloadcharacteristics, then use that benchmark as a guideline for platform comparisons. Very few business implementations come at all close to the popularindustry-standard benchmarks. Two examples that we can use as samples of market offerings areBMC (formerly BGS) Best/1 and TeamQuest. Best/1 Performance Console for UNIX has an internal database of hardware andoperating capabilities. Performance of a given system is quantified and can be used as input into the BEST/1 modeller for system sizing. Without strict resource management implementations, the rule ofthumb for target utilization is 70%. With resource management software and policies, target utilizations of 90% or more can be safely projected. The Sun Enterprise 10000 platform supports dynamic system domains (DSDs), allowing a single-server platform to act as several distinct servers. System resources can be migrated manually between system domains, allowing administrators to reallocate idle resources. Solaris OE processor sets allow the administrator to bind applications to a set ofdefined processors. processor sets create hard walls of CPU resource allotment foreach designated workload. Idle CPU resources within a processor set cannot be used by other workloads outside that processor set. Resources are defined as either static orrenewable , depending on their particularcharacteristics. A CPU is one example of a renewable resource because CPUresources are dynamic for any given point in time. There is no finite limit of CPUpower; as a machine runs, new processing power is always available. System platforms, operating systems, applicationsoftware, and the business functions that use them are in a continuous evolutionaryprogression. To properly consolidate server functions and satisfy business requirements, a complete understanding of the enterprise is necessary. Even the hardware and software procurement process is circular. Business groupscontract service levels with capacity planners. Capacity planners define a workloadmetric for the service levels contracted. Systems engineers specify a hardwareplatform that can satisfy the workload metric. The next chapter provides a recipe for successful scenario planning. The next chapter presents amethodology for modelling capacity and load. The last chapter suggests processes for quantifying capacity and consolidating workloads. Trending techniques use a mixture of step functions that track changes in yourconfiguration. This process extracts cyclic variations from your data so that you can see the underlying trends. With this technique, you use analytical models topredict the behavior of complex workloads. A Recipe for Successful Scenario Planning. Successful planning must be based on firm foundations. This recipe provides a step-by-step guide to the process. The model should concentrate o. he same job, show them as asimple replicated set. Be ruthless about paringdown the complexity to a minimum to start with. The bottlenecks will change over time, so they must be listed explicitly each time ascenario is modelled. It may be sufficient to look at just the CPU load on a central database server, or you may also need towatch network capacity or a wider range of systems. The primary bottlenecks appear to be the CPU and disk on the back-end database and the CPU on the searchengines. Disk utilizations were found to be very low on the other systems. Measure service levels and workload intensities. Performance modelling assumes that the workload is essentially constant. One of the main modes occurs when backups are taking place. Fast-growing environments will find that the amount of data to back up rapidly. e back-end server\u2019snetwork interface would be a bottleneck during the update. The utilization levels aredifferent in each mode, so they need to be recorded separately. The initial model should be a crude and oversimplified base for planning futurescenarios. The hardest thing to learn for successful modelling is to let go of thedetails. You can build a very useful model with incomplete and inaccurate data. As long as that mixture remains fairly constant, you can average all transactions into ageneric transaction rate. If the back-end server load level fluctuates a lot because of competing applications, you need to perform a workload breakdown. If this is the primary load on the system, then you can probably getaway with using the overall CPU utilization. You need to collect historical data that spans a time period similar to the period you are trying to predict. Lack of data is no excuse; however, you should start to plan immediately. The rest of the scenario modelling and planning process is illustrated with a simpleparameterized spreadsheet in the next section. This section explains how a spreadsheet can be used to quickly generate estimates of capacity and load. In the next section, a simple spreadsheet-based scenario planning model is explained. create a spreadsheet that estimates the \u201cgrowth factor\u201d from a baseline. If additionalinfluences have an impact on the load, they can be computed in a manner similar to the influences described below and can be combined with the other influences. The growth factor is simply thefactor that the original (baseline) load must be multiplied by to get the estimatedload supported at the time in question. If the baseline load was measured in Januaryand the estimated growth factor for June is 2.0, this means that the load can beexpected to increase two-fold from January to June. The units of measurement cancel each other out. The spreadsheet method outlined here is designed for systems with onlyone significant bottleneck. Systems with multiple bottlenecks need to use morecomplex methods.  spreadsheet model is flexible enough to allow fortailoring the models. More complex formulas can be used, or different formulas canbe used for different time spans. Modifying the spreadsheet so that it better estimates the load and capacity of the systems in question is discussed in\u201cTweaking the Model\u201d on page 74. The number of data points (spreadsheet cells) should also be the same for eachinfluence. It is important that the seasonal variations begin with thebaseline in the correct season. The important data is the prime time daily peak level. You can see that the system never sleeps. There is a predictable pattern that peaks in the evening during weekdays and falls off a little during the weekend. The chart inFIGURE 4-6 shows the peak level day by day. In this case, Monday isshown as the busiest day, with Friday a little lower, and a quieter weekend. Sundaysare often busier than Saturdays, partly because systems located in California arenear the end of the international dateline. Data is taken from 1996 Web site hit rate datacollected in 1996 on www.www .sun.com by Adrian Cockcroft. Analysis of variance, or ANOVA, is used to fit a model to the data. If you have at least a years\u2019 worth of collected data, you can use it to generate your \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0estimates for the monthly variations. Keep in mind that if you have limited historicaldata, it is likely that the perceived seasonal effects will be a combination of theactual seasonal effect and other (potentially large) influences. \u201cTweaking the Model\u201d on page 74 offers guidelines on how tomake the model fit the data. The growth of the Internet combined with a growth in awareness of your site maycause a geometric exponential growth in load levels. This can be easily expressed intwo numbers as a growth factor over a time period. For example, growth might beexpressed as 50% per quarter, or doubling in a year. A spreadsheet computes the growth factor and monthly factors for the coming months with a growth rate and duration provided by a user. ibed here is mathematically referred to as geometric exponential growth. It is usually (somewhat incorrectly) referred to asexponential growth. The load at a given time ( Lt) is computed based only on the time ( t-t0). The value t0is zero if tismeasured in relation to it. In the following table, the values used are G= .22 and L0= 1, which makes the following formula: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Lt= 1.14t. This formula is provided in a capacity planning spreadsheet. There is an underlying growth factor involved in the size of the total Internet marketplace. At one point this was doubling about every 100 days, but recentgrowth rates suggest a doubling period of a year or more. Several large Internet sites are seeing business grow at arate of about 50% every three months. Growth rate over several different time ranges (for instance, January to March,March to May, and May to July) and averaging the results may yield a much betterestimate. tune the growth rate as you obtain more data. The spreadsheet available athttp://www .sun.com/blueprints/tools uses a formula that estimates the marketing reach of a campaign. This formula is appropriate insome circumstances; however, other formulas may better represent a marketingimpact depending on your environment. Most formulas can be easily implementedin a spreadsheet to work with the rest of this procedure. A marketing campaign starts in the fourth month to counteract theexpected seasonal drop in load level. It continues for four months, then thememories fade away and a residual increased level of activity is seen. The short-termand residual gain should be something that the marketing department uses to justify the campaign. In practice, many smaller marketing boosts may be modelled rather than one large boost. modelled in the same way as itsimpact on the business. The campaign is not repeated in the second year; the chart inFIGURE 4-9 shows why it was not needed. The factors that affect capacity can be separately computed in aspreadsheet and then combined to get a combined capacity estimation. Manycapacity measurements can be modelled. The two influences on capacity shownbelow are efficiency variations and capacity increases. Capacity increases include anything that allows increased capacity because ofupgrades to the bottleneck. Large changes in the application or transaction mix need to beaccounted for in the scenario plan. Table 4-6 shows the effects of heavy application tuning on the CPU usage pertransaction in the above example. Although these results may be unrepresentative ofwhat can be achieved in many situations, it is sometimes possible to achieve a four-or five-fold performance improvement. The factors in TABLE 4-6 are plotted in FIGURE 4-11. To counteract the increased load levels, the application is tuned and the hardware is Upgrade. ompared to the untuned one. Database tuning or application software upgrades give the first two gains. Then there is a reversal as a new, more complex user interface is introduced. After a while, several more tuning improvements are made, until after two years, the computer resources used per transaction is reduced to 15% of that at the start. Appendix A, \u201dSun Constant Performance Metrics,\u201d shows some performancenumbers for various machines that may be appropriate for projecting the capacity ofnew hardware. A notional level of 70% capacity utilization was also factored in. The third column shows the CPU utilization in each month. Thisassumes a start point, which for this case is entered into the spreadsheet as 70% busyat month zero. The utilization is calculated by taking into account all the differentgrowth factors explained in the next section. The combination of efficiency and hardware capacity upgrades can be plotted on the same axis to see if the system is capable of sustaining the load. The plot in FIGURE 4-13 shows that the combination of a marketing campaign and theseasonal boost overloads is more effective. If this model is correct, something else needs to be done to sustain this futurebusiness scenario. Either a faster system could be installed earlier, or more efficiency could be squeezed out of the system. Another option is that the work itselfcould be split in some way, possibly by functional or geographical aspects. When manipulating the data, bear in mind that the analysis is based on peak loads, not average loads. If you take measurements atintervals of a few minutes, they can be averaged. As soon as you zoom out toconsidering intervals of an hour or more, you want to know the peak level duringthat hour and the peak hour during the day.  monthly data is based on the peak for the wholemonth, and it could occur at any time during the month. For example, if the weekly peak is a Monday, then the monthly peak could occur on any Monday in themonth. Set the workload growth parameters shown in TABLE 4-3 to get a first approximation of the overall long-term growth rate. Tweak the seasonality. Set up marketing factors. Spreadsheet-based models are a good starting point, but are extremely limited andcan be clumsy to use and extend. Large disk subsystems are expensive and may be the primary bottleneck of yourapplication. alues should be moved to be seasonal. This chapter examines the problems in more detail. By identifying the primary bottleneck, theexamination shows where the system can be expected to run out of headroom first. Solaris 7 OE includes the kstat command, which obtains and processes throughput counters. The priority_paging option is available in the appropriate kernel update patch. The latest SE toolkit is available at http://www .setoolkit.com. Solaris 8 OE changes the memorymanagement subsystem in several ways to make paging much simpler tounderstand. There is no need for priority paging because a moresophisticated scheme is used by default. You should see no scanning, a lot morepage reclaims, and a much higher free memory value reported byvmstat. A system can be modelled as a whole or broken down into individual services to build separate models for different functions. Separate SLAs might exist for system performance during system maintenance tasks, systemperformance during business hours. Resource management schemes, such as processor sets and SRM software, can be used to control the backup and reporting systems to reduce impact on OLTP. At this point in the capacity and performance management cycle, intimateknowledge of the business system being modelled is required to prioritize the workloads. The sample period should reflect the maximum period oftime for which degraded system performance is to be tolerated, with a critical threshold of maximum degradation to be allowed. In an online processing systemthat has direct interaction with the paying customer of the business, the sampleperiod could be as low as one minute. isolated from periods of customer interaction. H processing windows and transactional processing should be avoided where possible. Batch reports, database record purging, system backups,and other batch system maintenance should be scheduled into time windows. One immediate change to workload scheduling can be derived from this graph. There appear to be batch reports running during the system maintenance window. By moving these batch reports outside the defined maintenance window and creating a defined shift for reporting tasks, we can enforce resource management.  utilization of system resources when workloads could otherwise coexist without contention or minimized through the proper use of resource management tools. To illustrate this case, midday reports are allowed between 12:00 p.m. and 1:00 a.m., but all other report generation is assigned to the 6:00p.m.-0:00am window. Isolating the different system processing tasks greatly simplifies the implementationof monitoring tools and system resource data collection. Brian\u2019s method is commonly referred to as the \u201cM-value process,\u201d after thetraditional mainframe performance metric. By creating a series of generic terms todefine work on a computer, it is possible to quantify several statistics. These statistics describe the capacity of the machineplatform, as well as the performance characteristics of a given workload. SCPMs are a real-world estimation of Amdahl\u2019s Law applied to system architectures and operating system versions for cases of regular,predictable, and normal workloads. Occasionally, significant patches may influencethe scalability of an operating system. Amdahl\u2019s Law provides the calculations for predicting the benefit of running aworkload on a scalable multiprocessor environment. Measuring the potential parallelization of your workload provides a more accurate model of the performancecharacteristics. Application of Amdahl\u2019s Law measures CPUarchitectures and presumes that there are no bottlenecks in the disk or networksubsystems. Speed-up is calculated using the time taken to accomplish a defined set ofrepresentative transactions. The speed-up factor that is calculated with a single CPU and two CPUs can be used to calculate the scalability factor ( S) of a given workload. S is the percentage of the workload that is parallelized, and not the finite resource constrained.  scalability of a given machine for a given workload can be predicted with theScalability factor. Scalability factor may be reduced if there is resource contention. Adding CPUswill increase the contention, thereby reducing the percentage gains. Lower scalability factors can influence system architecture decisions toward wider server architectures. With deeper architectures, a direct relationship is establishedbetween the scalability of an application architecture and the potential total cost ofownership (TCO) With lower scalability factors, the progressively lower increase inperformance benefit of adding additional CPUs can outweigh thepotential savings in TCO of managing fewer systems. Larger system architecturesgenerally provide more reliability and serviceability features. The scalability of any particular software on a particular server might displaynongeometric scalability characteristics. For most normal workloads that we havemeasured, the variances from the geometric scalability curves are minimal. Factorsthat might cause a significant variance include:CPU cache/code efficiency. Limits to application software threading. The SCPM capacity planning process applies the basis of the Utilization Law to measure how much of a server\u2019s resources are being consumed during a sample period. Examples of resource contention areas include locking mechanisms forexclusive access to resources.  Processing consumes measurable resources in the serverplatform. An observer of the server during the observed time frame canidentify the amount of time that the server resource component is busy processing tasks. The SCPM process measures the amount of work being done on a machine. By measuring realworkloads and their relative disk I/O, CPU, memory, the process can determine how well a system is performing. You can predict the effective resource consumption of that workload. You can alsoproject resource consumption into other SCPM-rated platforms or modify thevolumes of work being performed within the model. The output of sar can easily be imported into a spreadsheet for further Calculations. The %wio column is not considered for this exercise, because it indicates the time that the CPUs werewaiting, and not processing. We can also project the measured system utilization into the system SCPM value to produce the quanta consumed (QC) over time (see FIGURE 5-9 ). This representationcan sometimes be easier to understand than percentage utilization graphs. If we divide the QC on a given platform by the number of users accomplishing that work, we now have the quanta consumed per user (QC \u00f7User). You can thencompute the total users that a given machine will support: SCPM\u00d7Target Utilization \u00f7(QC\u00f7User) Capacity planning with SCPM values provides a simple method for measuring,reporting, and predicting consumption of CPU resources. The same method can bemodified slightly and applied to disk subsystems. The activity being measured is only related to disk activity, so data generated by tape drive, Network File System (NFS), or CD-ROM activity mustbe \u201ccleaned\u201d before being used for capacity planning. The number of disk activities is measured and reported for total disk activity. The number of disk activities during the measured period can be referenced inrelation to the service levels provided in that sample time. By dividing the diskactivities per second by the transactional rate in transactions per second, wecan compute a metric indicating the disk activity per transaction. The relative disk I/O content for a given workload should remain almost constant. Changes to a workload (new features, new data structures) can change the R value. As the system grows, thisvalue should approach reality and become more accurate as we calibrate themeasured average I/O sizes against the data access I-O size.  err on the side of safety andoverestimate the maximum required disk throughput. The combination of these two trendswould cause the R value to reduce over time. Average CPU Utilization 75%Average Disk Reads per Second 3000Average Disk Writes per Second 1500Total Disk Avtivities per Second 4500Filesystem I/O Size 8KB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0QC (40,000 (Q))  *  (.75) 30,000 QQC / User  /  (300 (Users)  / (50 (Transactions)   600 Q*secS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W /  User (1500 (writes) / (300) 10 IOPS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0S    User  (4500 (I/Os) /   ( 300 (Use) (Use))   10 IOPSRSR   Users (3000 (reads) (3000) (1500) 5 IOPSSRSR (1000 ( Writes) (1000) (500) 10 QQQC   Transactions (30,000) (50) (100) 100 QQ C / Transaction   Transaction (30 During our monthly load planning meeting, the marketing department presented a growth projection of 50% for the next fiscal quarter. The server running the load has an SCPM value of 40,000 Q, and the load projection shows aprocessing requirement of 45,000Q. From these values, it is obvious that the currentserver will not be able to process the projected workload. If we upgrade the serverthrough module count increases, CPU module upgrades, or a \u201cforklift upgrade\u201d System Utilization =QC\u00f7SCPM (Q) 45,000\u00f772,000 =62.5% = 62.5%. The projected disk utilization can be predicted based on the capabilities of the disk subsystem. OLTP servers generally consume disk, CPU,and memory resources at a similar rate. The SCPM process has proved very effective in modellingand predicting workloads and resource consumption for a large percentage of themachines sampled. The SCPM process is very useful in a first pass of a TCO analysis as part of a serverconsolidation effort. The details of how disk measurements are derived are covered in-depth by Adrian Cockcroft. More complex systems do not follow the same rules as simple systems whenit comes to response time, throughput, and utilization. Even the simple systems are not so simple, so we start by looking at measurements available for a single disk. Then observe combinations, and finally show you how to combine the availablemeasurements into capacity estimates. The most important measurement to take on each disk is the average transfer size. For example, a high-end server 18.2 GB FC-AL drive is the Seagate ST318203FC. A standard disk is SCSI-based and has an embedded controller. The disk drive contains a small microprocessor andabout 1 MB of RAM. It can typically handle up to 64 outstanding requests via SCSItagged-command queuing. Large systems have another level of intelligence and buffering in a RAID controller. The service time, as measured by the device driver, varies according to the load leveland queue length. Disks that spin fasterand seek faster have lower (better) service times. If there is always a request waiting, the device is 100% busy. A busy disk can operate more efficiently than a lightly loaded disk. The difference is that the service time is lower for the busy disk, but theresponse time is longer. This difference is because all the requests are present in thequeue at the start. Solaris OE instrumentation measures a two-stage queue. A reador write command is issued to the device driver and sits in the wait queue until theSCSI bus and disk are both ready. When the command is sent to the disk device, itSector 0. iostat -x was introduced in Solaris 2.6 OE, and at the same time, disk slices, tapes, and NFS mountpoints were added to the underlying kernel data source. The problem with iostat is that it tries to report the new measurements in some of the subsystems. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. Theword wait is being used to mean two different things (seeFIGURE 5-15 ). The meaning of service time ( S) is as close as you can get to the old-style disk service time. To calculate it from iostat output, divide the utilization ( U) by the total number of reads and writes. %iostat -xn \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  r/s  w/s, kr/s and kw/s wait actv wsvc_t asvc_t  %w  %b device \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021.9 63.5 1159.1 2662.9 0.7 0.0 31.8 0 93 c3t15d0UwaitWait Queue Utilization iostat %w100Bwait\u00d7 worrisomeThires. iostat reports 31.6 vs. 31.8. The difference is due to rounding errors in the reported values, 2.7 and 85.4. With fullprecision, the result is identical, since this is how iostat calculates the response time. This method uses a theoretical model of response time. It assumes that as youapproach 100% utilization with a constant service time, the response time increasesto infinity. This occurs because the disk is much more complex than the model. When a simple system reaches 100% busy, it has alsoreached its maximum throughput. When the device being monitored is an NFS server, hardwareRAID disk subsystem, or striped volume, the situation is clearly much morecomplex. All of these can process many requests in parallel. As long as a single I/O is being serviced at all times, the utilization is reported by iostat as 100%. However, there is enough capacity for additionalI/Os to be serviced in parallel. Compared to a simple device, the service time is the same, but the queue is being drained more quickly. If several requests can be serviced at once, then when a queuing model is used, the model works as if the averageservice time was reduced by the same factor. So, there is some confusion here over how to handle this case. Cached Disk Subsystem Optimizations can be used to improve performance of complex disk systems. The cache allows many optimizations that combine small accesses into fewer largeraccesses. Some of the common optimizations are: read prefetch clustering and modified cacheblocks written back asynchronously. Multiple writes to the same block often occur when file systemmetadata is updating. Multiple writes to adjacent data are coalesced into a singlelarger write. If the larger write exceeds the underlying disk block size, then thereis no need to read a large block. The disk cache can be inserted into the disk subsystem in two places: in the hostcomputer or in the disk array. The Sun Prestoserve and Sun StorEdge\u2122 Fast WriteCache products add nonvolatile memory to the host computer and use it primarily as a write cache. A very small amount of  the large block back. The capacity modelfor reads is the same as that of a simple striped disk setup. The worst-case performance for writes is limited by the write-backrate. There can be a choice ofpolicies for the cache: it might cache only synchronous data. UltraSCSI runs at close to 40 MB/s for large transfers. This interconnect is used for simple disk packs and the Sun D1000and A1000. The transfer latency for small transfers over UltraSCSI is around 0.6 ms. This section provides an overview of performance factors and capacity planningmodels for complex disk subsystems. It includes RAID5 and cached stripe examples. The capacity model for a single disk is simple and has already been described. When disks are concatenated or the request size is less than the interlace, individual requests will only go to one disk. In practice, writes take a small hitand reads a small benefit, but these are second-order effects that can be ignored for the sake of simplicity. Small requests of 2 to 8kilobytes are used and it is impractical to attempt to make a stripe interlace thissmall. The reduced size of the request on each underlying stripe reduces service time for large transfers, but you need to wait for all disks to complete. The operation of RAID5 is described in detail in Configuration and Capacity Planningfor Sun Servers by Brian Wong. In essence, a parity disk is maintained alongside astripe to provide storage that is protected against failure, without the fullduplication overhead of mirroring. Reading the RAID5 combination performs like a normal stripe. It has similar read and write performance and thethroughput of N-1 disks. The service time for the transfer is reduced as the size ofeach write is reduced. The cache provides fast service time for all writes. Interconnect transfer time is the only component. The cache optimizes RAID5 operations because it allows all writes,whether large or small, to be converted to the optimal full-stripe operations. Write-caching for stripes provides greatly reduced service time. It is worthwhile for small transfers, but large transfers should not be cached. For consistently largertransfers, better performance is obtained without a write cache. The configuration parameters for this system are: number of data disks M= 4 (ignore mirrors) RAID5 parity disks I= 64K (use whole disk size if unstriped) Max SCSI rate Brw = 40000K/s or FC-AL Br=Bw= 100000K/ s. Max disk data rate D= 24500K/S. We use the notation of Mdisks, with a workload concurrency of N. mance varies from 1 to stripe width M, depending upon configuration and workload. The performance factor Pcan vary for read and write, but it does not include cacheeffects. There is an effective increase in concurrency at the disk level because each thread ofaccess in the workload is being subdivided by the disk interlace to cause morethreads of disk accesses. This division allows more performance to be obtained froma disk subsystem, up to the limit of the number of disks in the stripe. This example is based on the same iostat data shown above, but for M= 6 and I= 16K interlace. The change in the interlace would change the data reported by \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0iostat in practice, so the results are not comparable with Example 1. The primary benefit of cache is fast response time. There is also a small overhead from copying data to and from thecache on its way to the disk. You should make the pessimistic assumption thatthroughput is unchanged by the cache. Solid-state disks have H= 1.0 for both reads and writes as data is engulfed by the cache. Other disk types have varying values for H, but characteristic values for E. The measured service time calculated from iostat data is. Smeasured =Sdisk(1 \u2013 H)+H\u00d7Sdisk\u00f7E. Main memory provides the best cache for reads, and NVRAM for writes. The only disadvantage of the NVRam cache is that in clustered,highly available disk subsystems, it cannot be used because it is internal to eachsystem. The next chapter concentrates on the importance of observability requirements for information collection and presentation to three different audiences. This chapter looks in detail at gathering, organizing, andpresenting system performance data to these three audiences. Alert-based monitoring should be combined with high-level problem diagnosis. A single-page daily summary of status and problems is more appropriate. In the following sections, techniques and examples for implementing theseviewpoints are presented. The operations viewpoint is demonstrated with the SunManagement Center (SunMC, a.k.a. Sun Enterprise SyMON\u2122 2.1). The engineeringviewpoint is implemented with SunMC and the SE toolkit to collect additionaldetailed data. The management viewpoint is implemented as a spreadsheet. The steps covered in this section illustrate how to use a system monitor to automate the process of tracking multiple systems for unusual or problematic conditions. With SunMC, basic rules are enabled but you can load additional health monitoring tools. If you have never seen a problem with a particular metric, then set its threshold alittle higher than the highest value you have ever seen. This way, you \u201csurround\u201dthe systems being monitored with a multidimensional behavior envelope. If thebehavior goes beyond that envelope in any dimension, an alarm tells you thatsomething abnormal is happening. The number of alarms that occur at each of the problematic levels (e.g.,yellow and red) should be collected daily so it can be reported to management. The alarm log should bearchived. The number of problems analyzed each day matches the number ofproblems reported, so you are keeping on top of the situation. SunMC is becoming increasingly important as a platform for the management and monitoring of Sun systems. Many third-party tools could be used in this role. They are not Sun specific and can managemany kinds of systems from several vendors. SunMC has third-party vendor support for managing Windows NT-basedsystems, and relational databases for performing generic system administrationfunctions. Contact Halcyon Inc. for more details. SunMC can be used to monitor systems using complex rule-based alerts to diagnoseproblems. SunMC\u2019s SNMP version 2 with user security implementation ismore efficient at bulk transfer of data. The SunMC health monitor is based on a set of complex rule objects. The health monitor rules are based on thoseimplemented by Adrian Cockcroft in the SE Toolkit script virtual_adrian.se. A simple rule can be placed on any system. To load the health monitor module, start SunMC with the default administrativedomain. Select the system, pop up a menu, then select the Load Module option from the menu. The Browser tab of the host Details window shows the modules that are loaded. When it is selected, a secondwindow opens that is specific to the system being monitored. Under Local Applications, you will find the Health Monitor module. Inside that you find the eight rules that are implemented to monitor several system components. Handling Alarms in SunMC can monitor hundreds of systems from a single console. SunMC gives each group a \u201ccloud\u201d icon; or, you can load a backgroundimage to form a map or building plan and then position the icons on the map. When a simple rule or one of the health monitoring rules generates an alarm, it islogged by SunMC. At the domain-level console, the worst alarm state for eachsystem being monitored is counted. This means that with only one system beingmonitored, only one alarm will be indicated. If you double-click the alarm or select the alarm and click the Details...button, the Details window for that system opens with its Alarms tab selected. The next step is to select one or all of the alarms and acknowledge them by clicking the Acknowledge button. It takes some time to perform the acknowledgment, since it involves communicating all the way back to the agent on the server being monitored. The Sun BluePrints book Resource Management shows how to separate and monitormultiple workloads with SunMC. Use this capability to track application processes and define alarms that will tell you when something goes wrong. Almost all commercial performancetools supply data in real time for display in ways that are suitable for an operationsmonitoring display. An example of a useful display is a spreadsheet that reads in data logged to a file using the SE toolkit. Main memory is managed by reclaiming itself once it reaches a low threshold, so it will tend to hover around that threshold. The page residence time is another useful measure, as explained here. System is quiet overnight, gets busy and plateaus during the day, then peaks in theearly evening. CPU and disk plots also show that there is extra system CPU timeduring the night and a higher disk data rate associated with an online backup. There are a few times when the page residence time drops to a low level during thebackup period. This value is calculated by dividing the page scan rate into a systemparameter called the handspread pages. An optional setting for Solaris 7 OE turns on priority paging. Solaris 8 OE uses a completely different algorithm, which allows the file cache to be included in the free memoryvalue. The absolute value of free memory is used directly to see how much capacity is available for use. In Solaris8 OE, scanning only occurs when thesystem is extremely short of memory. The example report shown in FIGURE 6-15 is customized for use by a large Int. atus. The report must becustomized to your own situation, so we describe the construction and reasoningbehind the report in detail. Thereport is designed to be delivered each Tuesday. It covers from Monday of theprevious week to the Monday before the report and predicts whether there isenough site capacity to survive the load expected for the following Monday. The nightly online backup took too long and spilling over into the high-loaddaytime period, where it adversely affected user response times. The schedule for hardware upgrade on 12-May is at risk due to component delivery. The entire capacity planning process is described in more detail in Chapter 4,\u201cScenario Planning.\u201d The output from the scenario planning process includes aschedule of upgrade events that increase site capacity and a schedule of expectedincreases in load. The site ran fairly smoothly the past week. A new record bandwidth of 253.8 Mbit/s was seen on Mondaynight. User-visible problems caused by nightly backup overrunning into daytime operation on 14, 17, and18th. The schedule for hardware upgrade on 12-May is at risk due to component deliveryleadtimes. The upgrade is estimated to give a 22% performance increase, which (ifimplemented the day of the report] would reduce the 71% utilizationto 58%. This section gives a view of events over the previous week or so. It includes both problem counts and a graph of activity from Monday of the previousweek through to about midday the next Tuesday. The first row gives the total number of problems that were unresolved at the end ofeach day. The second row shows the overall status of the day according to the severity ofproblems that occurred at any time during the day. The graph is aligned to the daily problem summary. how many problems were resolved during a day and how many new problems were registered. Problems can occur at a relatively constant ratethroughout the week on average, but analysis and resolution tend to take placeduring the week when all the staff are available. Management Viewpoint Implementation 141Thursday night/Friday morning. might be better to schedule downtime at a different time. Two kinds of Web pages on a site can betested: a standardized page that allows comparisons between Web sites, and a site-specific page. A full Keynote report covers the performance of many components of the Web site. Other vendors in this market are Mercury Interactive and Envive Corp. Asingle measurement is sufficient for the management report. The SunMC product provides data on some of the Sun specific capabilities. It does not include a performance database for long-term historical data storage. For a full and detailed picture of what is happening on a Solaris OE system, additional information generally needs to be collected. The standard system utility sar is already set up through cron to collect data. It just needs to be enabled. The sar data files are collected in the directory /var/adm/sa. The default collection scripts overwrite data after one month. The Sun 100 Mb Ethernethme andqfe and the Gigabit Ethernet gecollect many detailed metrics that are not reported by the netstat utility. It is important to obtain byte count metrics for capacity planning. Web server access logs provide a useful source of performance information. The overhead of collecting data from the system utilities is low as long as the collection interval is set to areasonable level, such as 30 seconds. The best organization on disk is to build a collection script that maintains a dailydirectory that contains data files for each tool for each hour. The best starting point is the orcollator.se script, which is in use at many Sun based Internet sites. The disadvantage of using the SE toolkit is that it is unsupported free code. You must devote some engineering resources to maintaining the SE scripts yourself. SAS/ITSV has very good graphical and data analysis capabilities. Some Sun/Solaris OE data can be imported fromorcollator data files to MXG for analysis. Sun\u2019s own tools are currently focused on system managem. This chapter describes several useful Sun and third-party tools for capacityplanning. It also offers several ways to balance peak load both within and across systems. Over time, more of these functions are likely to be integratedinto Sun\u2019s product line. By analyzing the resource requirements of multiple applications on distinct servers,you may be able to consolidate those services on fewer machines. eavor for IT departments to decreasethe datacenter space, simplify management, increase reliability, and decrease costs. Solaris Resource Manager (DR) in the Sun Enterprise 10000 server provides discrete \u2018hard\u2019 partitioning ofdomains. DR also enables you to move a systemboard containing memory and CPUs from one domain to another. This tool can be extremely useful when you combine mixed workload applications on a single server. CPU and virtualmemory to specific applications or users within a single image of Solaris OE. Theseresource allocations act as a form of \u201csoft\u201d partitioning. The SRM software enables the administrator to establish resource usage policies for the CPU and virtual memory resources. Interactive users are alloted an 80% share and the batchprocesses a 10% share of the resources during working hours. Solaris Bandwidth Manager (SBM) software can be used to give each site itsown equal share. Poorly written cgi-binscripts and searches from a single Web site can saturate the server and affect the other sites. The SBM software provides the framework for quality of service (QoS) guarantees and SLAsfor critical networked applications. It allows the administrator to establish controls for the amount ofbandwidth that applications, users, and departments are allowed to use. This can be useful in corporate intranets and can also be used to constrain denial-of-service attacks on the network without shutting off access completely. load sharing facility (LSF) from Platform Computing Corporation is a tool that takes advantage of idle CPU cycles on a network to assist with batch processes. LSF enables users to submit batch jobs and gives the administrator the capability to set up rule sets to prioritize the requests. There are three roles to which hosts can be assigned within a cluster, and hosts can have more than one role. SunMC can be used to manage hundreds of systems running Solaris 2.5.1, 2.6, 7, and 8 OEs from a single console. This module is free of charge to download fromhttp://www .sun.com/softwar e/sunmanagementcenter/hwds/ and is an add-on solution for SunMC. SunMC Hardware Diagnostic Suite enables you to run comprehensive, data-safe, and non-resource-intensive hardware diagnostics testing in enterprise environments. The testing does not corrupt dataon the system and uses minimal system resources. SunMC modules tie into the SunMC alarm framework so users can executecorrective action scripts or programs that eliminate faults. Scheduled routine hardware validation can replace other maintenancethat requires system downtime. CST fills an important hole in the overall strategy of proactive system management. It provides a macroscopic view of the system configurationchanges caused by failures or upgrades and service patterns over extended periods. The CST agent creates a snapshot of the system configuration and determines the changes since the previous snapshot. It then catalogs the event andchanges in a change log file. The agent also provides a facility for maintaining an electronic log of service events. BMC Best/1 is a real-time performance monitor and prediction tool. Users can view the configuration report and change log for any host on the network. Best/1 is a set of tools for UNIX, PC and Mac computers. It can be used to create, edit and analyse data. The tool is not easy to use. Visualizer is one of the few tools that can analyze trend data in terms of cyclicfluctuations. It implements MASF, which is a sophisticated and automated approach to the problems discussed in Chapter 4, \u201cScenario Planning\u201d Patrol has been criticized for imposing a larger load on the system being managed than other tools. The product has a good underlying architecture that scales well in large, distributedinstallations with multiple data storage points. Foglight Software was initially known as Resolute Software and more recently hasbecome a division of Quest Software. RAPS uses very efficient data collectors and has anadvanced distributed storage architecture and a good GUI. SAS has a wide range of statistical analysis products. MXG Toolset is a notable alternative to the SAS/CPE functions that runs on the basic SAS platform. SES Workbench runs onUNIX systems and NT. SES Strategizer runs only on WindowsNT. rvices capability to assist in capacity planning simulation development. SarCheck is a relatively inexpensive tool that can be very useful to help withdiagnosis in an operations environment. On SolarisOE it reads data from sar andps, then writes a detailed explanation of what it sees, almost like an essay. Performance data is monitoredand recorded in real time, with both reporting and alert features. The data can beviewed live, showing system and workload performance measures, while the data isbeing recorded in the TeamQuest database. The performance data for a previouslyrecorded period can also be viewed in context. Capacity Planning with TeamQuest Model 155. The system being modelled is a database server, with the database instance and twomiddleware applications running on the same system. The Stretch Factor statistic represents the time spent waiting for a resource. A higherstretch factor indicates a higher relative quantity of resource wait time. The standard reports produced include severalinteresting statistics for further analysis. The Active Resource report includes resource consumption statistics such asthroughput, service time, wait time, average queue length, and number of server instances. Stretch factors greater than 2.0 shouldbe analyzed\u2014they indicate a significant resource shortage. The calibrated model, including the new workload growth calculations, can now be exported to create graphs and tables of performance data in Microsoft Excel. The first graph that we will examine is the Stretch Factor by Workload graph. The \u201cDatabase_App2\u201d workload appears to have a response time increase, which indicates a resource contention problem. This is to be expected, with the increase inrelative queue wait time to resource service time represented by the stretch factor. We can further analyze this workload\u2019s behavior by using the Components ofResponse Time graph for the \u201cDatabase_App2\u201d workload. The graph in FIGURE 7-7illustrates the time consumed by a transaction, or unit of work within the workload,with time spent in the workload. This is broken down by active resource service timeand active resource queue delay.  CPU Queue Delay is increasing at an alarming rate and that itaccounts for more than half of the per-transaction time by Step 6. This indicates that the system is on the verge of a serious performance degradation. The CPU resource utilization is projected over 95% average utilization by the time the model reaches the sixth step. Step 1 shows that we are running at approximately 70% utilization, with the active resourceutilization for the measured workloads showing very little resource queue delaytime. At Step 2, the workloads arebeginning to show some contention and CPU queue delay times, with CPUutilization just passing 75%. This trend would indicate that we should target 70% for the KPI maximum CPU utilization threshold. System was a Sun Enterprise 10000 serverwith six 250 MHz UltraSPARC CPUs with 4 MB of e-cache. The chosen solution inthis case is to perform a one-to-one upgrade of the CPUs in the system to400 MHz UltraSpARC CPUs. With this solution, we can return to the TeamQuest Model window, create alternatives based on the current model, and modify the system resources to predict the projectedperformance of the growing workload. The Stretch Factor graph now shows stretch factors under 2.0 in all steps, for all workloads modelled. In addition, the \u201cDatabase_App2\u201d workload displays anincremental increase in delay time per unit of work being done. We can also examine the total system CPU utilization by looking at the resultingspreadsheet table data or by viewing the Active Resource Utilization graph. The CPU and disk active resources are graphed over the current, measured workload, as well as over the five steps of compound growth. The Active Resource Utilization graph shows that disks are becoming rather busy throughout the model. Wecould now return to the model and upgrade the disk subsystems to provide moreresource capabilities or move workload resour. This chapter presented a detailed survey of useful tools for capacity planning anddiscussed the benefits of using both Sun and third-party tools. In addition, this chapter offered several ways to balance peak load both within and across systems. Alternative pathing (AP) works in conjunction with dynamicreconfiguration (DR) to provide redundant disk and network controllers and their respective physical links. The main purpose of AP is to sustain continuous network and disk I/O when system boards are detached from amachine or dynamic system domain. Short jobs are said to backfill processors reserved for largejobs. Short jobs fit into the time slot during \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0which the processors are reserved. The blacklist file is read andprocessed at startup. class-based queuing (CBQ) The underlying queuing technology used in the Solaris BandwidthManager (SBM) software. Cluster A collection of computers interconnected via a high-speed interface. Multiple servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if high availability is important, multiple servers are configured as siblings. A Workload Manager (WLM) component that communicates policies, metrics, and control data between Sysplex nodes. A function, used by dynamic reconfiguration, or DR (called from dr_driver ), provides the ability to attach a particular instance of a driver withoutaffecting other instances. E is a function, used by dynamic reconfiguration, or DR (called from dr_driver ), that provides the ability to detach a board that contains the kernel cage (OBPOpenBoot PROM), kernel, and non-pageable memory. The kernel cage canonly be relocated after all of the drivers throughout the entire dynamic system, or DSD (not just on the board being detached) are quiesced. The goal of this group is to tie together usersand applications with network elements, protocols, and services throughspecific relationships. By complying to this model, different network equipment and application vendors should be able to build interoperable network elements. direct control A means of control that operates on the resource you want to control. Dynamic internal service class created by the Workload Manager. Distributed queuing system. Sun Microsystems technology allows system boards to be added (attached) orremoved (detached) from a single server or domain. A Starfire independent hardware entity formed by the logicalassociation of its system boards. Exclusive scheduling is a type of scheduling used by the load sharing facility (LSF) A job only runs exclusively if it issubmitted to an exclusive queue. An exclusive job runs by itself on a host. LSFdoes not send any other jobs to the host until the exclusive job completes. Fairshare scheduling is an alternative to the default first come, first servedscheduling. Fairshare scheduling divides the processing power of the LSFcluster among users and groups. LSF allows fairshare policies to be defined at the queue level. Gigaplane-XB provides main memory access through a point-to-point data router. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Hostview is a graphical user interface (GUI) program that runs on the system serviceprocessor (SSP) machine. Hostview enables you to monitor and control an Enterprise 10000. SunNetManager or HP OpenView products collecting and displaying the data. IBM WorkloadManager (WLM) provides an automated resourcemanagement environment driven by high-level business goals. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. Intimate shared memory is often the largest component of a database\u2019s memoryrequirements. A JavaBeans based framework for developing and deploying dynamic management based applications. Autonomous agents can be deployed in realtime to perform management tasks for devices on the network. When dynamic reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. A lightly damped system is veryresponsive to sudden changes, but will probably oscillate back and forth. A node in a special resource tree used by the Solaris ResourceManager (SRM) software. The SRM is built around lnodes, which are afundamental addition to Solaris kernel. ln nodes correspond to UNIX UIDs, and may represent individual users, groups of users, applications,and special requirements. High-Performance Computing (HPC) package includes the LSF as a vehicle forlaunching parallel applications on an HPC cluster. In addition to starting batchjobs, the L SF also provides load balancing. The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager (SBM) software. The maximum bandwidth is dependent on thepercentage of bandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. NetFlow allows for detailed network measurements that can be sent to other software pack. Network File System (NFS) An application that uses TCP/IP to provide distributed file services. Sterling Software offers a distributed version of NQS calledNQS/Exec. PC NetLink is a product from Sun Microsystems that is based on the AT&T Advanced Server for UNIX. PC NetLink adds functionality that was not previously available onSolaris servers with products such as Samba. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. A network element that does not explicitly support policy control using mechanisms defined in the applicable standard policy. The policy protocol can be any combination ofCOPS, Simple Network Management Protocol (SNMP), and Telnet/CLI. priority A relative importance level that can be given to the work done by a system. priority decay Seeprocess priority decay. Application memory is allocated at a higherpriority than file system memory. processor reservation is a method that allows job slots to be reserved for a parallel job until enough slots are available to start the job. Processor reservation helps to ensure that large parallel jobs are able to run without underutilizing resources. A \u201cprovider domain\u201d is the domain where a system board gets logically detachedfrom to then have it attached to a \u201creceptor domain.\u201d A proxy caching Web server sits between a large number of users and the Internet, funneling all activity through thecache. The SolarisBandwidth Manager (SBM) software provides the means to manage yournetwork resources to provide QoS to network users. Dynamic reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider dynamic system domain. repository access protocol used to communicate between a policy repository and the repository client. RSVP provides a way for an application to communicateits desired level of service to the network components. Scheduler is a component of the Solaris Resource Manager (SRM) software that schedules users and applications. Security policy aims at preventing access to certain resources or allowing designated users to manage subsystems. Service class is a class that defines a set of goals, together with periods, duration, and importance. A number of individual processes and CICS/IMS transactions can be assigned membership to a service class. They will then become subject tospecified goals and constraints. service provider In a network policy, the service provider controls the network infrastructure and may be responsible for the charging and accounting of services. Service time The time it takes for an I/O device to service a request. The SE Toolkit is a freely available butunsupported product for Solaris systems. It can be downloaded fromhttp://www .sun.com/sun-on-net/performance/se3. The SHRScheduler manages the scheduling of individual threads. It alsoportions CPU resources between users. Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris ManagementConsole (SMC) An application that provides a generic framework for gathering together operating system administration tools. Service levelagreements (SLAs) can be defined and translated into SBM software controls and policies. Resource management is done on a per-network basis, often by controlling the priority of data flow through intelligent routers and switches. For High-Performance Computing (HPC), Sun HPC servers use the platform computingload sharing facility (LSF) to perform load balancing. System service processor; Starfire\u2019s system administrator and systemmonitoring interface. The SSP configures the Starfire hardware through aprivate Ethernet link to create domains. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 GB ofmemory and over 20 TB of disk space. StoreX enables management of any storageresource in a heterogeneous distributed environment. SunMC is a Java-based monitor with multiple userconsoles that can monitor multiple systems using the secure extensions to Simple Network Management Protocol version 2 (SNMPv2) to communicate over the network. System level measurements show the basic activity and utilization of the memory system and CPUs. Per-process activity can be aggregated at a per-system level, then combined withnetwork measurements to measure distributed applications. The Solaris BandwidthManager (SBM) software can use this information when classifying a packet. It can also change the information, to influence how the packet is routed. Virtual memory is not directly related to physical memory usage. Virtual Web hosting is a configuration where a single server is configured to respond to hundreds or thousands of Internet addresses. g system will create 16 MB of memory within that application\u2019saddress space, but will not allocate physical memory to it until that memory isread from or written to.",
        "metadata": {
          "word_count": 48047,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "engineering",
        "hpc",
        "infrastructure",
        "metrics",
        "monitoring",
        "observability",
        "performance",
        "platform",
        "security",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_book_28",
      "kind": "book",
      "subkind": "1,31-65,80-107,117-148,339-356",
      "title": "Resource Management",
      "source": "Sun Press Blueprints  - McDougall, adrianco etc.",
      "published_date": "1999",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/authors/virtual_adrianco/rm.pdf",
      "content": {
        "text": "901 San Antonio Road\nPalo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource Management\nPart No. 805-7268-10\nJuly 1999, Revision ASun Microsystems, Inc.\nSend comments about this document to:  blueprints@sun.com\n\n1CHAPTER1\nIntroduction\nThis chapter introduces the business problems that resource management addresses\nand explains the scope of this book.\nBusiness Problems\nIf you work in a large data center environment, the following questions might befamiliar to you.\n\u25a0The data center is full of systems. Many of those systems are lightly used, andmore applications are always waiting to be brought online. There is no room toadd more systems, and the available systems are getting more powerful. How canwe add a new application to an existing system, without affecting the level ofservice provided to its users?\n\u25a0There are so many small server systems that they are a nightmare to manage.Each server has its own custom setup, and unlike an environment containingdesktop machines, it\u2019s hard to automate a cloned installation process. How canwe combine lots of small servers into a few big ones?\n\u25a0Very large systems are installed in the data center, and many applications sharetheir resources. How can we measure and control these applications to meet aservice level agreement (SLA) that we have with users?\n\u25a0The Solaris operating environment is being installed as a replacement formainframes running MVS. How can mainframe techniques be applied to a UNIXsystem? What is the same and what is new?\n\u25a0Sun provides a base-level operating environment with many facilities and severalunbundled products that extend its capability. How can we tell whatcombinations of products work together to solve our business problems?\nThese are the questions answered by this book.\n\n2Resource ManagementScope\nThe scope of this book can be summarized as:\n\u25a0The best way to use combinations of Sun Microsystems products to manage\nresources.\n\u25a0Generic resource management concepts and methods.\n\u25a0Comparison of UNIX system practices with mainframe class practices.\nSun\u2019s range of products provide a lot of built-in flexibility that can be deployed in\nmany ways to meet the requirements of several markets. The documentation forthese product covers all the options but normally provides only basic informationabout how these product can be used with other products. This book, on the otherhand, focuses on resource management, looks at products that are relevant, givesdetailed and specific information about how to choose the right set of products andfeatures to solve resource management problems.\nTo solve the problem, products must fit into an overall methodology that addresses\nthe processes of workload consolidation and service level management. Theprinciples are explained with reference to tools and products that can helpimplement each scenario. To manage the service level you provide, you must be ableto measure and control the resources consumed by it.\nResource management is an established discipline in the mainframe operations\narena. As Solaris systems are deployed in the data center, mainframe staff mustfigure out how to apply existing management practices to these unfamiliar systems.One of the common complaints voiced by mainframe staff confronted with UNIXsystems is that they don\u2019t have the measurement data they need to do capacityplanning and performance management properly. These techniques are critical partsof a consolidation and resource management process. Thus, this book providesdetailed information on the tools and measurements available for Solaris systems.The Solaris operating environment is one of the best instrumented UNIXimplementations, and it is supported by all the vendors of performance tools.However, not all commercial tools report Solaris-specific metrics.\nResource management for UNIX systems is in its infancy compared to common\npractices under MVS. We are all trying to solve the same set of problems. So overtime, the Solaris operating environment will provide comparable resource andservice level management features.\n\n3CHAPTER2\nService Level Management\nThis chapter describes the overall methodology of service level management so that\nthe resource management component can be put into a wider context. It alsodescribes and compares several approaches to resource management.\nService Level Definitions and\nInteractions\nThis section starts with a high level view of service level management and defines a\nterminology that is based on existing practices.\nComputer systems are used to provide a service to end users. System and\napplication vendors provide a range of components that can be used to construct aservice. System managers are responsible for the quality of this service. A servicemust be available when it is needed and must have acceptable performancecharacteristics.\nService level management is the process by which information technology (IT)\ninfrastructure is planned, designed, and implemented to provide the levels offunctionality, performance, and availability required to meet business ororganizational demands.\nService level management involves interactions between end users, system\nmanagers, vendors and computer systems. A common way to capture some of theseinteractions is with a service level agreement (SLA) between the system managersand the end users. Often, many additional interactions and assumptions are notcaptured formally.\n\n4Resource ManagementService level management interactions are shown in FIGURE 2-1 . Each interaction\nconsists of a service definition combined with a workload definition. There are manykinds of service definitions and many views of the workload. The processesinvolved in Service Level Management include creating service and workloaddefinitions and translating from one definition to another.\nThe workload definition includes a schedule of the work that is run at different times\nof the day (for example, daytime interactive use, overnight batch, backup, andmaintenance periods). For each period, the workload mix is defined in terms ofapplications, transactions, numbers of users, and work rates.\nThe service level definition includes availability and performance for service classes\nthat map to key applications and transactions. Availability is specified as uptimeover a period of time and is often expressed as a percentage (for example 99.95percent per month). Performance may be specified as response time for interactivetransactions or throughput for batch transactions.\nFIGURE 2-1 Service Level Management Interactions\nSystem Level Requirements\nSystem managers first establish a workload definition and the service levelrequirements. The requirements are communicated to vendors, who respond byproposing a system that meets these requirements.\nSizing Estimates\nVendors measure the system performance using generic benchmarks. They may alsowork with system managers to define a customer-specific benchmark test. Vendorsprovide a sizing estimate based on the service level requirements and workloaddefinition. The basis of the sizing estimate can be a published benchmarkperformance. In some cases, the measured performance on a customer-definedVendorsRequirements\nSizing EstimatesSystem\nManagers\nPolicies\nand ControlsMeasurements\nReal User\nExperiencesBusiness\nUsers\nApplication\nSystemService Level\nAggrements\n\nChapter 2 Service Level Management 5benchmark is used as the basis of a sizing estimate. Vendors provide reliability data\nfor system components. They can also provide availability and performanceguarantees for production systems with a defined workload (at a price). Vendorscannot provide unqualified guarantees because typically, many application andenvironmental dependencies are outside their control.\nService Level Agreements\nSystem managers and end users negotiate an SLA that establishes a user-orientedview of the workload mix and the service levels required. This may take the form:95th percentile response time of under two seconds for the new-order transactionwith up to 600 users online during the peak hour. It is important to specify theworkload (in this case the number of users at the peak period), both to providebounds for what the system is expected to do and to be precise about themeasurement interval. Performance measures averaged over shorter intervals willhave higher variance and higher peaks.\nThe agreed-upon service levels could be too demanding or too lax. The system may\nbe quite usable and working well, but still failing an overly demanding service levelagreement. It could also be too slow when performing an operation that is notspecified in the SLA or whose agreed-to service level is too lax. The involved partiesmust agree to a continuous process of updating and refining the SLA.\nReal User Experiences\nThe actual service levels experienced by users with a real workload are subjectivemeasures that are very hard to capture. Often problems occur that affect parts of thesystem not covered explicitly by the service level agreement, or the workload variesfrom that defined in the service level agreement. One of the biggest challenges inperformance management is to obtain measurements that have a good correlationwith the real user experience.\nService Level Measurements\nThe real service levels cannot always be captured directly, but the measurementstaken are believed to be representative of the real user experience. Thesemeasurements are then compared against the service level agreement to determinewhether a problem exists. For example, suppose downtime during the interactiveshift is measured and reported. A problem could occur in the network between theusers and the application system that causes poor service levels from the end-userpoint of view but not from the system point of view. It is much easier to measure\n\n6Resource Managementservice levels inside a backend server system than at the user interface on the client\nsystem, but it is important to be aware of the limitations of such measurements. Atransaction may take place over a wide variety of systems. An order for goods willaffect systems inside and outside the company and across application boundaries.This problem must be carefully considered when the service level agreement is madeand when the service level measurements are being defined.\nPolicies and Controls\nSystem managers create policies that direct the resources of the computer system tomaintain service levels according to the workload definition specified in thosepolicies. A policy workload definition is closely related to the service levelagreement workload definition, but may be modified to satisfy operationalconstraints. It is translated into terms that map oto system features. Example policiesinclude:\n\u25a0A maximum of 600 interactive users of the order entry application at any time.\n\u25a0Order entry application has a 60 percent share of CPU, 30 percent share ofnetwork, and 40 percent share of memory.\n\u25a0If new-order response time is worse than its target, steal resources from otherworkloads that are overachieving.\nThe policy is only as effective as the measurements available to it. If the wrong\nthings are being measured, the policy will be ineffective. The policy can controlresources directly or indirectly. For example, direct control on CPU time andnetwork bandwidth usage might be used to implement indirect control on disk I/Orates by slowing or stopping a process.\nCapacity Planning and Exception Reporting\nThe measured workload and service levels should be analyzed to extract trends. Acapacity planning process can then be used to predict future scenarios anddetermine action plans to tune or upgrade systems, modify the service levelagreement, and proactively avoid service problems.\nIn cases where the measured workload from the users exceeds the agreed-upon\nworkload definition or where the measured service level falls short of the definedlevel, an exception report is produced.\n\nChapter 2 Service Level Management 7Accounting and Chargeback\nThe accrued usage of resources by each user or workload may be accumulated into\nan accounting system so that projects can be charged in proportion to the resourcesthey consume.\nResource Management Control Loop\nTo manage a resource, you must be able to measure and control it. A control loop isset up that measures the performance of an application subsystem, applies policiesand goals to decide what to do, then uses controls to change the resources beingprovided to that subsystem. This loop can be implemented manually on a time scalemeasured in days or weeks (by reconfiguring and upgrading entire systems), or itcan be automated in software and run as often as every few seconds. The controlloop is shown in\nFIGURE 2-2 .\nFIGURE 2-2 Resource Management Control Loop\nA complete system implements many control loops. A brief digression into basiccontrol theory is provided at this point to help explain the behavior of such systems.Performance\nMetrics\nGoals and\nPoliciesApplication\nSubsystem\nResource\nControls\n\n8Resource ManagementA Simple Approach to Control Theory\nWe spend so much of our lives operating control loops that it is actually quite\nintuitive to most people. Designing a control loop is more complex and requires amore explicit understanding of the situation.\nYou start with an objective, such as to steer a car around a corner while staying in\nthe lane. You apply a control input by turning the steering wheel to the point thatwill get you around the corner. After a delay. the car responds, You measure theresponse, compare it with what you wanted, and obtain the error difference. If thedifference is zero you don\u2019t need to change the control input. If the turn is too tight,you need to reduce the input. If the turn is too wide, you need to increase the input.You have to decide how much extra correction is needed to compensate for beingwrong the first time, and also decide whether the car has finished responding fullyto the initial input. You may decide to over- or under-correct, and apply thecorrection gradually or quickly (for example, if you are heading straight for a lamppost!).\nFIGURE 2-3 Example Control Loop\nThe first time you tried a car driving game on a computer, you probably swungwildly from side to side. This wild swinging is caused by over-correcting too latebecause you don\u2019t have the same motion sensing inputs you have in a real car. Whenthe car is oscillating, your corrections may end up being delayed to the point thatyou are turning the wheel the wrong way at the wrong time, and you might spin offor crash. Eventually, you learn to react based on what you see on the screen andmake smaller corrections more quickly to keep the car on track and stable.Metrics\n(apparent\nroad position)\nGoal\n(turn corner)Application\n(car)\nControls\n(steering angle)\n\nChapter 2 Service Level Management 9In control terms, you are applying negative feedback to the system. You take the error\ndifference between what you wanted and what you got, and apply the inverse of theerror to the system to reduce the error in the future. The rate at which you measureand apply corrections is called the control interval, and the rate at which the system\nresponds to changes is called the time constant for the loop. The amount of the error\nthat you feed back changes the characteristic behavior of the control loop. If youfeed back a large proportion of the error with a short control interval, the system islightly damped and will be very responsive to sudden changes but will probably\noscillate back and forth. If you feed back a small proportion of the error over alonger control interval, the system is heavily damped and will tend to be sluggish and\nunresponsive with a large time constant.\nWhen you apply these principles to computer system resource management you can\nsee that it is important to average measurements over an appropriate time scale andget the damping factor right. The resource manager needs to respond quicklyenough to cope with sudden changes in the workload such as many simultaneoususer logins at the start of a shift, while maintaining a steady flow of resources to allthe workloads on the system so that response times are consistent and predictable.\nViewpoints of Resource Management\nYou can measure and control a computer system in many ways. This sectionexamines various approaches to solving resource management problems. Themethodology used depends upon the starting point of the developers, for example anetwork-centric methodology can be extended for use in other areas, so can astorage-centric or server-centric viewpoint.\nDiverse Methods\nThis diversity of approach occurs both because of the products that are available andbecause of the expectations of the end users who are purchasing solutions. In a largeorganization, several groups (such as system administrators, network managers,database administrators, and security managers) are responsible for different partsof operations management.\nIn some organizations, each group is free to use its own methods and obtain its own\ntools. This can cause demarcation problems because there is so much overlap in thescope of each method. Or a single methodology and tool could be imposed by thedominant group. The problem with such an aproach is that the methodology may beoptimal for managing one aspect of the system only and could do a poor job in otherareas.\n\n10Resource ManagementIn an ideal world, one all-encompassing mega-tool would implement an integrated\nmethodology and solve all resource management problems. The closer you get tothis ideal, the more expensive and complex the tool becomes. And you may notwant all of its features. So it is harder to justify purchasing it.\nA more pragmatic approach is to integrate simpler, more specialized tools that share\ninformation with each other, have a similar look and feel, and can be used as astandalone product as needed.\nToday\u2019s diverse set of methodologies and tools have very little integration between\nthem and several different user interfaces. The products produced at SunMicrosystems, Inc. are converging on a common user interface style and on commontechnologies for sharing information to provide better integrated resourcemanagement components. This book\u2019s primary role is to explain to data centeroperations managers how to use combinations of the current set of products andtechnologies. It also indicates the direction of the development and integration workthat will produce the next generation of products.\nThe System-Centric Viewpoint\nThe system-centric viewpoint focuses on what can be done on a single server usinga mixture of hardware and operating system features. The operating systemprovides basic management capabilities for many components such as attachednetwork and storage devices. But it specializes in managing CPU and memoryresources for a single desktop or server system. Dynamic Reconfiguration (DR),processor sets, Solaris Resource Manager\u2122, and Sun Enterprise\u2122 10000 (also knownas Starfire\u2122) Dynamic System Domains (DSDs) are all system-centric resourcemanagement technologies. The Sun hardware system-centric resource managementtool is the Sun Enterprise SyMON\u2122 2.0 software. It is based on the Simple NetworkManagement Protocol (SNMP), so it also has some network managementcapabilities. The Solaris Management Console\u2122 software provides a more genericframework for gathering together operating system administration tools andinterfacing to industry standard initiatives such as the web-based managementinitiative (WebM) and the Common Information Model (CIM). The Starfire systemcurrently uses its own HostView interface running on a separate system serviceprocessor (SSP) to manage domains.\nThe system-centric viewpoint runs into problems when a lot of systems must be\nmanaged. Coordinating changes and allocating resources becomes complex quiterapidly. Tools like the Sun Enterprise SyMON 2.0 software can view many systemson one console, but cannot replicate and coordinate changes over multiple systems.One reason why the Sun Enterprise SyMON 2.0 software does not fully supportmanagement of the Starfire system is because each domain on the system runs aseparate copy of the Solaris operating environment and sees a different subset of the\n\nChapter 2 Service Level Management 11hardware configuration. The next release of this software will be extended to view\nan SSP and all the domains in a Starfire system as a special kind of cluster so it canbe used in place of the HostView interface.\nThe operating system and devices provide a large number of measurements of\nutilization, throughput, and component-level response times. There are several goodways to control CPU resources, but at present, there is no way to control the usage ofreal memory by a workload. The only way to constrain a workload that is using toomuch memory is to slow down or stop its CPU usage so that it stops referencing itsmemory, which will then be stolen by other, more active processes.\nManual resource management policies can be implemented using the Sun Enterprise\nSyMON Health Monitor, which generates alerts when a component of the systembecomes overloaded. The system administrator can then tune or reconfigure thesystem to avoid the problem. Automatic resource management policies areimplemented by Solaris Resource Manager, which dynamically adjusts the prioritiesof processes to ensure that their recent CPU usage tracks the share of the system thathas been given as a goal for that user.\nThe Cluster-Centric Viewpoint\nThe cluster-centric viewpoint concentrates on coordinating resource managementacross the cluster. Systems are clustered together to provide higher availability andhigher performance than that provided by a single system. A cluster is morecomplex to install and administer, so tools and methods attempt to automate clustermanagement to make it more like a single system. From a resource-managementviewpoint, the primary issue is load balancing and the extra costs of accessingnonlocal information over the cluster interconnect. Sun has two kinds of clusters.The highly integrated SPARCcluster\u2122 product range is focused on improvedavailability in commercial environments. Its management tools will eventuallybecome an integrated extension to the SyMON software. For high performancecomputing, Sun HPC Servers use the platform computing load share facility (LSF) toperform load balancing on much larger and more loosely coupled clusters.\nAt a cluster level, multiple system level measurements are compared to measure\nload balance and look for spare resources. The cluster interconnect utilization andproportion of remote data access are important additional measures. The primaryresource management control is the choice of where to run new work. It is notcurrently possible to migrate a running job from one node in a cluster to another, orto checkpoint a job to disk and restart it again later, either on the same node or on adifferent one.\n\n12Resource ManagementWhen deciding on the resources that are available on a node, it is easy to decide if\nthere is some spare CPU power, but very hard to decide if there is enough availablereal memory. The kernel maintains a free list of memory, but there is also someproportion of memory in use as a file system cache that could be reclaimed to run anew job if there was a way to measure it. This is a current issue for the LSF product.\nThe Network-Centric Viewpoint\nFrom a network point of view, there are a large number of devices to manage. Manyof them are network components with limited monitoring capabilities, such asbridges and routers. The primary resource that is managed is network capacity. Atthe intersection of servers and networks, there are products that perform protocolbased bandwidth management on a per-server basis (such as Solaris\u2122 BandwidthManager software) or act as secure firewalls. In the telecommunications industry,network management encompasses all the equipment required to run a globalsystem where the end-points are mostly telephones or mobile cellphones, and thetraffic is a mixture of speech and data. In this environment, SNMP is too simple, sothe more scalable OSI-based CMIP management protocol is often used. TheSolstice\u2122 Enterprise Manager product is a Telco-oriented CMIP and SNMPmanagement system that is used to manage cellular networks. In theory it could beused to manage computer systems and local area networks, but it was not developedto do this. Computer-oriented local and wide area networks are normally managedusing SNMP protocols, with the Solstice SunNet Manager\u2122 or HP OpenViewproducts collecting and displaying the data. Both products provide some visibilityinto what is happening in the computer systems on the network, but they are muchmore focused on network topology. At this level, resource management is done on aper-network basis, often by controlling the priority of data flows through intelligentrouters and switches. The Sun Enterprise SyMON 2.0 software can act as a networkmanagement platform as well as a system hardware management platform, whichmay help to integrate these two viewpoints.\nProtocol information along with the information found in packet headers and\nnetwork addresses form the basis for network measurements. There is no user orprocess identifier in a packet, so it is hard to directly map network activity to systemlevel activity unless an identifiable server process is dedicated to each protocol.Some protocols measure round trip times for their acknowledgments. This canprovide an estimate of network latency between two systems.\nNetwork controls are based on delaying and prioritizing packets based on the\nprotocol and destination data in the packet headers. This can occur at the serversthat make up the end points or in the routers that connect them.\n\nChapter 2 Service Level Management 13Storage-Centric Viewpoint\nStorage has recently moved from being a simple attached computer system\nperipheral to a complex managed entity in its own right. Networked storage usingfibre channel puts an interconnection layer in between multiple servers or clustersand multiple storage subsystems. This storage area network (SAN) can containswitches and routers just like local or wide area networks, but the protocol incommon use is SCSI over fibre channel rather than IP over Ethernet. A SAN mayalso span multiple sites, for example, where remote mirroring is being used fordisaster recovery. Storage is also now open for access in a heterogeneous multi-vendor environment, where multiple server and storage vendors can all beconnected over the SAN. This is an emerging technology, and tools to manage aSAN are still being developed. Sun provides one approach with an industry-wideinitiative called Project StoreX. It is based on a distributed pure Java\u2122 technologyplatform that can run anywhere a JVM is available, scaling from embedded devicesthrough open systems into mainframe and enterprise environments. Project StoreXenables management of any storage resource in a heterogeneous distributedenvironment, from storage hardware, like devices and switches, to storage softwarelike backup solutions and volume managers. Project StoreX is being promoted as anopen multi-vendor standard.\n\n14Resource ManagementFIGURE 2-4 Storage Area Network\nStorage management has two constraints that make it interesting: one is that it must\nbe distributed over many server systems and storage devices to be useful. The otheris that these servers and devices come from many vendors and run differentoperating software. So Project StoreX cannot include a core dependency on theSolaris operating environment or other Sun products. Project StoreX must solvesome of the generic problems of clustered and networked resource management. Inparticular, it manages the state of distributed devices in a persistent manner. ProjectStoreX must be capable of stand-alone operation, with component managementinterfaces to other products.\nProject StoreX enables resource management of capacity, performance, and\navailability of data storage. Backup and archival policies can be used to automatemigration of data to a tape library. Measurements of capacity and performancecharacteristics can be combined with availability policies, so the operator will bealerted of any problems. Ultimately storage subsystems will be reconfiguredautomatically.LAN/WAN/INTRANET\nServer\nOLTPServer\nDW/DSSServer\nInternetServer\nNFSServer\nFile\nSwitch To other SANS Storage Network Switch\n\nChapter 2 Service Level Management 15At present, Project StoreX does not include a general purpose rule script-based\npolicy engine. It does provide the infrastructure necessary to create polices,allowing the view of storage to be elevated to the Storage Service level.\nIntegration between the system viewpoint and the storage viewpoint has some of\nthe same problems as the integration of system and network viewpoints. The trafficon the SAN does not contain any indication of which user or process generated therequest. Within the Solaris software, it is possible to trace storage accesses on a per-process, per device basis. But the overhead of collecting and analyzing this data isquite high. There may be a need for a Project StoreX Solaris Storage BandwidthManager to bridge the two management viewpoints in a way that accounts for,prioritizes, and controls SAN bandwidth on a per process or per user basis.\nDatabase-Centric Viewpoint\nA database management system has its own notion of users, manages its ownmemory and storage allocation, and can appear as a \u201cblack box\u201d to the server systemon which it runs. Some database vendors have implemented their own resourcemanagement capability on a per user or per transaction basis. This may take theform of priorities, shares, or limits on CPU usage per user or per transaction. Thedatabase also contains its own logic that implements policies and dynamicallycontrols resources.\nThe database implementation can sometimes work well with server-based resource\nmanagement. This is described in more detail in Chapter 4.\nIntegration is needed between the internal database resource management\ncapabilities and the other resource management viewpoints.\nApplication-Centric Viewpoint\nLarge and complex applications such as SAP R/3, Baan, and Oracle Financialscontain their own resource management concepts and controls. For example, OracleFinancials implements its own batch queue system to decouple the generation oflarge reports from the interactive response time of the users. A report is sent to asubsystem called the concurrent manager, which is configured to have a number ofparallel streams of work of various types according to the local policy. Concurrentmanager activity can be scheduled to occur outside normal working hours, or it canbe used to soak up spare cycles during the day.\nSAP R/3 measures the response time of important transactions and breaks these\ndown into application server time and backend database server time. As many usersconnect to an application server and many database servers connect to a single\n\n16Resource Managementbackend database, there is no concept of which user is doing work on the backend\nsystem. The application itself must have the instrumentation to keep track of what isgoing on. The application directly implements the policies and controls.\nIntegrated Methods\nIn a large organization, all of the above viewpoints are useful, and somecombination of methodologies is probably implemented already.\nFIGURE 2-5 indicates\nthe relative breadth of coverage of each methodology. A darker box shows thatbetter coverage is available, a white box indicates that little or no support isprovided for a combination.\nFIGURE 2-5 Integrated Methodology by Viewpoint\nThe mature methods have remained specialized, but the emerging technologies ofSun Enterprise SyMON 2.0 and Project StoreX address a much broader scope ofproblems. The product overview section of this book discusses their capabilities inmuch greater detail.\nSo far the discussion has been quite abstract. The next chapter introduces several\nexample workloads, showing the appropriate methodologies to manage resourcesfor them.Application\nDatabase\nNet Manager\nProject StoreXCluster\nSyMON\nApplication\nDatabase\nNetwork\nStorageCluster\nServerMethodused\nto manage\nresources in:\n\nChapter 2 Service Level Management 17The Consolidation Process\nThe consolidation process starts when you identify candidate systems and\napplications. First measure the resource usage and service levels of those systems soyou can see which application workloads will fit together best.\nNext, migrate those systems to a common Solaris release and patch revision and do\nsome testing so you can be sure that everything works correctly in the sameenvironment. You also need to make sure that there are no conflicts in the nameservice configuration and network services files. For example, local password filesmay need to be merged, and any conflicting port numbers specified in the/etc/services file may need to be cleared. If you use a name service such as NIS\nfor all your password and services information, then the systems should already beseeing the same name space and definitions. Using a common name service eases theconsolidation process. If you prefer to make all the changes at one time, then youcan upgrade as the application is consolidated, but allow for more testing time and amore incremental installation process on the consolidated system\nFor each group of consolidated applications, you must choose appropriate resource\nmanagement controls. Once you have consolidated your applications to fewersystems, monitor and re-size the consolidated systems to allow for peak loads. Youcan either remove excess resources for use elsewhere or identify additionalcandidate applications to be consolidated onto these systems. Treat this as a rollingupgrade program rather than a one-time big change.\nAn obvious question that arises is how many systems should the new consolidation\ncontain. Circumstances vary, but the basic principles remain the same. If you treatconsolidation as a process, then the number of systems decreases over time and thesize of systems increases.\nDowntime impacts multiple applications on the consolidated systems. Therefore,\nwhen you increase the resources by adding an extra application, you want to do sowithout rebooting. Consolidated upgrades benefit from systems that can performdynamic reconfiguration. The midrange Sun Ultra\u2122 Enterprise\u2122 E3000-E6500servers can perform I/O board reconfiguration with the Solaris 2.6 release, but theyrequire the Solaris 7 release for dynamic reconfiguration of CPU and memory, whichcauses some application availability issues. The number of system footprints may betoo high with midrange servers, and it is hard to reduce the total number of serverseffectively. With the high-end Starfire system, Dynamic System Domains (DSDs)solve these problems. DSDs are supported on the Solaris 2.5.1, 2.6, and 7 releases.The total number of DSDs can be reduced as applications are consolidated.\nOne approach is to use each DSD for a different Solaris revision. You may have a\nlarge DSD for the bulk of your Solaris 2.6 applications, a smaller one for applicationsthat have not yet migrated from the Solaris 2.5.1 release, and a development and test\n\n18Resource ManagementDSD for the Solaris 7 release. Over time, the Solaris 2.5.1 DSD will shrink away and\nits resources will migrate into the other DSDs. Applications will also migrate intothe Solaris 7 DSD. The key benefit here is that this all happens under softwarecontrol, using a single system footprint in the data center. DSDs are described indetail Chapter 8.\nUse the Solaris Resource Manager or the Solaris Bandwidth Manager software or\nprocessor sets to control applications within a single copy of the Solaris operatingenvironment.\nA consolidated system runs a mixture of workloads. You have to choose relevant\nprocesses and aggregate to measure them. The remainder is overhead or unplannedactivity. If it is significant, it should be investigated. Break down network workloadsas well so that you know which applications are generating the network traffic.\nThere is a common set of measurements to collect per workload.\n\u25a0Number of processes and number of users\n\u25a0End user response times for a selection of operations\n\u25a0User and System CPU usage\n\u25a0Real and virtual memory usage and paging rates\n\u25a0I/O rates to disk and network devices\n\u25a0Microstate wait timers to see which resources are bottlenecks\nTo actually perform workload aggregation you have to match patterns in the data.\n\u25a0match processes on user name\n\u25a0match processes on command name and arguments\n\u25a0match processes using processor set binding\n\u25a0match system accounting data using user and command name\n\u25a0match network packets on port number and protocol\nYou usually have to assign disks and file systems to workloads manually. Dealing\nwith shared memory, libraries, and code makes RAM breakdown hard.\nWhen you are accumulating measurements don\u2019t accumulate the pscommand\nCPU% . It\u2019s a decayed average of recent CPU usage, not an accurate measure of actual\nCPU usage over an interval. You need to measure the actual process CPU time usedin each interval by taking the difference of two measurements. There is more detailon the available measurements and what they mean in Chapter 5.\n\n19CHAPTER3\nPolicies and Controls\nTo manage the resources of a system, you must be able to measure and control\nresource usage. You must also establish policies that determine the controls that areinvoked once the available measurements are interpreted. This chapter examinesvarious types of policies and controls and ways to implement them. It also looks ateach subsystem in turn to see the resources that can be managed.\nPolicy Types\nThere are many types of policies and many ways to implement them. Some of themain classifications are explained below. A recent draft standard defines theterminology of each kind of policy. That terminology has been adopted in this book,and the standard is summarized in this section.\nLimits and Error Event Rules\nOne of the simplest policies is to define limits on a measurement and associate itwith an action. This is often implemented as an \u201cif measure passes threshold thenaction\u201d rule. Products such as the Sun Enterprise SyMON 2.0 software (referred tohereafter as SyMON) predefine many simple limit rules and allow new rules to beset on any measurement. A limit can be defined as a simple rule with a single inputmeasurement. It is also common to have several thresholds with a warning levelaction and a critical problem level action for the same measure.\nAn error event is different because it is treated as a discrete on/off event rather than\na continuous variable to be compared against a limit.\nIn either case, an alert is generated and logged. The alert can be transitory and go\naway when the rule is re-evaluated, or it can be persistent and require a user toacknowledge that it has been seen.\n\n20Resource ManagementComplex Rules and Hierarchies\nMore complex rules take several inputs and can maintain historical information such\nas previous state and running averages. They can also be built out of several simplelimit rules. A complex rule is used to establish the state of a component or asubsystem. When rules are combined, they are ranked so that critical problems takeprecedence over warnings. A hierarchy of rules can be built for a network of systemsso that the overall state of the network is indicated by the state of the system thathas the worst problem. In turn, that state is based on the state of the subsystem thathas the worst problem. A rule state propagation hierarchy is provided as part of theSun Enterprise SyMON 2.0 product, and many other commercial tools implementthis mechanism.\nThe policy is inherent in the set of rules that are implemented, the thresholds that\nthe rules use, and the actions that occur when a rule becomes active.\nPriority\nA relative importance level can be given to the work done by a system as part of apolicy that prioritizes some activities over others. The Solaris Resource Managerproduct and others like it assign shares to each user according to a policy decided bythe administrator, then accumulate the CPU usage at a per-user level and implementa control based on the number of shares held by each user and the user\u2019s place in thehierarchy.\nAn alternative approach is to specify percentages directly. The Solaris Bandwidth\nManager software uses this mechanism to provide a way to specify policies on a per-network packet basis. Each packet is classified by address or protocol and each classis given a priority and a percentage of the total bandwidth that it can use.\nGoals\nGoal-based policies are prescriptitive rather than reactive. They operate at a higherlevel. A goal can be translated into a mixture of limits, priorities, and relativeimportance levels. Goals can include actions for when the goal cannot be met.\nA goal can also be thought of as a control loop, where the policy manipulates\ncontrols when a measurement deviates from its desired range. As described inChapter 3, a control loop is a complex thing to manage because its stabilitycharacteristics, time constant, and damping factor must be set correctly. Theinteraction of multiple inter-linked control loops can be problematic.\n\nChapter 3 Policies and Controls 21Goals can be expressed in several ways:\n\u25a0Response time goals try to monitor the end user response time of a system and\ncontrol resources so that high priority work maintains its response time goal bytaking resources from lower priority work.\n\u25a0Throughput goals monitor the rate of consumption of a resource for long runningjobs and control the relative priority to maintain the desired balance.\n\u25a0Deadline goals have a way of telling how far a repetitive batch job has gonethrough its work, and control resources to ensure that the entire job completes bya deadline. For example, a payroll application must complete on time andgenerate the correct number of pay slips. A goal-based workload manager couldmonitor the running total.\nAt present, automated goal-based workload management is a feature found only on\nmainframes running OS/390 software.\nOperational Policies\nSome policies are implemented manually as part of operations management. Forexample, an availability policy can include a goal for uptime and an automatic wayto measure and report the uptime over a period. There is no direct control in thesystem that affects uptime. It is handled by operations staff, who will reconfiguresoftware to work around problems, swap out unreliable hardware, or reconfigure thesystem into a more resilient configuration if the availability goal is not being met.\nIn most cases, goal-based policies require manual intervention to complete the\ncontrol loop. A measure of response time is monitored, and if its goal is not beingmet, the administrator manually varies the CPU shares, moves work from anoverloaded system to another system, or performs a hardware upgrade.\nNetworked Security Policies\nAccess to a system varies according to the role of the user. A security policy canprevent access to certain resources or allow designated users to manage subsystems.For example, the SyMON software includes access control lists for operations thatchange the state of a system, and multiple network domain views to give differentadministrative roles their own view of the resources being managed. Security andnetwork-based policies can be stored in an LDAP based name service. When usersdial into an Internet service provider, they are looked up in a RADIUSauthentication database, which can extract a profile from an LDAP server toconfigure the systems each user is allowed to access and the Solaris BandwidthManager configuration is updated to take into account that user\u2019s network address.\n\n22Resource ManagementControls\nControls are used to limit or redirect resources.\nLimits\nA limit prevents a resource from exceeding a preset value. Some limits are system\nwide (such as the total number of processes allowed on a system) and some operateon a per-user basis (such as a file system quota). Since many limits are implementedby the Solaris software, the action, when a limit is reached, is to return an error codeinside the application and possibly send a signal to the process. Well-writtenapplications handle the errors and catch the signals, but applications that do notexpect to ever run into a limit might misbehave or abort. During testing, it is a goodidea to run with very tight limits and test the behavior of applications as they hitthose limits.\nDirect and Indirect Controls\nA direct control operates on the resource you want to control. For example, theSolaris Resource Manager software controls CPU usage per user by implementing ascheduling class that determines each user\u2019s share of the CPU. An indirect controlworks via dependent resources. For example, to limit the I/O throughput of aprocess, it is sufficient to be able to measure the I/O throughput and limit the CPUresources for that process. The process could be stopped temporarily to prevent itfrom issuing read and write system calls at too high a rate. It might be more efficientto add a direct measurement and control capability to the code that implements readand write calls, but that is a more invasive approach that requires changes to theSolaris software itself.\nThe Solaris Bandwidth Manager product implements a direct control on network\npacket rates. This can be used to implement an indirect control on the CPU resourcestaken up by the NFS server code in the kernel. The Solaris Resource Managersoftware cannot control NFS service directly as NFS is implemented using kernelthreads that do not have an associated user-level process.\n\nChapter 3 Policies and Controls 23Standardized Policy Definitions\nThe Internet draft policy framework standard by Strassner and Ellesson defines its\nscope thus:\nThis document defines a set of terms that the Internet community can use\nto exchange ideas on how policy creation, administration, management,and distribution could work among policy servers and multiple devicetypes.\nThe terminology definitions are network oriented but apply equally well to system\nlevel policies. Some of the terms defined in this standard are listed here.\n\u25a0Administrative Domain: A collection of network elements under the sameadministrative control and grouped together for administrative purposes.\n\u25a0Network Element (also called a Node): A networking device, such as a router, aswitch, or a hub, where resource allocation decisions have to be made and thedecisions have to be enforced.\n\u25a0Policy: The combination of rules and services where rules define the criteria forresource access and usage.\n\u25a0Policy control: The application of rules to determine whether or not access to aparticular resource should be granted.\n\u25a0Policy Object: Contains policy-related info such as policy elements and is carriedin a request or response related to resource allocation decision.\n\u25a0Policy Element: Subdivision of policy objects; contains single units of informationnecessary for the evaluation of policy rules. A single policy element carries anuser or application identification whereas another policy element may carry usercredentials or credit card information. Examples of policy elements includeidentity of the requesting user or application, user/app credentials, and so on.The policy elements themselves are expected to be independent of which Qualityof Service signaling protocol is used.\n\u25a0Policy Decision Point (PDP): The point where policy decisions are made.\n\u25a0Policy Enforcement Point (PEP): The point where the policy decisions are actuallyenforced.\n\u25a0Policy Ignorant Node (PIN): A network element that does not explicitly supportpolicy control using the mechanisms defined in this standard.\n\u25a0Resource: Something of value in a network infrastructure to which rules or policycriteria are first applied before access is granted. Examples of resources includethe buffers in a router and bandwidth on an interface.\n\u25a0Service Provider: Controls the network infrastructure and may be responsible forthe charging and accounting of services.\n\n24Resource ManagementGeneral Policy Architecture\nThe general architecture shown FIGURE 3-1 illustrates one common implementation\nof a policy that combines the use of a policy repository, a PDP , and a PEP . Thisdiagram is not meant to imply that these entities must be located in physicallyseparate devices, nor is it meant to imply that the only protocols used forcommunicating policy are those illustrated. Rather, it simply shows oneimplementation containing the three important entities fundamental to policy: arepository, a PDP , and a PEP .\nFIGURE 3-1 Example Policy Architecture\nIt is assumed that policy decisions will always be made in the PDP and implementedin the PEP . Specifically, the PEP cannot make decisions on its own. This simplifiesthe definition and modeling of policy while leaving open the possibility for a singledevice to have both a local PDP (LPDP) as well as a PEP .\nIn general, the repository access protocol and the policy protocol are different\nprotocols. If the policy repository is a directory, then LDAP is one example of arepository access protocol. However, the policy protocol can be any combination ofCOPS, SNMP , and Telnet/CLI. Given this rich diversity, a common language isneeded to represent policy rules. The rest of the standard document describes theManagement tool\nRepository client\nRepository access protocol\nRepository access protocol\nPolicy protocolPolicy repository\n(directory server,database, etc.)\nRepository client\nPolicy Decision Point\n(PDP)\nPolicy Enforcement\nPoint (PEP)\n\nChapter 3 Policies and Controls 25terminology necessary to enable the definition of such a language and discusses how\npolicy is defined, manipulated, and used in the PDP and PEP . For more information,see the complete document at http://www.ietf.org/internet-drafts/\ndraft-strassner-policy-terms-01.txt .\nSubsystem Policies and Controls\nEach component and subsystem implements a set of measurements and controls that\nallows a policy to manage them.\nUser-level Controls\nUser-level controls include limits on the number of logins and limits on theresources used by each user.\nLogin Limits\nThe Solaris software implements a blanket login ban for non-root users as describedin thenologin(4) manual page. It also limits the total number of processes that\ncan be started via the nproc kernel tunable. This feature scales with the memory\nconfiguration. A further limit is placed on the total number of processes per userusing the maxuprc kernel tunable. This is a single global limit. The total number of\nlogins per user can also be limited. The current limit can be viewed with sysdef or\nsar. The example in\nFIGURE 3-2 was run on a 64 Mbyte desktop system.\n%sysdef -i | grep processes\n    1002maximum number of processes (v.v_proc)     997maximum processes per user id (v.v_maxup)%sar -v 1\nSunOS maddan 5.6 Generic sun4m    03/18/9918:25:12  proc-sz    ov  inod-sz    ov  file-sz    ov   lock-sz\n18:25:13  108/1002    0 4634/4634    0  495/495     0    0/0\nFIGURE 3-2 Determining Solaris Process Limits\n\n26Resource ManagementWhen the SRM software is in use, you can view the maximum number of processes\nthat can be limited on a per-user basis using the limadm command. You can view\nthe current number and limit using the liminfo command, as described in\nChapter 7.\nApplication-level Controls\nThe main class of application-level controls are those provided by relationaldatabases and transaction processing monitors. These consist mainly of accesscontrols, but the Oracle8 idatabase also implements controls on resource\nconsumption and policies for relative importance. This is described in more detail inthe Chapter 7.\nCPU Power-level Controls\nThere are many ways to control CPU power. The most basic one is to physicallychange the CPU configuration itself. A faster CPU shortens the CPU-intensivecomponent of response times. Additional CPUs allow more concurrent work to takeplace with a similar response time.\nPhysically changing the CPU configuration requires a power down and reboot on\nmany computer systems. But the Sun Enterprise Server systems allow CPU boardsto be added and removed from a running system without powering it down orrebooting it. On the Starfire system, the CPUs can be partitioned into dynamicsystem domains, and a separate copy of the Solaris operating environment booted ineach domain. CPU boards can then be moved from one dynamic system domain toanother. This is described in detail in Chapter 8.\nWith a single copy of the Solaris software, the CPUs can be partitioned into\nprocessor sets as described in Chapter 7. Each process is bound to a processor setand constrained to run only on the CPUs that are members of that set. Sets arecreated and removed dynamically. If one set is overloaded, its processes cannotmake use of the CPU power in a different set without manual intervention.Processor sets are most useful when there are a large number of CPUs to partition.This technique is obviously not useful on a uniprocessor system.\nThe SRM software also works within a single copy of the Solaris operating\nenvironment. Unlike sets, it has fine granularity and can be used on a uniprocessorsystem. When a system gets busy, all CPU power is used automatically. SRM worksby biasing CPU usage on a per-user basis, using shares to determine the relativeimportance of each user.\n\nChapter 3 Policies and Controls 27Thekill command sends a stop signal to a process and suspends it in the same\nway that typing Control-Z does in an interactive shell session. Sending a start signallets the process continue. Stopping and starting processes in this way can control theconcurrency of CPU-bound jobs on a system.\nThe Load Share Facility (LSF) software is described in detail in Chapter 10. LSF\nsoftware implements a distributed batch queuing system where jobs are submittedand, when resources are available, sent to be run on a system. LSF softwareimplements its own set of policies and controls.\nDisk I/O Policies\nAccess controls via file permissions and access control lists (ACLs) control who canread and write to a file system.\nCurrently, no direct measures or controls of the rate at which a process is writing to\na file system exist. The only information provided for each process is the total readplus write data rate. Block input and output counters are not incremented correctlyin current releases of the Solaris operating environment. The block counter problemis filed as bugid 1141605 and is fixed in the next release of Solaris software. This datais not made available by the standard commands; it is part of the \u201cusage\u201d structurethat includes microstate accounting as described in Chapter 5.\nBecause the Solaris software does not have any direct measurements or controls,\nadditional application-specific information is required (such as configuring thelocation of data files manually) to implement policies.\nDisk Space\nSee Chapter 6 for a description of the disk quota system. Disk quotas are currentlyimplemented separately on each file system type. They are available on the UFS filesystem and remote mounts of UFS via NFS only. Other file system types either haveno quota system or have a separately administered implementation.\nVirtual Memory\nAn application consumes virtual memory when it requests memory from theoperating system, and the memory is allocated from a central pool of resources.Virtual memory usage, however, is not directly related to physical memory usagebecause not all virtual memory has physical memory associated with it. If an\n\n28Resource Managementapplication requests 16 Mbytes from the operating system, the operating system will\ncreate 16 Mbytes of memory within that application\u2019s address space, but will notallocate physical memory to it until that memory is read from or written to.\nWhen you restrict or control the amount of virtual memory that an application can\nhave, you are not controlling the amount of RAM that application can have. Ratheryou are implementing a policy limit on the maximum amount of virtual addressspace that process can have. This is an important difference because the limit isenforced when the application first requests memory not while it is using it. Whenthe application hits the limit, it will probably fail because its requests to extend itsvirtual address space will fail. This may be what you want if the application is likelyto impact higher priority work. But careful testing and debugging is required tomake applications recover gracefully from memory allocation failures.\nVirtual memory can be limited at the system level and at the process level. At a\nsystem level, the total amount of virtual memory available is equal to the totalamount of swap space available. Each time virtual memory is used, the amount ofswap space available drops by the same amount. If one application requests a largeamount of virtual memory (for example, malloc (1 Gbyte), there is potential for that\napplication to exhaust the system-wide swap space, which will then cause otherapplications to fail when they request memory.\nYou can use resource management of virtual memory to prevent a single process\nfrom growing too large and consuming all virtual memory resources by limiting themaximum amount of memory that a process or group of processes can use. This canbe useful in two cases: to prevent any one user from using all of the available swapspace (a denial of service attack) and to prevent a runaway process or leakingprocess from consuming all of the available swap space.\nBase Solaris software can do simple resource management of a process\u2019s virtual\nmemory usage. The limits information described in Chapter 6 can be used to limitthe maximum amount of virtual memory used by a process. Limits are enforced perprocess, thus preventing any one process from using an unreasonably large amountof virtual memory. But a user can run many processes, so this does not preventdenial of service attacks.\nThe SRM software has a mechanism that can limit the maximum amount of virtual\nmemory per user. This implements a similar limits policy as the per-process limitbuilt into the Solaris operating environment, but it can be used to limit a user,regardless of how many processes are running.\nFIGURE 3-3 shows the three levels of\nvirtual memory that can be controlled.\n\nChapter 3 Policies and Controls 29FIGURE 3-3 Three Levels of Virtual Memory Limits\nRelationship Between Virtual Memory and Swap\nSpace\nIt is important to note that the amount of swap space used by a user does not\ncorrelate directly to the sum of all that user\u2019s processes. A user may have threeprocesses, where each shares a single shared memory segment between them. Eachprocess has the shared memory segment mapped into its address space, but swapspace is only accounted for once within these processes. For example, three userseach have a 1 Gbyte shared global area mapped into their address space, but eachincremental user is not consuming an additional 1 Gbyte of swap from the system-wide pool; they use three Gbytes of virtual memory space, but only one Gbyte ofswap space.\nIt is important to factor this in when using products like SRM software to control a\nuser\u2019s virtual memory usage. The virtual memory accounted for by the SRMsoftware is different from the amount of swap space used by the user. Since we aretrying to control virtual memory in an attempt to prevent a single user fromconsuming all of the swap space, we must take care to apply the correct virtualmemory limit policy. This sometimes makes it extremely difficult to control swapspace usage with SRM software, but the right parameters provide adequate controlin most environments.ProcessesSolaris Resource\nManager\nProcess\nlimits (ulimit)Swap space\nPer-process\nvirtualmemoryTotal system-wide\nswap space\nUser1 User2User3\n\n30Resource ManagementFIGURE 3-4 Swap Usage Only Accounts for Shared Segments Once\nIn the example illustrated in FIGURE 3-3 and FIGURE 3-4 , the SRM software is\nconfigured to limit the total virtual memory to 1.012 GBytes, which allows all threeprocesses to execute normally. If one of the three processes has a memory leak, thelimit would be hit for that user, affecting only the processes owned by that user. Thedisadvantage is that if the user starts another process, the same limit is reached. Theper-user limits must take into account the number of processes each user is expectedto run.\nPhysical Memory\nResource Management of physical memory means defining policies and controllingthe amount of RAM that is allocated to different workloads. In contrast to the virtualmemory limit policies, physical memory is controlled by applying importancepolicies to different types of memory. In the future, it may be possible to apply limitor allocation style policies to physical memory, but that capability is not available inthe Solaris operating environment today.\nThe physical memory management system in the Solaris operating environment can\nimplement different policies for different memory types. By default, the memorymanagement system applies an equal importance policy to different memorysubsystems, which sometimes results in unwanted behavior. Before we look at thepolicies, let\u2019s take a quick look at the different consumers of memory.Shared memory\n1 Gbyte\nPrivate memory\n4 MbytesShared memory\n1 Gbyte\nPrivate memory\n4 MbytesShared memory\n1 Gbyte\nPrivate memory\n4 Mbytes\nTotal swap space = 1 GByte + (4 Mbytes * 3) = 1.012 GBytesProcess 3 Process 2 Process 11 Gbyte memory\nis shared amongthese 3 processes\n\nChapter 3 Policies and Controls 31The most important consumers of memory in the Solaris operating environment are:\n\u25a0Kernel memory, used to run the operating system\n\u25a0Process memory, allocated to processes and applications\n\u25a0System V shared memory, allocated by the shared memory subsystem by\napplications such as databases\n\u25a0Memory used for file system caching\nThe Default Memory Allocation Policy\nMemory in the Solaris operating environment is, by default, allocated on a demandbasis with equal importance to each subsystem. When a subsystem requestsmemory, it is allocated from a central pool of free memory. If sufficient memory isavailable in the free pool, then an application\u2019s request is granted. If free memory isinsufficient, then memory is taken from other subsystems to satisfy the request. Theequal-importance policy means that the application with the most aggressivememory requests gets the majority of the memory assigned to it. For example,suppose a user starts a netscape process that uses 20 Mbytes of memory. That\nmemory is taken from the free pool of memory and allocated to the netscape\nprocess. When the user starts a gimp image editor tool, if there is no free memory in\nthe free memory pool, then memory will be taken from the netscape browser and\nallocated to the gimp image editor tool. The Solaris memory allocation policy takes\ninto account recent usage in an attempt to choose the correct application from whichto steal memory, but the usage history is very short (less than one minute) in mostcircumstances.\nFIGURE 3-5 shows an example where the memory allocated to the netscape process\nis reduced when the gimp process is started.\nFIGURE 3-5 Memory Allocation with netscape andgimpOS, CDE, etc.\n(23 Mbytes)\nnetscape\n(30 Mbytes)netscape\n(10 Mbytes)\nFree memory\n1 Mbytesgimp editor\n(31MBytes)\nFree memory\n10 MBytes64 MbytesOS, CDE, etc.\n(23 Mbytes)\n\n32Resource ManagementThis situation can be avoided by configuring physical memory in the system so that\nthere is always enough memory for each application\u2019s requirements. In exampleshown in\nFIGURE 3-5 , if we configured the system with 128 Mbytes of memory, then\nbothnetscape andgimp could execute at the same time without affecting each\nother.\nHowever, there is another consumer of memory in the operating system that often\ncauses application memory starvation. That consumer is the file system. The filesystem uses memory from the free memory pool just like any other application inthe system. Because the memory system implements equal importance by default,the file system can squeeze applications in the same way gimp did in\nFIGURE 3-5 .\nReading a file through the file system causes the file system to use physical memoryto cache the file. This memory is consumed in 8-kilobyte chunks as the file is read.The free memory pool is thus depleted and the memory system starts looking formemory that it can use to replenish the free pool. The memory system will takememory from other applications that haven\u2019t used portions of their memoryrecently. For example, if you start a file-based mail tool such as dtmail , the memory\nused to cache the file when dtmail reads a 23-Mbyte mail file will be taken from\nother portions of the system.\nLet\u2019s revisit the example shown\nFIGURE 3-5 and look at what happens when we factor\nin the file system memory usage. FIGURE 3-6 shows the same 64-Mbytes system\nwhere we start dtmail whilenetscape is running. Again, if we increase the\nmemory to 128 Mbytes, we will provide enough memory for netscape , thedtmail\napplication, and the /var/mail file. What happens if the file we are accessing is\nmany times larger than the memory in the system (for example, several gigabytes)?\nFIGURE 3-6 Memory Allocation with netscape anddtmailnetscape\n(30 Mbytes)\nFree Memory\n10 Mbytes Free Memory\n1 Mbytes/var/mail  file sys\n(26 Mbytes)dtmail\n(4 Mbytes)netscape\n(10 Mbytes)OS, CDE, etc\n(23 Mbytes)OS, CDE, etc\n(23 Mbytes)\n64 Mbytes\n\nChapter 3 Policies and Controls 33Priority Paging\u2014Memory Policy by Importance\nThe Solaris feature priority paging prevents the file system from consuming too\nmuch memory. Priority paging implements a memory policy with differentimportance factors for different memory types. Application memory is allocated at ahigher priority than file system memory thus preventing the file system fromstealing memory from other applications. Priority paging is implemented in theSolaris 7 operating environment, but it must be enabled with an/etc/system parameter as follows:\nTo use priority paging with the Solaris 2.6 release, use kernel patch 105181-13; with\nthe Solaris 2.5.1 release, use kernel patch 103640-26 or higher.\nWith priority paging enabled, the memory system behaves differently. In our\nexample, rather than shrinking Netscape from 30 Mbytes to 10 Mbytes, the systemlimits the amount of memory that is allocated to the file system.\nFIGURE 3-7 shows\nhow both netscape anddtmail can exist on the same system, while the size of the\nfile system cache is held to a reasonable limit.\nFIGURE 3-7 Memory Allocation with Priority Paging Enabled\nThe new memory allocation policy can be extremely important for larger systems,where memory paging problems cannot be resolved by adding additional memory.A large database system with a 50 Gbyte+ database on the file system willcontinuously put memory pressure on the database application with the default*\n* /etc/system file*set priority_paging=1\n64 MbytesOS, CDE, etc\n(23 Mbytes)OS, CDE, etc\n(23 Mbytes)\nnetscape\n(30 Mbytes)\nFree Memory\n10 Mbytesnetscape\n(30 Mbytes)\ndtmail\n(4 Mbytes)\n/var/mail  file sys\n(5 Mbytes)\nFree Memory\n1 Mbytes\n\n34Resource Managementmemory allocation policy. But priority paging will ensure that the file system only\nuses free memory for file system caching. As the application grows and shrinks, thesize of the file system cache will grow and shrink in keeping with the amount of freememory on the system.\nNetwork Interfaces and Network Services\nTCP/IP-based network services are configured using the /etc/services and\n/etc/inetd.conf files so that the server responds on a particular port number.\nAccess controls are implemented by removing and enabling particular portspecifications from these files. In addition, more sophisticated access control can beimplemented using TCP wrappers. These wrappers look at the incoming requestpackets and can deny access from unauthorized networks. To relay high volumes oftraffic in and out of a secured network, a firewall is used. Access policies can bebased on the destination address, protocol, or port number.\nThe Solaris Bandwidth Manager product provides controls on network traffic on\neach interface of a server. Chapter 9 explains how this product can be used as part ofa resource control framework.\nNetwork level resource managers can distribute incoming traffic over multiple\nsystems according to the current load on each system and the network delaysbetween the end user and the available servers. Cisco Local Director and ResonateCentral Dispatch, among other products, can be used to direct traffic within a singleweb site, while Cisco\u2019s Distributed Director and Resonate\u2019s Global Dispatchproducts distribute traffic over wide areas, optimizing the wide area network delays.Cisco\u2019s products are implemented as hardware routers, while Resonate\u2019s productsrun as a software package on the existing server systems.\n\n35CHAPTER4\nWorkload Management\nApplication workloads can be understood in terms of the resources they consume on\neach of the systems that they are distributed across.\nResource measurements are available at the system level and at the per-process\nlevel. Analysis of per-process measurements separates the raw data into applicationworkloads. When several applications are consolidated onto a single system,resource contention can occur. Analysis determines which resource is suffering fromcontention and which application workloads are involved. This chapter describesseveral tools that help perform process-based analysis.\nThis chapter also describes several diverse, real-life situations and the appropriate\ntools to use with them.\n\u25a0The first scenario covers resource management issues that occur in Internetservice provider (ISP) environments.\n\u25a0The second scenario portrays at consolidated commercial workloads, whichinclude databases and file services for UNIX systems using NFS and batchworkloads.\n\u25a0The third scenario looks at batch workloads, which are required by mostcommercial installations.\nWorkload Analysis Tools\nIn a distributed environment with discrete applications on separate systems,workloads are analyzed by monitoring the total resource usage of each wholesystem. The busiest systems can be identified and tuned or upgraded. This approachdoes not work when multiple workloads are combined on a single system. While allperformance monitoring tools can tell you how busy the CPU is in total, few of themcan aggregate all the processes that make up a workload and tell you the amount ofresource per-user that workload is using.\n\n50Resource ManagementCustomized Process Monitors\nThe Solaris software provides a great deal of per-process information that is not\ncollected and displayed by the pscommand or the SyMON software. The data can\nbe viewed and processed by a custom-written process monitor. You could write onefrom scratch or use the experimental scripts provided as part of the SE (SymbElEngine) Toolkit. The SE Toolkit is freely available for Solaris systems and is widelyused. However, it is not a Sun-supported product. It can be downloaded from theURLhttp://www.sun.com/sun-on-net/performance/se3 . The SE Toolkit is\nbased on an interpreter for a dialect of the C language and provides all the perprocess information in a convenient form that can then be processed further ordisplayed. The available data and the way the SE Toolkit summarizes it is explainedin detail in Chapter 5. For now, we are only interested in how this tool can be used.\nThe SE Process Class\nThe basic requirement for the process class is that it should collect both the psinfo\nandusage data for every process on the system. The psinfo data is what the ps\ncommand reads and summarizes. The usage data is extra information that includes\nthe microstate accounting timers. Sun\u2019s developer-oriented Workshop Analyzer usesthis data to help tune code during application development, but it is not normallycollected by system performance monitors. For consistency, all the data for allprocesses should be collected at one time and as quickly as possible, then offered fordisplay, one process at a time. This avoids the problem in the pscommand where\nthe data for the last process is measured after all the other processes have beenmeasured and displayed, so the data is not associated with a consistent timestamp.\nThepsinfo data contains a measure of recent average CPU usage, but what you\nreally want is all data measured over the time interval since the last reading. This iscomplex because new processes arrive and old ones die. Matching all the data is notas trivial as measuring the performance changes for the CPUs or disks in the system.Potentially, tens of thousands of processes must be tracked.\nThe code that implements this is quite complex, but all the complexity is hidden in\nthe class code in /opt/RICHPse/include/process_class.se . The result is that\nthe most interesting data is available in a very easy-to-use form and a simple scriptcan be written in SE to display selected process information in a text based formatsimilar to the pscommand. To change the displayed data, it is easy to edit the\nprintf format statement in the script directly.\n\nChapter 4 Workload Management 51Using the pea.se Script\nThepea.se script is an extended process monitor that acts as a test program for\nprocess_class.se and displays useful information that is not extracted by\nstandard tools. It is based on the microstate accounting information described inChapter 5.\nThe script runs continuously and reports on the average data for each active process\nin the measured interval. This reporting is very different from tools such as ps, that\nprint the current data only. There are two display modes: an 80-column format(which is the default and is shown in\nFIGURE 4-15 ) and the wide mode, which\ndisplays much more information and is shown in FIGURE 4-16 . The initial data\ndisplay includes all processes and shows their average data since the process wascreated. Any new processes that appear are also treated this way. When a process ismeasured a second time and is found to have consumed some CPU time, itsaverages for the measured interval are displayed. Idle processes are ignored. Theoutput is generated every ten seconds by default. The script can report only onprocesses that it has permission to access. So it must be run as root to see everythingin the Solaris 2.5.1 operating environment. However, it sees everything in the Solaris2.6 operating environment without root permissions.\nNote \u2013 The font size in FIGURE 4-16 has been severely reduced.Usage: se [-DWIDE] pea.se [interval]\n%se pea.se\n09:34:06 name lwp pid ppid uid usr% sys% wait% chld% size rss pfolwm 1 322 299 9506 0.01 0.01 0.03 0.00 2328 1032 0.0maker5X.exe 1 21508 1 9506 0.55 0.33 0.04 0.00 29696 19000 0.0perfmeter 1 348 1 9506 0.04 0.02 0.00 0.00 3776 1040 0.0cmdtool 1 351 1 9506 0.01 0.00 0.03 0.00 3616 960 0.0cmdtool 1 22815 322 9506 0.08 0.03 2.28 0.00 3616 1552 2.2xterm 1 22011 9180 9506 0.04 0.03 0.30 0.00 2840 1000 0.0se.sparc.5.5.1 1 23089 22818 9506 1.92 0.07 0.00 0.00 1744 1608 0.0fa.htmllite 1 21559 1 9506 0.00 0.00 0.00 0.00 1832 88 0.0fa.tooltalk 1 21574 1 9506 0.00 0.00 0.00 0.00 2904 1208 0.0nproc 31  newproc 0  deadproc 0\nFIGURE 4-15 Output from the pea.se Command\n\n52Resource ManagementThepea.se script is 90 lines of code containing a few simple printf s in a loop.\nThe real work is done in process_class.se (over 500 lines of code). It can be used\nby any other script. The default data shown by pea.se consists of:\n\u25a0Current time and process name\n\u25a0Number of lwps for the process, so you can see which are multithreaded\n\u25a0Process ID, parent process ID, and user ID\n\u25a0User and system CPU percentage measured accurately by microstate accounting\n\u25a0Process time percentage spent waiting in the run queue or for page faults to\ncomplete\n\u25a0CPU percentage accumulated from child processes that have exited\n\u25a0Virtual address space size and resident set size, in Kbytes\n\u25a0Page fault per second rate for the process over this interval\nWhen the command is run in wide mode, the following data is added:\n\u25a0Metadata input and output blocks per second\n\u25a0Characters transferred by read and write calls\n\u25a0System call per second rate over this interval\n\u25a0Voluntary context switches, where the process slept for a reason\n\u25a0Involuntary context switches where the process was interrupted by higher\npriority work or exceeded its time slice\n\u25a0Milliseconds per slice or the calculated average amount of CPU time consumedbetween each context switch\nSeveral additional metrics are available and can be substituted or added to the\ndisplay by editing a copy of the script.\nWorkload Based Summarization\nWhen you have numerous processes, group them together to make them moremanageable. If you group them by user name and command, then you can formworkloads, which is a powerful way to view the system. The SE Toolkit alsoincludes a workload class, which sits on top of the process class. It pattern matches%se -DWIDE pea.se\n09:34:51 name lwp pid ppid uid usr% sys% wait% chld% size rss pf inblk outblk chario sysc vctx\nictx   msps\nmaker5X.exe 1 21508 1 9506 0.86 0.36 0.10 0.00 29696 19088 0.0 0.00 0.00 5811 380 60.03 0.30 0.20perfmeter 1 348 1 9506 0.03 0.02 0.00 0.00 3776 1040 0.0 0.00 0.00 263 12 1.39 0.20 0.29cmdtool 1 22815 322 9506 0.04 0.00 0.04 0.00 3624 1928 0.0 0.00 0.00 229 2 0.20 0.30 0.96se.sparc.5.5.1 1 3792 341 9506 0.12 0.01 0.00 0.00 9832 3376 0.0 0.00 0.00 2 9 0.20 0.10 4.55se.sparc.5.5.1 1 23097 22818 9506 0.75 0.06 0.00 0.00 1752 1616 0.0 0.00 0.00 119 19 0.10\n0.30  20.45\nfa.htmllite 1 21559 1 9506 0.00 0.00 0.00 0.00 1832 88 0.0 0.00 0.00 0 0 0.10 0.00 0.06nproc 31  newproc 0  deadproc 0\nFIGURE 4-16 Output from the -DWIDE pea.se Command\n\nChapter 4 Workload Management 53on user name, command and arguments, and processor set membership. It can work\non a first-fit basis, where each process is included only in the first workload thatmatches. It can also work on a summary basis, where each process is included inevery workload that matches. By default, the code allows up to 10 workloads to bespecified.\nThepw.se Test Program for Workload Class\nSpecifying workloads is the challenge. For simplicity, pw.se uses environment\nvariables. The first variable is PW_COUNT , the number of workloads. This is followed\nbyPW_CMD_n ,PW_ARGS_n ,PW_USER_n , andPW_PRSET_n where ngoes from 0 to\nPW_COUNT -1 . If no pattern is provided, pw.se automatically matches anything. If\nyou run pw.se with nothing specified all processes are accumulated into a single\ncatch-all workload. The size value is accumulated because it is related to the total\nswap space usage for the workload although it is inflated due to shared memory.Therss value is not, as too much memory is shared for the result to have any useful\nmeaning. The final line also shows the total accumulated over all workloads.\n16:00:09 nproc 61  newproc 0  deadproc 0\nwk command args user procs usr% sys% wait% chld% size pf\n0 59 26.3 2.0 5.6 0.0 180900 2\n1 0 0.0 0.0 0.0 0.0 0 02 0 0.0 0.0 0.0 0.0 0 03 0 0.0 0.0 0.0 0.0 0 04 0 0.0 0.0 0.0 0.0 0 05 0 0.0 0.0 0.0 0.0 0 06 0 0.0 0.0 0.0 0.0 0 07 0 0.0 0.0 0.0 0.0 0 08 0 0.0 0.0 0.0 0.0 0 09 0 0.0 0.0 0.0 0.0 0 0\n10 Total * * 59 26.3 2.0 5.6 0.0 180900 2\nFIGURE 4-17 Example Output from pw.se\n\n54Resource ManagementIt is easier to use the pw.sh script that sets up a workload suitable for monitoring a\ndesktop workstation that is also running a Netscape web server.\nThe script runs with a one minute update rate and uses the wide mode by default. It\nis useful to note that a workload that has a high wait% is either being starved of\nmemory (waiting for page faults) or of CPU power. A high number of page faults fora workload indicates that it is either starting a lot of new processes, doing a lot of filesystem I/O, or that it is short of memory.%more pw.sh\n#!/bin/csh\nsetenv PW_CMD_0 ns-httpd\nsetenv PW_CMD_1 'se.sparc'setenv PW_CMD_2 'dtmail'setenv PW_CMD_3 'dt'setenv PW_CMD_4 'roam'setenv PW_CMD_5 'netscape'setenv PW_CMD_6 'X'setenv PW_USER_7 $USERsetenv PW_USER_8 'root'setenv PW_COUNT 10exec /opt/RICHPse/bin/se -DWIDE pw.se 60\nFIGURE 4-18 Sample pw.se Configuration Script\n16:00:09 nproc 61  newproc 59  deadproc 0\nwk command args user count usr% sys% wait% chld% size pf pwait% ulkwt% chario sysc vctx ictx msps\n0 ns-httpd 0 0.0 0.0 0.0 0.0 0 0 0.0 0.0 0 0 0 0 0.00\n1 se.sparc 1 26.0 13.8 0.8 24.4 2792 6 0.8 0.0 20531 0 9 15 16.55\n2 dtmail 0 0.0 0.0 0.0 0.0 0 0 0.0 0.0 0 0 0 0 0.003 dt 3 0.0 0.0 0.0 0.0 11132 0 0.0 0.0 64 0 0 0 0.00\n4 roam 2 1.3 0.2 0.0 0.0 31840 0 0.0 0.0 897 8022 3.74\n5 netscape 2 0.1 0.0 0.0 0.0 34668 0 0.0 0.0 108 1000 0.00\n6 X 2 3.2 0.8 0.0 0.0 27188 0 0.0 0.0 4571 0 10 3 3.09\n7 adrianc 16 0.0 0.0 0.1 0.8 32048 1 0.1 0.0 10 2093 0.03\n8 root 33 0.0 0.0 0.0 6.4 41224 1 0.0 0.0 47 0 0 3 0.09\n9 0 0 . 0 0 . 0 0 . 0 0 . 00 0 0 . 0 0 . 00000 0.00\n10 Total * * 59 30.6 14.9 0.9 31.5 180892 7 0.9 0.0 35375 0 21 23 10.32\nFIGURE 4-19 Example Output from Configured pw.se\n\nChapter 4 Workload Management 55Process Rule\nOnce you have collected the data, you can write a rule that examines each process or\nworkload and determines, using thresholds, whether that workload is CPU-bound,memory-bound, I/O bound, or suffering from some other problem. This informationcan then be used as input to determine the resources that must be increased forworkloads that are under-performing. A prototype of this rule is implemented in theSE Toolkit, and it can produce the kind of information shown in\nFIGURE 4-20 . The\nexample was taken while saving changes to a large file, hence the process wasdetected to be I/O bound. In this example the threshold is set to zero. In practice,you would set higher thresholds.\nNow we will look at some sample workloads and see how the tools for workload\nanalysis can be used in practice.\nInternet Service Provider Workloads\nThe Internet provides a challenge for managing computer systems. Instead of asmall population of known users who can connect to a server system, millions ofusers can connect to any server that is on the Internet. External synchronizing eventscan cause a tidal wave of users to arrive at the server at the same time, as shown inthe following examples.\n\u25a0Sports-related web sites in the USA get a peak load during key games in the\u201cMarch madness\u201d college basketball season. In general, any television or newsmedia event may advertise a URL to which users can connect, and cause a spikein activity.\n\u25a0In countries where local phone calls are charged by the minute, the onset of cheaprate calls at 6:00 p.m. each day causes a big spike in the number of dial-inconnections at an ISP , creating a major load on the local servers at that time.% se pry.se 791\n...monpid 791  nproc 94  newproc 0  deadproc 015:34:45 name lwmx pid ppid uid usr% sys% wait% chld% size rss pf inblk outblk chario\nsysc   vctx   ictx   msps\nmaker5X.exe 1 791 1 9506 4.77 1.41 0.45 0.00 25600 22352 1.7 0.00 0.00 139538\n858  27.13   5.84   1.87\n  amber: IObound\namber:  0.2%   [ 0.0%  ] process is delayed by page faults in data file access\nFIGURE 4-20\nSample Process Monitoring Rule\n\n56Resource Management\u25a0Proxy caching web servers sit between a large number of users and the Internet\nand funnel all activity through the cache. They are used in corporate intranets andat ISPs. When all the users are active at once, regardless of where they areconnecting to, these proxy cache servers get very busy.\nOn the other hand, many web sites get little or no activity most of the time. It is too\nexpensive to dedicate a single computer system to each web site, so a single systemmay be configured to respond to hundreds or thousands of internet addresses. Thisis sometimes known as virtual web hosting. From a user perspective it is notpossible to tell that accesses to the different web sites are going to the same physicalsystem. If one of the virtual web sites becomes very busy, the performance ofaccesses to all the other virtual sites can be affected, and resource management toolsare needed to help maintain a fair quality of service for all the sites.\nThis section looks at three case studies: a proxy web cache workload, virtual web\nhosting, and a content provider site.\nProxy Web Cache Workload\nA caching web server acts as an invisible intermediary between a client browser andthe servers that provide content. It cuts down on overall network traffic andprovides administrative control over web traffic routing. Performance requirementsare quite different from those for a regular web server. After discussing the issues,we\u2019ll look at several metrics that must be collected and analyzed to determine theworkload mix and the management of resources.\nWe start by remembering the caching principle of temporal and spacial locality. In\nthe case of a web cache, cacheable objects are always separate and are always read intheir entirety with no prefetching. The proxy web cache mostly works by usingtemporal locality. If cached items are read more than once by different users in areasonably short time interval, the cache will work well. If every user readscompletely different pages, the cache will just get in the way. If one user rereads thesame pages, that browser will tend to cache the pages on the client. So the proxyserver cache won\u2019t be used effectively.\nNot all web content is cacheable. Some of the busiest traffic is dynamic by nature,\nand caching it would prevent the browser from seeing an update.\nCache Effectiveness\nCaches commonly have a hit rate of about 20 to 30 percent, with only 60 percent ofthe data being cacheable. That figure does not take into account the size of eachaccess\u2014just the total number of accesses. Each cache transaction takes some time tocomplete and adds significant latency to the connection time. In other words, thecache slows down end users significantly, and only a small proportion of the data\n\nChapter 4 Workload Management 57read is supplied from the cache. All the users go through the cache regardless of\ntheir final destination. This funnel effect can increase network load at a local levelbecause accesses to local web servers will normally go via the cache, causing twotrips over the local network rather than one.\nDirect web service, as shown in\nFIGURE 4-21 , allows the browser to contact web\nservers directly whenever possible. This contact may involve a firewall proxy thatdoes not provide caching to get out to the public Internet.\nFIGURE 4-21 Direct Web Service Without Proxy Cache\nIn contrast, when a proxy is introduced, as shown in FIGURE 4-22 , all the browsers\nconnect to the proxy by default. An exception can be made for named systems anddomains that can be accessed more efficiently by a direct connection.Home server\nClient\nbrowsers\nLocal siteIntranet server\nRemote site\nIntranet server\nRemote sitejava.sun.com\nInternetwww.foo.com\nwww.bar.comanywhere.net\nVirtual HostsFirewall\n\n58Resource ManagementFIGURE 4-22 Indirect Web Service\nCaching for Administrative Control\nThe reduction in wide-area network traffic from a 20 to 30 percent hit rate is worth\nhaving, especially since wide-area links are expensive and may be saturated at peaktimes.\nThe real reason to set up a proxy cache intranet infrastructure is the administrative\ncontrol. Security is a big problem. One approach is to set up the firewall gateways tothe Internet so that they route web traffic only to and from the proxy caches. Theproxy caches can make intelligent routing decisions. For example, Sun hasconnections to the Internet in Europe and the USA. If a user in Europe accesses anInternet web site in Europe, that access is routed via the local connection. When anaccess is made to a site in the USA, a choice can be made. The route could go to theUSA over Sun\u2019s private network to the USA-based gateway, or it could go directly tothe Internet in Europe and find its own way to the USA. The second option reducesthe load on expensive private transatlantic links, but the first option can be used as afallback if the European connection goes down.Client browsersHome server\nno\nproxyfor home\nLocal site\nProxy cache Intranet server\nRemote sitejava.sun.com\nwww.foo.com\nwww.bar.comanywhere.netVirtual Hosts\nInternetFirewallIntranet server\nRemote site\n\nChapter 4 Workload Management 59Restricted routing also forces every end user who wants to get out to the Internet to\ndo so via a proxy cache. The cache makes routing and filtering decisions and logsevery access with the URL and the client IP address (which is usually sufficient toidentify the end user). If the corporate policy is \u201cInternet access is provided forbusiness use only during business hours,\u201d then employees who clog up thenetworks with non-business traffic can be identified. It is probably sufficient to makeit known to the employees that their activity is being logged and that they can betraced. Posting an analysis of the most visited sites during working hours foreveryone to look at would probably have a sobering effect. Filtering can be added todeny access to popular but unwelcome sites.\nThe point is that you have a limited amount of expensive wide-area network\ncapacity. There is a strong tendency for end users to consume all the capacity thatyou provide. Using gentle peer pressure to keep the usage productive seemsreasonable and can radically improve the performance of your network for realbusiness use.\nSolaris Bandwidth Manager software can be used effectively in this environment to\nimplement policies based on the URL, domain or protocol mix that is desired. Forexample, if traffic to an Internet stock quote server is crowding out more importanttraffic in the wide area links of a company intranet, Solaris Bandwidth Managersoftware can throttle access to that site to a specified percentage of the total. Thecombination of proxy cache controls and bandwidth management is a powerful andflexible way to optimize use of wide area network and Internet gateway bandwidth.\nClustered Proxy Cache Architectures\nThe Apache and Netscape\u2122 proxy caches are extensions of a conventional webserver. They are used singly, and although they can be used in a hierarchy of caches,there is no optimization between the caches. An alternative is a clustered cache. TheHarvest Cache was the original development and is now a commercial product. TheSquid cache is a freely available spin-off and is the basis for some commercialproducts. Squid is used at the biggest cache installations, caching traffic at thecountry level for very large Internet service providers.\nClustered caches use an intercache protocol (ICP) to talk among themselves and\nform an explicit hierarchy of siblings and parents. If the load would overwhelm asingle machine or if high availability is important, multiple systems are configuredas siblings. Each sibling stores data in its cache but also uses the ICP to search thecaches of other siblings. The net result: the effective cache size is that of all thesiblings combined, the hit rate is improved, and it doesn\u2019t matter which sibling aclient visits. The parent-child relationships also form a more efficient hierarchybecause the ICP-based connections are much more efficient than individual HTTPtransfers. ICP connections are kept open, and transfers have lower latency. When\n\n60Resource Managementusing bandwidth management in a clustered cache architecture, give the inter-cache\nprotocol high priority so that the response time for cache queries does not getaffected when the caches are generating a lot of network traffic.\nVirtual Web Hosting\nA huge number of people and businesses have registered their own personal domainnames and want the domain to be active on the internet with a home page. In manycases, there is little activity on these names, but they are assigned a fixed IP address,and somewhere a web server must respond to that address. In the Solaris operatingenvironment, it is possible to configure more than one IP address on a singlenetwork interface. The default limit is 256 virtual addresses per interface, but it hastested up to 8000 addresses per interface and could go higher. Use the ifconfig\ncommand to assign IP addresses to an interface and specify virtual interfaces with atrailing colon and number on the interface name. Use the ndd command to query or\nset the maximum number of addresses per interface. In the example below, twoaddresses are configured on hme0, and the default of up to 256 addresses is set.\nThe real limit to the number of virtual addresses is the time it takes to run\nifconfig command on all of them. When thousands of virtual addresses are\nconfigured, the reboot time is affected, which can be a significant problem in a highavailability failover scenario. The Solaris 2.6 operating environment was tuned tospeed up this process.\nWeb servers can be configured in several ways to manage many virtual addresses.\nThe most efficient way is to use a feature of the web server so that a single instanceof the server listens to many addresses and responds with the appropriate homepage. With a multithreaded web server, one process can support many addresses.With web servers like Apache that use a separate process for each connection, moreprocesses are spawned for the addresses that are most active at any point.%ifconfig -a\nlo0: flags=849<UP,LOOPBACK,RUNNING,MULTICAST> mtu 8232        inet 127.0.0.1 netmask ff000000hme0: flags=863<UP,BROADCAST,NOTRAILERS,RUNNING,MULTICAST> mtu 1500        inet 129.136.134.27 netmask ffffff00 broadcast 129.136.134.255hme0:1: flags=863<UP,BROADCAST,NOTRAILERS,RUNNING,MULTICAST> mtu 1500        inet 129.136.134.26 netmask ffffff00 broadcast 129.136.134.255hme1: flags=863<UP,BROADCAST,NOTRAILERS,RUNNING,MULTICAST> mtu 1500        inet 129.136.23.11 netmask ffffff00 broadcast 129.136.23.255\n# /usr/sbin/ndd -get /dev/ip ip_addrs_per_if\n256\n\nChapter 4 Workload Management 61Administrative problems may rule out this approach. It may be necessary to run a\ncompletely separate web instance of the web server for each address. This way theweb server runs using a distinct userid. The administrator of the web site can log inas that user to configure their own site. The web server and any search engine orcgi-bin scripts that it starts all run as that user so that accounting, performance\nmonitoring and resource management tools can manage each virtual web site\u2019sactivity separately. It is common for cgi-bin scripts to have errors and be resource\nintensive. They can get stuck in infinite loops and consume an entire CPU. Withthousands of customer web sites setup on a single system, the central administratorcannot control the quality of the custom cgi-bin code that is set up on each site.\nIn the end, this workload is similar in many ways to an old-fashioned central\ntimeshare system. Resource management using the SRM software to control the CPUusage and the number of processes and logins on a per-user basis is extremelyeffective. The Solaris Bandwidth Manager software can also be used effectively, asthere is an explicit administrative mapping from each network address to the useridthat is running that web server. Users can be allocated shares of the CPU andnetwork resources according to their need, or can pay more for a larger share and beconfident that they will get a better quality of service if the server saturates.\nManaging Web Servers With SRM Software\nThe SRM software manages resources on web servers by controlling the amount ofCPU and virtual memory. Three basic topologies are used on systems hosting webservers.\nResource Managing a Consolidated Web Server\nA web server can be managed by controlling the resources it can use. In anenvironment where a web server is being consolidated with other workloads, thisbasic form of resource management prevents other workloads from affecting theperformance of the web server, and vice versa.\n\n62Resource ManagementFIGURE 4-23 Resource Management of a Consolidated Web Server\nInFIGURE 4-23 , the web server is allocated 20 shares, which means that it is\nguaranteed at least 20 percent of the processor resources should the database placeexcessive demands on the processor. In addition, if a cgi-bin process in the web\nserver runs out of control with a memory leak, the entire system will not run out ofswap space; only the web server will be affected.\nManaging Resources Within a Single Web Server\nIt is often necessary to use resource management to control the behavior within asingle web server. For example, a single web server may be shared among manyusers, each with their own CGI-BIN programs. An error in one CGI-BIN programcould cause the entire web server to run slowly or, in the case of a memory leak,could even bring down the web server. To prevent this from happening, use the per-process limits.root\nonline\ncpu.shares=20 sharesbatch\ncpu.shares=1 share\nChuck MarkSales Database Web Server\ncpu.shares=80 sharescpu.shares=20shares\nmemory=50MB\n\nChapter 4 Workload Management 63FIGURE 4-24 Resource Management of a Single Web Server\nResource Management of Multiple Virtual Web Servers\nSingle machines often host multiple virtual web servers. In such cases, there are\nmultiple instances of the httpd web server process, and far greater opportunity\nexists to exploit resource control using the SRM software.\nYou can run each web server as a different UNIX userid by setting a parameter in the\nweb server configuration file. This effectively attaches each web server to a differentlnode in the SRM hierarchy. For example, the Solaris Web Server has the followingparameter, as shown in\nFIGURE 4-25 , in the configuration file, /etc/http/\nhttpd.confWeb Server\ncpu.shares=20 shares\nmemory=50 Mbytes/usr/lib/httpd\nWeb Server, httpd\nprocess.memory=5 Mbytes\n/cgi-bin/lookup.shCGI-BIN perl program\nlimited to 5 Mbytes ofvirtual memory\nShell program\nlimited to 5 Mbytes of\nvirtual memorylimited to 5 Mbytes of virtual memory\n/cgi-bin/form.pl\n\n64Resource ManagementFIGURE 4-25 Solaris Web Server Parameter File\nBy configuring each web server to run as a different UNIX userid, you can set\ndifferent limits on each web server. This is particularly useful for control and foraccounting for resource usage on a machine hosting many web servers. You canmake use of most or all of the SRM resource controls and limits:\nShares [ cpu.shares ] The CPU shares can proportionally allocate\nresources to the different web servers.\nMem limit [ memory.limit ] The memory limit can limit the amount of virtual\nmemory that the web server can use. Thisprevents any one web server from causinganother to fail from memory allocation.# Server parameters\nserver  {  server_root                   \"/var/http/\"  server_user                   \"webserver1\"  mime_file                     \"/etc/http/mime.types\"  mime_default_type             text/plain  acl_enable                    \"yes\"  acl_file                      \"/etc/http/access.acl\"  acl_delegate_depth            3  cache_enable                  \"yes\"\ncache_small_file_cache_size 8 # megabytes\ncache_large_file_cache_size 256 # megabytes\ncache_max_file_size 1 # megabytes\ncache_verification_time 10 # seconds\ncomment \"Sun WebServer Default Configuration\"\n  # The following are the server wide aliases\nmap /cgi-bin/ /var/http/cgi-bin/ cgi\n  map   /sws-icons/             /var/http/demo/sws-icons/  map   /admin/                 /usr/http/admin/\n# To enable viewing of server stats via command line,\n# uncomment the following line\nmap /sws-stats dummy stats\n}\n\nChapter 4 Workload Management 65Proc mem limit [ memory.plimit ] The per-process memory limit can limit the\namount of virtual memory a singlecgi-bin process can use. This stops any cgi-\nbin process from bringing down its respective\nweb server.\nProcess limit [ process.limit ] The maximum number of processes allowed to\nattach to a web server. It effectively limits thenumber of concurrent cgi-bin processes.\nCommercial Workloads\nIf you want to consolidate or control the relative importance of applications in\ncommercial workloads, you must manage system resources. For example, if youwant to mix decision support and OLTP applications on the same system, you needresource management so that the decision support does not affect the response timesfor the users of the OLTP system. Even without consolidation, workload resourcemanagement is an important tool that can provide resource allocation for usersaccording to importance. (For example, in the case where telesales order users havea higher importance than general accounting users.)\nThis section examines some aspects of the resource management that apply to these\ntypes of problems. It focuses on workload consolidation and resource managementof databases and analyzes the following workloads:\n\u25a0NFS Servers\n\u25a0Databases\n\u25a0Batch workloads\n\u25a0Mail servers\nWorkload Consolidation\nWorkload consolidation is the most requested form of commercial workloadresource management. It allows multiple applications to co-exist in the same Solarisoperating environment with managed resource allocation so that one workload doesnot adversely affect another. The success of workload consolidation is bound closelyto the ability to partition resources between the applications, and it is important tounderstand the types of resources being managed and how they interact.\nThe resources used by commercial applications fall into the following categories:\n\u25a0CPU cycles\n\n66Resource Management\u25a0Physical memory\n\u25a0Virtual memory and swap space\n\u25a0Disk bandwidth\n\u25a0Disk space\n\u25a0Network bandwidth\nEach application uses these resources differently, and the ability to consolidate one\napplication with another is governed by how effectively you can manage eachresource. Sometimes resources cannot be managed by the Solaris operatingenvironment. These types of applications cannot be mix with others. However, inmost cases, combinations of resource management products can solve this problem.\nThe NFS Server Workload\nThe unique characteristics of an NFS workload place multiple resource demands onthe system. No single product can provide complete resource management of anNFS workload. The resources used by NFS and the different products or techniquesthat can be used to manage them are shown in\nTABLE 4-1 .\nTABLE 4-1 NFS Workload Resources\nResource TypeManagement Product/\nMethod Comments and Issues\nCPU Processor Sets /\nnfsd/ SolarisBandwidth ManagerLimit by constraining the system processor\nset.Control the number of nfsd threads\nnfsd(1M).Indirectly limit the number of NFSoperations by using Solaris BandwidthManager.\nPhysical Memory Priority Paging Install and enable priority paging so that\nfile systems will not consume all physicalmemory.\nSwap Space N/R NFS uses very little swap space.Disk Bandwidth Separate Disks/\nSolaris BandwidthManagerEither put the NFS file systems on a\ndifferent storage device or indirectly useSolaris Bandwidth Manager to control theNFS request rate.\nDisk Space File System Quotas Use file system quotas to limit disk space\nallocation. See \u201cBase Solaris OperatingEnvironment\u201d on page 136.\nNetwork Bandwidth Solaris Bandwidth\nManagerUse Solaris Bandwidth Manager to control\nthe rate of NFS operations.\n\nChapter 4 Workload Management 67Controlling NFS CPU Usage\nNFS servers are implemented in the Solaris kernel as kernel threads and run in the\nsystem class. You can control the amount of resource allocated to the NFS server inthree ways:\n\u25a0Limit the number of NFS threads with nfsd\n\u25a0Limit the amount of CPU allocated to the system class with psrset\n\u25a0Indirectly control the number of NFS ops with Solaris Bandwidth Manager.\nYou can control the number of NFS threads by changing the parameters to the NFS\ndaemon when the NFS server is started. Edit the start-up line in/etc/rc3.d/S15nfs.server :\nThe actual number of threads required will vary according to the number of requests\ncoming in and the time each thread spends waiting for disk I/O to service therequest. There is no hard tie between the maximum number of threads and thenumber of NFS threads that will be on the CPU concurrently. The best approach is toapproximate the number of threads required (somewhere between 16 to 64 perCPU), and then find out if the NFS server is doing its job or using too much CPUtime.\nNFS is a fairly lightweight operation, so it is unlikely that the NFS server CPU usage\nis an issue. The CPU time consumed by the NFS server threads accumulates assystem time. If the system time is high, and the NFS server statistics show a highrate of NFS server activity, then curtail CPU usage by reducing the number ofthreads.\nA far more effective way to control NFS servers is to use the Solaris Bandwidth\nManager product to limit the traffic on the NFS port, 2049, and indirectly cap theamount of CPU used by the NFS server. The disadvantage is that spare CPUcapacity can be wasted because managing by bandwidth usage does not reveal howmuch spare CPU is available.\nTo understand if you have over constrained NFS\u2019s allocation of resources you can\nuse the new NFS iostat metrics and look at the %busy column.\nNFS Metrics\nLocal disk and NFS usage are functionally interchangeable, so the Solaris 2.6\noperating environment was changed to instrument NFS client mount points thesame way it does disks. NFS mounts are always shown by iostat andsar. With/usr/lib/nfs/nfsd -a 16  (change 16 to number of threads)\n\n68Resource Managementautomounted directories coming and going more often than disks coming online,\nthat change may cause problems for performance tools that don\u2019t expect the numberofiostat orsar records to change often.\nThe full instrumentation includes the wait queue for commands in the client ( biod\nwait ) that have not yet been sent to the server. The active queue measures\ncommands currently in the server. Utilization ( %busy ) indicates the server mount-\npoint activity level. Note that unlike the case with simple disks, 100% busy does not\nindicate that the server itself is saturated ; it just indicates that the client always has\noutstanding requests to that server. An NFS server is much more complex than adisk drive and can handle many more simultaneous requests than a single disk drivecan. This is explained in more detail later in this chapter.\nThe following is an example of the new -xnP option, although NFS mounts appear\nin all formats. Note that the Poption suppresses disks and shows only disk\npartitions. The xnoption breaks down the response time, svc_t , into wait and\nactive times and puts the device name at the end of the line. The vold entry\nautomounts floppy and CD-ROM devices.\nNFS Physical Memory Usage\nNFS uses physical memory in two ways: In the kernel each NFS thread consumessome space for a stack and local data, and outside the kernel the data being servedby NFS consumes memory as it is cached in the file system. The amount of kernelmemory used by NFS is easily manageable because its size doesn\u2019t change much andthe amount of memory required is small.\nThe amount of memory used by the file systems for NFS servers is, however, very\nlarge and much harder to manage. By default, any non-sequential I/O and non-8KBI/O uses memory at the rate of data passed though the file system. The amount ofmemory used grows continuously. When there is no more free memory, memory istaken from other applications on the system.\nBy using priority paging (see \u201cPriority Paging\u2014Memory Policy by Importance\u201d on\npage 33) you can apply a different resource policy to the memory system to preventthis from happening. With priority paging, the file system can still grow to use free%iostat -xnP\n                              extended device statistics  r/s  w/s   kr/s   kw/s wait actv wsvc_t asvc_t  %w  %b device  0.0  0.0    0.0    0.0  0.0  0.0    0.0    0.0   0   0 vold(pid363)  0.0  0.0    0.0    0.0  0.0  0.0    0.0    0.0   0   0 servdist:/usr/dist\n0.0 0.5 0.0 7.9 0.0 0.0 0.0 20.7 0 1 servhome:/export/home2\n  0.0  0.0    0.0    0.0  0.0  0.0    0.0    0.0   0   0 servmail:/var/mail  0.0  1.3    0.0   10.4  0.0  0.2    0.0  128.0   0   2 c0t2d0s0  0.0  0.0    0.0    0.0  0.0  0.0    0.0    0.0   0   0 c0t2d0s2\n\nChapter 4 Workload Management 69memory, but cannot take memory from other applications on the system. Priority\npaging should be mandatory for any system that has NFS as one of the consolidationapplications.\nNFS and Swap Space\nNFS uses a very small amount of swap space, and there should be no inter-workloadswap space issues from NFS.\nNFS Disk Storage Management\nYou can manage NFS disk storage using UFS disk quotas. See \u201cDisk Quotas\u201d onpage 130 for more information.\nControlling NFS with Solaris Bandwidth Manager Software\nYou can control amount of resources consumed by NFS indirectly by curtailing theamount of network bandwidth on port 2049. The Solaris Bandwidth Managerproduct provides the means to do this.\nFirst assess the network interfaces that need to be controlled. If clients come in over\nseveral network interfaces, all of these interfaces must be brought under control bythe Solaris Bandwidth Manager software.\nWhen defining interfaces in the Solaris Bandwidth Manager software, you must\nspecify whether incoming or outgoing traffic needs to be managed. In the case ofNFS, network traffic could go in both directions (reads and writes). In the SolarisBandwidth Manager configuration, this would look as follows:\ninterface hme0_in\nrate      100000000         /* (bits/sec) */activate  yes\ninterface hme_out\nrate 100000000activate yes\n\n70Resource ManagementNext define the service you want to manage. The Solaris Bandwidth Manager\nsoftware already has two pre-defined classes for NFS:\nPut in place a filter that can categorize network traffic in NFS and non-NFS traffic:\nThe filter in the above example is for managing outgoing NFS traffic to the\n129.146.121.0 network. You could decide to leave out the destination and manageNFS traffic to all clients, from wherever they come.\nCreate another nfs_in filter for NFS traffic in the opposite direction. Only the src\nanddst parts need to be reversed.\nLastly, create a class that will allocate a specific bandwidth to this filter:service nfs_udp\nprotocol udpports 2049, *ports *, 2049\nservice nfs_tcp\nprotocol tcpports 2049, *ports *, 2049\nfilter nfs_out\nsrc\ntype    hostaddress servername\ndst\ntype    subnetmask    255.255.255.0address 129.146.121.0\nservice\nnfs_udp, nfs_tcp\nclass managed_nfs\ninterface     hme_outbandwidth     10max_bandwidth 10priority       2filter        nfs_out\n\nChapter 4 Workload Management 71This class sets a guaranteed bandwidth of 10 percent of the available bandwidth (10\nMbytes in case of fast Ethernet). Control the maximum bandwidth by setting anupper bound to the CPU resources that NFS consumes on the host. The key variableismax_bandwidth ; it specifies an upper bound to the consumed bandwidth that\nnever will be exceeded. You could even set the bandwidth variable to 0, but this\ncould lead to NFS starvation if other types of traffic will be managed as well.\nThepriority variable is less important. It will be a factor if other types of traffic\nare being managed. Generally, higher priorities will have lower average latencies,because the scheduler gives them higher priority if it has the choice (within thebandwidth limitations that were configured).\nIt is not easy to find a clear correlation between NFS network bandwidth and NFS\nserver CPU utilization. That correlation depends very much on the type of NFSworkload for your server. A data-intense NFS environment that moves large filesand tends to saturate networks is very different from an attribute-intenseenvironment, which tends to saturate CPUs with lots of small requests for fileattributes, such as a software development compile and build server.Experimentation will determine what\u2019s good for you. Your administrator could evendevelop a program that monitors NFS CPU utilization, and if it gets too high, usethe Solaris Bandwidth Manager APIs to dynamically limit the bandwidth more, allautomatically and in real time.\nDatabase Workloads\nResource management of databases can be somewhat simplified by viewing thedatabase as a \u201cblack box\u201d and managing resources around it. This strategy isparticularly useful when doing server consolidation because the main objective withconsolidation is to partition one workload from an other.\nIn most commercial database systems several packages interact with the database to\nprovide the overall application environment to the user. The typical data centerstrategy is to consolidate more workloads onto each system. Resource managementof the whole environment requires careful planning and a solid understanding of theinteraction between these packages and the database.\nA look at a typical example where a single database instance provides database\nservices for two different applications that require access to the same data, uncoverssome of the complexities of database resource management.\nFIGURE 4-26 shows an\ntypical backend database server with a web application used for customer orderentry. and an ad-hoc decision support user that generates large queries against adatabase.\n\n72Resource ManagementFIGURE 4-26 Practical Example with a Database Server and a descision support process\nThe life cycle of each transaction crosses several management boundaries. No single\ncontrol or policy can assign resources to ensure that adequate CPU is provided tomeet response times. For example, if the\ndescision support user is retrieving a large\nnumber of transactions from the database, the web user may not be able to getadequate response from the database during that time. Ideally, you want to controlthe allocation of resources as the transaction proceeds though the system. A timeline of the transaction though the system is shown in\nFIGURE 4-27 .\nFIGURE 4-27 Transaction Flow and Resource Management Product Assignment\n1. Network Latency\u2014The user issues an http request from the desktop, which\nmust travel across the intranet/Internet to the web server. This component can bemanaged by controlling the bandwidth and network priorities into the web serverWeb User\nDecisionWeb Server\ncgi-bin\nuserid=nobodywebform.pl Backend Database\nuserid=db\nDatabase\nListener\nDatabase Shadow\nProcesses\nuserid=dbDecision\nSupport\nuserid=sasuserSupport\nUserProcess\n1. Network\nLatency2. Web\nserverSBM\nURL FilterSRM\nuserid=nobodySRM\nlnode per scriptDB Resource Manager\nprsetper instance\nSRM\nuserid=nobodySBM\nURL Filter\n3. cgi-bin\nPerl exec timeand script execution4. DB\nListenerconnect5. DB\nSQL txnlatency6. Web\nserverresponse7. Network\nLatency\n\nChapter 4 Workload Management 73with the Solaris Bandwidth Manager software. Often the network infrastructure\nbetween the client browser and the web server is outside the control ofmanagement (for example, the internet). Managing the bandwidth on the networkport on the machine that hosts the web server is a useful strategy to ensure thatkeyhttp requests get the required bandwidth, while other lower priority data\ntransfers (such as an ftp transfer) are constrained. The Solaris Bandwidth\nManager software can control the bandwidth allocation into the web server, andcgi-bin can be controlled using bandwidth manager URL prioritization. (See\nChapter 4.)\n2. The Web Server\u2014The web server usually runs as a single user, and often as the\nusernobody . Ensure that the web server as a whole gets a sufficient share of CPU.\nUse SRM software to ensure that adequate CPU is allocated by assigning CPUshares to the nobody user.\n3. The Web Server cgi-bin script\u2014The cgi-bin script is forked by the web server,\nand by default also runs as the userid nobody . Use the SRM software to limit the\namount of CPU the cgi-bin script uses by creating a node in the SRM hierarchy\nfor that cgi-bin script and using the srm user command as a wrapper to the\ncgi-bin script to assign resource control limits to that point in the hierarchy. See\n\u201cManaging Resources Within a Single Web Server\u201d on page 62 for moreinformation on how to control cgi-bin scripts.\n4. The Database Listener\u2014The database listener process accepts the incoming\nconnection from database client applications. (in this case the cgi-bin Perl\nscript). The database listener forks a database process and connects the clientapplication to the database.\n5. The Database Server Session\u2014The Database session can either be a new per-client\nprocess or some portion of the existing server processes. You can control theamount of CPU allocated to the database processes by using database vendor-specific resource control tools.\n6. Web Server Response\u2014Control passes back to the web server to output the data\nto the client browser. The same control as in the web server component (2) isimplemented here.\n7. Network Latency\u2014The response from the web server is transmitted over the\nnetwork. The Solaris Bandwidth Manager software again controls the bandwidthout of the port on the host to the internet/intranet.\nEach stage of the transaction uses different resources. You can apply different\ntechniques to each stage. The more difficult part of using today\u2019s technologies ismanaging the priority of the transaction once it enters the database. You can usefine-grained resource management to control the amount of CPU allocated to theusers in the database, but you must ensure that external control of the database byresource management products is fully supported for the database in question.\n\n74Resource ManagementVirtual Memory and Databases\nThe SRM software and resource limits can limit the amount of virtual memory used\nby processes, users, and workloads. This capability does not manage physicalmemory, but it effectively restricts the amount of global swap space consumed byeach user.\nWhen a user or workload reaches the virtual memory limit, the system returns a\nmemory allocation error to the application, that is calls to malloc() fail. The same\nerror code is reported to the application as if the application had run out of swapspace.\nIn practice, many applications do not respond well to memory allocation errors\nbecause it is very hard to recover from a lack-of-memory error without making someadditional demand for memory. Because of this, it is a high risk to ever let adatabase server reach its virtual memory limit. If the limit is reached, the databaseengine may hang or crash, and the database may have to be restarted.\nIt is best to set a high virtual memory limit so that the limit can never be reached\nunder normal circumstances. However, the virtual memory limit can be used toplace a ceiling on the entire database server to stop a user with a memory leak fromaffecting other databases or workloads on the system.\nDatabase Consolidation\nYou can consolidate databases using a number of resource management techniques.Choose the technique based on the supportability and functionality of the solution.Not all databases are supported under each of the resource management facilities.Because most consolidated database servers are mission-critical production servers,this should be the primary concern.\nThe objective for successful consolidation is to provide strong insulation between\neach database instance. This requires careful resource management of CPU, memory,I/O, and network traffic.\nInsulating CPU Usage\nProcessor usage is a relatively easy resource to isolate. There are several mechanismsby which this can be achieved. There is a trade-off between the strength of insulationand the flexibility of resource allocation. At one end of the scale is Starfire dynamicsystem domains, which provides the same level of insulation as separate machinesbut requires manual intervention to shift resources between database instances.There is also a large granularity of resource allocation, which is four processors at atime. At the other end of the spectrum is the SRM software, which provides\n\nChapter 4 Workload Management 75fine-grained resource management with the ability to dynamically assign resources\nwhenever they are needed. But it also provides much lower insulation between theapplications.\nFIGURE 4-28 Consolidation of Two Databases with Solaris Resource Manager\nThe advantage with the SRM software is that resources are completely dynamic. Ifone database is not using the resources, they are available to the other database.System resources are not wasted because spare CPU cycles can be used by any otherworkload requiring them. The more workloads consolidated, the flatter the totalresource usage becomes. By doing this, you can often size a system for the averageresource requirements instead of the peaks.\nProcessor sets provide a middle ground. The granularity is one CPU, and the\nresources are easier to reallocate than with domains, but still relatively static innature. Processor sets and domains, however, provide the static processorassignment and granularity required by some databases. In some cases, this is theonly supported way of dividing resources between databases. Unfortunately,resources are statically assigned. Although some movement of resources is possible,it means that you must size each database instance for most of its peakrequirements.\nInsulating Memory Usage\nDatabases have three types of memory usage, which have very differentcharacteristics:\n\u25a0shared memory area\n\u25a0private memory area\n\u25a0file system paging memory\nThe shared memory area is often the largest component of the database\u2019s memory\nrequirements. It is the easiest to insulate between instances because the memory isallocated as shared memory. All mainstream databases allocate this as intimateroot\nSales Database Finance Database\ncpu.shares=60 shares cpu.shares=40 shares\n\n76Resource Managementshared memory (ISM), which wires down the memory so that it can\u2019t be paged.\nBecause this memory is wired down, the memory allocated to each instance staysallocated. One instance cannot steal memory from another. However, this poses achallenge for dynamic memory resource allocation, such as dynamic reconfigurationbecause the wired-down memory must be unallocated before it can be removed fromthe system, requiring quiescing of the database.\nThe private memory is the additional memory that a database uses, over and above\nthe large shared memory segment that is typical of most databases. The privatememory is the regular process memory used by each process that the databaseengine comprises, and it is regular pageable memory. For example, the Oracleshadow processes have a small private memory size of about 500k per process. TheOracle parallel query processes may use several megabytes per process. The privatememory area requirements for each database vary, but they are generally manytimes smaller than the global shared memory requirements. The amount of memoryused is sometimes configurable. Simple capacity planning ensures that sufficientsystem-wide memory exists to cater to all of the database private memoryrequirements. Take care when including\nDecision Support System ( DSS) database\ninstances because they often have huge private memory requirements.\nOften overlooked are the memory requirements of the file system. By using raw\ndevices for all tablespaces or by using the direct I/O feature of the UFS file system,the file system bypasses the Solaris page cache. In this case, we do not need to takeinto consideration the memory use of the file system.\nThe Solaris page cache causes a huge memory demand, which places undue\npressure on the database private memory and breaks down any insulation betweenestablished databases. Priority paging puts a hard fence between the file systemsand applications, and the file system will only use free memory in the system for itscache. If you plan to run a database on file systems, consider this a mandatoryrequirement. See Chapter 3 for more information.\nIn summary, to provide proper memory insulation between databases, ensure the\nfollowing:\n\u25a0Use ISM for the shared global areas to wire down memory.\n\u25a0Use proper capacity planning and parameters to ensure there is sufficient RAMfor private memory requirements.\n\u25a0Use either raw disk devices or ensure that priority paging is installed toprevent file system memory pressure.\nInsulating I/O\nEnsure that I/O activity from one application does not affect another application inan undesirable way. The best way to insulate I/O within a single database instanceis to make sure that application tables are placed on separate I/O devices. This still\n\nChapter 4 Workload Management 77leaves shared log devices, which can be a source of I/O contention. Ensure that\nadequate I/O bandwidth, spindles, and, where possible, hardware caching areprovided for adequate performance for the log devices.\nDatabases can be consolidated by simply ensuring that each database is assigned its\nown I/O devices. This completely insulates one database from another. However,you must also make sure that all data paths from the server to the storage device arecarefully isolated. For example, two databases can be placed on their own set ofdisks within a Sun StorEdge\u2122 A3500 storage system. However, because both sets ofdisks use the same SCSI channel to the host the databases, I/O is not properlyinsulated.\nIf you must use a single storage controller, use capacity planning so that sufficient\nbandwidth is available to combine both. For OLTP applications this is rarely an issuebecause the bandwidth requirements are so low. For example, an OLTP systemgenerating 2000 8-kilobyte IOPS only requires 16 Mbytes of I/O bandwidth. Adecision support application is completely different. A single decision supportworkload can generate several hundred megabytes a second of I/O. In addition tothis, decision support often generates enough I/O to overwrite the cachecontinuously (cache wiping), which hurts the OLTP random I/O that makesextensive use of the storage cache. Do not consider a single storage controller forconsolidation of decision support workloads with OLTP workloads.\nDatabases in a Resource Managed Environment\nRelationships with databases and resource management products are many and varywith each database type. For example, if you add CPU resources to an instance ofOracle 7, the Oracle engine automatically picks up those resources, whereas someother databases must be restarted to use the newly available resources (it should benoted however that internal table sizes and latch sizing will not be automaticallyadjusted, and performance may improve after a restart of the database). Tounderstand how different databases work within a resource managementenvironment, look at the different database topologies and explore what happens toeach database when you add or remove CPU, memory, or I/O resources.\n\n87CHAPTER5\nWorkload Measurements\nIf you want to manage a resource, you must measure its usage.\nThe generic types of measurements that you can obtain directly or indirectly via\nrelated measurements are throughput, utilization, queue length and response time.These measurements are made at several levels, including business operations,application, user, system, network, process, and device level.\nThis chapter discusses measurement sources and the meaning of the available\nmeasurements that are important for resource management.\nOne problem in documenting the details of Solaris measurements is that far more\nmeasurements are available than can be displayed by the standard tools such asvmstat ,sar, andps. Most commercial performance tools read the kernel directly\nand have access to the full range of measurements, although they do not use all ofthem. The SyMON product collects a large proportion of the availablemeasurements. The most convenient way to explore this data is to use the SE Toolkit.This freely available but unsupported tool provides full access to all the raw datasources in the Solaris operating environment and generates higher level processeddata. It is used to prototype ideas that can then be promoted for incorporation inproducts, in particular for the Sun Enterprise SyMON 2.0 software.\nThe SE Toolkit\nThe SE Toolkit is based on a C language interpreter that is extended to make all theSolaris measurement interfaces available in an easy form. The code that takesmetrics and processes them is provided as C source code and runs on the interpreterso that it is easy to trace where data comes from and how it is processed. You canthen write your own programs in any language to obtain the same data. The SEToolkit has been jointly developed by Richard Pettit and Adrian Cockcroft as a\u201cspare time\u201d activity since 1993. Richard worked at Sun but is currently at Foglight\n\n88Resource ManagementSoftware, and Adrian is one of the authors of this book. The SE Toolkit can be\ndownloaded from http://www.sun.com/sun-on-net/performance/se3 .\nDetailed information and examples of how to write code that reads the kernel andthe SE Toolkit itself can be found in Sun Performance and Tuning, Java and the Internet ,\nby Adrian Cockcroft and Richard Pettit, Sun Press 1998.\nMeasurement Levels\nMeasurements can be classified into several levels. Data from lower levels isaggregated and merged with new data as more valuable higher level measurementsare produced.\n\u25a0Business operations\nBusiness workloads are broad-based and are not only computer oriented. Use a\nform that makes sense to managers and non-technical staff to represent the part ofthe business that is automated by the computer system.\n\u25a0Applications\nThe business operation can be broken down into several applications such as\nsales and distribution, e-commerce web service, email, file, and print. Examples ofapplication-specific measurements are order entry rate, emails relayed, and webserver response time.\n\u25a0Users\nEach class of user interacts with several applications. The number of users and\nthe work pattern of each class of users should be understood.\n\u25a0Networks\nNetworks connect the users to the applications and link together multiple systems\nto provide applications that are replicated or distributed. Measure traffic patternsand protocol mixes for each network segment.\n\u25a0Systems\nSystem-level measurements show the basic activity and utilization of the memory\nsystem and CPUs. Some network measurements, such as TCP/IP throughput, arealso available on a per system basis. Per process activity can be aggregated at aper system level then combined with network measurements to measuredistributed applications.\n\u25a0Processes\nProcess measurements show the activity of each user and each application.\nCurrent process activity can be monitored, and accounting logs provide a recordof who ran what when.\n\nChapter 5 Workload Measurements 89\u25a0Devices\nDevices, such as disks and network interfaces, are measured independently and\naggregated at the system level. There are few ways to link the usage of a device toa process or a user automatically, so detailed information about the configurationof devices and the usage of file systems by applications is needed.\nThe Application Resource Measurement\nStandard\nThe Application Resource Measurement (ARM) standard aims to instrument\napplication response times. The intent is to measure end-to-end response time bytagging each ARM call with a transaction identifier and tracking these transactionsas they move from system to system. If a user transaction slows down, the system inthe end-to-end chain of measurements that shows the biggest increase in responsetime can be pinpointed.\nThis technique sounds very useful, and there is both good and bad news about\nARM. The good news is that all vendors support the one standard, and severalimplementations exist, from Hewlett-Packard and Tivoli (who jointly invented thestandard) to the more recently developed BMC Best/1 and Landmark. The bad newsis that application code must be instrumented to measure user response time, andsophisticated tools are needed to handle the problems of distributed transactiontracking. Application vendors have shown interest, so more measurements willbecome available. The latest versions of Baan products include ARM-basedinstrumentation.\nSAP R/3 Measurements\nThe SAP R/3 enterprise resource planning package has three tiers of systems: userdesktops, application servers, and a backend database. The application instrumentsitself and measures the performance of key user transactions and the response timeof the backend database for each application. This information is provided by afacility called CCMS, and it is used by several tools such as BMC Best/1 to providemore detailed application-level management than can be done with just system anddevice-level information.\n\n90Resource ManagementInternet Server Measurements\nAn Internet server complex often provides a range of services and uses applications\nsuch as firewalls, web servers, and content generation databases. Each of theseapplications supports a range of operations that you can log or monitor to discoverthe rate of use and the mixture of operations. In some cases, the time taken for eachoperation is also logged, so you can obtain a response time measurement.\nOne of the most common applications is the proxy web cache service. Because this\napplication often acts as a funnel for all the web traffic going out of a corporateintranet to the Internet itself, a good measure of overall Internet response time canbe obtained. An example scenario follows.\nConfiguring and Monitoring a Proxy Cache\nData is collected on a Netscape 2.5 proxy cache that is serving most of the thousandor so employees in Sun\u2019s United Kingdom facility. The server is situated at thelargest site, so some users have LAN-based access speeds. However, several othersites in the UK, run over wide area links to the central site. Additionally, a numberof users are connected over ISDN and modem dial-up connections.\nThe access log contains the same data that a normal web server log contains, but\nseveral additional values are logged. Netscape supports two extended formats andalso allows a custom format to be specified. The interesting data to extract is themixture of possible cache outcomes that determines whether it is a cache hit or not,as well as the routing information and transfer time. The SE Toolkitpercollator.se script can parse Netscape and Squid proxy cache log formats and\nsummarize the data.\nThe route data is analyzed to count all the entries that go to PROXY or SOCKS and\nreport the percentage that are indirect. This is the percentage that gets routed to theInternet, rather than being DIRECT references to other servers in the corporateintranet or incomplete transfers marked with a \u201c-\u201d.\nThe cache finish status was analyzed, and operations are divided into four\ncategories. The NO-CHECK and UP-TO-DATE states are cache hits. The WRITTEN,REFRESHED, and CL-MISMATCH states are misses that cause cache writes. TheDO-NOT-CACHE and NON-CACHEABLE states are uncacheable, and anything elseis an error or incomplete transfer. The percentages for the first three are recorded.\nThe total transfer time is recorded. The average transfer time can be calculated, but\nsince the transfer size varies from zero to several megabytes, the average transfertime for each of the size ranges for the mix must also be worked out. The mix in thepercollator.se code is based on the SPECweb96 size boundaries of up to\n\nChapter 5 Workload Measurements 911 Kbyte, 1 Kbyte up to 10 Kbytes, 10 Kbytes up to 100 Kbytes, 100 Kbytes up to\n1 Mbyte, and over 1 Mbyte. We end up with the percentage of ops and the average\ntransfer time in each size range.\nObserved Results in Practice\nOn a fairly quiet weekday, 280,000 accesses went via this cache, and 56 percent of theaccesses went out to the Internet. The cache breakdown was as follows: 34 percent ofthe accesses hit in the cache, 16 percent missed and caused a cache write, 49 percentof the accesses were not cacheable, and 1 percent ended in some kind of error. Aweek\u2019s worth of data showed that the indirect and cache hit rates vary by 5 to 10percent of the total from day to day.\nThe transfer time averaged about 2.3 seconds. The problem with this number is that\nit included a small number of very large or very slow transfers. The average fortransfers up to 1 Kbyte was 1.8 seconds; for 1 to 10 Kbytes, it was 2.5 seconds; for10 to 100 Kbytes, it was 6 seconds; for 100 Kbytes to 1 Mbyte, it was 40 seconds; andfor over 1 Mbyte, it was 188 seconds. Within each of these size bands, connections tothe client browser took place over everything from 100-Mbit Ethernet to 28.8-Kbitmodems.\nTransfer Time Versus Size Distribution\nTo calculate the transfer time versus size distribution, 10,000 access log entries weretaken. After removing all zero content length and zero time transfers, size wasplotted against transfer time using log-to-log axes. The result showed that transfertime is not very dependent on size until the size gets into the 10 to 100 Kbyte region,as shown in\nFIGURE 5-1 .\n\n92Resource ManagementFIGURE 5-1 Log-to-Log Plot of Response Time Versus Size\nThe plot in FIGURE 5-1 shows bands of transfer times that depend upon the user\u2019s\nlocation. Many users are locally connected, but others are operating over slowernetworks. The transfer time includes the time spent waiting for the remote server torespond. Although it does not represent the extra time imposed by the cache, itactually gives a reasonable summary of the response time that users areexperiencing with their browsers. This information is useful because you can collectit at a central point rather than attempting to measure the response time at anindividual user site.\nA probability density histogram of the same data is shown in\nFIGURE 5-2 .10^00.1 1.0 10.0 100.0 1000.0\n10^1 10^2 10^3 10^4 10^5 10^6 10^7\n\nChapter 5 Workload Measurements 93FIGURE 5-2 Perspective Log-Log Plot of Response Time Versus Size Distribution\nThe plot in FIGURE 5-2 shows that the most probable response is at about 0.5 seconds\nand 1 Kbyte. Remember that the average response time is 2.3 seconds on this data. Itis clear that the average does not tell the whole story.\nInternet Servers Summary\nLog files are a rich source of performance information. Watching the rate at whichthey grow and breaking down the mix of logged operations is a powerful techniquefor getting good application-level performance measurements. You can apply thistechnique to many other applications, such as ftp and mail servers, as well as to\nany other application that can write a line to a file when it does some work.-2-1 65432100\n0.1\n0.2\n0.3\n0.4\n0.5\n012\n\n94Resource ManagementProcess Information\nMany users are familiar with the psprocess status command. But the pscommand\ndoes not provide access to all the information available from the Solaris operatingenvironment. It does provide a common set of data that is generally available on allUNIX systems, so we start by looking at that data, then move to the Solaris-specificdata to gain a better insight into what is happening to a process.\nThe underlying data structures provided by the Solaris operating environment are\ndescribed in full in the proc(4) manual page. The interface to /proc involves\nsending ioctl commands or opening special pseudo-files and reading them (a new\nfeature of the Solaris 2.6 release). The data that psuses is called PIOCPSINFO , and\nthis is what you get back from ioctl . The data is slightly different if you read it\nfrom the pseudo-file.\nproc(4)                   File Formats                    proc(4)\n  PIOCPSINFO\n     This returns miscellaneous process information such as that     reported by ps(1). p is a pointer to a prpsinfo structure     containing at least the following fields:\n     typedef struct prpsinfo {\nchar pr_state; /* numeric process state (see pr_sname) */\nchar pr_sname; /* printable character representing pr_state\n*/\nchar pr_zomb; /* !=0: process terminated but not waited\nfor */\n       char        pr_nice;     /* nice for cpu usage */       u_long      pr_flag;     /* process flags */       int         pr_wstat;    /* if zombie, the wait() status */       uid_t       pr_uid;      /* real user id */       uid_t       pr_euid;     /* effective user id */       gid_t       pr_gid;      /* real group id */       gid_t       pr_egid;     /* effective group id */       pid_t       pr_pid;      /* process id */       pid_t       pr_ppid;     /* process id of parent */       pid_t       pr_pgrp;     /* pid of process group leader */       pid_t       pr_sid;      /* session id */       caddr_t     pr_addr;     /* physical address of process */       long        pr_size;     /* size of process image in pages */\nFIGURE 5-3 Process Information Used as Input by the psCommand\n\nChapter 5 Workload Measurements 95For a multithreaded process, you can get the data for each lightweight process\nseparately. There\u2019s a lot more useful-looking information there, but no sign of thehigh-resolution microstate accounting that /usr/proc/bin/ptime and several SE\nToolkit scripts display. They use a separate ioctl ,PIOCUSAGE .       long        pr_rssize;   /* resident set size in pages */\n       u_long      pr_bysize;   /* size of process image in bytes */       u_long      pr_byrssize; /* resident set size in bytes */       caddr_t     pr_wchan;    /* wait addr for sleeping process */\nshort      pr_syscall;  /* system call number (if in syscall) */\nid_t pr_aslwpid; /* lwp id of the aslwp; zero if no aslwp */\ntimestruc_t pr_start; /* process start time, sec+nsec since epoch\n*/\n       timestruc_t pr_time;     /* usr+sys cpu time for this process */\ntimestruc_t pr_ctime; /* usr+sys cpu time for reaped children */\nlong pr_pri; /* priority, high value is high priority */\nchar pr_oldpri; /* pre-SVR4, low value is high priority */\n       char        pr_cpu;      /* pre-SVR4, cpu usage for scheduling */\nu_short pr_pctcpu; /* % of recent cpu time, one or all lwps */\nu_short pr_pctmem; /* % of system memory used by the process */\ndev_t pr_ttydev; /* controlling tty device (PRNODEV if none) */\n       char        pr_clname[PRCLSZ]; /* scheduling class name */\nchar pr_fname[PRFNSZ]; /* last component of exec()ed pathname */\nchar pr_psargs[PRARGSZ];/* initial characters of arg list */\n       int         pr_argc;     /* initial argument count */       char        **pr_argv;   /* initial argument vector */       char        **pr_envp;   /* initial environment vector */     } prpsinfo_t;\nproc(4)                   File Formats                    proc(4)\n  PIOCUSAGE\n     When applied to the process file descriptor, PIOCUSAGE     returns the process usage information; when applied to an     lwp file descriptor, it returns usage information for the     specific lwp.   p points to a prusage structure which is     filled by the operation. The prusage structure contains at     least the following fields:\n     typedef struct prusage {\nid_t pr_lwpid; /* lwp id. 0: process or defunct */\n          u_long         pr_count;    /* number of contributing lwps */\nFIGURE 5-4 Additional Microstate-based Process InformationFIGURE 5-3 Process Information Used as Input by the psCommand (Continued)\n\n96Resource Management          timestruc_t    pr_tstamp;   /* current time stamp */\ntimestruc_t pr_create; /* process/lwp creation time stamp */\ntimestruc_t pr_term; /* process/lwp termination timestamp */\ntimestruc_t pr_rtime; /* total lwp real (elapsed) time */\n          timestruc_t    pr_utime;    /* user level CPU time */          timestruc_t    pr_stime;    /* system call CPU time */          timestruc_t    pr_ttime;    /* other system trap CPU time */          timestruc_t    pr_tftime;   /* text page fault sleep time */          timestruc_t    pr_dftime;   /* data page fault sleep time */          timestruc_t    pr_kftime;   /* kernel page fault sleep time */          timestruc_t    pr_ltime;    /* user lock wait sleep time *          timestruc_t    pr_slptime;  /* all other sleep time */          timestruc_t    pr_wtime;    /* wait-cpu (latency) time */          timestruc_t    pr_stoptime; /* stopped time */          u_long         pr_minf;     /* minor page faults */          u_long         pr_majf;     /* major page faults */          u_long         pr_nswap;    /* swaps */          u_long         pr_inblk;    /* input blocks */          u_long         pr_oublk;    /* output blocks */          u_long         pr_msnd;     /* messages sent */          u_long         pr_mrcv;     /* messages received */          u_long         pr_sigs;     /* signals received */          u_long         pr_vctx;     /* voluntary context switches */          u_long         pr_ictx;     /* involuntary context switches */          u_long         pr_sysc;     /* system calls */          u_long         pr_ioch;     /* chars read and written */     } prusage_t;\n     PIOCUSAGE can be applied to a zombie   process   (see\n     PIOCPSINFO).\n     Applying PIOCUSAGE to a process that does not have micro-\n     state accounting enabled will enable microstate accounting     and return an estimate of times spent in the various states     up to this point.   Further invocations of PIOCUSAGE will     yield accurate microstate time accounting from this point.     To disable microstate accounting, use PIOCRESET with the     PR_MSACCT flag.\nFIGURE 5-4 Additional Microstate-based Process Information (Continued)\n\nChapter 5 Workload Measurements 97There is a lot of useful data here. The time spent waiting for various events is a key\nmeasure. It is summarized by msacct.se in the following figure:\nData Access Permissions\nTo access process data you must have access permissions for entries in /proc or run\nas a setuid root command. In the Solaris 2.5.1 release, using the ioctl access\nmethod for /proc , you can only access processes that you own, unless you log in as\nroot, and the ps command is setuid root . In the Solaris 2.6 release, although you\ncannot access the /proc/pid entry for every process, you canread/proc/pid/\npsinfo and/proc/pid/usage for every process. This means that any user can use\nthe full functionality of psand the process. The code for process_class.se\nconditionally uses the new Solaris 2.6 access method and the slightly changeddefinition of the psinfo data structure.\nMicrostate Accounting\nMicrostate accounting is not turned on by default. It slows the system down slightly.\nIt was on by default until the Solaris 2.3 release. From the Solaris 2.4 release on, it isenabled the first time you issue an ioctl to read the data. For the Solaris 2.6 access\nmethod, the process flags that enable microstate data collection and an inherit onfork option must be set directly via the /proc/pid/ctl interface.\nThe CPU time is normally measured by sampling 100 times a second, the state of all\nthe CPUs from the clock interrupt. Microstate accounting works as follows: A high-resolution timestamp is taken on every state change, every system call, every pagefault, and every scheduler change. Microstate accounting doesn\u2019t miss anything, andthe results are much more accurate than those from sampled measurements. Thenormal measures of CPU user and system time made by sampling can be wrong by20 percent or more because the sample is biased, not random. Process schedulinguses the same clock interrupt used to measure CPU usage. This approach leads tosystematic errors in the sampled data. The microstate-measured CPU usage datadoes not suffer from those errors.Elapsed time         3:20:50.049  Current time Fri Jul 26 12:49:28 1996\nUser CPU time           2:11.723  System call time        1:54.890System trap time           0.006  Text pfault sleep          0.000Data pfault sleep          0.023  Kernel pfault sleep        0.000User lock sleep            0.000  Other sleep time     3:16:43.022Wait for CPU time          0.382  Stopped time               0.000\nFIGURE 5-5 Summary of Microstate Process Data Reported by msacct.se\n\n98Resource ManagementFor example, consider a performance monitor that wakes up every ten seconds,\nreads some data from the kernel, then prints the results and sleeps. On a fast system,the total CPU time consumed per wake-up might be a few milliseconds. On exitfrom the clock interrupt, the scheduler wakes up processes and kernel threads thathave been sleeping until that time. Processes that sleep then consume less than theirallotted CPU time quanta always run at the highest timeshare priority. On a lightlyloaded system there is no queue for access to the CPU, so immediately after theclock interrupt, it is likely that the performance monitor will be scheduled. If it runsfor less than 10 milliseconds, it will have completed and be sleeping again before thenext clock interrupt. If you remember that the only way CPU time is allocated isbased on what was running when the clock interrupt occurs, you can see that theperformance monitor could be sneaking a bite of CPU time whenever the clockinterrupt isn\u2019t looking. When there is a significant amount of queuing for CPU time,the performance monitor is delayed by a random amount of time, so sometimes theclock interrupt sees it, and the error level decreases.\nThe error is an artifact of the dual functions of the clock interrupt. If two\nindependent, unsynchronized interrupts are used (one for scheduling and one forperformance measurement), then the errors will be averaged away over time.Another approach to the problem is to sample more frequently by running the clockinterrupt more often. This does not remove the bias, but it makes it harder to hidesmall bites of the CPU. The overhead of splitting up the interrupts is not worthimplementing. You can increase the CPU clock rate to get more accuratemeasurements, but the overhead is higher than using direct microstatemeasurement, which is much more useful and accurate because it measures moreinteresting state transitions than just CPU activity.\nThe case where this problem matters most is when a sizing exercise occurs by\nmeasuring a lightly loaded system, then scaling the results up for a higher loadestimate. Performance models that are calibrated at a light load also suffer from thisproblem. The best solution is to use a microstate accounting-based tool, or to disablesome of the CPUs so that the measurements are made on a fairly busy system.\nSome performance tool vendors are currently working to incorporate microstate-\nbased data in their products. An example implementation of a microstate-basedmonitor was built using the SE Toolkit. The data provided by the SE processmonitoring class is shown in\nFIGURE 5-6 .\n/* output data for specified process */\ndouble interval; /* measured time interval for averages\n*/\ndouble timestamp;       /* last time process was measured */double creation;        /* process start time */double termination;     /* process termination time stamp */\nFIGURE 5-6 Combined Process Information from SE Toolkit Process Class\n\nChapter 5 Workload Measurements 99double elapsed; /* elapsed time for all lwps in process\n*/\ndouble total_user;      /* current totals in seconds */double total_system;double total_child; /* child processes that have exited */double user_time;       /* user time in this interval */double system_time; /* system call time in this interval */double trap_time;       /* system trap time in interval */double child_time;      /* child CPU in this interval */double text_pf_time; /* text page fault wait in interval */double data_pf_time; /* data page fault wait in interval */double kernel_pf_time; /* kernel page fault wait in interval\n*/\ndouble user_lock_time;  /* user lock wait in interval */double sleep_time;      /* all other sleep time */double cpu_wait_time; /* time on runqueue waiting for CPU */double stoptime;        /* time stopped from ^Z */ulong syscalls; /* syscall/interval for this process */ulong inblocks;         /* input blocks/interval - metadata\nonly - not interesting */\nulong outblocks;        /* output blocks/interval - metadata\nonly - not interesting */\nulong vmem_size;        /* size in KB */ulong rmem_size;        /* RSS in KB */ulong maj_faults;       /* majf/interval */ulong min_faults; /* minf/interval - always zero - bug? */ulong total_swaps;      /* swapout count */long  priority;         /* current sched priority */long  niceness;         /* current nice value */char  sched_class[PRCLSZ];/* name of class */ulong messages;         /* msgin+msgout/interval */ulong signals;          /* signals/interval */ulong vcontexts; /* voluntary context switches/interval\n*/\nulong icontexts; /* involuntary context switches/interval\n*/\nulong charios;          /* characters in and out/interval */ulong lwp_count;        /* number of lwps for the process */int   uid;              /* current uid */long  ppid;             /* parent pid */char fname[PRFNSZ]; /* last component of exec'd pathname */char args[PRARGSZ]; /* initial part of command name and arg\nlist */\nFIGURE 5-6 Combined Process Information from SE Toolkit Process Class (Continued)\n\n100Resource ManagementMost of the data in FIGURE 5-6 is self explanatory. All times are in seconds in double\nprecision with microsecond accuracy. The minor fault counter seems to be brokenbecause it always reports zero. The inblock andoutblock counters are not\ninteresting because they only refer to file system metadata for the old-style buffercache. The charios counter includes all read and write data for all file descriptors,\nso you can see the file I/O rate. The lwp_count is not the current number of lwps.\nIt is a count of how many lwps the process has ever had. If the count is more than\none, then the process is multithreaded. It is possible to access each lwp in turn and\nread its psinfo andusage data. The process data is the sum of these.\nChild data is accumulated when a child process exits. The CPU used by the child is\nadded into the data for the parent. This can be used to find processes that areforking many short-lived commands.\nAccounting\nWho ran what, when, and how much resource was used?\nMany processes have very short life spans. You cannot see such processes with ps,\nbut they may be so frequent that they dominate the load on your system. The onlyway to catch them is to have the system keep a record of every process that has run,who ran it, what it was, when it started and ended, and how much resource it used.The answers come from the system accounting subsystem. While you may havesome concerns about accounting because of the \u201cbig brother is watching you\u201dconnotation or the cost of additional overhead, the information is important andvaluable. The overhead of collecting accounting data is always present but is\ninsignificant . When you turn on accounting, you are just enabling the storage of a few\nbytes of useful data when a process exits.\nAccounting data is most useful when it is measured over a long period of time. This\ntemporal information is useful on a network of workstations as well as on a single,time-shared server. From this information, you can identify how often programs run,how much CPU time, I/O, and memory each program uses, and what work patternsthroughout the week look like. To enable accounting to start immediately, enter thethree commands shown below.\n#ln /etc/init.d/acct /etc/rc0.d/K22acct\n#ln /etc/init.d/acct /etc/rc2.d/S22acct\n#/etc/init.d/acct start\nStarting process accounting\n\nChapter 5 Workload Measurements 101Refer to the accounting section in the Solaris System Administration Answerbook and\nsee the acctcom command. Add some crontab entries to summarize and\ncheckpoint the accounting logs. Collecting and checkpointing the accounting dataitself puts a negligible additional load onto the system, but the summary scripts thatrun once a day or once a week can have a noticeable effect, so schedule them to runoutside business hours.\nYourcrontab file for the adm user should contain the following:\nYou get a daily accounting summary, but it is best to keep track of the monthly\nsummary stored in /var/adm/acct/fiscal . Following is an excerpt from\nfiscrpt07 , which is the report for July on this desktop system.\nThe commands reported are sorted by KCOREMIN, which is the product of the\namount of CPU time used and the amount of RAM used while the command wasactive. CPU-MIN is the number of minutes of CPU time. REAL_MIN is the elapsedtime for the commands. SIZE-K is an average value for the RSS over the activelifetime of the process. It does not include times when the process was not actuallyrunning. (In Solaris 2.4 and earlier releases, a bug causes this measure to be invalid.)HOG FACTOR is the ratio of CPU-MIN to REAL-MIN; a high factor means that thiscommand hogs the CPU whenever it is running. CHARS TRNSFD counts thenumber of characters read and written. BLOCKS READ counts data read from block#crontab -l adm\n#ident  \u201c@(#)adm        1.5     92/07/14 SMI\u201d   /* SVr4.0 1.2   */#min    hour    day     month   weekday0       *       *       *       *       /usr/lib/acct/ckpacct3 0 2*** /usr/lib/acct/runacct 2> /var/adm/\nacct/nite/fd2log30      9       *       *       5       /usr/lib/acct/monacct\nJul 26 09:30 1996 TOTAL COMMAND SUMMARY FOR FISCAL 07 Page 1\nTOTAL COMMAND SUMMARY\nCOMMAND NUMBER TOTAL TOTAL TOTAL MEAN MEAN HOG CHARS BLOCKS\nNAME CMDS KCOREMIN CPU-MIN REAL-MIN SIZE-K CPU-MIN FACTOR TRNSFD READ\nTOTALS 26488 16062007.75 3960.11 494612.41 4055.95 0.15 0.01 17427899648 39944\nmae 36 7142887.25 1501.73 2128.50 4756.45 41.71 0.71 2059814144 1653\nsundgado 16 3668645.19 964.83 1074.34 3802.36 60.30 0.90 139549181 76Xsun 29 1342108.55 251.32 9991.62 5340.18 8.67 0.03 2784769024 1295xlock 32 1027099.38 726.87 4253.34 1413.04 22.71 0.17 4009349888 15fountain 2 803036.25 165.11 333.65 4863.71 82.55 0.49 378388 1netscape 22 489512.97 72.39 3647.61 6762.19 3.29 0.02 887353080 2649maker4X. 10 426182.31 43.77 5004.30 9736.27 4.38 0.01 803267592 3434wabiprog 53 355574.99 44.32 972.44 8022.87 0.84 0.05 355871360 570imagetoo 21 257617.08 15.65 688.46 16456.60 0.75 0.02 64291840 387java 235 203963.64 37.96 346.35 5373.76 0.16 0.11 155950720 240aviator 2 101012.82 22.93 29.26 4406.20 11.46 0.78 2335744 40se.sparc 18 46793.09 19.30 6535.43 2424.47 1.07 0.00 631756294 20xv 3 40930.98 5.58 46.37 7337.93 1.86 0.12 109690880 28\n\n102Resource Managementdevices (basically, local disk file system reads and writes). The underlying data that\nis collected can be seen in the acct (4) manual page. The data structure is very\ncompact\u2014about 40 bytes, as shown FIGURE 5-7 :\nSolaris Resource Manager Accounting\nThe SRM software provides the ability to generate accounting data based onresource usage. This is made available by the accumulation of resource informationin the lnode tree. Although the SRM software does not provide resource accounting,it does provide the data and data-extraction tools used to develop a system togenerate accounting (or billing) information. See Chapter 7 for more information onSRM Accounting.DESCRIPTION\n     Files produced as a result of calling acct(2) have records     in the form defined by <sys/acct.h>, whose contents are:\ntypedef ushort comp_t; /* pseudo \"floating point\" representation */\n                                  /* 3-bit base-8 exponent in the high */                                  /* order bits, and a 13-bit fraction */                                  /* in the low order bits. */\n     struct  acct\n     {             char    ac_flag;     /* Accounting flag */             char    ac_stat;     /* Exit status */             uid_t   ac_uid;      /* Accounting user ID */             gid_t   ac_gid;      /* Accounting group ID */             dev_t   ac_tty;      /* control tty */             time_t  ac_btime;    /* Beginning time */             comp_t  ac_utime;    /* accounting user time in clock */                                  /* ticks */             comp_t  ac_stime;    /* accounting system time in clock */                                  /* ticks */\ncomp_t ac_etime; /* accounting total elapsed time in clock */\n                                  /* ticks */             comp_t  ac_mem;      /* memory usage in clicks (pages) */             comp_t  ac_io;       /* chars transferred by read/write */             comp_t  ac_rw;       /* number of block reads/writes */             char    ac_comm[8];  /* command name */     };\nFIGURE 5-7 Accounting Data Format\n\nChapter 5 Workload Measurements 103Network Accounting Using NetFlow\nThe Solaris Bandwidth Manager software has built-in support for Cisco NetFlow\u2122\nsoftware. This feature allows for detailed network measurements that can be sent toother software packages that can process and analyze this data.\nBesides the Solaris Bandwidth Manager software, NetFlow data collection is\nsupported on a variety of Cisco devices, such as the Cisco 7000 and 7500 seriesrouters.\nWhen NetFlow is enabled on the Solaris Bandwidth Manager software or any Cisco\nequipment supporting NetFlow, network packets are processed and classified intoflows. A flow is a unidirectional series of packets that have several parameters incommon:\n\u25a0Source and destination IP address\n\u25a0Source and destination application port number\n\u25a0IP protocol type\n\u25a0IP ToS (Type of Service)\nA flow ends when either a FIN or RST packet is received for that flow, or when it\ntimes out.\nNetFlow-enabled devices send out NetFlow datagrams, which contain records for\none or more flows. Combining multiple flow records in one datagram reducesnetwork overhead caused by NetFlow. These datagrams are UDP based, so they canget lost in transit in case of a busy network. NetFlow version 5 and higher add asequence number to the datagram header, so the receiver of these datagrams can atleast know that a datagram was lost.\nEach NetFlow datagram record contains detailed flow information, such as the\nnumber of bytes in the flow, the number of packets, the duration, source anddestination IP address, and so on.\nCisco offers applications that read and process NetFlow datagrams. NetFlow\nFlowCollector\u2122 is a NetFlow datagram consumer for one or more NetFlow devices.These devices simply point to the host and port number on which the FlowCollectorsoftware is running. The FlowCollector aggregates this data, does preprocessing andfiltering, and provides several options to save this data to disk (such as flat files).Other applications, such as network analyzing, planning, and billing, can use thesefiles as input.\nThe NetFlow FlowAnalyzer\u2122 application uses the output from NetFlow\nFlowCollector. It provides elaborate processing, graphing, and reporting options fornetwork analysis, planning, troubleshooting, and more.\n\n104Resource ManagementYou can run the Solaris Bandwidth Manager software in statistics mode, where it\nwill process packets but not provide the priority scheduling. By using the NetFlowoutput of the Solaris Bandwidth Manager software, you can obtain detailed networkmeasurements that are not easily measured otherwise. For example, you can find outthe bandwidth used by a specific application or user by using NetFlow dataanalysis. The flow data can also provide latency information of specific applications.Trend data can show the long term impact of the deployment of certain applications.\nFor more information about Cisco NetFlow, see\nhttp://www.cisco.com/warp/public/732/netflow/index.html .\nStorage Measurements\nIndividual disks are well instrumented, but storage subsystems are now much more\ncomplex than a few individual disks. This section explains the basic Solaris softwaremeasures and discusses more complex disk subsystems.\nDisk Workloads\nThere are six basic disk access patterns. Read, write, and update operations caneither be sequentially or randomly distributed. Sequential read and write occurwhen files are copied and created or when large amounts of data are processed.Random read and write can occur in indexed database reads or can be due to page-in or page-out to a file. Update consists of a read-modify-write sequence and can becaused by a database system committing a sequence of transactions in either asequential or random pattern. When you are working to understand or improve theperformance of your disk subsystem, spend some time identifying which of theaccess-pattern categories are most important to you.\nYou cannot automatically tell which processes are causing disk activity. The kernel\ndoes not collect this information. You may be able to work out where the workloadcomes from by looking at how an application was installed, but often you mustresort to using truss on a process or the TNF tracing system. The Solaris software\ndoes not offer a way of getting I/O activity on a per-file descriptor basis, soapplication-specific instrumentation is all we have. Databases such as Oracle cancollect and report data on a per-tablespace basis, so if you can map the tablespacesto physical disks, you can tell what is going on programmatically. Such collectionand analysis is performed by the BMC Best/1 performance modeling tool so thatchanges in disk workload can be modeled.\n\nChapter 5 Workload Measurements 105Output Formats and Options for iostat\nTheiostat command produces output in many forms. When a large number of\ndisks are being reported, the iostat -x variant provides extended statistics and is\neasier to read because each disk is summarized on a separate line (see FIGURE 5-8 ).\nThe following values are reported: the number of transfers and kilobytes per secondwith read and write shown separately, the average number of commands waiting inthe queue, the average number of commands actively being processed by the drive,the I/O service time, and the percentages of the time that commands were waitingin the queue, and commands that were active on the drive.\nDisk configurations have become extremely large and complex on big server\nsystems. The Starfire system supports a maximum configuration of several thousanddisk drives, but dealing with even a few hundred is a problem. When large numbersof disks are configured, the overall failure rate also increases. It is hard to keep aninventory of all the disks, and tools like the SyMON software have to depend uponparsing messages from syslog to see if any faults are reported. The size of each\ndisk is also growing. When more than one type of data is stored on a disk, it\u2019s hardto determine which disk partition is active. A series of new features has beenintroduced in the Solaris 2.6 release to help solve these problems.\n\u25a0Per-partition data identical to existing per-disk data\nIt is now possible to separate root, swap, and home directory activity even if these\nfile systems are on the same disk.\n\u25a0New \u201cerror and identity\u201d data per disk\nYou no longer need to scan syslog for errors. Full data is saved from the first\nSCSI probe to a disk. This data includes vendor, product, revision, serial number,RPM, heads, and size. Soft, hard, and transport error counter categories sum upany problems. The detail option adds the Media Error, Device not ready, No%iostat -txc 5\nextended disk statistics tty cpu\ndisk r/s w/s Kr/s Kw/s wait actv svc_t %w %b tin tout us sy wt idfd0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 77 42 9 9 39sd0    0.0  3.5    0.0   21.2  0.0  0.1   41.6   0  14sd1    0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0sd3    0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0\nextended disk statistics tty cpu\ndisk r/s w/s Kr/s Kw/s wait actv svc_t %w %b tin tout us sy wt idfd0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 84 37 17 45 1sd0    0.0 16.8    0.0  102.4  0.0  0.7   43.1   2  61sd1    0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0sd3    0.0  1.0    0.0    8.0  0.0  0.1  114.3   2   4\nFIGURE 5-8 iostat -x Output\n\n106Resource Managementdevice, Recoverable, Illegal request, and Predictive failure analysis information.\nDead or missing disks can still be identified because there is no need to send themanother SCSI probe.\n\u25a0Newiostat options to present these metrics\nOne option ( iostat -M ) shows throughput in Mbytes/s rather than Kbytes/s\nfor high-performance systems. Another option ( -n) translates disk names into a\nmore useful form, so you don\u2019t have to deal with the sd43b format\u2014instead, youget c1t2d5s1. This feature makes it easier to keep track of per-controller loadlevels in large configurations.\nFast tapes now match the performance impact of disks. We recently ran a tape\nbackup benchmark to see if there were any scalability or throughput limits in theSolaris software, and we were pleased to find that the only real limit is the speed ofthe tape drives. The final result was a backup rate of an Oracle database at 1terabyte/hour or about 350 Mbytes/s, which was as fast as the disk subsystem wehad configured could perform. To sustain this rate, we used every accessible tapedrive, including 24 StorageTEK Redwood tape transports, which run at around 15Mbytes/s each. We ran this test using the Solaris 2.5.1 operating environment, butthere are no measurements of tape drive throughput in the Solaris 2.5.1 release. Tapemetrics were added to the Solaris 2.6 release, and you can see the tape drive that isactive, the throughput, average transfer size, and service time for each tape drive.\nTapes are instrumented in the same way as disks; they appear in sar andiostat\nautomatically. Tape read/write operations are instrumented with all the samemeasures that are used for disks. Rewind and scan/seek are omitted from theservice time.\nThe output format and options of sar(1) are fixed by the generic UNIX standard\nSVID3, but the format and options for iostat can be changed. In the Solaris 2.6\nrelease, existing iostat options are unchanged. Apart from extra entries that\nappear for tape drives and NFS mount points, anyone storing iostat data from a\nmixture of Solaris 2.x systems will get a consistent format. New options that extendiostat are as follows:\n-E full error statistics\n-e error summary statistics\n-n disk name and NFS mount point translation, extended service time\n-M Mbytes/s instead of Kbytes/s\n-P partitions only\n-p disks and partitions\n\nChapter 5 Workload Measurements 107Here are examples of some of the new iostat formats:\nThe Solaris 2.5 Trace Capability\nUNIX systems have had a kernel trace capability for many years. It was designed for\ndevelopment and debugging, not for end users. The production kernel is normallybuilt without the trace capability for performance reasons. One of the firstproduction kernels to include tracing was IBM\u2019s AIX kernel on the RS/6000 servers.They left it on during early releases to assist in debugging, then decided that tracingwas useful enough to pay its way, and the overhead was low. So it is now apermanent feature. Sun also recognized the value of trace information but decided toextend the trace capability to make it more generally useful and to implement italongside the existing kernel trace system. It was introduced in the Solaris 2.5operating environment and consists of the following features.\n\u25a0A self-describing trace output format, called Trace Normal Form (TNF), allowsdata structures to be embedded in the trace file without the need for an externaldefinition of their types and contents.\n\u25a0A set of libraries allows user-level programs to generate trace data. In particular,this trace data helps analyze and debug multithreaded applications.%iostat -xp\n                               extended device statisticsdevice    r/s  w/s   kr/s   kw/s wait actv  svc_t  %w  %bsd106     0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0sd106,a   0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0sd106,b   0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0sd106,c   0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0st47      0.0  0.0    0.0    0.0  0.0  0.0    0.0   0   0\n%iostat -xe\nextended device statistics ---- errors ----\ndevice r/s w/s kr/s kw/s wait actv svc_t %w %b s/w h/w trn totsd106 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0 0 0 0 0 0st47 0.0 0.0 0.0 0.0 0.0 0.0 0. 0000000\n%iostat -E\nsd106   Soft Errors: 0 Hard Errors: 0 Transport Errors: 0\nVendor: SEAGATE  Product: ST15230W SUN4.2G Revision: 0626 Serial No:00193749RPM: 7200 Heads: 16 Size: 4.29GB <4292075520 bytes>Media Error: 0 Device Not Ready: 0 No Device: 0 Recoverable: 0Illegal Request: 0 Predictive Failure Analysis: 0\nst47    Soft Errors: 0 Hard Errors: 0 Transport Errors: 0\nVendor: EXABYTE Product: EXB-8505SMBANSH2 Revision: 0793 Serial No:\n\n108Resource Management\u25a0A well-defined set of kernel probe points covering most important events was\nimplemented.\n\u25a0A program prex (1) controls probe execution for both user and kernel traces.\n\u25a0A program tnfxtract (1) reads out the kernel trace buffer, and tnfdump (1)\ndisplays TNF data in human-readable ASCII.\n\u25a0Manual pages exist for all the commands and library calls. The set ofimplemented kernel probes is documented in tnf_probes (4).\nA few things about kernel probes are inconvenient. While user-level probes can\nwrite to trace files, the kernel probes write to a ring buffer. This buffer is not a singleglobal ring; it is a buffer per kernel thread. This buffer scheme avoids any need tolock the data structures, so there is no performance loss or contention onmultiprocessor systems. You cannot easily tell how big the buffer needs to be. Onehighly active probe point might loop right round its buffer while others have hardlystarted. If you are trying to capture every single probe, make the buffer as big as youcan. In general, it is best to work with low event rate probes or rely on sampling andput up with missing probes. The tnfxtract routine just takes a snapshot of the\nbuffer, so a second snapshot will include anything left over from the first one. Thetnfdump program does quite a lot of work to sort the probe events into time order.\nI/O Trace: Commands and Features\nThe command sequence to initiate an I/O trace is quite simple. You run the\ncommands as root, and you need a directory to hold the output. It\u2019s easiest to havetwo windows open: one to run prex , and the other to go through a cycle of\nextracting, dumping, and viewing the data as required. The command sequence forprex is to first allocate a buffer (the default is 384 Kbytes; you can make it bigger),\nenable the iogroup of probes, make them trace accesses, then turn on the global\nflag that enables all kernel tracing.\nNow, wait a while or run the program you want to trace. In this case, we ran\niostat -x 10 in another window, didn\u2019t try to cause any activity, and waited for\nsome slow service time to appear. After approximately one minute, we stoppedcollecting.#prex -k\nType \"help\" for help ...prex>buffer alloc\nBuffer of size 393216 bytes allocatedprex>enable io\nprex>trace io\nprex>ktrace on\nprex>ktrace off\n\nChapter 5 Workload Measurements 109In the other window we extracted and dumped the data to take a look at it.\nUnderstanding I/O Measurements\nTo really understand the data presented by iostat ,sar, and other tools, you must\nlook at the raw data being collected by the kernel, remember some history, and dosome simple mathematics.\nIn the old days, disk controllers controlled the disks directly. All the intelligence was\nin the controller. The disk heads were directly connected to the controller, and thedevice driver knew exactly which track the disk was reading. As each bit was readfrom disk, it was buffered in the controller until a whole disk block was ready to bepassed to the device driver.\nThe device driver maintained a queue of waiting requests, which were serviced one\nat a time by the disk as shown in\nFIGURE 5-9 . From this, the system could report the\nservice time directly as milliseconds-per-seek. The throughput in transfers persecond was also reported, as was the percentage of the time that the disk was busy(the utilization). The terms utilization ,service time ,wait time ,throughput , and wait\nqueue length have well-defined meanings in this scenario because the setup is so\nsimple that a very basic queueing model fits it well. A set of simple equations fromqueuing theory can be used to derive these values from underlying measurements.\nFIGURE 5-9 Simple Old Disk Model\nNowadays, a standard disk is SCSI based and has an embedded controller. The diskdrive contains a small microprocessor and about 1 Mbyte of RAM. It can typicallyhandle up to 64 outstanding requests via SCSI tagged-command queuing. Thesystem uses a SCSI Host Bus adapter (HBA) to talk to the disk. In large systems,there is another level of intelligence and buffering in a hardware RAID controller.The simple model of a disk used by iostat and its terminology have become#mkdir /tmp/tnf\n#cd /tmp/tnf\n#tnfxtract io.tnf\n#tnfdump io.tnf | more\nI/O I/O I/O\nI/Os being serviced I/Os waiting in device driver\nqueue one at a time by the disk\n\n110Resource Managementconfused. In addition, the same reporting mechanism is used for client side NFS\nmount points and complex disk volumes setup using Solstice\u2122 DiskSuite\u2122software.\nIn the old days, if the device driver sent a request to the disk, the disk would do\nnothing else until it completed the request. The time it took was the service time,and the average service time was a property of the disk itself. Disks that spin fasterand seek faster have lower (better) service times. With today\u2019s systems, if the devicedriver issues a request, that request is queued internally by the RAID controller andthe disk drive, and several more requests can be sent before the first one comes back.The service time, as measured by the device driver, varies according to the load leveland queue length and is not directly comparable to the old-style service time of asimple disk drive.\nFIGURE 5-10 Two-Stage Disk Model Used By Solaris 2\nThe instrumentation provided in the Solaris operating environment takes thischange into account by explicitly measuring a two-stage queue: one queue, calledthe wait queue, in the device driver; and another queue, called the active queue, inthe device itself. A read or write command is issued to the device driver and sits inthe wait queue until the SCSI bus and disk are both ready. When the command issent to the disk device, it moves to the active queue until the disk sends its response.The problem with iostat is that it tries to report the new measurements in some of\nthe original terminology. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case, and theword \u201cwait\u201d is used to mean two different things. To sort out what we really have,we must do some mathematics.\nLet\u2019s start with the actual measurements made by the kernel. For each disk drive\n(and each disk partition, tape drive, and NFS mount in the Solaris 2.6 release), asmall set of counters is updated. An annotated copy of the kstat (3K)-based data\nstructure that the SE Toolkit uses is shown in\nFIGURE 5-11 .I/O I/O I/O\nI/Os active in I/Os waiting in\ndevice driver deviceI/O I/O I/O\nI/O being sent\nto deviceI/O being serviced\nby device\nwait, wsvc_t actv, asvc_t %w %bSCSIbus\n\nChapter 5 Workload Measurements 111None of these values are printed out directly by iostat , so the basic arithmetic\nstarts here. The first thing to note is that the underlying metrics are cumulativecounters or instantaneous values. The values printed by iostat are averages over a\ntime interval. We need to take two copies of the above data structure together with ahigh resolution timestamp for each and do some subtraction. We then get the average\nvalues between the start and endtimes. We\u2019ll write it out as plainly as possible, with\npseudocode that assumes an array of two values for each measure, indexed by start\nand end.T\nhires is in units of nanoseconds, so we divide to get seconds as T.struct ks_disks {\n    long      number$;    /* linear disk number */    string    name$;      /* name of the device */\n    ulonglong nread;      /* number of bytes read */\n    ulonglong nwritten;   /* number of bytes written */    ulong     reads;      /* number of reads */    ulong     writes;     /* number of writes */    longlong  wtime;      /* wait queue - time spent waiting */\nlonglong wlentime; /* wait queue - sum of queue length multiplied\nby time at that length */    longlong  wlastupdate;/* wait queue - time of last update */\nlonglong rtime; /* active/run queue - time spent active/running */\nlonglong rlentime; /* active/run queue - sum of queue length * time\nat that length */    longlong  rlastupdate;/* active/run queue - time of last update */    ulong     wcnt;       /* wait queue - current queue length */    ulong     rcnt;       /* active/run queue - current queue length */};\nFIGURE 5-11 Kernel Disk Information Statistics Data Structure\nThireshires elapsed time EndTime StartTime \u2013 timestamp end []timestamp start [] \u2013 == =\nTThires\n100000000-------------------------- - =\nBwaithires busy time for wait queue wtime end[]wtime start[]\u2013 ==\nBrunhires busy time for run queue rtime end[]rtime start[]\u2013 ==\nQBwaitwait queue length * time wlentime end []wlentime start [] \u2013 ==\nQBrunrun queue length * time rlentime end []rlentime start [] \u2013 ==\n\n112Resource ManagementWe assume that all disk commands complete fairly quickly, so the arrival and\ncompletion rates are the same in a steady state average, and the throughput of bothqueues is the same. We\u2019ll use completions below because they seem more intuitive inthis case.\nA similar calculation gets us the data rate in kilobytes per second.\nNext, we obtain the utilization or the busy time as a percentage of the total time:Creadcompleted reads reads end[]reads start[]\u2013 ==\nXreadread throughput iostat rpsCread\nT-------------- == =\nCwritecompleted writes writes end[]writes start[]\u2013 ==\nXwritewrite throughput iostat wpsCwrite\nT--------------- == =\nCtotal commands completed CreadCwrite+ ==\nXthroughput in commands per second iostat tpsC\nT----XreadXwrite+ == = =\nKreadKbytes read in the intervalnread end[]nread start[]\u2013\n1024--------------------------------------------------------------------- ==\nXkreadread Kbytes throughput iostat Kr/sKread\nT-------------- == =\nKwriteKbytes written in the intervalnwritten end []nwritten start [] \u2013\n1024----------------------------------------------------------------------------------- ==\nXkwritewrite Kbytes throughput iostat Kw/sKwrite\nT--------------- == =\nXktotal data rate in Kbytes per second iostat Kps XkreadXkwrite+ == =\nUwaitwait queue utilization iostat %w100Bwait\u00d7\nThires--------------------------- - == =\nUrunrun queue utilization iostat %b100Brun\u00d7\nThires------------------------- - == =\n\nChapter 5 Workload Measurements 113Now, we get to service time, but it is notwhatiostat prints out and calls service\ntime. This is the real thing!\nThe meaning of Srunis as close as you can get to the old-style disk service time.\nRemember that the disk can run more than one command at a time and can returnthose commands in a different order than they were issued.\nThe data structure contains an instantaneous measure of queue length, but we want\nthe average over the time interval. We get this from that strange \u201clength time\u201dproduct by dividing it by the busy time:\nFinally, we get the number that iostat calls service time. It is defined as the queue\nlength divided by the throughput, but it is actually the residence or response timeand includes all queuing effects:Swaitaverage wait queue service time in millisecondsBwait\nC100000\u00d7--------------------------- - ==\nSrunaverage run queue service time in millisecondsBrun\nC100000\u00d7--------------------------- - ==\nQwaitaverage wait queue length iostat waitQBwait\nBwait----------------- == =\nQrunaverage run queue length iostat actvQBrun\nBrun--------------- == =\nRwaitaverage wait queue response time iostat  wsvc_tQwait\nX------------- == =\nRrunaverage run queue response time iostat asvc_tQrun\nX----------- == =\n\n114Resource ManagementAnother way to express response time is in terms of service time and utilization:\nThis uses a theoretical model of response time that assumes that as you approach\n100 percent utilization with a constant service time the response time increases toinfinity.\nThe real definition of service time is the time taken for the first command in line to\nbe processed. Its value is not printed out by iostat . Using the SE Toolkit, this\ndeficiency is easily fixed. A corrected version of iostat written in SE prints out the\ndata, using the format shown in\nFIGURE 5-12 .\nThe Solaris 2.6 disk instrumentation is complete and accurate. Now that it has been\nextended to tapes, partitions, and client NFS mount points, there much more can bedone with it.\nUnderstanding Resource Utilization Characteristics\nOne important characteristic of complex I/O subsystems is that the utilizationmeasure can be confusing. When a simple system reaches 100 percent busy, it hasalso reached its maximum throughput. This is because only one thing is beingprocessed at a time in the I/O device. When the device being monitored is an NFSserver, hardware RAID disk subsystem, or a striped volume, it is clearly a muchmore complex situation. All of these can process many requests in parallel.%se siostat.se 10\n03:42:50 ------throughput------ -----wait queue----- ----active queue----disk r/s w/s Kr/s Kw/s qlen res_t svc_t %ut qlen res_t svc_t %utc0t2d0s0 0.0 0.2 0.0 1.2 0.00 0.02 0.02 0 0.00 22.87 22.87 003:43:00 ------throughput------ -----wait queue----- ----active queue----disk r/s w/s Kr/s Kw/s qlen res_t svc_t %ut qlen res_t svc_t %utc0t2d0s0 0.0 3.2 0.0 23.1 0.00 0.01 0.01 0 0.72 225.45 16.20 5\nFIGURE 5-12 SE-based Rewrite of iostat to Show Service Time CorrectlyRwaitaverage wait queue response time prediction iostat  wsvc_tSwait\n1Bwait\u2013---------------------- == =\nRrunaverage run queue response time prediction iostat asvc_tSrun\n1Brun\u2013-------------------- == =\n\nChapter 5 Workload Measurements 115FIGURE 5-13 Complex I/O Device Queue Model\nAs long as a single I/O is being serviced at all times, the utilization is reported as\n100 percent. This makes sense because it means that the pool of devices is alwaysbusy doing something. However, there is enough capacity for additional I/Os to beserviced in parallel. Compared to a simple device, the service time for each I/O isthe same, but the queue is being drained more quickly so the average queue lengthand response time is less, and the peak throughput is more. In effect, the load oneach disk is divided by the number of disks, so the underlying utilization is U/M.The approximated model for response time in this case changes so that responsetime stays lower for longer, but it still heads for infinity at a true 100 percentutilization.\nIn practice, some other effects come into play. The drives optimize head movement,\nso that as the queue gets longer, the average service time decreases, since it is morelikely that there is a block nearby to make a shorter seek to. This in itself increasesthroughput more than traditional simple queueing models would predict.I/O I/O I/O\nI/Os active in I/Os waiting in\ndevice driver deviceI/O\nI/OI/O\nI/O being sentto device\nI/O being serviced\nby M devicesQwait, Rwait, Uwait Qrun Rrun, UrunSwait\nSrunI/O\nRwaitaverage wait queue response time prediction iostat  wsvc_tSwait\n1BwaitM\u2044()M\u2013---------------------------------------- == =\nRrunaverage run queue response time prediction iostat asvc_tSrun\n1BrunM\u2044()M\u2013-------------------------------------- == =\n\n116Resource ManagementNetwork Measurements\nThe way network measurements can and should be made depends on the objective\nof the measurement. Do you want to manage the overall health of a network link?Are you interested in the TCP/IP statistics of your server? Do you want to knowhow much bandwidth is used by a specific application? Each of these questions canbe answered if the right tools and techniques are used to do the measuring.\nNetwork Throughput\nUsing SNMP counters is a good way to get an overall view of network throughput.The Solaris software provides an SNMP daemon which provides the data you need.If your host is managed by the SyMON software, you can browse the SNMPcounters through the SyMON GUI, and it is very easy to graph network throughputdata by combining some of these counters in a simple formula. For example, to get agraph depicting how much percent of the (half duplex) network segment that isbusy, use the following formula:\n((ifInOctets + ifOutOctets) / time_interval)/ ifSpeed) * 100\nMost networking devices support SNMP , and the SyMON software can incorporateany third-party MIBs so all links from a switch can be monitored at the same time.\nBesides the SyMON software, many other commercial and free applications and\nutilities manage SNMP devices. One example of an excellent free utility is MRTGand can be downloaded fromhttp://ee-staff.ethz.ch/~oetiker/webtools/mrtg/mrtg.html .\nEthernet Statistics\nTraditionally, administrators have used collisions as an indicator of network health\nfor Ethernet networks. Collisions are reported using netstat with the -iflag.\ndoghouse%netstat -i\nName Mtu Net/Dest Address Ipkts Ierrs Opkts Oerrs Collis Queue\nlo0 8232 loopback localhost 40986 0 40986 0 0 0hme0 1500 doghouse doghouse 128128305 0 125004514 0 77754 0\n\nChapter 5 Workload Measurements 117The collision rate is (Collis / Opkts) * 100% . In our case, that is less than a\ntenth of a percent. Collisions are absolutely normal and should cause no concernunless collision rates become very high (in the order of 10 to 20 percent or higher). Ifyou have high collision rates, check the total bandwidth utilization on your networklink.\nToday, more and more switched and full duplex Ethernet networks are being\ndeployed. In these environments, there are no collisions since each port on theswitch has its own dedicated bandwidth, and hosts can send network packetswhenever they want. If a switch gets overloaded, it just drops packets that it can\u2019tprocess fast enough. Dropped packets have to be retransmitted by the sender.\nNetstat has an undocumented option that is invoked with the -k flag. It will dump a\nlot of statistics that are kept in kernel structures (the kstats ). You can look at for\nexample the statistics from the hme card as follows:\nMost of these statistics are related to the Ethernet MAC and the network interface\ncard hardware itself. In general, these counters help troubleshoot hardwareproblems. Every network interface card typically has different counters, which canchange with the operating system releases. Sun does not officially support thenetstat option.doghouse%netstat -k hme0\nhme0:ipackets 133465658 ierrors 0 opackets 130244542 oerrors 0collisions 83672 defer 115 framing 0 crc 0 sqe 0 code_violations0 len_errors 0 ifspeed 10 buff 0 oflo 0 uflo 0 missed 0tx_late_collisions 0 retry_error 0 first_collisions 0 nocarrier 0inits 6 nocanput 0 allocbfail 0 runt 0 jabber 0 babble 0 tmd_error0 tx_late_error 0 rx_late_error 0 slv_parity_error 0tx_parity_error 0 rx_parity_error 0 slv_error_ack 0 tx_error_ack0 rx_error_ack 0 tx_tag_error 0 rx_tag_error 0 eop_error 0 no_tmds0 no_tbufs 0 no_rbufs 0 rx_late_collisions 0 rbytes 1523404831obytes 813184078 multircv 33440 multixmt 3 brdcstrcv 3312500brdcstxmt 14694 norcvbuf 0 noxmtbuf 0 phy_failures 0\n\n118Resource ManagementProtocol Statistics\nTo view TCP , UDP , IP , ICMP , and IGMP statistics, you can use netstat -s . The\ncommand prints lots of protocol-specific information. Adrian Cockcroft and RichPettit describe this in their book Sun Performance and Tuning, Java and the Internet . The\nmore interesting counters are incorporated into the SE Toolkit script nx.se . The\noutput of nx.se looks as follows:\nThe script lists TCP as if it were an interface, with input and output segment and\ndata rates, resets per second, outgoing connection attempt fails per second,percentage of bytes retransmitted, and incoming and outgoing connections persecond. It then lists all the interfaces. For interfaces that provide this information (atpresent, only leandhme)nx.se reports kilobytes in and out. NoCP is nocanput.\nThis counts the number of times a received packet was discarded due to slowprocessing in the TCP/IP stack and lack of buffering on input. Defr shows the\nnumber of defers that took place. A defer happens when an Ethernet tries to sendout a packet, but it finds the medium to be busy, and it has to hold off (defer) thesending until the line is clear.% /opt/RICHPse/bin/se nx.se\nCurrent tcp RtoMin is 200, interval 5, start Thu Aug 27 00:23:09 199800:23:14 Iseg/s Oseg/s InKB/s OuKB/s Rst/s  Atf/s  Ret%  Icn/s  Ocn/stcp        0.0    0.2   0.00   0.01   0.00   0.00   0.0   0.00   0.00Name    Ipkt/s Opkt/s InKB/s OuKB/s IErr/s OErr/s Coll% NoCP/s Defr/shme0       1.2    1.2   0.09   0.20  0.000  0.000   0.0   0.00   0.00hme1       0.0    0.0   0.00   0.00  0.000  0.000   0.0   0.00   0.00hme2       0.0    0.0   0.00   0.00  0.000  0.000   0.0   0.00   0.00\n\nGlossary 309Glossary\naccess control list (ACL) A file that specifies which users can access a particular resource, such as\na filesystem.\naccounting Keeping track of resource usage on a machine. SRM provides accounting\nfeatures.\nACL Seeaccess control list .\nadministration Tool A GUI tool for configuring Solaris Bandwidth Manager.\nadministrative\ndomain A collection of network elements under the same administrative control and\ngrouped together for administrative purposes.\nADOP Seeautomatic degree of parallelism .\nalarm The means by which notification is sent when an exception occurs.\nattaching SeeDR Attach .\nAlternate Pathing (AP) A software mechanism which works in conjunction with Dynamic\nReconfiguration (DR) to provide redundant disk and network controllers andtheir respective physical links. The main purpose of Alternate Pathing is tosustain continuous network and disk I/O when system boards are detachedfrom a machine or DSD (in the case of the Starfire) that is running a live copyof the Solaris operating environment.\nAP SeeAlternate Pathing .\napplication resource\nmeasurement (ARM) A means of measuring the end-to-end response time of a system.\nARM Seeapplication resource measurement .\nASE Sybase Adaptive Server Enterprise.\n\n310Resource Managementautomatic degree of\nparallelism (ADOP) A feature of the Oracle8 iDatabase Resource Manager that attempts to\noptimize system utilization by automatically adjusting the degree ofparallelism for parallel query operations.\nbackfilling The execution of a job that is short enough to fit into the time slot during\nwhich the processors are reserved, allowing for more efficient use of theavailable resources. Short jobs are said to backfill processors reserved for largejobs.\nBBSRAM Boot Bus Static Random Access Memory.\nblacklist A file that enables you to specify components, such as system boards, that\nshould not be configured into the system. The blacklist file is read andprocessed at bringup time.\nBMC Best/1 BMC Software\u2019s BEST/1 products provide tools to address performance\nmanagement requirements across OS/390, Parallel Sysplex, SAP R/3, Unix,Windows NT, VM and AS/400 environments.\nCBQ SeeClass Based Queuing .\nCCMS A tool that provides information to SAP R/3 allowing it to measure the\nperformance of key user transactions and the response time of the backenddatabase for applications.\nCICS SeeCustomer Information Control System .\nCIM SeeCommon Information Model .\nClass Based Queuing (CBQ) The underlying queuing technology used in Solaris Bandwidth\nManager.\nClasses of Service A feature supported by Solaris Bandwidth Manager that allows network traffic\nto be organized so that urgent traffic gets higher priority than less importanttraffic.\nclassifier A component of the Solaris Bandwidth Manager that allocates packets to a\nclass queue. When a packet arrives, the classifier analyzes the packet protocol,ToS value, URL information, source information, destination information andallocates the packet to a class queue where it waits to be processed.\nCLI Command Line Interface, as opposed to Graphical User Interface (GUI).\ncluster A collection of computers interconnected via a high-speed interface that allows\nthe environment to behave as a single unified computing resource.\nclustered cache A method of caching web pages where multiple servers use the intercache\nprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if highavailability is important, multiple servers are configured as siblings. Eachsibling stores data in its cache but also uses ICP to search the caches of othersiblings.\n\nGlossary 311CMIP A scalable OSI-based network management protocol that is used in situations\nwhere SNMP is not powerful enough.\ncodepoint A construct used by the ToS facility of the Solaris Bandwidth Manager. The\nway that the DSCP bits are set in a packet is called its codepoint , and network\ndevices translate DSCP codepoints to device configurations. For example, a DScompliant router that has two priority queues (one for regular priority trafficand one for high priority traffic) could be configured to have severalcodepoints map to the high priority queue, and several others to the regularpriority queue.\nCommon Information\nModel (CIM) A metamodel based on the Unified Modeling Language (UML) that\nsupplies a set of classes with properties and associations. The CommonInformation Model provides a conceptual framework within which it ispossible to organize information about a managed environment.\ncontrol interval In control theory, the rate at which measurements are made and corrections are\napplied.\nCoS SeeClasses of Service .\nCross-System Coupling\nFacility A WLM component that communicates policies, metrics, and control data\nbetween Sysplex nodes.\nCustomer Information\nControl System (CICS) An interactive transaction processing system from IBM.\nOracle8 iDatabase\nResource Manager A feature of Oracle8 ithat is used to allocate and manage CPU resources among\ndatabase users and applications. In addition, the Oracle8 iDatabase Resource\nManager enables the administrator to limit the degree of parallelism of anyoperation. It is possible to balance one user\u2019s resource consumption againstother users\u2019 by allocating flexibility among tasks of varying importance toachieve overall enterprise goals.\nDDI_ATTACH A function, used by DR (called from the dr_driver ) that provides the ability\nto attach a particular instance of a driver without affecting other instances thatare servicing separate devices.\nDDI/DKI Device Driver Interface/Device Kernel Interface. These are function call entry\npoints that device drivers should implement in order to fully support DR.DDI/DKI is specified in the \u201cWriting Device Drivers\u201d section of the Driver\nDeveloper Site 1.0 AnswerBook (http://docs.sun.com ).\nDDI_DETACH A function, used by DR (called from the dr_driver ), that provides the ability\nto detach a particular instance of a driver without affecting other instances thatare servicing separate devices.\n\n312Resource ManagementDDI_RESUME A function, used by DR (called from the dr_driver ), that provides the ability\nto detach a board that contains the kernel cage (OBP , kernel, and non-pageablememory). The kernel cage can only be relocated after all of the driversthroughout the entire DSD (not just on the board being detached) are quiescedto guarantee the data integrity of the kernel cage relocation. DDI_RESUME\nresumes the drivers after the quiesce period.\nDDI_SUSPEND A function, used by DR (called from the dr_driver ), that provides the ability\nto detach a board that contains the kernel cage (OBP , kernel, and non-pageablememory). The kernel cage can only be relocated after all of the driversthroughout the entire DSD (not just on the board being detached) are quiescedto guarantee the data integrity of the kernel cage relocation. DDI_SUSPEND\nsuspends the drivers to begin the quiesce period.\ndecay The period by which historical usage is discounted.\nDEN The Directory Enabled Networks working group. The goal of this group is to\noffer a standard information model and directory schemas to tie together usersand applications with network elements, protocols, and services throughspecific relationships. By complying to this information model and the DENschemas, different network equipment and application vendors should be ableto build interoperable network elements around a central directory.\ndetaching SeeDR Detach .\nDiff-Serv The Differentiated Services working group of the Internet Engineering Task\nForce (IETF). Diff-Serv addresses network management issues related to end-to-end Quality of Service (QoS) within diverse and complex networks.\nDIMM Dual In-Line memory Module. A memory module with higher capacity and\nfaster performance than SIMMs (Single In-Line Memory Module) It is currentlyused as the memory source for all Sun Microsystems platforms.\ndirect control A means of control that operates on the resource you want to control. For\nexample the Solaris Resource Manager software controls CPU usage per userby implementing a scheduling class that decides who should get what share ofthe CPU.\nDISC Dynamic internal service classes created by WLM. These classes enable WLM\nto manage transactions. Each DISC is associated with one or more normalservice classes and a given server component. The number of transactionsusing each route then allow the DISCs to be weighted. Thus, if the external orstandard service class goal is not being met, the associated DISCs can bemanaged (if that is where a bottleneck lies).\ndistributed queuing\nsystem A batch system product from Florida State University that is available in the\npublic domain. The set of system resources it understands is host (by name),system architecture, operating system type, amount of memory, and CPUusage.\nDMTF Desktop Management Task Force.\n\nGlossary 313DQS Seedistributed queuing system .\nDR SeeDynamic Reconfiguration .\nDR Attach The process of bringing a system board under the control of the Solaris\noperating environment through use of the DR mechanism.\nDR Detach The process of removing a system board from Solaris operating system control\nthrough use of the DR mechanism.\nDSD SeeDynamic System Domains .\nDSS Decision Support System.\nDSS/DW Decision Support System / Data Warehousing.\nDynamic\nReconfiguration (DR) A Sun Microsystems technology supported on the Starfire and other Sun\nEnterprise servers which allows system boards to be added (attached) orremoved (detached) from a single server or domain.\nDynamic System\nDomains (DSD) Starfire independent hardware entities formed by the logical association\nof its system boards. Each domain on the Starfire enjoys complete hardwareisolation from other domains, executes its own private version of the Solarisoperating system, and is centrally managed by the SSP .\nELIM SeeExtended Load Information Manager .\nEnterprise 10000 SeeSun Enterprise 10000 .\nERP Enterprise Resource Planning\nerror event A discrete on/off event, as opposed to a continuous variable to be compared\nagainst a limit.\nexception A condition that represents a problem in processing a job. LSF can watch for\nseveral types of exception conditions during a job\u2019s life cycle.\nexclusive scheduling A type of scheduling used by LSF that makes it possible to run exclusive jobs\non a host. A job only runs exclusively if it is submitted to an exclusive queue.An exclusive job runs by itself on a host. LSF does not send any other jobs tothe host until the exclusive job completes.\nExtended Load\nInformation Manager (ELIM) LSF uses the Load Information Manager (LIM) as its resource\nmonitoring tool. To modify or add load indices, an Extended Load InformationManager can be written.\nfairshare A form of scheduling used by LSF to prevent a single user from using up all\nthe available job slots, thus locking out other users. Fairshare scheduling is analternative to the default first come, first served scheduling. Fairsharescheduling divides the processing power of the LSF cluster among users and\n\n314Resource Managementgroups to provide fair access to resources for all jobs in a queue. LSF allows\nfairshare policies to be defined at the queue level so that different queues canhave different sharing policies.\nFlowAnalyzer\n(NetFlow) An application that uses the output from NetFlow FlowCollector. It provides\nvery elaborate processing, graphing, and reporting options that can be used fornetwork analysis, planning, trouble shooting, and more\nFlowCollector\n(NetFlow) A NetFlow datagram consumer for one or more NetFlow devices. These\ndevices simply point to the host and port number on which the FlowCollectorsoftware is running. The FlowCollector aggregates this data, doespreprocessing and filtering, and provides several options to save this data todisk (such as flat files). Other applications such as network analyzing,planning, and billing can use these files as input.\nGigaplane-XB The interconnect on the Starfire that provides main memory access through a\npoint-to-point data router which isolates data traffic between system boardsand minimizes any performance degradation when memory interleaving isdisabled.\ngoal Goal-based policies are prescriptitive rather than reactive. A goal can be\ntranslated into a mixture of limits, priorities, and relative importance levels.Goals can include actions to be performed when the goal cannot be met.\nHealth Monitor SeeSyMON Health Monitor .\nheavily damped A system is heavily damped if you feed back a small proportion of an error\nover a longer control interval. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used.\nhierarchical fairshare A method of sharing resources, supported by LSF. Hierarchical fairshare\nenables resources to be allocated to users in a hierarchical manner. Groups ofusers can collectively be allocated a share, and that share can be furthersubdivided and given to subgroups, resulting in a share tree.\nhost based resources Resources that are not shared among hosts, but are tied to individual hosts. An\napplication must run on that host to access such resources. Examples are CPU,memory, and swap space. Using up these resources on one host does not affectthe operation of another host.\nHostview A GUI program that runs on the SSP machine (which is a component of an\nEnterprise 10000 system). Hostview enables you to monitor and control theEnterprise 10000. For example, Hostview can display continuous readouts ofpower and temperature levels at various locations within the Enterprise 10000server.\nHPC High Performance Computing\n\nGlossary 315HP OpenView Computer-oriented local and wide area networks are normally managed using\nSNMP protocols, with Solstice SunNet Manager or HP OpenView productscollecting and displaying the data. Both products provide some visibility intowhat is happening in the computer systems on the network, but they arefocused on network topology. Resource management is done on a per-networkbasis, often by controlling the priority of data flows through intelligent routersand switches.\nHTTP Hypertext Transfer Protocol. HTTP is used by Web servers to host content and\nrespond to HTTP requests from Web browsers.\nIBM Workload\nManager A comprehensive tool set for MVS that provides an automated resource\nmanagement environment, driven by high level business goals, and that, inmany cases, is self tuning. Tools are provided to define the business goals orobjectives, to control system resources, and to feed metrics concerning theseresources back to the resource controller, which attempts to ensure that thegoals are met.\nIETF Internet Engineering Task Force.\nindirect control A means of control that works via resources that are dependent upon the\nresource that is being controlled. For example, to limit the I/O throughput of aprocess, it is sufficient to be able to measure the I/O throughput and limit theCPU resources for that process.\nintercache protocol (ICP) A protocol used to implement clustered caches. (See clustered cache .)\ninterleaving Seememory interleaving .\nintimate shared\nmemory (ISM) A way of allocating memory so that it can\u2019t be paged. The shared\nmemory area is often the largest component of a database\u2019s memoryrequirements, and is the easiest to insulate between database instances.Because intimate shared memory is wired down, the memory allocated to eachdatabase instance stays allocated and one instance cannot steal memory fromanother.\nInt-Serv The Integrated Services working group of the Engineering Task Force (IETF).\nIP Internet Protocol. IP is the foundation of the TCP/IP architecture. It operates\non the network layer and supports addressing. IP enables data packets to berouted.\nISM Seeintimate shared memory .\nISP Internet Service Provider, a company that provides Point-of-Presence access to\nthe Internet.\nISPF Interactive System Productivity Facility, a generic MVS interface that can be\nused by the operator/administrator to define, activate, and deactivate policies.\n\n316Resource ManagementJava Dynamic\nManagement Kit A JavaBeans based framework for developing and deploying dynamic\nmanagement based applications. Autonomous agents can be deployed in real-time to perform management tasks for devices on the network.\nJTAG Joint Test Action Group, IEEE Std. 1149.1. JTAG is an alternate communications\ninterface between the SSP machine and the Enterprise 10000 server, and is usedwhen the standard network connection between the SSP and the Enterprise10000 is unavailable.\nJava Virtual Machine The machine image, implemented in software, upon which Java code runs.\nJVM SeeJava Virtual Machine .\nkernel cage A special data structure (normally contained within a single system board) that\ncontrols the dynamic growth of all non-relocatable memory, including theOpenBoot PROM (OBP) and kernel memory. When Dynamic Reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system to ensure that no I/O or kernelactivity occurs while the kernel cage is being relocated.\nkernel memory Memory that is used to run the operating system.\nkernel module A Solaris Bandwidth Manager module that contains the classifier and the\nscheduler .\nLAN Seelocal area network .\nlightly damped If you feed back a large proportion of an error with a short control interval, the\nsystem is said to be lightly damped. A lightly damped system is veryresponsive to sudden changes but will probably oscillate back and forth.\nLIM SeeLoad Information Manager .\nlimit A simple rule with a single input measurement. It is also common to have\nseveral thresholds with a warning level action and a critical problem levelaction for the same measure.\nlnode Limit node, a node in a special resource tree used by Solaris Resource Manager\n(SRM). SRM is built around lnodes which are a fundamental addition to theSolaris kernel. lnodes correspond to UNIX UIDs, and may represent individualusers, groups of users, applications, and special requirements. The lnodes areindexed by UID and are used to record resource allocations policies andaccrued resource usage data by processes at the user, group of users, andapplication level.\nLoad Information\nManager (LIM) The resource monitoring tool used by LSF. The Load Information\nManager process running on each execution host is responsible for collectingload information. The load indices that are collected include: host status,length of run queue, CPU utilization, paging activity, available swap space,available memory, and I/O activity.\n\nGlossary 317Load Share Facility (LSF) A software facility that provides the capability of executing batch and\ninteractive jobs on a pool of networked computers. The Sun MicrosystemsHigh Performance Computing (HPC) package includes the Load Share Facilityas a vehicle for launching parallel applications on an HPC cluster. In additionto starting batch jobs, the Load Share Facility also provides load balancing.\nlocal area network A set of computer systems in relatively close proximity that can communicate\nby means of networking hardware and software.\nLPAR Logical Partitions, an IBM S/390\u2122 logical entity which runs its own operating\nsystem instance and it\u2019s allocated resources and managed by PR/SM.\nLSF SeeLoad Share Facility .\nLWP Lightweight Process\nManagement\nInformation Base A database that contains network management variables and can be accessed\nvia SNMP .\nmaster host The node where the LSF batch queues reside. When the LSF software\ninitializes, one of the nodes in the cluster is elected to be the master host. Thiselection is based on the order of nodes listed in a configuration file. If the firstnode listed in the configuration file is inoperative, the next node is chosen, andso forth.\nmaximum bandwidth The amount of spare bandwidth allocated to a class by the Solaris Bandwidth\nManager. The maximum bandwidth is dependent upon the percentage ofbandwidth the class can borrow.\nMDF\nTM Multiple Domain Facility, an Amdahl Corporation\u2122 technology which\nprovides logical partitioning for its mainframes. By integrating specialhardware for each logical partition or domain, Amdahl processor complexescould run multiple operating systems at close to native performance.\nmemory interleaving A method of using computer memory that helps increase memory subsystem\nperformance by reducing the probability of hot spots or contention in a fewmemory banks. This is accomplished by spreading access to multiple memorybanks.\nMessage Passing\nInterface (MPI) An industry standard interface used to parallelize applications.\nMIB SeeManagement Information Base .\nmicrostate accounting A method of accounting for resource usage where a high-resolution timestamp\nis taken on every state change, every system call, every page fault, and everyscheduler change. Microstate accounting provides much greater accuracy thansampled measurements.\nMPI SeeMessage Passing Interface .\nMTS SeeMulti-Threaded Mode .\n\n318Resource ManagementMulti-Threaded\nMode A database topology where a single process serves many users.\nnegative feedback A method of applying feedback to a system where you take the error difference\nbetween what you wanted and what you got, and apply the inverse of theerror to the system to reduce the error in the future.\nNetFlow A product from Cisco that is supported by Solaris Bandwidth Manager.\nNetFlow allows for detailed network measurements that can be sent to othersoftware packages which can process and analyze the data.\nNetwork File System An application that utilizes TCP/IP to provide distributed file services.\nnetwork queuing\nsystem (NQS) A public domain software product that has been enhanced by many\nhardware vendors. Sterling Software offers a distributed version of NQS calledNQS/Exec which is geared toward a supercomputer environment. Limitedload balancing is provided as there is no concept of demand queues, since ituses traditional push queues instead. There is also no control over interactivebatch jobs.\nNFS SeeNetwork File System .\nNQS SeeNetwork Queuing System .\nNVRAM Non-Volatile Random Access Memory\nOBP OpenBoot PROM\nODS Informix OnLine Dynamic Server\nOLTP Online Transaction Processing\noperational policy A policy that is implemented manually as part of operations management. For\nexample, an availability policy can include a goal for uptime and an automaticway to measure and report the uptime over a period. There is no direct controlin the system that affects uptime. It is handled by operations staff.\nOracle8 iResource\nManager An Oracle facility that ensures system resources are applied to the most\nimportant tasks of the enterprise at the levels required to meet enterprise goals.\nPC NetLink A product from Sun Microsystems that is based on the AT&T Advanced Server\nfor UNIX. PC NetLink adds functionality that was not previously available onSolaris servers with products such Samba and SunLink\u2122 PC\u2122 (a.k.a. SyntaxTotalNET Advanced Server). PC NetLink adds file and print services, andenables Solaris servers to act as Microsoft\u00ae Windows NT\u2122 Primary DomainControllers (PDC) or Backup Domain Controllers (BDC). For enterprises withmixed NT and Solaris servers and desktops, PC NetLink 1.0 offers many newoptions for utilizing hardware resources and minimizing systemadministration overhead.\n\nGlossary 319Platform Computing\nLoad Share Facility SeeLoad Share Facility .\nPDP Seepolicy decision point .\nPEP Seepolicy enforcement point .\nperformance index The ratio of work completed versus the amount of work which should have\nbeen completed in order to meet the goal.\nPIN Seepolicy ignorant node .\npolicy agent A component of the Solaris Bandwidth Manager that implements the\nconfiguration and handles communication with the kernel module .\npolicy control The application of rules to determine whether or not access to a particular\nresource should be granted.\npolicy decision point In policy administration, the point where policy decisions are made.\npolicy element A subdivision of policy objects. A policy element contains single units of\ninformation necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. The policy elements themselves areexpected to be independent of which Quality of Service signaling protocol isused.\npolicy enforcement\npoint In policy administration, the point where policy decisions are enforced.\npolicy ignorant node A network element that does not explicitly support policy control using the\nmechanisms defined in the applicable standard policy.\npolicy object An object that contains policy-related information, such as policy elements , and\nis carried in a request or response related to resource allocation decisions.\npolicy protocol A protocol for communication between the policy decision point and policy\nenforcement point. The policy protocol can be any combination of COPS,SNMP , and Telnet/CLI.\nPOST Power-ON self tests, a suite of hardware diagnostic tests which ensure full\nfunctionality of a system board.\npreemptive\nscheduling A method of scheduling where a high priority job can bump a lower priority\njob that is currently running. LSF provides several resource controls toprioritize the order in which batch jobs are run. Batch jobs can be scheduled torun on a first come first served basis, fair sharing between all batch jobs, andpreemptive scheduling.\npriority A relative importance level that can be given to the work done by a system as\npart of a policy that prioritizes some activities over others.\n\n320Resource Managementpriority decay Seeprocess priority decay .\npriority paging A method of implementing a memory policy with different importance factors\nfor different memory types. Application memory is allocated at a higherpriority than file system memory, which prevents the file system from stealingmemory from other applications. Priority paging is implemented in the Solaris7 operating environment.\nprocess\nmeasurements Measurements that show the activity of each user and each application.\nprocess memory Memory allocated to processes and applications.\nProcess Monitor An optional module within Sun Enterprise SyMON that can be used to view all\nthe processes on a system. The Process Monitor can also be configured topattern match and accumulate all the processes that make up a workload.\nprocess priority decay A process decay method used by SRM, where each processes priority is\ndecayed according to a fixed decay factor at regular intervals (each second).\nprocessor reservation A method that allows job slots to be reserved for a parallel job until enough are\navailable to start the job. When a job slot is reserved for a job, it is unavailableto other jobs. Processor reservation helps to ensure that large parallel jobs areable to run without under utilizing resources.\nprocessor set The set of processors available to a system.\nProject StoreX A technology being developed at Sun to address modern storage issues.\nStorage is now open for access in a heterogeneous multivendor environment,where multiple server and storage vendors can all be connected over theStorage Area Network (SAN). This is an emerging technology, and tools tomanage a SAN are still being developed. Project StoreX is based on adistributed pure Java framework that can run on servers from any vendor,interface to other storage management software, and manage any kind ofattached storage.\nprovider DSD Dynamic Reconfiguration (DR) on the Starfire allows the logical detachment of\na system board from a provider DSD (the DSD from which resources areborrowed) and the logical attachment of the same system board to a receptorDSD (the DSD where loaned resources are applied).\nprovider domain When relocating resources between DSDs, a \u201cprovider domain\u201d is the domain\nwhere a system board gets logically detached from to then have it attached to a\u201creceptor domain\u201d.\nproxy cache A method of caching Web pages. A proxy caching Web server sits between a\nlarge number of users and the Internet, funneling all activity through thecache. Proxy caches are used in corporate intranets and at ISPs. When all theusers are active at once, regardless of where they are connecting to, the proxycache server will get very busy\n\nGlossary 321PR/SM\u2122 Processor Resource/Systems Manager), an IBM S/390 hardware feature which\nallows customers to statically allocate processor and I/O resources to LPARs toconcurrently run multiple operating system instances on the same machine.\nQoS SeeQuality of Service .\nQuality of Service A measure of the speed and reliability of a service. Solaris Bandwidth Manager\nprovides the means to manage your network resources to provide Quality ofService to network users. QoS is a network-wide issue;\nif congestion takes\nplace anywhere on the network, it affects the overall quality of service.\nRAS Reliability, Accessibility and Serviceability\nreceptor DSD Dynamic Reconfiguration (DR) on the Starfire allows the logical detachment of\na system board from a provider DSD (the DSD from which resources areborrowed) and the logical attachment of the same system board to a receptorDSD (the DSD where loaned resources are applied).\nreceptor domain When relocating resources between DSDs, a \u201creceptor domain\u201d is the domain\nwhich receives a system board after having it logically detached from a\u201cprovider domain.\u201d\nrepository access\nprotocol The protocol used to communicate between a policy repository and the\nrepository client. LDAP is one example of a repository access protocol.\nResource Management\nFacility A component of WLM that tracks metrics including progress against goals.\nRMF SeeResource Management Facility .\nRSVP A protocol (part of the Int-Serv framework), that provides applications the\nability to have multiple levels of Quality of Service (QoS) when delivering dataacross the network. RSVP provides a way for an application to communicateits desired level of service to the network components. It requires each hopfrom end-to-end be RSVP-enabled, including the application itself (through anAPI). Bandwidth is reserved at each hop along the way before transmittingbegins, guaranteeing that enough resources will be available for the durationof the connection.\nSAN SeeStorage Area Network .\nscheduler A component of the Solaris Resource Manager (SRM) that schedules users and\napplications.\nscheduler term The period of time during which the Solaris Resource Manager (SRM) ensures\nthat a particular user or application receives its fair share of resources.\n\n322Resource Managementsecurity policy A type of policy that aims at preventing access to certain resources or allowing\ndesignated users to manage subsystems. For example, Sun Enterprise SyMON2.0 software includes access control lists for operations that change the state ofa system, and multiple network domain views to give different administrativeroles their own view of the resources being managed.\nSE Toolkit A toolkit that can be used to develop customized process monitors. The Solaris\nsoftware can provide a great deal of per-process information that is notcollected and displayed by the ps command or Sun Enterprise SyMON 2.0software. The data can be viewed and processed by a custom written processmonitor. You could write one from scratch or use the experimental scriptsprovided as part of the SE Toolkit. The SE Toolkit is a freely available butunsupported product for Solaris systems. It can be downloaded from thehttp://www.sun.com/sun-on-net/performance/se3 .\nserver consolidation A current trend by data centers to reduce cost of server ownership by reducing\nphysical footprint and reducing number and management cost of multivendorplatforms. The basis of server consolidation is to combine applications anddata contained in several smaller servers into a single larger server.\nservice class A class that defines a set of goals, together with periods, duration, and\nimportance. A number of individual processes and CICS/IMS transactions canbe assigned membership of a service class. They will then become subject tothe specified goals and constraints, including those imposed by any resourcegroup subscribed to by the class. In essence, this is analogous to the SRMlnode, which effectively defines a resource management policy that can besubscribed to.\nService Level\nAgreement A written agreement between system managers and end users that captures\nthe expectations and interactions between end users, system managers,vendors, and computer systems. Often, many additional interactions andassumptions are not captured formally.\nService Level\nManagement The process by which information technology (IT) infrastructure is planned,\ndesigned, and implemented to provide the levels of functionality, performance,and availability required to meet business or organizational demands.\nservice provider In a network policy, the service provider controls the network infrastructure\nand may be responsible for the charging and accounting of services.\nservice time The time it takes for an I/O device to service a request. This can be complex to\nmeasure. For example, with today\u2019s disk storage systems, the device driverissues a request, that request is queued internally by the RAID controller andthe disk drive, and several more requests can be sent before the first one comesback. The service time, as measured by the device driver, varies according tothe load level and queue length and is not directly comparable to the old-styleservice time of a simple disk drive.\n\nGlossary 323SEVM Sun Enterprise Volume Manager, technically equivalent to Veritas Volume\nManager.\nShareII A resource management product from product from Softway. The Solaris\nResource Manager (SRM) is based on ShareII.\nshared resources A resource that is not tied to a specific host, but is associated with the entire\ncluster, or a specific subset of hosts within the cluster. Examples of sharedresources include: floating licenses for software packages, disk space on a fileserver which is mounted by several machines, and the physical networkconnecting the hosts.\nSHR Scheduler A component of the Solaris Resource Manager (SRM) that controls the CPU\nresources. Users are dynamically allocated CPU time in proportion to thenumber of shares they possess (analogous to shares in a company), and ininverse proportion to their recent usage. The important feature of the SHRscheduler is that while it manages the scheduling of individual threads, it alsoportions CPU resources between users.\nSimple Network\nManagement Protocol (SNMP) An open network protocol used by network management systems that\nare based on TCP/IP .\nSLA SeeService Level Agreement .\nSNIA Storage Network Industry Association.\nSNMP SeeSimple Network Management Protocol .\nSolaris Bandwidth\nManager A product from Sun that provides the means to manage your network\nresources to provide Quality of Service (QoS) to network users. It allowsnetwork traffic to be allocated to separate Classes of Service (CoS), so thaturgent traffic gets higher priority than less important traffic. Different classesof service can be guaranteed a portion of the network bandwidth, leading tomore predictable network loads and overall system behavior. Service LevelAgreements can be defined and translated into Solaris Bandwidth Managercontrols and policies. Tools and APIs provide an interface for monitoring,billing, and accounting options.\nSolaris Management\nConsole An application that provides a generic framework for gathering together\noperating system administration tools and interfacing to industry standardinitiatives such as the Web-based management initiative (WebM) and theCommon Information Model (CIM).\nSolaris Resource\nManager (SRM) A software tool for enabling resource availability for users, groups, and\napplications. The Solaris Resource Manager provides the ability to allocate andcontrol major system resources such as CPU, virtual memory, and number ofprocesses. The Solaris Resource Manager software is the key enabler for serverconsolidation and increased system resource utilization.\n\n324Resource ManagementSolstice SunNet\nManager Computer-oriented local and wide area networks are normally managed using\nSNMP protocols, with Solstice SunNet Manager or HP OpenView productscollecting and displaying the data. Both products provide some visibility intowhat is happening in the computer systems on the network, but they arefocused on network topology. Resource management is done on a per-networkbasis, often by controlling the priority of data flows through intelligent routersand switches.\nSPARCcluster A highly integrated product line that is focused on improved availability in\ncommercial environments. Its management tools will eventually become anintegrated extension to the Sun Enterprise SyMON2.0 software. For HighPerformance Computing, Sun HPC Servers use the Platform Computing LoadShare Facility (LSF) to perform load balancing on much larger and moreloosely coupled clusters.\nSRM SeeSolaris Resource Manager .\nSRM(IBM) The\nSystem Resource Manager of WLM. The term SRM(IBM) is used in this\nbook to differentiate it from Solaris Resource Manager. SRM(IBM) providesthe algorithms for managing resources and caters for dynamic switchingbetween compatibility and goal modes.\nSSP System Service Processor. Starfire\u2019s system administrator & system monitoring\ninterface. The SSP configures the Starfire hardware, through a private ethernetlink, to create domains. The SSP collects hardware logs, provides bootfunctions, and produces consoles for each domain.\nStarfire SeeSun Enterprise 10000 .\nstatic resources Host information that does not change over time, such as the maximum RAM\navailable to processes running on the host.\nStorage Area\nNetwork (SAN) A complex managed storage system, where networked storage using\nfiber channel makes up an interconnection layer between multiple servers orclusters and multiple storage subsystems. A storage area network can containswitches and routers just like local or wide area networks, but the protocol incommon use is SCSI over fiber channel rather than IP over ethernet. A storagearea network may also span multiple sites, for example where remotemirroring is being used for disaster recovery.\nStoreX A technology developed at Sun that enables management of any storage\nresource in a heterogeneous distributed environment, from storage hardwarelike devices and switches, to storage software like backup solutions andvolume managers. For more information about StoreX, seehttp://www.sun.com/storage/storex/ .\nsubmission host In a typical LSF workload configuration, the submission host is the node where\nthe user or operator submits the task to be performed.\n\nGlossary 325Sun Enterprise 10000 A highly scalable 64-processor (UltraSparc II) SMP server with up to 64 Gbytes\nof memory and over 20 Tbytes of disk space.\nSun Enterprise SyMON\n2.0 A product developed by Sun to act as a user interface to hardware features. It\nis a powerful and extensible system and network monitoring platform that isused to manage other products. Sun Enterprise SyMON 2.0 is a Java-basedmonitor with multiple user consoles that can monitor multiple systems usingthe secure extensions to SNMPv2 to communicate over the network.\nSunNet Manager SeeSolstice SunNet Manager .\nSyMON SeeSun Enterprise SyMON 2.0 .\nSyMON Health\nMonitor A SyMON module that can be used in a resource management scenario to\ndetermine if a system has enough resources to run comfortably. For example, ifthe CPU state is reported as \u201cred\u201d, then either less work or more CPU powermay be needed on that system. Similarly, if the memory rule reports \u201cred\u201d thenthe system may need more memory\nsystem level\nmeasurements A type of measurement. System level measurements show the basic activity\nand utilization of the memory system and CPUs. Some network measurementssuch as TCP/IP throughput are also available on a per system basis. Perprocess activity can be aggregated at a per system level then combined withnetwork measurements to measure distributed applications.\nTeamquest A workload analysis product. See www.teamquest.com.\ntime constant In control theory, the rate at which a system responds to changes.\nTNF Seetrace normal form .\nToS SeeType of Service .\ntrace normal form (TNF) A format used to implement tracing (which makes it possible to trace\nthe execution steps of user and kernel processes). Trace normal form, which issupported by the Solaris operating environment, provides a self-describingtrace output format. Trace normal form allows data structures to be embeddedin the trace file without the need for an external definition of their types andcontents.\nType of Service (ToS) A header field contained in IP packets. Its purpose is to convey\ninformation about how the packet should be routed. The Solaris BandwidthManager can use this information when classifying a packet. It can also changethe information, to influence how the packet is routed.\nUDB DB2 Universal Database.\n\n326Resource Managementusage decay A form of decay used by SRM. The user scheduler is the most important and\nvisible portion of SRM and it implements usage decays which control longterm CPU allocation responsiveness.\nvirtual memory A type of memory that is allocated from a central resource pool and is\nconsumed by an application when it requests memory from the operatingsystem. Virtual memory is not directly related to physical memory usagebecause virtual memory is not always associated with physical memory. Forexample, if an application requests 16 Mbytes from the operating system, theoperating system will create 16 Mbytes of memory within that application\u2019saddress space but will not allocate physical memory to it until that memory isread from or written to.\nvirtual Web hosting A web server configuration where a single server is configured to respond to\nhundreds or thousands of Internet addresses. Virtual web hosting is often usedin situations where web sites receive little or no activity most of the time. Inthese situations, it is usually too expensive to dedicate a single computersystem to each web site.\nWAN Seewide area network .\nWebM Web-based management initiative\nwide area network A network that provides connectivity across a large geographical area.\nWLM SeeIBM Workload Manager .\nWorkload Manager SeeIBM Workload Manager .\nXCF SeeCross-System Coupling Facility .",
        "summary": "Resource Management is published by Sun Microsystems, Inc. 901 San Antonio Road \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource ManagementPart No. 805-7268-10July 1999, Revision ASun MicroSystems, Jr. The scope of this book can be summarized as: The best way to use combinations of Sun Microsystems products to manageresources. How can we combine lots of small servers into a few big ones? Sun\u2019s range of products provide a lot of built-in flexibility that can be deployed in many ways to meet the requirements of several markets. To manage the service level you provide, you must be able to measure and control the resources consumed by it. The Solaris operating environment is one of the best instrumented UNIXimplementations. Not all commercial tools report Solaris-specific metrics. Resource management for UNIX systems is in its infancy. This chapter describes the overall methodology of service level management. It alsodescribes and compares several approaches to resource management. Service level management is the process by which information technology (IT) infrastructure is planned, designed and implemented. Service level management interactions are shown in FIGURE 2-1. Each interactionconsists of a service definition combined with a workload definition. The processes involved in Service Level Management include creating service and workload definitions and translating from one definition to another. System managers first establish a workload definition and the service level requirements. Vendors then propose a system that meets these requirements. The basis of the sizing estimate can be a published benchmarkperformance. Service Level Management 5benchmark is used as the basis of a sizing estimate. Vendors provide reliability data for system components. They can also provide availability and performance guarantees for production systems. The agreed-upon service levels could be too demanding or too lax. The actual service levels experienced by users with a real workload are subjective measures that are hard to capture. The real service levels cannot always be captured directly, but the measurementstaken are believed to be representative of the real user experience. Atransaction may take place over a wide variety of systems. An order for goods willaffect systems inside and outside the company. System managers create policies that direct the resources of the computer system. A policy workload definition is closely related to the service levelagreement workload definition. The policy can controlresources directly or indirectly. The measured workload and service levels should be analyzed to extract trends. Acapacity planning process can then be used to predict future scenarios. The accrued usage of resources by each user or workload may be accumulated into an accounting system. A complete system implements many control loops. Designing a control loop is more complex and requires explicit understanding of the situation. A brief digression into basiccontrol theory is provided to help explain the behavior of such systems. You apply a control input by turning the steering wheel to the point thatwill get you around the corner. After a delay the car responds, You measure theresponse, compare it with what you wanted, and obtain the error difference. If the difference is zero you don\u2019t need to change the control input. You may decide to over- or under-correct, and apply the correction gradually or quickly. In control terms, you are applying negative feedback to the system. The rate at which you measureand apply corrections is called the control interval. The amount of the error that you feed back changes the behavior of the control loop. When you apply these principles to computer system resource management you can see that it is important to average measurements over an appropriate time scale. l, the system islightly damped and will be very responsive to sudden changes but will probablyoscillate back and forth. If you feed back a small proportion of the error over alonger control interval, thesystem is heavily damped. Diverse Methods occurs both because of the products that are available andbecause of the expectations of the end users who are purchasing solutions. Anetwork-centric methodology can be extended for use in other areas, so can astorage-centric or server-centric viewpoint. SunMicrosystems, Inc. egratedmethodology and solve all resource management problems. The closer you get tothis ideal, the more expensive and complex the tool becomes. So it is harder to justify purchasing it. The system-centric viewpoint focuses on what can be done on a single server using a mixture of hardware and operating system features. Dynamic Reconfiguration (DR),processor sets, Solaris Resource Manager\u2122, and Sun Enterprise\u2122 10000 (also knownas Starfire\u2122) Dynamic System Domains (DSDs) are all system-focused resourcemanagement technologies. The Starfire system uses its own HostView interface running on a separate system serviceprocessor (SSP) to manage domains. The next release of this software will be extended to viewan SSP and all the domains in a Starfiresystem as a special kind of cluster. There is no way to control the usage of memory by a workload. The only way to constrain a workload is to slow down or stop its CPU usage. Sun has two kinds of clusters. The highly integrated SPARCcluster\u2122 product range is focused on improvedavailability in commercial environments. Its management tools will eventuallybecome an integrated extension to the SyMON software. It is notcurrently possible to migrate a running job from one node in a cluster to another, orto checkpoint a job to disk and restart it again later. erconnect utilization andproportion of remote data access are important additional measures. The Solstice\u2122 Enterprise Manager product is a Telco-oriented CMIP and SNMPmanagement system. In theory it could beused to manage computer systems and local area networks, but it was not developed to do this. Network controls are based on delaying and prioritizing packets. There is no user orprocess identifier in a packet, so it is hard to directly map network activity to system activity. Storage has recently moved from being a simple attached computer systemperipheral to a complex managed entity in its own right. Networked storage usingfibre channel puts an interconnection layer in between multiple servers or clusters. Project StoreX is based on a distributed pure Java technologyplatform that can run anywhere a JVM is available. Project StoreXenables management of any storage resource in a heterogeneous distributedenvironment. Project StoreX enables resource management of capacity, performance, andavailability of data storage. Measurements of capacity and performancecharacteristics can be combined with availability policies. Backup and archival policies can be used to automatemigration of data to a tape library. Solaris software can trace storage accesses on a per-process, per device basis. There may be a need for a Project StoreX Solaris Storage BandwidthManager to bridge the two management viewpoints. Large and complex applications such as SAP R/3, Baan, and Oracle Financialscontain their own resource management concepts and controls. For example, OracleFinancials implements its own batch queue system to decouple the generation oflarge reports from the interactive response time of the users. SAP R/3 measures the response time of important transactions and breaks these down into application server time and backend database server time. As many usersconnect to an application server and many database servers connect to a single database, there is no concept of which user is doing work on the backend. The application itself must have the instrumentation to keep track of what is going on. The consolidation process starts when you identify candidate systems and Applications. The next chapter introduces several example workloads. The product overview section of this book discusses their capabilities inmuch greater detail. Local password filesmay need to be merged, and any conflicting port numbers specified in the/etc/services file may need to being cleared. If you use a name service such as NISfor all your password and services information, then the systems should already beseeing the same name space and definitions. The number of system footprints may betoo high with midrange servers. It is hard to reduce the total number of serverseffectively. Consolidated upgrades benefit from systems that can perform dynamic reconfiguration. DSDs are supported on the Solaris 2.5.1, 2.6, and 7 releases. The total number of DSDs can be reduced as applications are consolidated. There is a common set of measurements to collect per workload. o choose relevantprocesses and aggregate to measure them. The remainder is overhead or unplannedactivity. If it is significant, it should be investigated. When you are accumulating measurements don\u2019t accumulate the pscommandCPU% . It\u2019s a decayed average of recent CPU usage, not an accurate measure of actual CPU usage over an interval. You need to measure the actual process CPU time used in each interval by taking the difference of two measurements. One of the simplest policies is to define limits on a measurement and associate itwith an action. Products such as the Sun Enterprise SyMON 2.0 predefine many simple limit rules. An error event is different because it is treated as a discrete on/off event. A complex rule is used to establish the state of a component or asubsystem. When rules are combined, they are ranked so that critical problems takeprecedence over warnings. A hierarchy of rules can be built for a network of systems. Solaris Resource Manager and others like it assign shares to each user according to a policy decided by an administrator. The Solaris BandwidthManager software uses this mechanism to provide a way to specify policies on a per-network packet basis. A control loop is a complex thing to manage because its stabilitycharacteristics, time constant, and damping factor must be set correctly. Policy manipulatescontrols when a measurement deviates from its desired range. Automated goal-based workload management is a feature found only on mainframes running OS/390 software. Some policies are implemented manually as part of operations management. Security and network-based policies can be stored in an LDAP based name service. The Solaris BandwidthManager configuration is updated to take into account that user\u2019s network address. A direct control operates on the resource you want to control. An indirect controlworks via dependent resources. some operate on a per-user basis (such as a file system quota) Solaris Bandwidth Manager product implements a direct control on networkpacket rates. This can be used to implement an indirect control on the CPU resourcestaken up by the NFS server code in the kernel. The terminology definitions are network oriented but apply equally well to systemlevel policies. Some of the terms defined in this standard are listed here. ultiple devicetypes. Policy elements are expected to be independent of which Qualityof Service signaling protocol is used. Examples of policy elements includeidentity of the requesting user or application, user/app credentials, and so on. The general architecture shown FIGURE 3-1 illustrates one common implementationof a policy that combines the use of a policy repository, a PDP , and a PEP. This diagram is not meant to imply that these entities must be located in physicallyseparate devices. The policy protocol can be any combination ofCOPS, SNMP , and Telnet/CLI. Given this rich diversity, a common language isneeded to represent policy rules. The rest of the standard document describes theManagement tool. The Solaris software implements a blanket login ban for non-root users. It also limits the total number of processes that can be started via the nproc kernel tunable. This feature scales with the memoryconfiguration. A further limit is placed on the total amount of processes per user. The main class of application-level controls are those provided by relationaldatabases and transaction processing monitors. The Oracle8 idatabase also implements controls on resourceconsumption and policies for relative importance. On the Starfire system, the CPUs can be partitioned into dynamicsystem domains, and a separate copy of the Solaris operating environment booted in each domain. CPU boards can then be moved from one dynamic system domain toanother. This is described in detail in Chapter 8. SRM works by biasing CPU usage on a per-user basis, using shares to determine the relativeimportance of each user. LSF software implements a distributed batch queuing system where jobs are submitted and, when resources are available, sent to be run on a system. Block input and output counters are not incremented correctly in current releases of the Solaris operating environment. The block counter problem is filed as bugid 1141605 and is fixed in the next release of Solaris software. An application consumes virtual memory when it requests memory from the operating system. Virtual memory usage is not directly related to physical memory usage. Not all virtual memory has physical memory associated with it. When you restrict or control the amount of virtual memory that an application canhave, you are not controlling the amounts of RAM it can have. Virtual memory can be limited at the system level and at the process level. At asystem level, the total amount of virtual memory available is equal to the totalamount of swap space available. Each time virtual memory is used, the amount ofswap space available drops by the same amount. Base Solaris software can do simple resource management of a process\u2019s virtualmemory usage. Limits are enforced perprocess, thus preventing any one process from using an unreasonably large amount of virtual memory. But a user can run many processes, so this does not preventdenial of service attacks. It is important to note that the amount of swap space used by a user does notrelate directly to the sum of all that user\u2019s processes. A user may have threeprocesses, where each shares a single shared memory segment between them. SRM software isconfigured to limit the total virtual memory to 1.012 Gbytes. If one of the three processes has a memory leak, the limit would be hit for that user. Theper-user limits mus mus be set to 1 Gbytes for each user. Physical memory is controlled by applying importancepolicies to different types of memory. In the future, it may be possible to apply limitor allocation style policies to physical memory, but that capability is not available in Solaris today. The Default Memory Allocation Policy is, by default, allocated on a demandbasis with equal importance to each subsystem. When a subsystem requestsmemory, it is allocated from a central pool of free memory. If sufficient memory isavailable in the free pool, then an application\u2019s request is granted. The Solaris memory allocation policy takes into account recent usage in an attempt to choose the correct application from which to steal memory. The most aggressivememory requests gets the majority of the memory assigned to it. This situation can be avoided by configuring physical memory in the system so that there is always enough memory for each application\u2019s requirements. In examples, if we configured the system with 128 Mbytes of memory, then bothnetscape andgimp could execute at the same time. The memory system will takememory from other applications that haven\u2019t used portions of their memoryrecently. For example, if you start a file-based mail tool such as dtmail , the memoryused to cache the file when dt mail reads a 23-Mbyte mail file will be taken from other portions of the system. Priority paging prevents the file system from consuming toomuch memory. Priority paging implements a memory policy with differentimportance factors for different memory types. Application memory is allocated at a higher priority than file system memory. The new memory allocation policy can be extremely important for larger systems. A large database system with a 50 Gbyte+ database on the file system willcontinuously put memory pressure on the database application. But priority paging will ensure that thefile system onlyuses free memory for file system caching. The Solaris Bandwidth Manager product provides controls on network traffic. The product can be used as part of a resource control framework. It can distribute incoming traffic over multiplesystems according to the current load on each system. This chapter describes tools that help perform process-based analysis. Analysis of per-process measurements separates the raw data into applicationworkloads. When several applications are consolidated onto a single system,resource contention can occur. In a distributed environment with discrete applications on separate systems,workloads are analyzed by monitoring the total resource usage of each wholesystem. The busiest systems can be identified and tuned or upgraded. This approach does not work when multiple workloads are combined on a single system. The Solaris software provides a great deal of per-process information. The data can be viewed and processed by a custom-written process monitor. The SE Toolkit is freely available for Solaris systems and is widelyused. The psinfo data is what the pscommand reads and summarizes. The usage data is extra information that includesthe microstate accounting timers. Sun\u2019s developer-oriented Workshop Analyzer usesthis data to help tune code during application development. Thepea.se script is an extended process monitor that acts as a test program for the system. It is based on the microstate accounting information described in chapter 5 of Workload Management 51. There are two display modes: an 80-column format(which is the default and is shown inFIGURE 4-15 ) and the wide mode, which displays much more information. The initial data display includes all processes and shows their average data since the process wascreated. Idle processes are ignored. Thepea.se script is 90 lines of code containing a few simple printf s in a loop. The real work is done in process_class.se (over 500 lines ofcode) It can be used by any other script. When the command is run in wide mode, the following data is added:Metadata input and output blocks per second.Characters transferred by read and write calls. System call per second rate over this interval. CPU time consumed between each context switch. The SE Toolkit alsoincludes a workload class, which sits on top of the process class. If you group them by user name and command, then you can formworkloads, which is a powerful way to view the system. Thepw.se Test Program for Workload Class. 53on user name, command and arguments, and processor set membership. It can work on a first-fit basis, where each process is included only in the first workload that matches. The pw.sh script sets up a workload suitable for monitoring a desktop that is also running a Netscape web server. The script runs with a one minute update rate and uses the wide mode by default. A high number of page faults fora workload indicates that it is either starting a lot of new processes, doing a lot. of I/O, or tha %more pw.sh. t it is short of memory. The script is compiled with the following command: #!/bin/csh -DWIDE pW.se 60. The command is followed by a list of command lines. Once you have collected the data, you can write a rule that examines each process orworkload and determines, using thresholds, whether that workload is CPU-bound,memory-bound or I/O bound. A prototype of this rule is implemented in theSE Toolkit, and it can produce the kind of information shown in the example. The Internet provides a challenge for managing computer systems. External synchronizing events can cause a tidal wave of users to arrive at the server at the same time. Sports-related web sites in the USA get a peak load during key games in the\u201cMarch madness\u201d college basketball season.  proxy caching web servers sit between a large number of users and the Internet. When all the users are active at once, regardless of where they areconnecting to, these proxy cache servers get very busy. Many web sites get little or no activity most of the time. A caching web server acts as an invisible intermediary between a client browser and servers that provide content. It cuts down on overall network traffic andprovides administrative control over web traffic routing. Performance of accesses to all the other virtual sites can be affected, and resource management tools are needed. Caches commonly have a hit rate of about 20 to 30 percent, with only 60 percent ofthe data being cacheable. If cached items are read more than once by different users in areasonably short time interval, the cache will work well. Each cache transaction takes some time tocomplete and adds significant latency. Direct web service allows the browser to contact web servers directly whenever possible. This contact may involve a firewall proxy thatdoes not provide caching to get out to the public Internet. This funnel effect can increase network load at a local level. The real reason to set up a proxy cache intranet infrastructure is the administrativecontrol. Security is a big problem. Setting up the firewall gateways to the Internet so that they route web traffic only to and from the proxy caches. Restricted routing also forces every end user who wants to get out to the Internet to use a proxy cache. The cache makes routing and filtering decisions and logsevery access with the URL and the client IP address. If the corporate policy is \u201cInternet access is provided forbusiness use only during business hours,\u201d then employees who clog up thenetworks with non-business traffic can be identified. Solaris Bandwidth Manager software can be used effectively in this environment to implement policies based on the URL, domain or protocol mix. Apache and Netscape\u2122 proxy caches are extensions of a conventional webserver. The Harvest Cache was the original development and is now a commercial product. TheSquid cache is a freely available spin-off and is the basis for some commercial products. Squid is used at the biggest cache installations, caching traffic at thecountry level for very large Internet service providers. ICP-based connections are much more efficient than individual HTTPtransfers. ICP connections are kept open, and transfers have lower latency. In the Solaris operatingenvironment, it is possible to configure more than one IP address. The Solaris 2.6 operating environment was tuned tospeed up this process. P addresses to an interface and specify virtual interfaces with atrailing colon and number. Use the ndd command to query or set the maximum number of addresses per interface. %ifconfig -a \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0lo0: flags=849<UP,LOOPBACK,RUNNING,MULTICAST> mtu 8232. or each connection, moreprocesses are spawned for the addresses that are most active at any point. The administrator of the web site can log inas that user to configure their own site. Cgi-bin scripts that it starts all run as that user so that accounting, performancemonitoring and resource management tools can manage each virtual web site\u2019sactivity separately. Solaris Bandwidth Manager software can also be used effectively, asthere is an explicit administrative mapping from each network address to the useridthat is running that web server. The SRM software manages resources on web servers by controlling the amount ofCPU and virtual memory. Three basic topologies are used on systems hosting webservers. Resource management is used to control the behavior of a single web server. For example, a web server may be shared among many users. An error in one CGI-BIN program could cause the entire web server to run slowly. You can run each web server as a different UNIX userid by setting a parameter in the web server configuration file. For example, the Solaris Web Server has the followingparameter, as shown inFIGURE 4-25. The memory limit can limit the amount of virtualmemory that the web server can use. Thisprevents any one web server from causinganother to fail from memory allocation. The following are the server wide aliases. Proc mem limit [ memory.plimit ] The per-process memory limit can limit theamount of virtual memory a single CGI-bin process can use. It effectively limits thenumber of concurrent cgi-bin processes. This section examines some aspects of the resource management that apply to thesetypes of problems. It focuses on workload consolidation and resource managementof databases. The success of workload consolidation is bound closely to the ability to partition resources between the applications. The unique characteristics of an NFS workload place multiple resource demands on the system. No single product can provide complete resource management of anNFS workload. The resources used by NFS and the different products or techniquesthat can be used to manage them. NFS servers are implemented in the Solaris kernel as kernel threads and run in thesystem class. You can control the amount of resource allocated to the NFS server inthree ways:Limit the number of NFS threads with nfsd. The actual number of threads required will vary according to the number of requestscoming in and the time each thread spends waiting for disk I/O to service therequest. There is no hard tie between the maximum number of. threads and thenumber of NFS threads that will be on the CPU concurrently. The best approach is to approximate the number. of threads needed (somewhere between 16 to 64 perCPU), and then find out if the NFS. server is doing its job or using too much CPUtime. Solaris 2.6 changed to instrument NFS client mount points thesame way it does disks. NFS mounts are always shown by iostat andsar. The full instrumentation includes the wait queue for commands in the client that have not yet been sent to the server. Utilization ( %busy ) indicates the server mount-point activity level. An NFS server is much more complex than adisk drive and can handle many more simultaneous requests than a single disk drive. NFS uses physical memory in two ways. In the kernel each NFS thread consumessome space for a stack and local data. Outside the kernel the data being servedby NFS consumes memory as it is cached in the file system. The amount of memory used by the file systems for NFS servers is verylarge and much harder to manage. NFS uses a very small amount of swap space, and there should be no inter-workloadswap space issues from NFS. You can manage NFS disk storage using UFS disk quotas. Prioritypaging should be mandatory for any system that has NFS as one of the consolidation applications. Solaris Bandwidth Manager provides the means to do this. You can control amount of resources consumed by NFS indirectly by curtailing theamount of network bandwidth on port 2049. The filter in the above example is for managing outgoing NFS traffic to the 129.146.121.0 network. You could decide to leave out the destination and manageNFS Traffic to all clients, from wherever they come. The key variableismax_bandwidth specifies an upper bound to the consumed bandwidth that never will be exceeded. Thepriority variable is less important. It will be a factor if other types of traffic are being managed. Higher priorities will have lower average latencies. The typical data center strategy is to consolidate more workloads onto each system. Resource management of the whole environment requires careful planning and a solid understanding of theinteraction between these packages and the database. A single database instance provides database services for two different applications that require access to the same data. No single control or policy can assign resources to ensure that adequate CPU is provided tomeet response times. The user issues an http request from the desktop, which must travel across the intranet/Internet to the web server. This component can bemanaged by controlling the bandwidth and network priorities into the web servers. Managing the bandwidth on the networkport on the machine that hosts the web server is a useful strategy to ensure thatkeyhttp requests get the required bandwidth. The Solaris BandwidthManager software can control the bandwidth allocation into the web servers. The database listener process accepts the incomingconnection from database client applications. The database listener forks a database process and connects the clientapplication to the database. The same control as in the web server component (2) is implemented here. The SRM software and resource limits can limit the amount of virtual memory used by processes, users, and workloads. This capability does not manage physicalmemory, but it effectively restricts theamount of global swap space consumed by each user. It is a high risk to ever let adatabase server reach its virtual memory limit. If the limit is reached, the databaseengine may hang or crash, and the database may have to be restarted. The objective for successful consolidation is to provide strong insulation between each database instance. This requires careful resource management of CPU, memory,I/O, and network traffic. System resources are not wasted because spare CPU cycles can be used by any otherworkload requiring them. The more workloads consolidated, the flatter the totalresource usage becomes. The shared memory area is often the largest component of the database\u2019s memory usage. It is the easiest to insulate between instances because the memory isallocated as shared memory. Resource Managements wires down the memory so that it can\u2019t be paged. One instance cannot steal memory from another. D-down memory must be unallocated before it can be removed from the system, requiring quiescing of the database. Private memory is the regular process memory used by each process. The Solaris page cache causes a huge memory demand, which places undue pressure on the database private memory. Priority paging puts a hard fence between the file systems and applications. If you plan to run a database on file systems, consider this a mandatoryrequirement. Ensure that I/O activity from one application does not affect another application inan undesirable way. The best way to insulate I/o within a single database instance is to make sure that application tables are placed on separate I-O devices. This completely insulates one database from another. If you must use a single storage controller, use capacity planning so that sufficient bandwidth is available to combine both. For OLTP applications this is rarely an issue because the bandwidth requirements are so low. Adecision support application is completely different. A single decision support workload can generate several hundred megabytes a second of I/O. When you add CPU resources to an instance ofOracle 7, the Oracle engine automatically picks up those resources. Someother databases must be restarted to use the newly available resources. Tounderstand how different databases work within a resource managementenvironment. The SE Toolkit is based on a C language interpreter that is extended to make all theSolaris measurement interfaces available in an easy form. It is used to prototype ideas that can then be promoted for incorporation inproducts, in particular for Sun Enterprise SyMON 2.0 software. The SEToolkit has been jointly developed by Richard Pettit and Adrian Cockcroft as a\u201cspare time\u201d activity since 1993. e code that takesmetrics and processes them is provided as C source code and runs on the interpreter. The business operation can be broken down into several applications such as sales and distribution, e-commerce web service, email, file, and print. Use aform that makes sense to managers and non-technical staff to represent the part ofthe business that is automated by the computer system. The Application Resource Measurement (ARM) standard aims to instrumentapplication response times. Per process activity can be aggregated at aper system level then combined with network measurements to measuredistributed applications. ARM can be used to measure user response time. All vendors support the one standard, and severalplementations exist. Application vendors have shown interest, so more measurements will become available. CCMS is used by several tools such as BMC Best/1 to providemore detailed application-level management than can be done with just system and device-level information. This information is provided by afacility called CCMS. Data is collected on a Netscape 2.5 proxy cache that is serving most of the thousandor so employees in Sun\u2019s United Kingdom facility. SE Toolkitpercollator.se script can parse Netscape and Squid proxy cache log formats andsummarize the data. The cache finish status was analyzed, and operations are divided into four categories. The NO-CHECK and UP-TO-DATE states are cache hits. The WRITTEN,REFRESHED, and CL-MISMATCH states are misses that cause cache writes. TheDO-NOT-CACHE and NON- CACHEABLE states are uncacheable, and anything elseis an error or incomplete transfer. On a quiet weekday, 280,000 accesses went via this cache, and 56 percent of theaccesses went out to the Internet. 34 percent ofthe accesses hit in the cache, 16 percent missed and caused a cache write, 49 percentof the accesses were not cacheable, and 1 percent ended in some kind of error. The plot in FIGURE 5-1 shows bands of transfer times that depend upon the user\u2019s location. Many users are locally connected, but others are operating over slowernetworks. The transfer time includes the time spent waiting for the remote server torespond. Log files are a rich source of performance information. You can apply thistechnique to many other applications, such as ftp and mail servers, as well as toany other application that can write a line to a file. The underlying data structures provided by the Solaris operating environment are described in full in the proc(4) manual page. The data that psuses is called PIOCPSINFO , and this is what you get back from ioctl . The data is slightly different if you read it from the pseudo-file.  p is a pointer to a prpsinfo structure containing at least the following fields. The structure is a typedef structure with a number of fields for each process. For a multithreaded process, you can get the data for each lightweight process separately. There\u2019s a lot more useful-looking information there, but no sign of thehigh-resolution microstate accounting that /usr/proc/bin/ptime and SEToolkit scripts display. proc(4) returns the process usage information; when applied to an   process file descriptor, PIOCUSAGE returns theprocess usage information. The file format is: proc(4), proc(5),proc(6) and proc(7) P points to a pr usage structure which is filled by the operation. The pr Usage structure contains at least the following fields. The psCommand returns usage information for the specific lWP file descriptor. Pr_dftime;   /* data page fault sleep time */    Timestruc_t   pr_slptime;  /* all other sleep time /* kernel page faults sleep time. pr_wtime; /* wait-cpu (latency) time   u_long,   Pr_majf,  /* major page faults, */ pr_nswap, /* swaps */ u_mrcv, u_msnd, u-msnd. PIOCUSAGE can be applied to a zombie process. PIOCRESET can be used to disable microstate accounting. To access process data you must have access permissions for entries in /proc. In the Solaris 2.6 release, you can access the /proc/pid entry for every process. This means that any user can use the full functionality of ps. Microstate accounting is not turned on by default. It slows the system down slightly. Microstate accounting works as follows: A high-resolution timestamp is taken on every state change, every system call, every pagefault, and every scheduler change. Thenormal measures of CPU user and system time made by sampling can be wrong by20 percent or more because the sample is biased, not random. For example, consider a performance monitor that wakes up every ten seconds,reads some data from the kernel, then prints the results and sleeps. Processes that sleep then consume less than their CPU time quanta always run at the highest timeshare priority. The error is an artifact of the dual functions of the clock interrupt. If twoindependent, unsynchronized interrupts are used (one for scheduling and one forperformance measurement), then the errors will be averaged away over time. You can increase the CPU clock rate to get more accuracy. The best solution is to use a microstate accounting-based tool, or to disablesome of the CPUs so that the measurements are made on a fairly busy system. The data provided by the SE processmonitoring class is shown in Figure 5. double elapsed; /* elapsed time for all lwps in process */double total_user;  \u00a0    /* current totals in seconds */ double total_system;doubletotal_child; /* child processes that have exited */double user_time; \u00a0 /* user time in this interval */double system_time, double trap_time and double child_time. ulong outblocks;   /* output blocks/interval - metadata - metadata only - not interesting */ulong vmem_size; \u00a0 /* size in KB */ ulong maj_faults; /* minf/intervals - always zero - bug? */ulONG total_swaps; /* swapout count */long  priority;\u00a0 /* current sched priority */ long  niceness; /* current nice value */char  sched_class[PRCLSZ]; /* name of class */ulongs messages;  /* msgin+msgout/interVAL */ulongo signals;  \ufffd    \u201c  \u201d; /* signals/Interval */ul Tong signals; /* voluntary context switches/inter Validation. Most of the data in FIGURE 5-6 is self explanatory. All times are in seconds in double precision with microsecond accuracy. The minor fault counter seems to be brokenbecause it always reports zero. The inblock andoutblock counters only refer to file system metadata for the old-style buffercache. Many processes have very short life spans. You cannot see such processes with ps, but they may be so frequent that they dominate the load on your system. The overhead of collecting accounting data is always present but isinsignificant. Accounting can be started immediately by using the acctcom command. Add some crontab entries to summarize and check the accounting logs. Collecting and checkpointing the accounting data puts a negligible additional load onto the system. The commands reported are sorted by KCOREMIN, which is the product of theamount of CPU time used and the amount of RAM used while the command wasactive. A high factor means that thiscommand hogs the CPU whenever it is running. CHARS TRNSFD counts thenumber of characters read and written.  /usr/lib/acct/monacct. og30    8   10   9   12   13   14   15   16  17  18  19  20  21  22  24  25  257617.08 15.65 688.46 16456.60 0.02 88735308080 2649maker4X. 10 426182.31 43.77 5004.30 9736.27 4.38 0.01 803267592 3434wabiprog 53 355574.99 44.32 972.44 8022.44 822.87 0.05 355871360 570imagetoo 21 2576 17.65 15. 65 688,000.16 0.03 2784769024 1295xlock 32 1027099.38 726.87 4253.34 1413.04 Resource Management devices (basically, local disk file system reads and writes) The underlying data that is collected can be seen in the acct (4) manual page. See Chapter 7 for more information onSRM Accounting.  /* 3-bit base-8 exponent in the high */ /* order bits, and a 13-bit fraction */ /* in the low order bits. */ /* memory usage in clicks (pages) */ /* ticks */ The Solaris Bandwidth Manager software has built-in support for Cisco NetFlow\u2122software. This feature allows for detailed network measurements that can be sent to other software packages. NetFlow-enabled devices send out NetFlow datagrams, which contain records for one or more flows. Combining multiple flow records in one datagram reducesnetwork overhead caused by NetFlow. The NetFlow FlowAnalyzer application uses the output from NetFlowFlowCollector. It provides elaborate processing, graphing, and reporting options for network analysis, planning, troubleshooting and more. There are six basic disk access patterns. Read, write, and update operations caneither be sequentially or randomly distributed. This section explains the basic Solaris softwaremeasures and discusses more complex disk subsystems. You cannot automatically tell which processes are causing disk activity. You may be able to work out where the workloadcomes from by looking at how an application was installed. When a large number ofdisks are being reported, the iostat -x variant provides extended statistics. The Starfire system supports a maximum configuration of several thousanddisk drives. When more than one type of data is stored on a disk, it\u2019s  to read because each disk is summarized on a separate line. The size of eachdisk is also growing. Solaris 2.6 has a number of new features to help solve problems. It is now possible to separate root, swap, and home directory activity. Full data is saved from the first SCSI probe to a disk. iostat -M shows throughput in Mbytes/s rather than K bytes/s for high-performance systems. Dead or missing disks can still be identified because there is no need to send themanother SCSI probe. Another option ( -n) translates disk names into a more useful form. Tapes are instrumented in the same way as disks; they appear in sar andiostatautomatically. Tape read/write operations are instrumenting with all the samemeasures that are used for disks. Rewind and scan/seek are omitted from these. The output format and options of sar(1) are fixed by the generic UNIX standard. In the Solaris 2.6release, existing iostat options are unchanged. New options that extendiostat are as follows. Solaris 2.5 includes a self-describing trace output format. A set of libraries allows user-level programs to generate trace data. The trace data helps analyze and debug multithreaded applications. Device statistics: Device Not Ready: 0 No Device: 0 Recoverable: 0Illegal Request: 0 Predictive Failure Analysis: 0. 0. 0000000%iostat -E deities. The tnfxtract routin controls probe execution for both user and kernel traces. While user-level probes canwrite to trace files, the kernel probes write to a ring buffer. This buffer scheme avoids any need tolock the data structures, so there is no performance loss. The command sequence to initiate an I/O trace is quite simple. You run thecommands as root, and you need a directory to hold the output. Thetnfdump program does quite a lot of work to sort the probe events into time order. In the other window we extracted and dumped the data to take a look at it. To really understand the data presented by iostat ,sar, and other tools, you must look at the raw data being collected. A standard disk is SCSI based and has an embedded controller. The diskdrive contains a small microprocessor and about 1 Mbyte of RAM. It can typicallyhandle up to 64 outstanding requests via SCSI tagged-command queuing. In large systems,there is another level of intelligence and buffering. The same reporting mechanism is used for client side NFSmount points and complex disk volumes setup using Solstice\u2122 DiskSuite\u2122software. In the old days, if the device driver sent a request to the disk, the disk would donothing else until it completed the request. Disks that spin fasterand seek faster have lower service times. The problem with iostat is that it tries to report the new measurements in some of the original terminology. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. The SE Toolkit uses the kstat (3K)-based data structure. The underlying metrics are cumulativecounters or instantaneous values. We need to take two copies of the above data structure together with ahigh resolution timestamp for each and do some subtraction. Ong    number$;   /* linear disk number */   string   name$;\u00a0 /* name of the device */ /* number of bytes read */\u00a0 ulonglong nread; \u00a0 /* number  of bytes written */\u00a0  longlong  wtime; /* wait queue - time spent waiting */\u00a0 longlong wlentime; /* active/run queue - sum of queue length multiplied by time at that length */}; We assume that all disk commands complete fairly quickly. We obtain the utilization or the busy time as a percentage of the total time. A similar calculation gets us the data rate in kilobytes per second. The meaning of Srunis as close as you can get to the old-style disk service time. The disk can run more than one command at a time. The data structure contains an instantaneous measure of queue length, but we want the average over the time interval. We get this from that strange \u201clength time\u201dproduct by dividing it by the busy time. The Solaris 2.6 disk instrumentation is complete and accurate. Now that it has been extended to tapes, partitions, and client NFS mount points, there much more can bedone with it. As long as a single I/O is being serviced at all times, a single queue is being used. When the device being monitored is an NFSserver, hardware RAID disk subsystem, or a striped volume, it is clearly a muchmore complex situation. All of these can process many requests in parallel. In practice, some other effects come into play. The drives optimize head movement, so that as the queue gets longer, the average service time decreases. In effect, the load oneach disk is divided by the number of disks. Using SNMP counters is a good way to get an overall view of network throughput. The Solaris software provides an SNMP daemon which provides the dat. Most networking devices support SNMP , and the SyMON software can incorporateany third-party MIBs so all links from a switch can be monitored at the same time. Many other commercial and free applications and utilities manage SNMP devices. The collision rate is (Collis / Opkts) * 100% . In our case, that is less than a tenth of a percent. Collisions are absolutely normal and should cause no concernunless collision rates become very high (in the order of 10 to 20 percent or higher) Most of these statistics are related to the Ethernet MAC and the network interfacecard hardware itself. Every network interface card typically has different counters, which canchange with the operating system releases. Sun does not officially support thenetstat option. The SE Toolkit script nx.se lists TCP as if it were an interface, with input and output segment and data rates. For interfaces that provide this information (at present, only leandhme)nx. se reports kilobytes in and out. % /opt/RICHPse/bin/se nx.se.se current. sing in the TCP/IP stack and lack of buffering on input. Defr shows thenumber of defers that took place. A defer happens when an Ethernet tries to sendout a packet, but it finds the medium to be busy. Solaris Bandwidth Manager is a tool for configuring Solaris. The main purpose of Alternate Pathing is to sustain continuous network and disk I/O when system boards are detached. Short jobs are said to backfill processors reserved for large jobs. Blacklist is a file that enables you to specify components, such as system boards, that should not be configured into the system. Class Based Queuing (CBQ) is the underlying queuing technology used in Solaris BandwidthManager. The classifier analyzes the packet protocol,ToS value, URL information, source information, destination information and allocates the packet to a class queue. E servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if highavailability is important, multiple servers are configured as siblings. The CommonInformation Model provides a conceptual framework within which it ispossible to organize information about a managed environment. The Oracle8 iDatabase Resource Manager enables the administrator to limit the degree of parallelism of any operation. DDI/DKI is specified in the \u201cWriting Device Drivers\u2019 section of the DriverDeveloper Site 1.0 AnswerBook. These are function call entrypoints that device drivers should implement in order to support DR. DDI_SUSPENDsuspends the drivers to begin the quiesce period. DDI_RESUMEresumes the drivers after the quyingce period, and so on. The goal of this group is tooffer a standard information model and directory schemas. DIMM Dual In-Line memory Module. Diff-Serv addresses network management issues related to end-to-end Quality of Service (QoS) within diverse and complex networks. The set of system resources it understands is host (by name),system architecture, operating system type, amount of memory, and CPU usage. Thus, if the external orstandard service class goal is not being met, the associated DISCs can bemanaged. Dynamic SystemDomains (DSD) Starfire independent hardware entities formed by the logical association of its system boards. rvers which allows system boards to be added (attached) orremoved (detached) from a single server or domain. LSF uses the Load Information Manager (LIM) as its resourcemonitoring tool. To modify or add load indices, an Extended Load InformationManager can be written. Fairsharescheduling divides the processing power of the LSF cluster among users. The FlowCollector aggregates this data, doespreprocessing and filtering, and provides several options to save this data todisk. Other applications such as network analyzing,planning, and billing can use these files as input. Hierarchical fairshare enables resources to be allocated to users in a hierarchical manner. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Computer-oriented local and wide area networks are normally managed usingSNMP protocols. Both products provide some visibility intowhat is happening in the computer systems on the network, but they arefocused on network topology. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. The sharedmemory area is often the largest component of a database\u2019s memoryrequirements. ISP Internet Service Provider, a company that provides Point-of-Presence access to the Internet. JTAG is an alternate communicationsinterface between the SSP machine and the Enterprise 10000 server. A special data structure that controls the dynamic growth of all non-relocatable memory. When Dynamic Reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. The Load InformationManager process running on each execution host is responsible for collectingload information. The load indices that are collected include: host status,length of run queue, CPU utilization, paging activity, available swap space,available memory, and I/O activity. The Load Share Facility is a vehicle for launching parallel applications on an HPC cluster. When the LSF softwareinitializes, one of the nodes in the cluster is elected to be the master host. maximum bandwidth The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager. The maximum bandwidth is dependent upon the percentage ofbandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. mestamp is taken on every state change, every system call, every page fault, and everyscheduler change. A database topology where a single process serves many users. NQS/Exec is geared toward a supercomputer environment. Limitedload balancing is provided as there is no concept of demand queues. There is also no control over interactivebatch jobs. PC NetLink adds file and print services, andenables Solaris servers to act as Microsoft\u00ae Windows NT\u2122 Primary DomainControllers (PDC) or Backup Domain Controllers (BDC) PC NetLink 1.0 offers many new options for utilizing hardware resources and minimizing systemadministration overhead. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. LSF provides several resource controls toprioritize the order in which batch jobs are run. Batch jobs can be scheduled torun on a first come first served basis, fair sharing between all batch jobs, andpreemptive scheduling. Process Monitor is an optional module within Sun Enterprise SyMON that can be used to view all processes on a system. The Process Monitor can also be configured topattern match and accumulate all the processes that make up a workload. Project StoreX is based on adistributed pure Java framework. It can run on servers from any vendor, interface to other storage management software, and manage any kind of attachment.  Proxy caches are used in corporate intranets and at ISPs. Solaris Bandwidth Manager provides the means to manage your network resources to provide Quality of Service to network users. receptor DSD Dynamic Reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider DSD. The same system board is then attached to a receptorDSD. The Solaris Resource Manager (SRM) ensures that a particular user or application receives its fair share of resources. It requires each hopfrom end-to-end be RSVP-enabled, including the application itself. Solaris software can provide a great deal of per-process information that is notcollected and displayed by the ps command or Sun Enterprise SyMON 2.0software. The data can be viewed and processed by a custom written processmonitor. Service Level Agreement captures expectations and interactions between end users, system managers,vendors, and computer systems. In essence, this is analogous to the SRMlnode, which effectively defines a resource management policy that can be besubscribed to. The time it takes for an I/O device to service a request can be complex tomeasure. The SolarisResource Manager (SRM) is based on ShareII. SHR Scheduler is a component of the Solaris Resource Manager (SRM) Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris Resource Manager software is the key enabler for serverconsolidation and increased system resource utilization. Service LevelAgreements can be defined and translated into Solaris Bandwidth Managercontrols and policies. Computer-oriented local and wide area networks are normally managed using SNMP protocols. Resource management is done on a per-networkbasis, often by controlling the priority of data flows. The SSP configures the Starfire hardware, through a private ethernetlink, to create domains. The SSP collects hardware logs, provides bootfunctions, and produces consoles for each domain. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 Gbytes of memory and over 20 Tbytes of disk space. Sun Enterprise SyMON 2.0 is a Java-basedmonitor with multiple user consoles that can monitor  storage hardware. System level measurements show the basic activity and utilization of the memory system and CPUs. Perprocess activity can be aggregated at a per system level then combined withnetwork measurements to measure distributed applications. Trace normal form (TNF) is a format used to implement tracing. It makes it possible to trace the execution steps of user and kernel processes. ToS is a header field contained in IP packets. Virtual memory is not directly related to physical memory usage. Virtual web hosting is often used in situations where web sites receive little or no activity most of the time.  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0WLM SeeIBM Workload Manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0XCF SeeCross-System Coupling Facility. a large geographical area. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W LM SeeIBm Workload manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 XCF See cross-system coupling facility.",
        "metadata": {
          "word_count": 35852,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "engineering",
        "hpc",
        "infrastructure",
        "metrics",
        "monitoring",
        "performance",
        "platform",
        "scaling",
        "security",
        "tracing"
      ]
    },
    {
      "id": "virtual_adrianco_book_29",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: Java and the Internet",
      "source": "Prentice Hall - adrianco and Rich Pettit",
      "published_date": "1998",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_book_30",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: SPARC and Solaris",
      "source": "Prentice Hall - adrianco",
      "published_date": "1995",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_story_31",
      "kind": "story",
      "subkind": "",
      "title": "Microservices and DevOps",
      "source": "Infoq Charles Humble",
      "published_date": "7/11/2014",
      "url": "https://www.infoq.com/interviews/adrian-cockcroft-microservices-devops/",
      "content": {},
      "tags": [
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_story_32",
      "kind": "story",
      "subkind": "",
      "title": "High availability",
      "source": "Richard Seroter on Infoq",
      "published_date": "",
      "url": "https://www.infoq.com/articles/cockcroft-high-availability/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_33",
      "kind": "story",
      "subkind": "",
      "title": "Failing Over Without Falling Over",
      "source": "Stackoverflow",
      "published_date": "2020",
      "url": "https://stackoverflow.blog/2020/10/23/adrian-cockcroft-aws-failover-chaos-engineering-fault-tolerance-distaster-recovery/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_36",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "Adrian Cockcroft Appearance Playlist on YouTube",
      "source": "Many different accounts - technical content",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8Ce-3VPplFg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_37",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Enterprise Indigestion at NVIDIA GTC",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 2111,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_38",
      "kind": "story",
      "subkind": "",
      "title": "SC23 update",
      "source": "InsideHPC",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/sc23-top500-trends-the-ai-hpc-crossover-chiplet-standardization-the-emergence-of-ucie-and-cxl-advancements/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_39",
      "kind": "story",
      "subkind": "",
      "title": "SC22 predictions",
      "source": "InsideHPC",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/12/sc22-cxl3-0-the-future-of-hpc-interconnects-and-frontier-vs-fugaku/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_40",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Why did Docker Catch on Quickly and Why is it so Interesting",
      "source": "The New Stack",
      "published_date": "2014",
      "url": "https://thenewstack.io/why-did-docker-catch-on-quickly-and-why-is-it-so-interesting/",
      "content": {
        "text": "\n\nDocker has rapidly become \u201cthe new thing\u201d for computing in 2014. Why did that happen so quickly, and how has a Linux-application container become so popular? Broadly, I think, Docker is a great example of how to build a viral, developer-oriented product.\n\nA developer can figure out what Docker does, install it and do something useful with it in 15 minutes. I first heard this \u201crule\u201d from Marten Mickos when talking about why MySQL was so successful: low friction to try it out, a simple concept and useful functionality.\n\nDocker is a great name and it has a cute logo. It resonates with what the product does and is easy to remember. Engineering-oriented founders sometimes seem to think that names and logos don\u2019t matter if the product is good enough, but a great name can turbocharge adoption and build a valuable brand.\n\nThe Docker product came from a non-threatening source, a small startup (DotCloud) that was able to broadly partner across the whole industry. If the same product had come from an established enterprise technology player, there would have been much more push-back from that player\u2019s competitors, and the market would probably have split into several competing technologies.\n\nThe rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. So everyone is having to cooperate around a single container solution. This is great for the developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny, somewhat, as Docker did an end-run around their strategy.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nGet ubiquity first, become the category leader, then convert that into business value and revenue opportunities later.\n\nThat leaves the remaining question of, what does Docker do that is interesting, and who might it compete with? I have four separate answers:\n\nPortability : Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. This is a similar benefit to the Java \u201cwrite once, run anyware\u201d idea, but is more robust and is generalized to \u201cbuild anything once, run anywhere.\u201d\n\nSpeed : Start-up time for a container is around a second. Public-cloud virtual machines (VMs) take from tens of seconds to several minutes, because they boot a full operating system every time, and booting a VM on a laptop can take minutes. To counter this advantage, VMware has just announced (but not shipped) a technology called Fargo, that clones an existing VM in a second or so.\n\nConfiguration : The Docker container captures the exact configuration of a version of an application. To upgrade the application in production, the container is usually replaced with a new version, which takes a few seconds. The layers of components that go into the configuration are kept separate and can be inspected and rebuilt easily. This changes configuration management to be largely a build-time activity, so, for example, a Chef recipe might be used to build a Docker container, but at runtime there is no need to use the Chef services to create many identical copies of a Docker container. Used in this way, Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack.\n\nDocker Hub App-store : Docker containers are shared in a public registry at hub.docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Because containers are very portable, this provides a very useful cross platform \u201capp store\u201d for applications and component microservices that can be assembled into applications. Other attempts to build \u201capp stores\u201d are tied to a specific platform (e.g., the AWS Marketplace or Ubuntu\u2019s Juju Charms) or tool (e.g., the Chef Supermarket), and it seems likely that Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities.\n\nOne reason Docker is interesting is that all four answers are each individually useful, but can be used in combination. This causes cross-pollination of ideas and patterns. For example, someone might start using Docker because they like the speed and portability, but find that they end up adopting the configuration and Docker hub patterns as well.\n\nThe Docker technology is still fairly new; work is underway to add missing features, and a large ecosystem of related projects and companies is forming around it. There\u2019s a lot of interest in the technology from the VC community, as we try to figure out whom to fund to do what, and how the story will play out in the longer term.\n\nAdrian Cockcroft is a technology fellow\u00a0at Battery Ventures.",
        "summary": "Docker is a great example of how to build a viral, developer-oriented product. A developer can figure out what Docker does, install it and do something useful with it in 15 minutes. The rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. This is great for developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny. Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack. Docker containers are shared in a public registry at hub. docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities. There\u2019s a lot of interest in the technology from the VC community.",
        "metadata": {
          "word_count": 834,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "containers",
        "docker",
        "microservices",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_41",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "A Conference to Discuss Developer-Driven Infrastructure",
      "source": "The New Stack",
      "published_date": "2015",
      "url": "https://thenewstack.io/a-conference-to-discuss-developer-driven-infrastructure/",
      "content": {
        "text": "\n\nCompetitive pressures have pushed speed of development to be one of the highest\u00a0priorities for business today. Improved tools and techniques have moved the state\u00a0of the art in agile development from monolithic updates every few weeks, to\u00a0continuous delivery of microservices several times a day. The move to \u201crun what\u00a0you wrote\u201d and developer-driven infrastructure means that developers are not only\u00a0delivering products faster, but are also responsible for the efficiency and safety of those\u00a0products in production. Lean development techniques that take waste out of the\u00a0process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. The security blanket approach of wrapping a firewall around an\u00a0insecure monolithic product has been replaced by penetration testing of every\u00a0microservice, sophisticated use of identity and access management mechanisms,\u00a0encryption as the default and fine-grain security key management.\n\nThese three core areas form a story arc through GOTO London: Agile, Lean and Rugged. In each of them we will discuss the state of the art and emerging directions\u00a0that will set you up with a broad view of these key concerns for developers in 2016.\n\nThis is the first ever GOTO London conference. The core story is being structured as\u00a0a single track. Many conferences have an initial keynote session for everyone,\u00a0followed by a wide selection of talks running at the same time. This forces you to\u00a0decide what to miss and move from room to room. It also means that an individual\u00a0track has trouble building and maintaining context, and encourages repetitive, self-contained presentations. The approach is great for variety, but can be frustrating.\n\nFor GOTO London, we will start the week with two optional training days on Monday and Tuesday, Sept. 14-15, then kick off the event with two full days of single-track talks Wednesday and Thursday, Sept. 16 -17. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences,\u00a0presentations about open source tools and talks about the latest hot products.\n\nFor the single-track days, everyone will be in the same room, building up a shared\u00a0context as the curated story arc unfolds. You won\u2019t miss anything, you won\u2019t waste\u00a0time moving from room to room. You will get to know each other and the speakers\u00a0much better. The talks will be a bit shorter, more focused, and will build on each other, reducing repetition and elementary introductions.\n\nThere is a repeated pattern for the single-track days. In each half day, there will be\u00a0four half-hour talks, followed by a half-hour panel session with all four speakers\u00a0discussing each other\u2019s talks with each other and the audience.\n\nAfter leading a workshop on microservices, I\u00a0will also introduce the\u00a0conference as a whole on Wednesday morning.\n\nIntroducing Agile , Dan North will\u00a0focus on the impact of new ideas and tooling supporting faster development.\n\nNicole Forsgren, Ph.D., will introduce\u00a0Lean, showing how DevOps culture and practices\u00a0reduce waste and improve outcomes.\n\nIntroducing Rugged , Joshua Corman will discuss how developers can build the core concepts of security into their\u00a0applications to protect information from attack. These four speakers will end the\u00a0morning with the first panel session.\n\nThe afternoon continues with four speakers digging deeper into the relatively\u00a0unfamiliar territory of Rugged systems.\n\nWhat kind of attacks are we seeing? What\u00a0tools are available for developers to automate security testing?\n\nHow can we\u00a0manipulate keys, identity and access, safely and easily?\n\nThe afternoon ends with\u00a0Joshua Corman joining the four speakers for a panel discussion.\n\nThe first evening is the conference party, with an entertaining and informative\u00a0keynote presentation by Ines Sombra and Adrian Colyer. Ines runs the San Francisco Papers-We-Love meetup, and Adrian publishes a daily blog post called The Morning\u00a0Paper. Over the last year, there has been a rush of interest in academic research\u00a0papers resulting in Papers-We-Love meetups all over the world. People are finding\u00a0hidden gems, radical ideas, or fundamental turning points in computer science, and\u00a0having fun presenting their own interpretations. We hope you will be inspired to\u00a0attend your local meetup, read The Morning Paper and share your own ideas.\n\nThe second day starts with four presentations on Lean. The Lean Enterprise book\u00a0has become required reading as these ideas spread into the mainstream. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer as their top priority. We\u00a0will also look at developer-oriented techniques for measurement, analysis and\u00a0optimization. Nicole will return to lead the panel session.\n\nDigging into Agile, we will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave , and see what it takes for a startup to \u201ccatch and surf the\u00a0wave\u201d when a product goes viral and everyone else dives in. At the other extreme, we will examine the latest ideas to speed up development at a financial institution\u00a0and for embedded hardware, and finish with Rachel Davies on Extreme\u00a0Programming in the 21st Century. Dan North then leads the closing panel.\n\nWe wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups.\n\nThe final day of the conference puts the concepts into practice, with deep dives into\u00a0open source tools and the latest products. The multi-track talks are longer, with\u00a0more time between them to switch rooms and for hallway conversations.\n\nWe expect that GOTO London attendees will have an enjoyable and memorable\u00a0experience learning the concepts and tools needed to be agile, lean and rugged for\u00a02016 and beyond.\n\nWeaveworks is a sponsor of The New Stack.\n\nFeatured image via Flickr Creative Commons.",
        "summary": "Developers are responsible for the efficiency and safety of products in production. Lean development techniques that take waste out of the process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. This is the first ever GOTO London conference. The core story is being structured as a single track. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences, presentations about open source. There is a repeated pattern for the single-track days. In each half day, there will be four half-hour talks. The talks will be a bit shorter, more focused, and will build on each other. The first evening is the conference party, with an entertaining and informative presentation by Ines Sombra and Adrian Colyer. Over the last year, there has been a rush of interest in academic research papers resulting in Papers-We-Love meetups all over the world. The second day starts with four presentations on Lean. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer. We will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave. We wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups. The final day of the conference puts the concepts into practice, with deep dives into open source tools and the latest products.",
        "metadata": {
          "word_count": 944,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "agile",
        "architecture",
        "devops",
        "docker",
        "infrastructure",
        "lean",
        "microservices",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_story_42",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "The Rise of Continuous Resilience",
      "source": "The New Stack",
      "published_date": "2020",
      "url": "https://thenewstack.io/the-rise-of-continuous-resilience/",
      "content": {
        "text": "\n\nAmazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21.\n\nI sometimes ask a CIO whether they have a backup datacenter. Most will say yes, as it\u2019s a normal part of a business continuity plan for disaster recovery. In some industries, like financial services, it\u2019s a regulated requirement and there\u2019s an annual visit from an auditor to make sure it\u2019s in place.\n\nWhen I ask how often they test their failover process, people start to look uncomfortable. Some admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. When I ask what the failover test looks like, it\u2019s normally one application at a time, in a planned migration from the primary data center to the backup. It\u2019s rare for people to test an entire data center by cutting its power or network connections at an arbitrary time. I did hear once from a bank that had two data centers and switched between them every weekend so that one data center was primary on even-numbered weeks and the other was primary on odd-numbered weeks. If they ever had a problem mid-week, they knew what to do and that they could rely on it working smoothly. I\u2019ve asked this question a lot over several years, and have had only a handful of good answers.\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nDisasters don\u2019t happen very often, but unfortunately, when data centers lose power, drop network connectivity, lose their cooling system, catch fire or are flooded, the whole data center goes offline. Usually at an inconvenient time, with little or no warning. During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater, and they still don\u2019t work once the water recedes \u2014 as they are then full of mud and other debris. Even if the data center isn\u2019t in the basement, sometimes the backup generators are, or the fuel tanks for the generators, or some critical network equipment. There are lots of examples of disasters like this in the press, so why are so many companies that have a business continuity plan in the news when disaster strikes?\n\nThe short answer is that it\u2019s hard to get a disaster recovery data center implemented, and too much work and too risky to test it frequently. Each installation is a very complex, fully customized \u201csnowflake.\u201d The configuration of the two data centers drifts apart, so that when the failover is needed the failover process itself fails and the applications don\u2019t work. Even such basic things as backups for data need to be tested regularly, by attempting the restore process. I\u2019ve heard of some embarrassing data loss situations, where the backups failed and this wasn\u2019t discovered until months later when a database failed and a restore was needed.\n\nI call this \u201cavailability theater\u201d \u2014 everyone is going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nSo how can we make this better? There are two technology trends coming together to create a more productized solution, that is tested frequently enough to be sure it\u2019s going to work when it\u2019s needed. The combination of cloud computing and chaos engineering is leading to \u201c continuous resilience. \u201d\n\nThe last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. They ended up doing region evacuation testing about once every two weeks.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nIn these tests, they drain all the traffic from a region and show that they can still run Netflix on the two remaining regions; and no-one notices! (Netflix operates from AWS regions in Virginia, Oregon and Dublin). The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope \u2014 a \u201c Chaos first \u201d policy.\n\nThe reason Netflix was able to implement this is that cloud regions are different to data centers in two critical ways. Firstly, they are completely API driven, and the entire state of an AWS account can be extracted and compared across regions. Secondly, the versions and behaviors of the AWS services in each region don\u2019t drift apart over time the way data centers do.\n\nMost customers will have a mixture of multiregion workloads. Some customer-facing services, like a mobile back-end that needs to be online all the time, can be built active-active, with traffic spread across multiple regions. Workloads like a marketplace, which needs a consistent view of trades, are more likely to be operating in a primary region \u2014 with failover to a secondary, after a hopefully short outage.\n\nNetflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong, and to make sure that the system has enough resilience to absorb failures without causing customer-visible problems. Nowadays more companies are setting up chaos engineering teams, hardening their systems, running game day exercises, and using some of the chaos engineering tools and services that are developing as the market matures.\n\nAWS has been investing in our services to provide support for multiregion applications, for both active-active operation and primary-secondary failover use cases. In the last few years, we\u2019ve added cross-region data replication and global table support to Amazon S3, DynamoDB, Aurora MySQL and Aurora Postgres . AWS also acquired a disaster recovery service called CloudEndure , which continuously replicates application instances and databases across regions. We\u2019ve also extended AWS Cloudwatch to support cross-account and multiregion dashboards .\n\nAs usual, we are listening to what our customers ask for, and our partners and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.\n\nReaders might also like the Architecting resilient systems on AWS session from AWS re:Invent 2019.\n\nFeature image via Pixabay.",
        "summary": "Amazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21. Some CIOs admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. The combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater. It's hard to get a disaster recovery data center implemented. Each installation is a very complex, fully customized \u201csnowflake.\u201d Even such basic things as backups for data need to be tested regularly. The last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. Netflix operates from AWS regions in Virginia, Oregon and Dublin. The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope. Netflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong. Nowadays more companies are setting up chaos engineering teams. AWS Cloudwatch extended to support cross-account and multiregion dashboards. Partner and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.",
        "metadata": {
          "word_count": 1077,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "netflix",
        "resilience"
      ]
    },
    {
      "id": "virtual_adrianco_story_43",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Nvidia GTC: Hyperscaler Happiness and Enterprise Indigestion",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 2111,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_44",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Sustainability: Comparing AWS, Azure and Google",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/sustainability-how-did-amazon-azure-google-perform-in-2023/",
      "content": {
        "text": "\n\nThe leading cloud providers and the mammoth global companies that run them are adopting environmentally sustainable practices, but it is a complicated work in progress.\n\nAs Amazon , Google and Microsoft continue to grow, they\u2019re all grappling with the challenges of reducing their carbon emissions in the f ace of increased resource usage of AI, while making the right investments in renewable energy infrastructure around th\u200b\u200be world.\n\nCustomers of cloud providers need detailed information to understand and optimize the sustainability of their own workloads . While Amazon is making the biggest sustainability investments and getting some good results, it has historically been the least transparent cloud provider.\n\nAmazon released its sustainability report covering the whole of 2023 on July 9, Google released its report on July 2 , and Microsoft released its in May .\n\nI\u2019ve been leading the Real Time Cloud project at the Green Software Foundation for the last year or so. We have made a deep dive into the information available, identifying gaps and subtle differences in the data from each cloud provider.\u00a0 I\u2019m going to compare these three reports to see what\u2019s new since last year; my analysis here is my own, not the project\u2019s.\n\nAmazon Leads in Carbon Output Reductions\n\nAmazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 MmTCO2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ (grams of carbon dioxide per gross merchandise dollar) to 80.8 gCO2/$.\n\nThis continues a trend that started last year, of a small reduction in total carbon despite growing the business, as more of the internal projects started over the last few years mature and begin to have material impact. There\u2019s a long way still to go but it\u2019s a commendable result.\n\nGoogle however, increased its total carbon by 13% to 14.3 MmTCO2e for 2023, on top of an increase the previous year, slipping on carbon progress due to investment in data centers for AI, which was also the story at Microsoft, up from 16.5 to 17.2 MmTCO2e.\n\nThe physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors, and Amazon\u2019s revenue growth rate is a bit lower. But over the years, Amazon has also been more aggressive in buying renewable energy than Google and Microsoft.\n\nAmazon\u2019s carbon footprint was reduced for all categories apart from fossil fuels, as its delivery fleet continues to grow as a business, faster than those vehicles are being electrified. This trend is driven in part by Amazon bringing more deliveries in-house; in the report, the category that includes third-party delivery carbon declined 4%.\n\nIncreased focus on rail-based delivery in Europe and India is reducing delivery carbon. The electric delivery fleet grew more than sevenfold, to 19,000 vans, but still has a long way to go before it makes a material difference.\n\nOver time, the electricity use of Amazon for retail and delivery is adding a lot more demand in addition to the growth of data centers, compared to\u00a0 Microsoft and Google. This is why Amazon is investing in so many more energy projects.\n\nRenewable Energy Projects: Making Good Progress\n\nThe most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon , including retail and deliveries.\n\nThis is much more than just the AWS data center usage. Energy matching means that renewable energy is being generated and put into a grid somewhere in the world in an amount that matches any non-renewable energy that\u2019s being used.\n\nThe energy projects that make up the portfolio are geographically positioned as close as possible to where the power is needed. But it\u2019s not always possible for them to be located nearby, and the amount of energy generated doesn\u2019t exactly match how much is currently being used locally.\n\nThe Amazon generation portfolio increased from 20GW to 28GW, making it the world\u2019s largest purchaser of renewable energy for the fourth year in a row. Google added half as much, 4GW in 2023, which is still a record for the company.\n\nForty-two new utility-scale projects came online for Amazon in 2023. The Amazon sustainability report also states that the company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has also purchased a nuclear-powered data center in Pennsylvania.\n\nAmazon\u2019s energy storage increased from 445MW in 2022 to 1.3GW in 2023 to help smooth out the daily supply and demand mismatch as solar and wind come and go. It\u2019s now become common for new solar power developments to include battery storage, as the peak daily output could be curtailed and the excess can be stored then supplied to the grid in the evening.\n\n100% Carbon-Free Energy Match Still Emits Carbon\n\nThe 100% carbon-free energy match metric uses the annual \u201cmarket-based\u201d method, which is based on energy purchases, averaged over the whole year. As we noted previously, the in-house generating capacity won\u2019t ever match the right location at the right time, so there\u2019s a market for trading excess renewable energy.\n\nRenewable energy certificates (RECs) can be traded for up to a year, so when companies have final data for 2023, additional \u201cunbundled RECs\u201d can be purchased as needed to cover for the non-renewable energy that was used, and obtain any desired percentage, at a cost.\n\nAmazon committed to reach 100% by 2030, with a goal of 2025, and is proud to get to 100% in 2023. It looks less impressive to realize that Amazon has finally caught up with Google, which has been at 100% carbon-free energy match using the same annual market-based method for the last seven years. The Amazon sustainability report states that there was 2.79 MmTCO2e of energy-related carbon emissions (known as Scope 2). In other words, when Amazon reached 100% carbon-free energy, there were still 2.79 million tons of carbon equivalent emitted.\n\nA few years ago Google started measuring and reporting a much more stringent \u201c24\u00d77\u201d hourly matching location-based method. Microsoft has dabbled with 24\u00d77, but only for the Azure Sweden region, and AWS hasn\u2019t done anything with it.\n\nAmazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report : \u201cAmazon\u2019s Scope 2 (Indirect) GHG emissions: Location-based method (LBM) are 15.67 MmTCO2e.\u201d Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by the location-based method. When the methods were defined, the report\u2019s authors didn\u2019t consider that large end users of energy could also be large generators of energy across multiple locations.\n\nGoogle discloses location-based data to customers for all its cloud regions individually, and AWS needs to follow suit and start to report Scope 2 location-based data on a per-region basis.\n\nGiven the locations and sizes of the Amazon renewable power projects, I think AWS will often have lower location-based method carbon than Google and Azure. But AWS needs to publish its results \u2014 regardless of whether those numbers are better or not.\n\nAmazon Is Leaning Into Future Energy Capacity\n\nThe new Amazon report includes a statement that the company is \u201cpurchasing additional environmental attributes (such as renewable energy credits) to signal our support for renewable energy in the grids where we operate, in line with the expected generation of the projects we have contracted.\u201d\n\nIt takes two to three years to bring a wind or solar farm online, so Amazon buys RECs on the open market that match the future generation capacity that they have committed to building. This is a good policy, as unbundled REC purchases have a bad reputation when they are used across regions and countries as a cheap substitute for actual investments. But here they are being purchased to match investments in new generating capacity.\n\nHowever, there is also a footnote on page 26 of the sustainability report:\n\n\u201cAWS aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases (e.g., renewable energy in the same grid is not available), AWS may procure renewable energy attributes in other locations.\u201d\n\nA similar footnote in the audit report on energy also reads:\n\n\u201cAmazon takes a global approach to calculating the percentage of electricity consumed by Amazon\u2019s global operations matched by renewable energy sources. Amazon aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases, Amazon may procure renewable energy in other locations.\u201d\n\nThis is an unfortunate fact of life: there just isn\u2019t enough renewable energy in regions in Asia in particular, so cross-border RECs are used. This is the first time I\u2019ve seen Amazon make this statement, and Amazon joins Google and Microsoft, who\u2019ve been buying cross-border RECs and carbon offsets for many years.\n\nAmazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. This covers all of the Canada, China, Europe, India, Japan and U.S. regions. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Indonesia, Israel, Korea, Singapore, South Africa, and the United Arab Emirates. The difference since 2022 is that the \u201cin-market\u201d wording was added and Japan was added to the previously published 100% list.\n\nHowever Amazon has far more generating capacity in Asia than either Google or Microsoft, and Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances.\n\nThe problem with this is that by the market method, cloud energy use in Asia will be reported as zero carbon, because cloud providers will pay extra by buying RECs elsewhere. However, increased use of regions in Asia will cause extra carbon to be emitted compared to the same use in Europe or the U.S., and to measure the difference the location-based method needs to be used and reported.\n\nMore Investments in De-Carbonizing Supply Chains\n\nAmazon announced a new program to engage major suppliers to decarbonize its supply chain with Amazon Sustainability Exchange , sharing internally developed playbooks and sustainability science models for things like renewable concrete and hydrogen. There\u2019s some interesting work and good advice here; it was put together by sustainability scientists in the Amazon central sustainability team.\n\nLow-carbon concrete and steel for construction is a good investment and is growing fast, up from 16 AWS data centers to 36 in 2023, but it\u2019s still a small impact on the carbon total. Efficiency and electrification upgrades and the use of local solar power on existing buildings are having a bigger effect.\n\nAWS, Azure and Google have all started to transition to bio-diesel (mostly from recycled cooking oil) for backup generators, and AWS has reduced the use of air freight for deliveries and started to use electric vehicles for trucking equipment to data centers. This is worthwhile, as it helps develop a market for the supply of biodiesel globally, although this is currently a very small part of the total carbon footprint.\n\nAmazon is correctly positioning investment in nature-based solutions and carbon capture as a second priority to direct reductions in carbon emissions. But it\u2019s a worthwhile additional investment, as it\u2019s expected to be necessary to scale these solutions over time to meet the 2040 Net Zero Climate Pledge goal, as the remaining direct emissions become harder to eliminate.\n\nSustainable Water Resources: Good Progress\n\nSustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers, which has increased a lot in recent years. A few years ago the cloud providers started to measure and report their water usage, and they all have targets to use less, and to return more clean water to the locations where they operate.\n\nThere are two ways to measure water, one is how much water is used as a proportion of the energy being used in a data center. Water usage effectiveness (WUE) is measured in liters per kilowatt hour. The other is how much water is replenished as a proportion of what is used.\n\nAWS\u2019s WUE improved by 5% from 0.19 to 0.18 Liters/kWh in 2023, averaged across all the facilities AWS runs globally. It\u2019s unfortunate that AWS only provided a global average, which removes any opportunity to optimize via workload placement.\n\nMicrosoft Azure provided WUE on a region-by-region basis, varying from zero to over 2 Liters/kWh. Google did not report WUE figures.\n\nAWS reports 41% \u201cwater positive\u201d as a new metric in 2023, with a 100% goal. This appears to be another name for \u201cwater replenishment.\u201d\n\nGoogle reports improving from 6% to 18% water replenishment in 2023, but the company has a goal of eventually returning more water than it uses. Google was dinged for poor water use a few years ago, and is clearly playing catch up, while AWS has already made good progress.\n\nAzure doesn\u2019t provide a replenishment figure. This is the kind of thing the Green Software Foundation\u2019s Real Time Cloud project is working on, trying to get all the cloud providers to standardize on the same metrics with the same names by pointing out the gaps.\n\nAmazon\u2019s Reporting Needs More Transparency.\n\nAWS had previously published data center versus cloud carbon reduction comparison reports for Asia, Europe and the U.S. that were used as the basis for a lot of marketing claims over the years. They were getting a bit old and were updated by a new carbon reduction comparison report in June.\n\nAccenture was commissioned to write the report, and while it\u2019s a helpful document and the claims it makes do seem reasonable, there\u2019s no transparency in the calculations, and a lot of private AWS data was used by Accenture that isn\u2019t available for customers to use to model their own specific situations.\n\nCustomers need more than marketing claims, and the report could have included a lot more of the underlying data and calculations, not just the headline results.\n\nAWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago and releases quarterly updates. Microsoft Azure publishes PUE data for all its regions.\n\nWhen you measure energy use for a workload in a cloud region (e.g. by collecting NVIDIA GPU energy use metrics), you need to multiply by PUE to account for how much energy was supplied to the data center to cover cooling and transmission inefficiencies then multiply by the carbon content of that energy.\n\nPUE is much higher in hot and humid climates. Of course, some workloads have to be in a specific region. But a significant amount of compute and storage capacity (like backup/archives) could be located anywhere. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data.\n\nThe other data provided by both Google and Azure is the companies\u2019 carbon-free energy (CFE) proportion on a per-region basis. This takes the local method grid carbon mix into account, then subtracts out the locally generated capacity from renewable projects that are up and running. It\u2019s a good way of indicating which regions are benefiting from low-carbon energy that can be used for optimizing global workload placement.\n\nThere is still no mention of Scope 3 supply chain reporting for AWS customers (Scope 3 refers to emissions from assets not owned or controlled by an organization, but that the organization\u2019s supply chain directly affects.)\n\nIt has been many years since Azure and GCP started to report Scope 3 and it is now years since AWS promised that Scope 3 would be provided. They\u2019ve told me they have a team working on it, but until something is released, this is a huge gap. Microsoft released a good white paper on their Scope 3 methodology back in 2021.\n\nThe AWS Customer Carbon Footprint Tool (CCFT) was embarrassing when it was initially released in 2022, and it has made no progress in the years since then. It is going to report zero for everyone for their Scope 1 and 2 carbon footprint, according to the market methodology, has no Scope 3 data, and aggregates too much data together.\n\nI recently tried to use the CCFT data to track progress for a company, and the three usage categories EC2, S3 and Other combined with three geographies EU, Americas and Asia made it impossible to figure out what was going on. The Other category in the Americas is dominating, but that\u2019s all it tells you.\n\nCustomer carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. Completely useless.\n\nIn summary, Amazon is making decent progress towards a reduced carbon footprint, while Google and Microsoft are slipping. However, someone at AWS needs to read and follow the advice given in the Amazon Exchange Carbon Measurement Guide . The guidance says that metrics and transparency are needed, and AWS is still at ground zero, with no transparency, and no progress on metrics that their customers have been asking for for years.\n\nAdditional Resources\n\nAmazon Reporting website\n\nAmazon Carbon Methodology\n\nAmazon Renewable Energy Methodology\n\nCarbon Free Energy\n\nAWS Sustainability Reporting Framework Summary\n\nAmazon Exchange Carbon Measurement and Reporting guidelines\n\nWater Positive Methodology\n\nAccenture report on AWS carbon reduction\n\nAmazon Energy Audit\n\nAmazon Scope 1 and 2 Audit\n\nAmazon Scope 3 Audit",
        "summary": "Amazon released its sustainability report covering the whole of 2023 on July 9. Google released its report on July 2 and Microsoft released its in May. Customers of cloud providers need detailed information to understand and optimize their own workloads. Amazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 Mm TCO 2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ to 80.8 g CO2/$. This continues a trend that started last year, of a small reduction in total carbon despite growing the business. The physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors. Amazon has also been more aggressive in buying renewable energy than Google and Microsoft. The most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon. Amazon generation portfolio increased from 20GW to 28GW. Forty-two new utility-scale projects came online for Amazon in 2023. The company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has finally caught up with Google, which has been at 100% carbon-free energy match for the last seven years. Amazon committed to reach 100% by 2030, with a goal of 2025. Amazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report. Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by theLocation-based method. It takes two to three years to bring a wind or solar farm online. Amazon buys RECs on the open market that match the future generation capacity. This is a good policy, as unbundled REC purchases have a bad reputation. Amazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Japan and U.S. Amazon has far more generating capacity in Asia than either Google or Microsoft. Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances. Sustainability scientists in the Amazon central sustainability team. Low-carbon concrete and steel for construction is a good investment and is growing fast. Amazon is correctly positioning investment in nature-based solutions. Sustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers. Cloud providers have targets to use less, and to return more clean water to the locations where they operate. In 2023, water use in the U.S. is expected to rise from 0.3% to 0.4%. The U.N. has set a goal of 0.5% water use by the end of the year. AWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago. PUE is much higher in hot and humid climates. Microsoft Azure publishes PUE data for all its regions. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data. There is still no mention of Scope 3 supply chain reporting for AWS customers. Scope 3 refers to emissions from assets not owned or controlled by an organization. It has been many years since Azure and GCP started to report Scope 3. AWS carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. AWS Sustainability Reporting Framework Summary. Water Positive Methodology. Accenture report on AWS carbon reduction. Amazon Exchange Carbon Measurement and Reporting guidelines. Amazon Energy Audit.",
        "metadata": {
          "word_count": 2890,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "metrics",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_45",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Cloud PUE: Comparing AWS, Azure and GCP Global Regions",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/cloud-pue-comparing-aws-azure-and-gcp-global-regions/",
      "content": {
        "text": "\n\nIn December 2024 Amazon released Power Usage Effectiveness (PUE) data for many of their AWS cloud regions, and along with existing data from Microsoft Azure and Google Cloud , there is finally enough data to make some comparisons for regions spread around the world and see what changed from 2022 to 2023.\n\nPUE is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center, and losses in the transmission and conversion of electricity on its way to the computers. Energy is measured (and paid for) at the meter as it enters the building, and a PUE of 1.15 means that an extra 15% of the total energy used by the computers is used for cooling and overheads. PUE varies between about 1.04 and 2.0 in practice.\n\nAll the cloud providers run very efficient data center hardware configurations, and they have, in general, become more efficient over time. However, it is harder to cool data centers in warm and humid environments, so PUE for data centers in the tropics will tend to be at the high end of the range, and PUE for data centers in cold and dry parts of the world are at the low end of the range.\n\nOne way to use less energy for cooling is to use more water, so there is also a natural tension between PUE and Water Usage Effectiveness (WUE in liters/kWh). The cloud providers have all recently invested heavily in optimizing for WUE as well, so the latest data centers tend to have good PUE and good WUE, but this requires the best and latest technology. Lower-cost and older data centers typical of the kind that enterprises own tend to be closer to 2.0 PUE and have a high WUE as well. WUE was explained and compared across cloud providers in the story I wrote for The New Stack in July 2024.\n\nSome of the regions deployed by cloud providers around the world are hosted by local service providers rather than in dedicated data centers built by the cloud providers. In this case the PUE is not included in the public numbers for two reasons, one is that it\u2019s hard to attribute and allocate part of a shared resource when there is no information about what is in the rest of the datacenter, and the other is that the over-all PUE is often proprietary information that is owned by the service provider and they don\u2019t allow it to be shared openly. Some additional PUE estimates may be available privately under a non-disclosure agreement so it\u2019s worth asking your provider if you are operating in a region that doesn\u2019t have a public PUE number.\n\nAmazon/AWS PUE Data\n\nThe new PUE information from Amazon is described on their sustainability page . They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. There is a short PUE Methodology pdf that basically says that they are following the relevant international (ISO) and European (CEN) standards. The global average 2023 PUE for AWS is 1.15, with AMER at 1.14, EMEA at 1.12 and APAC at 1.28. They say their best individual data center facility in Europe has a PUE of 1.04. Still, their best region is Melbourne, Australia, with a PUE of 1.08, and their worst is Hyderabad, India, with a PUE of 1.50. From year to year, AWS shows improvements in many regions.\n\nAWS PUE data as stated in January 2025.\n\nMicrosoft Azure PUE Data\n\nMicrosoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions, but when they were updated in December 2024 the numeric data was omitted. Instead, a webpage provides a summary that provides information for only 11 out of 27 regions, that is offset from the calendar year and doesn\u2019t cover all of 2023. We contacted the currently responsible teams within Microsoft and found that whoever disclosed the original 2022 data had moved on, and the 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table .\n\nThe Microsoft web page PUE data as stated in January 2025.\n\nMicrosoft discloses their best PUE of 1.11 in Wyoming, USA, and their worst at 1.35 in Illinois. Their sustainability targets don\u2019t mention PUE.\n\nGoogle Cloud Platform GCP PUE Data\n\nGoogle has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. For the same reasons as AWS and Azure discussed above, they don\u2019t report public PUE data on every region they operate in.\n\nGCP PUE data as stated in January 2025.\n\nGoogle\u2019s best PUE result is 1.07 in Oregon, and its worst is 1.19 in both Singapore and Nevada. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while until they start to fill up with equipment. This can be seen in the GCP regional data. Some locations support Google products that are not part of GCP regions, and there is a mapping of GCP cloud regions to the Google PUE data that we performed with help from Google engineers as part of producing the GSF cloud region metadata table .\n\nComparisons Between AWS, GCP and Azure\n\nWhile all the major cloud providers have industry-leading PUE numbers that are likely to be much better than local data center alternatives, the numbers above show that GCP has the best transparency, with more data over a longer time period, and the best overall PUE for the regions being disclosed. Microsoft went from providing the most data for 2022 to the least data for 2023 and also has the highest PUE values overall. AWS sits in-between, with two years of data, and PUE values that are worse than GCP but mostly better than Azure overall. AWS includes data for regions like Hyderabad with higher PUE that aren\u2019t disclosed at all by other cloud providers, and they have an excellent stated goal of 1.08 for their new data center builds.\n\nRegional Comparisons\n\nIf you can choose which cloud provider to use in a particular region, and you\u2019d like to make the most efficient use of the energy needed, then the above PUE data provides data for some scenarios.\n\nVirginia is the biggest cloud region in the world, AWS PUE is 1.15, Azure is 1.14 and GCP is 1.08. For the nearby regions in Ohio AWS PUE is 1.12, GCP is 1.10.\n\nSingapore is a major region in Asia for all the cloud providers, with a challenging tropical climate. AWS PUE is 1.30, Azure is 1.34, and GCP is 1.13 or 1.19 for each of their two facilities.\n\nIreland is one of the largest regions in Europe. AWS PUE is 1.10, Azure is 1.19, and GCP is 1.08.\n\nWhat Is a Good PUE Assumption for the GPUs Powering the AI Boom?\n\nThere is a lot of concern about how much power the massive build-out of GPU capacity is going to need. Whatever power the GPUs use directly needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. Looking at the above data, I think there are two situations to think about. GPUs that are put into older enterprise data centers tend to overwhelm the existing infrastructure that wasn\u2019t designed for very high power density, so the PUE is likely to be bad, I\u2019d guess 1.5 or higher. However, the massive new data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. The latest, most efficient designs will allow more GPUs to be powered and cooled in a given location, and I\u2019d assume a PUE of 1.08, regardless of who is building it.\n\nGreen Software Foundation Real-Time Cloud Project\n\nI\u2019ve been leading the GSF real-time-cloud project for the last year or so. We\u2019ve spent a lot of time discovering many sources, interfaces and products that collect and report energy and carbon data and documented their relationships in a large Miro flow chart. We\u2019ve also published regional metadata collected from GCP, AWS and Azure and summarized it into a single table covering the 2022 data sets. We\u2019re in the process of updating it to include the latest data releases to cover 2023 and are planning to produce estimates for 2024 and 2025 before the cloud providers disclose their data so that workloads that are running today can have some data to use that we think is the best guess available.",
        "summary": "Power Usage Effectiveness (PUE) is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center. PUE varies between about 1.04 and 2.0 in practice. PUE for data centers in the tropics will tend to be at the high end of the range. PUE for centers in cold and dry parts of the world are at the low end. Lower-cost and older data centers typical of the kind enterprises own tend to have a high WUE as well. The new PUE information from Amazon is described on their sustainability page. They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. Some additional PUE estimates may be available privately under a non-disclosure agreement. Microsoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions. The 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table. Google has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while. GCP has the best transparency, with more data over a longer time period. Microsoft went from providing the most data for 2022 to the least data for 2023. AWS sits in-between, with two years of data, and PUE values that are worse than GCP.  AWS PUE is 1.15 in Virginia, Azure 1.14 in Ireland and GCP 1.08 in Ohio. PUE needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. New data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. I\u2019d assume a PUE of 1.08, regardless of who is building it. . at are running today can have some data to use that we think is the best guess available. at the time of this article. We are happy to provide the best data we can.",
        "metadata": {
          "word_count": 1469,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "platform",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_46",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "NVIDIA Unveils Next Gen Rubin and Feynman Architectures",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/nvidia-unveils-next-gen-rubin-and-feynman-architectures-pushing-ai-power-limits/",
      "content": {
        "text": "\n\nSAN JOSE, Calif \u2014 NVIDIA has cemented its central position in the technology industry over the last few years. It has the most in-demand products and amongst the biggest revenue and biggest market capitalization. It\u2019s a once-in-a-lifetime moment for everyone involved, as the Talking Heads once sang, but the torrent of product releases is extremely hard to keep up with and make sense of.\n\nThere is an impressive amount of business agility and relentless progress, but sometimes headline-making announcements are quietly forgotten or superseded by even better products before people have had a chance to get their hands on them. NVIDIA is announcing new hardware that effectively obsoletes its previous products well before they are available, but customers are buying whatever they can get, as it appears.\n\nMeanwhile \u201cthe new stack\u201d of AI software is maturing rapidly, easy-to-use, pre-tuned, high-performance open source models are available, and a plethora of AI-based services and tools are competing for everyone\u2019s attention. Development is also being accelerated by a variety of AI-backed tools that are improving rapidly.\n\nLast year, I predicted that enterprise customers would find it extremely difficult to manage the pace of change, and get \u201centerprise indigestion,\u201d but that hyperscalers would be happy. This year, I think the hyperscaler customers and cloud providers have indigestion as well. It takes a few years to plan and build a data center, but the specification of the racks and the power density keeps going up, so they must be replanning before they are finished building. NVIDIA is responding by releasing a very wide range of packaging options to target various points in the market and providing more information about its future roadmap, which helps but is making it even harder to keep track of what\u2019s going on.\n\nHow Did We Get Here?\n\nBefore we can make sense of what was announced by Jensen Huang , the CEO of NVIDIA, in his annual GTC keynote Tuesday , we need to establish some context by looking back at previous keynotes and additional announcements over the last year. There is a big disconnect between the GPUs that most people are using now (the ones that just started shipping, but that most people haven\u2019t used yet) and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail.\n\nThe GPU servers that most people are familiar with right now have two Intel CPUs and eight NVIDIA Hopper H100 GPUs as the nodes that can be clustered. They were announced three years ago and are the mainstream workhorse underpinning most of the AI-based products and services we use today.\n\nTwo years ago in his keynote, Jensen announced the Grace Hopper GH200 combined CPU/GPU architecture. Grace was the first CPU designed by NVIDIA and uses ARM architecture rather than Intel, with an NVlink interface to couple it directly to a slightly upgraded Hopper H200 GPU. While it shipped in volume, the GH200 one-to-one mix of CPU to GPU is the wrong ratio for many AI training customers, who wanted more of their dollars and power budget to be spent purely on additional GPU capacity. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as the H100s.\n\nGH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. This is reversed from conventional architectures where the CPUs are in the center (that\u2019s what the C in CPU stands for!) driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when an H200 GPU wants to send data to another H200 GPU in the same system in a cluster, it goes directly across NVlink. This is far faster than a traditional GPU sending via a PCI bus to an Intel CPU then over a network to another Intel CPU, then via PCI to the other GPU. However, when one Grace CPUs wants to communicate with another, the data passes through at least two Hopper GPUs to get there.\n\nGH200 systems seem to be more suited to the HPC Supercomputer market than the much bigger AI training and inference market. NVIDIA told me in 2024:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nIn 2024, Jensen announced the Blackwell architecture GPU and the GB200 system. To get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (addressing the issue with the 1:1 CPU/GPU ratio in GH200).\n\nBlackwell GB200 modules have around four times the silicon area dedicated to GPU, and the same area for CPU, compared to GH200 modules. Two modules fit on a 1 rack-unit-high, water-cooled motherboard, and 18 compute boards fit in a 72 GPU NVL72 rack, along with switchboards to interconnect them.\n\nNVIDIA has introduced several variations of the Blackwell and packaging options since launch, including a reduced-performance single-chip B200A variation that can be configured to be air-cooled as part of its MGX line of products. In the MGX GB200A NVL36 rack, each board takes two rack units and has a single Grace CPU resulting in a 9-CPU/36-GPU rack that needs 40kW of cooling, which is still a challenge but is easier to deploy in existing data centers.\n\nAt the end of 2024, it was disclosed that a new process from chip supplier TSMC allows for 50% higher performance Blackwell Ultra \u2013 B300 GPUs. These also use even more power, pushing the NVL72 rack from 132kW to 163kW, but it appears that many of the orders and plans that were in place for B200 have now switched to Blackwell Ultra.\n\nIt also has more flexibility in its design options, allowing customers like AWS to customize its deployments to integrate with its custom packaging and network architecture. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72 so that existing installations can extend their lifespan, and work done on productizing data centers to support the original NVL72 rack isn\u2019t wasted.\n\nNVIDIA revealing Blackwell Ultra NVL72 onstage at GTC.\n\nThe highly structured AWS cloud architecture doesn\u2019t naturally support dropping in a product like NVL72, and while Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave , the latest AWS GPU that is generally available as of March 18, 2025 is the p5 that has eight H200 with two Intel CPUs . Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. In the header shot for this story, you can see the variants of Blackwell-based racks that are deployed by each provider.\n\nLooking closely, it appears that most of the racks including Azure and Oracle cloud are NVL72, but AWS and GCP are using the air-cooled MGX36, and HP has its own custom Cray architecture rack for HPC that hosts 244 Blackwell GPUs. I haven\u2019t seen any new announcements from AWS at GTC and confirmed that it has no announced Blackwell-based products right now after visiting its expo booth. And you may ask yourself, how did they get here?\n\nThere was already some disclosure in mid-2024 but Jensen provided more details of the next-generation 2026 Vera CPU and Rubin GPU , which has even higher per-rack power requirements. To reduce \u201chyperscaler indigestion,\u201d the first version of Rubin also delivers into the same NVL72 rack architecture, with another bump in power consumption and a change in naming convention to Vera Rubin NVL144. Jensen said that they are reverting to calling a two-die module two GPUs for Rubin rather than one, as Blackwell was too confusing.\n\nNVIDIA revealing Vera Rubin NVL144 onstage at GTC.\n\nThe Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance, up from 1.1 exaflops for Blackwell Ultra GB300, and 1.2 exaflops of FP8 for training, up from 0.36, which is 3.3x the performance. It\u2019s targeted to ship in the second half of 2026.\n\nEscalating per-rack power requirements are just one of the problems, but the next-generation Rubin Ultra racks are up to 600kW according to Jensen, containing 576 GPUs in 144 modules that each have four Rubin GPUs sharing 1 terabyte of high-bandwidth memory. This is useful information for people designing for data center builds completing 2-3 years out who want to take these next-generation racks. However, it emphasizes that running an AI factory data center is a new set of challenges, even for the hyperscalers and cloud providers, let alone the more traditional enterprise customers.\n\nNVIDIA revealing Robin Ultra NVL576 onstage at GTC.\n\nLooking beyond Rubin Ultra in 2027, the next-generation architecture will be named Feynman in 2028, so we can expect to hear more about the performance of that GPU architecture next year.\n\nNVIDIA showing its next-gen architecture onstage at GTC.\n\nAt the lower end of the scale, NVIDIA is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. These are particularly useful for accelerated computer-aided engineering (CAE) workloads and Jensen mentioned that many of the standard CAE tools for chip simulation and design, mechanical simulation, etc., are now accelerated.\n\nNVIDIA revealing AI infrastructure for enterprise computing onstage at GTC.\n\nBenchmarking\n\nLast year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. Blackwell adds a new FP4 capability for inference that doubles the performance and there are additional optimizations for inference that give rise to a 30x improvement claim. I analyzed the 2024 benchmark claims in detail and decided that a more reasonable expectation for configurations of up to eight Blackwell GPUs that most people actually use would be 2.5x the H100 (not 4x) for training, and 8-10x the H100 (not 30x) for inference. These are good improvements in any case. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.\n\nThe way Jensen positioned performance was in the context of an AI Factory, which needs to operate in a sweet spot between large efficient batches of work vs. quick responses to interactive queries. That\u2019s where the extra memory and memory bandwidth makes the GPU operate closer to its maximum potential capacity. To further optimize this workload, NVIDIA will release a new open source distributed inference serving library called Dynamo (which isn\u2019t on GitHub as I write this). This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.\n\nNVIDIA announcing Dynamo onstage at GTC.\n\nThe latest performance claim is that with Dynamo and scale-up benefits from the larger NVL72, Blackwell is \u201c40x better\u201d than Hopper at AI Factory hyperscale inference serving. I think the benchmark was explained better this year, than last year, and it\u2019s a relevant application-level benchmark, but I also hope that people don\u2019t assume that a single Blackwell is 30x or 40x faster than a single H100.\n\nRepeated attempts to architect the largest-scale SuperPods at GTC.\n\nWater-Cooled Optical Switches\n\nA big announcement from two years ago was that NVlink could replace the network for up to 256 GH200s combined into a single system connected by 900GB/s interfaces to NVswitch chips. That would have contained 120TB of CPU-hosted memory and 24TB of GPU-hosted high-bandwidth memory, but this configuration seems to have never shipped in volume.\n\nNVlink is a copper-cabling system that operates inside a rack and can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. So in addition to the over-provisioning of CPU capacity with GH200, an additional issue was the high cost of NVlink optical transceivers for building such a big system. For AI training workloads, it\u2019s common to arrange GPUs in a \u201crail\u201d architecture where they are connected in parallel lines that don\u2019t need to cross-connect. This seems plausible to configure using copper NVlink from one rack to the next on each side, but a system configured this way is only useful for training workloads.\n\nThere was an update in 2024 to the shared memory cluster plans, abandoning the GH200 approach and focusing on a GB200-based configuration where the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576. Again, this seems like a theoretical architecture at this point.\n\nI haven\u2019t been able to find anyone talking about more than 36 or 72 GPUs, and Coreweave \u2019s documentation implies that they configure the NVL72 racks as individual 4-GPU boards networked together. The previously announced deal with AWS to deploy a cluster of GH200s for NVIDIA called Project Ceiba was upgraded (and delayed somewhat) to be a GB200 NVL72-based system with more than 20,000 GPUs instead, as of October 2024. It was not mentioned this year at GTC. I expect that to switch again to be based on GB300 if it\u2019s ever completed.\n\nA key new announcement for 2025 is a very efficient, highly integrated water-cooled optical switch, NVIDIA Photonics. This is going to take out a lot of the complexity, cost and power consumption of larger clusters. The optics terminate directly on the switch chip without needing expensive transceivers. This was probably the biggest surprise announcement in the GTC keynote.\n\nThe latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d and it can all be wired together with low-latency copper cables. This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. A prototype of the packaging using Blackwell GPUs and current switch chips was shown in the expo and I took a few pictures.\n\nCan You Afford To Own GPUs?\n\nSame as it ever was. The challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years. That makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation.\n\nOld GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU. Jensen said during his keynote that H100 GPUs aren\u2019t generally useful at this point for building AI factories, and sales of Blackwell are much higher than Hopper already.\n\nNew Capabilities for Building AI Apps and More\n\nJust like last year, there were many more announcements for markets like robotics, automotive, healthcare, quantum computing, digital twins and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new GPU hardware specifications and the optical interconnect.\n\nThe new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU, the new Dynamo software to operate inference efficiently, optimized models, a large range of third-party packages to get started with and a choice of development tools to help write the remaining code for you.\n\nJensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors at building the software that makes it easy to build applications and has open sourced most of it. Letting the days go by, once in a lifetime.\n\nNVIDIA onstage at GTC showing CUDA-X for every industry.",
        "summary": " NVIDIA is announcing new hardware that effectively obsoletes its previous products. Customers are buying whatever they can get, as it appears. \"The new stack\" of AI software is maturing rapidly. Last year I predicted that enterprise customers would find it extremely difficult to manage the pace of change. This year, I think the hyperscaler customers and cloud providers have indigestion as well. Development is also being accelerated by a variety of AI-backed tools. There is a big disconnect between the GPUs that most people are using now and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail. GH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as H100s. The largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019 and the J\u00fclich Supercom computing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Blackwell is made from two of the largest possible chips, connected directly together. Two modules fit on a 1 rack-unit-high, water-cooled motherboard. 18 compute boards fit in a 72 GPU NVL72 rack. Blackwell Ultra \u2013 B300 has 50% higher performance than B200. New process from chip supplier TSMC allows for even more power. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72. Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave. The Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance. It\u2019s targeted to ship in the second half of 2026. The next-generation Rubin Ultra racks are up to 600kW. The next-generation architecture will be named Feynman in 2028. The company is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. Last year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.  NVIDIA will release a new open source distributed inference serving library called Dynamo. This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.  NVlink is a copper-cabling system that operates inside a rack. It can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. The NVL72-based system with more than 20,000 GPUs will be deployed by October 2024. GPUs in the shared memory cluster more than doubles from 256 to 576. The latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. The new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU. Jensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors.",
        "metadata": {
          "word_count": 2780,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "engineering",
        "gcp",
        "hpc",
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_48",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable: Chips, AI, Search, Quantum, Bitcoin, Fusion \u2013 OXD26",
      "source": "OrionX Download",
      "published_date": "2025",
      "url": "https://orionx.net/2025/03/analyst-roundtable-chips-ai-search-quantum-bitcoin-fusion-oxd26/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_49",
      "kind": "podcast",
      "subkind": "",
      "title": "SC24, Supercomputing, CryptoSuper500, Quantum, RISC-V Summit \u2013 OXD25",
      "source": "OrionX Download",
      "published_date": "2024",
      "url": "https://orionx.net/2024/12/sc24-supercomputing-cryptosuper500-quantum-risc-v-summit-oxd25/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_50",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Scaling AWS Costs to Match the Business",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/scaling-aws-costs-to-match-the-business-f9k",
      "content": {},
      "tags": [
        "aws",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_story_51",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Why are services slow sometimes?",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/why-are-services-slow-sometimes-mn3",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_52",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "If at first you don't get an answer...",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/if-at-first-you-don-t-get-an-answer-3e85",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_53",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Sustainability transformation and DevSusOps",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/what-is-sustainability-transformation-32hi",
      "content": {},
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_54",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Measuring energy usage",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/adrianco/measuring-energy-usage-5ip",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_cadcf9fb",
      "kind": "file",
      "subkind": "presentation",
      "title": "Systems for Innovation Villanova",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Systems for Innovation Villanova.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5bcbe0dc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Speeding Innovation Singapore 2019",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Speeding Innovation Singapore 2019.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b665be9d",
      "kind": "file",
      "subkind": "presentation",
      "title": "YOW22-Greatest-Hits-Brisbane",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/YOW22-Greatest-Hits-Brisbane.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_76a699cd",
      "kind": "file",
      "subkind": "presentation",
      "title": "ChoosingUsingLosing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/ChoosingUsingLosing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_27b829e5",
      "kind": "file",
      "subkind": "presentation",
      "title": "Cloud Trends 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Cloud Trends 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_60526469",
      "kind": "file",
      "subkind": "presentation",
      "title": "OSSV Lock-In 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/OSSV Lock-In 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ce8da570",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices TempleUniv Cool AWS Stuff",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices TempleUniv Cool AWS Stuff.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_7b574db1",
      "kind": "file",
      "subkind": "presentation",
      "title": "QConLondon Netflix Retrospective",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/QConLondon Netflix Retrospective.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_7d8d13fc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Gophercon 2016 CSG",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Gophercon 2016 CSG.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f0b33bd4",
      "kind": "file",
      "subkind": "presentation",
      "title": "NetflixWorkshop",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/NetflixWorkshop.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d90ba45d",
      "kind": "file",
      "subkind": "presentation",
      "title": "CMG-cloud-computing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CMG-cloud-computing.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_db153f9c",
      "kind": "file",
      "subkind": "presentation",
      "title": "bil2010-millicomputing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bil2010-millicomputing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d02dc96d",
      "kind": "file",
      "subkind": "presentation",
      "title": "netflixoncloudfordevandops",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/netflixoncloudfordevandops.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_c6fb1e62",
      "kind": "file",
      "subkind": "presentation",
      "title": "bottleneckanalysis",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bottleneckanalysis.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_821640e2",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices Workshop 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices Workshop 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_f150f910",
      "kind": "file",
      "subkind": "presentation",
      "title": "Resiliency - Failure Modes and STPA",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Resiliency - Failure Modes and STPA.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_100f72ca",
      "kind": "file",
      "subkind": "presentation",
      "title": "migratingnetflixtocassandra",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/migratingnetflixtocassandra.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fe8671ea",
      "kind": "file",
      "subkind": "presentation",
      "title": "CloudNative",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CloudNative.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_951fa883",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "061 Open source  open data  open standards with Adrian Cockcroft and Zaheda Bhorat  AWS ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htUGrDBKAMg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_eeaa0d1d",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "086 Keynote  Cloud Trends  DevOps and Microservices",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=c0wSmr-u5vQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_80cdf542",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "040 Innovating at speed   AWS and Formula 1",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=D7usPAR9a1k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3733c7ad",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "001 SC24  Supercomputing  CryptoSuper500  Quantum  RISC V Summit   OXD25",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=d5mr6Ib5ygQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_787fb720",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "015 Ep 113 Platform Engineering Teams Done RIGHT with Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_16014850",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "060 AWS Summit Series 2017   Chicago  Keynote",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=tMUKC6eBjL8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c3f015ec",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "035 AWS Executive Forum 2019   Positioning Large Transformation Efforts to the Board",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=L2N9y9ovKuk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_e8c8e73a",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "009 Adrian Cockcroft   A Journey from Chaos Monkey to Sustainability",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jwRVtUEND8c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_d5f9baf9",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "023 Coffee with Mr  IoT  Adrian Cockcroft  Amazon s pathway to sustainability through technology",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hFVxkdHh9To&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_67c72125",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "088 Systems for Innovation   Adrian Cockcroft  at USI",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=-vlOG3UIp9c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_71a9366a",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "051 Migrating to Cloud   Lessons from Netflix  Brought Up to Date",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=XrWII4ewrXA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_54b715c7",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "062 AWS Summit Stockholm 2017  Opening Keynote with Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=GgkAhTtZugc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_cb35d110",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "063 AWS Summit Seoul 2017   Day2 \uae30\uc870\uc5f0\uc124 Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=061bsq0jVYU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_115dc1fa",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "029 Keynote   Modern App Development   Adrian Cockcroft  VP Cloud Architecture Strategy  AWS",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=wYCLbLrEoqs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_7eb1f382",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "045 Open Source in the enterprise with Zaheda Bhorat and Adrian Cockcroft  AWS ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=YMKsEmlw6V4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_92754b58",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "080 DOES15   Adrian Cockcroft   Systems for Innovation",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SaMIiLF1w20&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_42a586a4",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "008 Speeding Up Innovation   Adrian Cockcroft   YOW  2019",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=QlqQmHLXNiY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_804c4369",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "042 Developing a Chaos Architecture Mindset   Adrian Cockcroft   GOTO 2018",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vHl7EZ5o0uY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_42d534fd",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "017 Mik   One  Adrian Cockcroft  Episode 24 ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=fDgqiA2yZ7g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_d8950de9",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "043 Rapid Development   Why Serverless First Is Like Building with Lego",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5siD210Grr4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_9192c555",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "049 Adrian Cockcroft on Chaos Architecture",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ja6n5etN8hk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_98dafaa1",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "010 Don t Miss   How Cloud Architect Adrian Cockcroft Transformed Tech   A Candid Conversation with Jani",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vNIDssi4FK4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_ae43484b",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "082 Panel   Nicole Forsgren  Joshua Corman  Dan North   Adrian Cockcroft   GOTO 2015",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=1qyljMck9Ng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_190f19af",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "012 2023 Monitorama Live Stream Day 1",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CRJcc1TqBhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_1354b4cb",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "048 Adrian Cockcroft on Mapping Your Stack",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mzIdKGCOf1g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_64253f0e",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "028 Failing Over without Falling Over   Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R3_ccsuPoD8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c272be4e",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "005 From Code to Climate with Adrian Cockcroft  Clip ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5YF68KFGSzk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_a5299866",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "018 Mik   One  Adrian Cockcroft Episode 15",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=nFiauu-qMV4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_65620fe2",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "036 Speeding Up Innovation   Adrian Cockcroft   Craft 2019",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=rnygCAvVBj8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_f9cf2dda",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "084 Welcome Keynote   Adrian Cockcroft   GOTO 2015",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SBp7AWelOhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_373e2d35",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "078 microXchg 2016   Adrian Cockcroft   Analyzing Response Time Distributions for Microservices",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5DPr4x76nvQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_24288b27",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "033 Adrian Cockcroft   Speeding Up Innovation",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4jYYrkmAnS0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_1679c42f",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "032 Episode 216  Adrian Cockcroft on the Modern Cloud based Platform",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qQzPmzVfmYA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_1e6bfc50",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "054 Adrian Cockcroft   Migrating to Cloud  Netflix Cloud Journey ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=U88QkaDGx6k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c4c389c9",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "068 Paul Borrill on Time clocks and the reordering of events",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CWF3QnfihL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_57c52234",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "020 ACD22 1 01 Adrian Cockcroft AWS",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Qu1MLtHg2so&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c03c94b2",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "094 Fast Delivery   Adrian Cockcroft   GOTO 2014",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hvFo3Q2PIQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_07d6530b",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "073 Purpose Driven Development   Adrian Cockcroft   SpringOne Platform",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=sll9RNHz3t0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_500e5ef2",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "007  HPCpodcast 77  Adrian Cockcroft on Future Architectures",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=2jrCvon9oTE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_0cac4e48",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "096 Fast Delivery   Adrian Cockcroft   adrianco ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=icvtIK83I_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_8d6db02e",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "090 Reflections on Monitorama 2015 with Adrian Cockcroft and James Turnbull",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LuA1AjorCQs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_95ce6b03",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "011 Microservices Retrospective   What We Learned  and Didn t Learn  from Netflix",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=TOM6UhCetQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_4377a633",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "069 Simplifying the Future   Adrian Cockcroft  Battery Ventures",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DGK6jjamzfY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_2482bda5",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "044 Top Technical Talent Programs   Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CSL-vNQW3SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_777deef1",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "025 Day 1 Keynote by  Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=O2UzvvtHkL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_1420d740",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "019 Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=6aC_nes9LIk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_35576314",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "052 Developing a Chaos Architecture Mindset   Adrian Cockcroft  AWS ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4VleTKY0QAM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3d0218df",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "098 Deep Dive into the Cloud Native Open Source with NetflixOSS   Adrian Cockcroft   GOTO 2014",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R2kKmMyqTfc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_89895a4e",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "050 Adrian Cockcroft  AWS   KubeCon   CloudNativeCon EU 2018",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=iqVtGmNgSJk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_302761eb",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "014 Chaos Carnival 2023 Day 2 Keynote 1   Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=noKPM4UlJDk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_ea2d012f",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "041 Adrian Cockroft   Chaos Engineering   What is it  and where it s going    Chaos Conf 2018",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=cefJd2v037U&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_377868ec",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "030 Managing Failure Modes in Microservice Architectures",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=v5Gwi9AYvm4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_9d0e9948",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "038 Safety Margins and Availability with Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NRCQngvRNng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_4449e385",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "079 Adrian Cockroft   DevOps Enterprise Summit 2015   theCUBE    DOES15",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pfEKVDWUknU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_cd125e22",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "056 Keynote  Cloud Native at AWS   Adrian Cockcroft  Amazon Web Services",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5U-6sxR5DaQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_b23f3d66",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "046 Dynamic Non Events   Adrian Cockcroft   GOTO 2018",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mFQRn_m2mP4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_259d42a3",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "067 GopherCon 2016  Communicating Sequential Goroutines   Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=gO1qF19y6KQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_20107970",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "057 Cloud Trends   Adrian Cockcroft   GOTO 2017",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=EDZBYbEwhm8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6466ddfb",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "097 Fast Delivery  Adrian Cockcroft  nginxconf 2014",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5qJ_BibbMLw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3b583ca2",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "099 AWS re Invent 2014    ARC201  Cloud Native Cost Optimization",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LZwlkqERv2g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_f2d154d4",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "075 OSCON Roundtable  The Future of IT Infrastructure",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=9q08veg5WkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "infrastructure"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_befeedd7",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "053 Adrian Cockcroft on The New De Normal   Untangling  Kitchen Sink  Database Schemas",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Y6nKD-sK6tg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_30fc2a90",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "059 Live from the NY Summit   Interview with Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7CJrAhpZlXE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_a926f53d",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "002 It s Complicated    Adrian Cockcroft   YOW  2015",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NJ-3eNx8iBo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_68114147",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "066 The Evolution of Microservices",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=b8TDodu5E0k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6ef2b329",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "085 OpenStack Silicon Valley 2015   Web Services and Microservices  The Effect on Vendor Lock In",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ewuw1s4cnJA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3aafdc07",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "022 DevOpsDays Silicon Valley 2018   Dynamic Non Events by Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=C9VchTAd7AM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_2c496658",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "058 AWS Summit Tel Aviv 2017  Keynote with Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Cg79AaEAPwA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c057b69d",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "016 Fireside Chat  Adrian Cockcroft Talks with the Authors of The Value Flywheel Effect",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=713_6MBW7q0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3d7bc203",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "087 OpenStack Now Podcast  Episode 5  Adrian Cockcroft  Battery Ventures",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jOxFFmoRkzw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_29ad458b",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "071  The Evolution of Microservices   Adrian Cockroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Mg4Cs2K7f98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_7d59869a",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "031 AWS re Invent 2019  Innovation at speed  ARC203 ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8ona5ZTu4_E&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_0aa723a6",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "095 Migrating to Cloud Native with Microservices   Adrian Cockcroft   GOTO 2014",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DvLvHnHNT2w&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_56b5f9de",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "024 Adrian Cockcroft Presents  The Startup Lifecycle",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=uRcSFXafIWM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_d6c56b5f",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "091 Adrian Cockcroft on Microservices and the Importance of Next Gen Apps for Businesses",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4ClmJxVz1SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_5f4fa47d",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "064 microXchg 2017   Adrian Cockcroft  Shrinking Microservices to Functions",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ZgxZCXouBkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_70e09c9a",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "004  022   Kubernetes for Humans with Adrian Cockcroft  Nubank ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=xSy-usyvWC4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_195319bf",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "070 Monitoring Microservices   A Challenge   Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Q3-XKQbMkXg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_3dd8223b",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "006 Adrian Cockcroft  OrionX   Supercloud 5",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=phZabEbs2-4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_f34c92b2",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "089 Adrian Cockcroft   DockerCon 2015",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=K4rcnaiyc_M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6dc90cfb",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "027 Map Camp 2020   Maps  Games and Morality",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qtMHOuGw0lI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_712cfd77",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "092  Monitoring Microservices  A Challenge    Adrian Cockcroft Keynote",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=smEuX-Hq6RI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_2214dc0a",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "083 AWS re Invent 2015    SPOT304  How Adrian Cockcroft Helped Move Customers to AWS",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LMYYJuh9t70&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_2dfd0089",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "081 Adrian Cockcroft Interview at DevOps Summit 2015",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htIO8ydywa4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_7a2d0f63",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "013 Adrian s Greatest Hits  B Sides   Re issues   Adrian Cockcroft   YOW  2022",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=kc9dyTF2PjI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_0252811b",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "047 Adrian Cockcroft on Digital Transformation",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7lDWXtNjVyQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_65efc3e6",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "021 The Lotus Elise was unlike any other car ever made   Revelations with Jason Cammisa   Ep  21",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=h0jXhOmL7Xg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_5c96d39f",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "055 Adrian Cockcroft on the Evolution of Business Logic from Monoliths  to Microservices  to Functions",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=aBcG57Gw9k0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_c82e0a5c",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "026 AWS re Invent 2020  Adrian Cockcroft s architecture trends and topics for 2021",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4rLVJFHfK98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_b4111520",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "076 Microservices  What s Missing ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=p848Dr3EtQg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_f99866b7",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "037 Adrian Cockcroft on the importance of the culture at Amazon",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=eMXMJ2lV1_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6f4889c5",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "039 Adrian Cockcroft  AWS   AWS re Invent 2018",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=lhHmXNDvd3M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_91c8b7bd",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "065 It s Simple      Adrian Cockcroft   GOTO 2016",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=JSnOk-4m7C4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_9ef244b9",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "074 Mendix World 2016 Adrian Cockcroft Keynote  It s Simple",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Bn2WLIpPxX8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6d2fc92d",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "072 It s Simple    Adrian Cockcroft   GOTO 2016",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=a8Re9Cvv6nU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_48d9d7e0",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "093 The State of the Art in Microservices by Adrian Cockcroft",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pwpxq9-uw_0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_playlist_6b905594",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "034 Open Source Force Multipliers   Adrian Cockcroft  Amazon Web Services ",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=E1aAG2iftMo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_9294eecb",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "100 Keynote Speakers Adrian Cockcroft and Nicholas Heller Interview at TM Forum Live 2014",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=dWPdsd8m8KM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_playlist_8aeb3627",
      "kind": "youtube_playlist",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "youtube_playlist",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Ev3rIMLujTM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_bafc0c54",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn t learn from Netflix",
      "source": "infoqvideo",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/infoqvideo/What_we_learned_and_didn_t_learn_from_Netflix.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_1b3f4a2e",
      "kind": "youtube",
      "subkind": "",
      "title": "From Netflix to the Cloud DevOps Microservices and Sustainability",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/From_Netflix_to_the_Cloud_DevOps_Microservices_and_Sustainability.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_48a8bc62",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/Platform_Engineering_Done_Right.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_c3ac9176",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_76e7a5e0",
      "kind": "podcast",
      "subkind": "",
      "title": "Supercomputing 24 CryptoSuper500 Quantum RISC V OXD25.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Supercomputing_24_CryptoSuper500_Quantum_RISC_V_OXD25.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_3a5123e2",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/940fbd39-3936-441f-8087-4ce99b7be568/audio/050961ea-530c-49a0-ae27-046982856d34/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_363e396b",
      "kind": "podcast",
      "subkind": "",
      "title": "022 Kubernetes for Humans with Adrian Cockcroft Nubank Komodor info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/022_Kubernetes_for_Humans_with_Adrian_Cockcroft_Nubank_Komodor_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_869d1844",
      "kind": "podcast",
      "subkind": "",
      "title": "From Netflix to the Cloud Adrian Cockroft on DevOps Microservices and Sustainability.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628032&ei=YBDeZ-z8KIKRsfIPj6-B4Qo&ip=71.198.156.212&id=o-ALRdyd3qfKsy1G9HxamFg5Jlp29gbc4zvxFPOtQwDmJy&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606432%2C&mh=8F&mm=31%2C26&mn=sn-n4v7sns7%2Csn-a5msenek&ms=au%2Conr&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcMw4zqjQxITUESI3F4OdfRvFZOxrqDk-TUOZn4x7eCCs205IRITjiaZ0dzbr0GdxJw9s8JX8zL8&spc=_S3wKgfT7Xqfy-T0aMoIB3PSIXueOFKlyiIvrfsx4jJ7TPkKtHTx8VviE8JJhRrTMnY7&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAMckZWnpsB3fBlBTaiYaBnYTtPLf7dxFgvURPohpW8q6AiEA2NBOIgXi3JB21VU2yn8fZq8omdk17NCABrNukbiJMNo%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRgIhANvmiGQSkP7c4tahT-l3TG0XWq4jeEcTCua8y5FqJnCqAiEAzd4tM3x1zU7kiX_g6k48CXhwlGAIQfqZEgKqK60yRQY%3D",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_06bf9f18",
      "kind": "podcast",
      "subkind": "",
      "title": "youtube FY3asCV9qOE.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628024&ei=WBDeZ7nKBbOWsfIPhpCtgA4&ip=71.198.156.212&id=o-AJVQQivjY88dhhg9gljXdceHbQmr6Lyc6-02WD9zBIiZ&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606424%2C&mh=8F&mm=31%2C29&mn=sn-n4v7sns7%2Csn-o097znsd&ms=au%2Crdu&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcNS3vFqoapn2rXkMIxcI1MX3Z2bIKmie9TZ-dqQIiYCvHUuJdg73wmlfWCcdJ8yTntmdBmuruuL&spc=_S3wKi4z4DPgT8nu312evHyNCFxrXsReJNelRbqUtemQiLgCxg34NboTSt1hY9o94IFI&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhANKPHEhIXUjzklRAhlQU1ZhVlRkwrVgx3EH94x7mBDQFAiBJKmW4DbD5xYvMlvB-rKLq6fr3QTxp9wjfHFmTCFWWnA%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRAIgEDNtYM-MWxZhuooRi_jNP5Q51AN6oKRsEnxFblq3qJYCIBhhL1wzEun4pSgzvsfjcyq_PzgIST3xi5naspt8wASR",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4457c57a",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_AWS_Sun_eBay_Netflix_and_Others_Vet_Adrian_Cockcroft_Talks_Cloud_HPC_AI_and_the_Amazon_Sustainability_Data_Initiative_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f3e33df0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "http://orionx.net/wp-content/uploads/2022/09/036@HPCpodcas_Adrian-Cockcroft_Cloud_ESG_20220914.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9b33411",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ1MzB9fX1dfQ__&Signature=M7LCJcOysGLmo0khzuJvnby05s8k6RXaFozS0DQzzfhv41VUmroFPw5UmZJRY6CXvE4lwVXQ8kK4hnRzIqmYJijoP7f8Shxb46VOfwtKvJPjLO~Nv9zWOHkfJH0x2Z0SZvD-cMzI1zQJHYEbAMmAp3sSmy04vfQ9TMpzmHNvtHFWb~xa3hwXIlEWJQ538OSY2ahtPOnFZgrsxCQTaXC0hVihgRWlWMLSsQ0MF-4JvXc4kXl75j6GAPC7LAKWxc6mBTPrst8MDV6dgPHcUFURE5aIkQceyUjGSM5lun1T0tjyp8vcwiEe~yjt64gh6gXpIGAPJDHJmCX7aDMPgfS0Kg__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_af2c8e31",
      "kind": "podcast",
      "subkind": "",
      "title": "Podcast Adrian Cockcroft on Serverless Continuous Resilience Wardley Mapping Large Memory Systems and Sustainability info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Podcast_Adrian_Cockcroft_on_Serverless_Continuous_Resilience_Wardley_Mapping_Large_Memory_Systems_and_Sustainability_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast",
        "resilience",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_aec0d230",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_New_Monitoring_for_Services_That_Feed_from_LLMs.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_a2854b1c",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_10b527e1",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Nvidia_s_Superchips_for_AI_Radical_but_a_Work_in_Progress.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4d217b30",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/05/057@HPCpodcast_Adrian-Cockcroft_Carbon-ESG_20230503.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4ca2fe2b",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/c27ff629-b50f-4c9d-a4b8-154adb18e4a5/audio/21249ddc-8d38-4138-8e5d-612df9cfb2b5/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9e2e9e5",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://traffic.megaphone.fm/SED1616041688.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1d8d897c",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Architecture for the Cloud InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "http://ress.infoq.com/downloads/mp3downloads/interviews/infoq-12-nov-adrian-cockcroft.mp3?Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6IioiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDkxNzN9LCJJcEFkZHJlc3MiOnsiQVdTOlNvdXJjZUlwIjoiMC4wLjAuMC8wIn19fV19&Signature=Fj4ki7BoXgztHB73H9~FwnuDJgKS1LnMG2XTsJD6D8Gbk0-6sMON8rsDY1mdGExfsssrSZnvsbdfLTcmYEhd8w07G0RyJzxmFRa25asaM~MSJIOgdZjRZ3A7j0WfQ631a3YOC2blChyTN62yUQl1YFNxSFF8yQeHkMOqiugL8v4_&Key-Pair-Id=APKAIMZVI7QH4C5YKH6Q",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f131fbf4",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Schedulers_with_Adrian_Cockcroft_Software_Engineering_Daily.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7262cb55",
      "kind": "podcast",
      "subkind": "",
      "title": "Sam Newman Magpie Talkshow Episode 22 Adrian Cockcroft.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Sam_Newman_Magpie_Talkshow_Episode_22_Adrian_Cockcroft.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b9c74b59",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_764ae438",
      "kind": "podcast",
      "subkind": "",
      "title": "unknown info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/unknown_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_af469a08",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Decarbonization_ESG_w_Adrian_Cockcroft_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b2a0efa0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_Tech_Analyst_Adrian_Cockcroft_on_Trends_Driving_Future_HPC_Architectures_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_33a2686c",
      "kind": "podcast",
      "subkind": "",
      "title": "When Netflix Bet On AWS.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/62cedf3d-2540-4354-85b4-8a5e293d1104/episodes/bb12eeb0-7db8-413c-b7b1-aa3a17a1ae64/audio/91c97a87-12d7-4be9-b628-85e82601b948/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_132b3cb0",
      "kind": "podcast",
      "subkind": "",
      "title": "The Evolution of Microservices with Adrian Cockroft info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_Evolution_of_Microservices_with_Adrian_Cockroft_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_e90e6bf6",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable Chips AI Search Quantum Bitcoin Fusion OXD26.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Analyst_Roundtable_Chips_AI_Search_Quantum_Bitcoin_Fusion_OXD26.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_c264cb7b",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_ffd1f118",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDM4MTJ9fX1dfQ__&Signature=XDo1OiQ7kO0RHU3Y5WAHECtOp57K7B1AXwhxGedTt2jQibFn0Kv8dhm-Oa~YtSbBpiXaF~C2wu5qPqmOPcz~dxoxiprzRxlr-IZ1dsiT0KrCV6Z-b9uQFTsXlwXtGHmABI4oBCocspLkVf-Zi4znHEPFE7HmjnscSrw~rvmiuLY9eLHc4pR3hLQtFkg-ALmovjKh281H8Pa0RZSK3rQcNe-vKREBK6MZ7D9KyP3AhQ8RAvOEvRFhdoFDlOQ~9F1HuY3Ak06u7SNvHnk7qq1Ut~f61NAN0Fzz-4~S0844RCfzMDNQuM3z6zWQjYArarAEYMnRvOEKcFN2fjiPMOxgaw__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7bd25a6f",
      "kind": "podcast",
      "subkind": "",
      "title": "Magpie Talkshow Episode 22 Adrian Cockcroft Spaghetti Monster Simulation Edition .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/DwsToh00RUjc.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvRHdzVG9oMDBSVWpjLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ3OTR9fX1dfQ__&Signature=Gvi1r7yNumawbEeJuvVaDfsMUiBQDoJHIR1e7r~QADr7yNVD1D03NomYjlAfDXQaKNpRcomVgYN~bAJ03fJuNjTUn39oWi33Vh9-8S6J0pAXhZWXC6FyamS7ea4ZJ5DKgrhsWQn5v~FVeGQ5K-K87~lOqNDsmFUHNA2DeShEiNZz3uWjmgFNj-SVhCfFhZDQreJac117RfAZNmNB4eIifgHa806fPBl2FelYrdo17yhaHWE2Yak6FEjBtJQEjDEn3cyFikDrtbh2~~5idW6P6VyBrmWtNMJpwm5RJNrSNKIJpleK5XTZ3xhGQF6GM93tMe~ahD37nnDVTLxrvWpkWA__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_99cc1983",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Adrian_Cockcroft_on_Future_Architectures_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    }
  ]
}