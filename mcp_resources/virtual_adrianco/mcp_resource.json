{
  "metadata": {
    "author": "virtual_adrianco",
    "version": "1.0",
    "last_updated": "2025-06-01T09:34:19.957998+00:00",
    "content_count": 211,
    "content_types": {
      "youtube": 104,
      "podcast": 52,
      "infoqvideo": 2,
      "book": 10,
      "story": 18,
      "file": 18,
      "summaries": 7
    },
    "processing_stats": {
      "total_items": 211,
      "processed_items": 211,
      "failed_items": 0,
      "processing_time": 0.064295
    }
  },
  "content": [
    {
      "id": "virtual_adrianco_youtube_0",
      "kind": "youtube",
      "subkind": "",
      "title": "From Netflix to the Cloud: DevOps, Microservices and Sustainability",
      "source": "Platform Engineering Podcast",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=FY3asCV9qOE",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1",
      "kind": "podcast",
      "subkind": "",
      "title": "Chat about open source and platforms",
      "source": "Kubernetes for Humans",
      "published_date": "3/27/2024",
      "url": "https://komodor.com/resources/022-kubernetes-for-humans-with-adrian-cockcroft-nubank/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_2",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia's Superchips for AI: Talk about GH200 and flip.ai",
      "source": "The New Stack Analysts with Alex Williams and Sunil Mallya",
      "published_date": "3/14/2024",
      "url": "https://thenewstack.io/nvidias-superchips-for-ai-radical-but-a-work-in-progress/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_3",
      "kind": "podcast",
      "subkind": "",
      "title": "Monitoring for AI",
      "source": "The New Stack Analysts with Alex Williams",
      "published_date": "3/28/2024",
      "url": "https://thenewstack.io/the-new-monitoring-for-services-that-feed-from-llms/",
      "content": {},
      "tags": [
        "ai",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4",
      "kind": "podcast",
      "subkind": "",
      "title": "From Code to Climate",
      "source": "Unlearn Podcast with Barry O'Reilly",
      "published_date": "1/31/2024",
      "url": "https://barryoreilly.com/explore/podcast/sustainable-future-for-technology/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_5",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices and Teraservices",
      "source": "Infoq with Wesley Reisz",
      "published_date": "",
      "url": "https://www.infoq.com/podcasts/adrian-cockcroft/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_6",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_7",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24",
      "source": "MIK+1",
      "published_date": "",
      "url": "https://flowframework.org/ffc-podcast/adrian-cockcroft-ep-24/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "John Myer",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_9",
      "kind": "podcast",
      "subkind": "",
      "title": "Netflix on AWS",
      "source": "CloudFix",
      "published_date": "",
      "url": "https://cloudfix.com/podcast/when-netflix-bet-on-aws/",
      "content": {},
      "tags": [
        "aws",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_10",
      "kind": "podcast",
      "subkind": "",
      "title": "Serverless and continuous resilience",
      "source": "Charles Humble/Container Solutions",
      "published_date": "",
      "url": "https://blog.container-solutions.com/adrian-cockcroft-on-serverless-continuous-resilience",
      "content": {},
      "tags": [
        "resilience",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_11",
      "kind": "podcast",
      "subkind": "",
      "title": "Future Architectures",
      "source": "HPC Podcastmwith Shahin/OrionX",
      "published_date": "11/1/2023",
      "url": "https://orionx.net/2023/11/hpcpodcast-77-adrian-cockcroft-on-future-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_12",
      "kind": "podcast",
      "subkind": "",
      "title": "ESG and HPC",
      "source": "HPC Podcast with Shahin/OrionX",
      "published_date": "5/1/2023",
      "url": "https://orionx.net/2023/05/hpcpodcast-57-decarbonization-renewable-energy-esg-w-adrian-cockcroft/",
      "content": {},
      "tags": [
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_13",
      "kind": "podcast",
      "subkind": "",
      "title": "Evolution of Microservices",
      "source": "ACM Bytecast",
      "published_date": "6/17/2016",
      "url": "https://learning.acm.org/techtalks/microservices",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_14",
      "kind": "podcast",
      "subkind": "",
      "title": "Scheduling",
      "source": "Software Engineering Daily",
      "published_date": "7/6/2016",
      "url": "https://softwareengineeringdaily.com/2016/07/06/schedulers-with-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_15",
      "kind": "podcast",
      "subkind": "",
      "title": "Architecting for cloud",
      "source": "Infoq interview with Michael Floyd",
      "published_date": "1/24/2013",
      "url": "https://www.infoq.com/interviews/Adrian-Cockcroft-Netflix/",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_16",
      "kind": "podcast",
      "subkind": "",
      "title": "Interview, early life etc",
      "source": "Sam Newman's Magpie Talkshow",
      "published_date": "10/8/2016",
      "url": "https://samnewman.io/blog/2016/10/08/magpie-talkshow-episode-22-adrian-cockcroft/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_17",
      "kind": "podcast",
      "subkind": "",
      "title": "Microservices",
      "source": "SE Radio with Stefan Tilkov",
      "published_date": "12/10/2014",
      "url": "https://se-radio.net/2014/12/episode-216-adrian-cockcroft-on-the-modern-cloud-based-platform/",
      "content": {},
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_18",
      "kind": "podcast",
      "subkind": "",
      "title": "Cloud and HPC",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/09/hpcpodcast-aws-sun-ebay-netflix-and-others-vet-adrian-cockcroft-talks-cloud-hpc-ai-and-the-amazon-sustainability-data-initiative/",
      "content": {},
      "tags": [
        "cloud",
        "hpc"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_19",
      "kind": "podcast",
      "subkind": "",
      "title": "SC23 summary",
      "source": "Inside HPC with Shahin Khan and Doug Black",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/hpcpodcast-tech-analyst-adrian-cockcroft-on-trends-driving-future-hpc-architectures/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_20",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn't learn from Netflix",
      "source": "Infoq - QCon London talk/video integrated together",
      "published_date": "3/28/2023",
      "url": "https://www.infoq.com/presentations/microservices-netflix-industry/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_book_21",
      "kind": "book",
      "subkind": "",
      "title": "Q&A: Serverless Development on AWS - sustainability expert section",
      "source": "O'Reilly - Sheen Brisals and Luke Hedger",
      "published_date": "3/14/2024",
      "url": "",
      "content": {},
      "tags": [
        "aws",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_book_22",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Building Green Software",
      "source": "O'Reilly - Anne Curry",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_23",
      "kind": "book",
      "subkind": "",
      "title": "Foreword1: The Value Flywheel Effect",
      "source": "IT Revolution - David Anderson",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_24",
      "kind": "book",
      "subkind": "",
      "title": "Foreword2: Reaching Cloud Velocity",
      "source": "AWS: Jonathan Allen, Thomas Blood",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_book_25",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: In Search of Certainty 2nd Edition",
      "source": "Mark Burgess",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_26",
      "kind": "book",
      "subkind": "",
      "title": "Foreword: Irresistable APIs",
      "source": "Kirsten Hunter",
      "published_date": "",
      "url": "",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_27",
      "kind": "book",
      "subkind": "1,21-187,215-233",
      "title": "Capacity Planning for Internet Services",
      "source": "Sun Press Blueprints - adrianco, Walker",
      "published_date": "2000",
      "url": "./mcp_resources/virtual_adrianco/pdfs/caphi_extracted_1_21-187_215-233.pdf",
      "content": {
        "metadata": {
          "word_count": 48047,
          "processing_status": "success",
          "processing_errors": [],
          "character_count": 320294,
          "excerpt": "Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131CapacityPlanningfor\n\nAlmost every business, from a corner shop to a multinational corporation, is faced\n\nwith competitive pressure to \u201cgo online\u201d and provide services via an Internet site. Inaddition, a large number of new online businesses are being implemented in a maddash to capture the attention and wallets of a huge and fast-growing number ofInternet users. Success is measured by growth in the number of pages viewed,registered users, ..."
        },
        "summary": "Quick planning techniques for high growth rates. Success is measured by growth in the number of pages viewed,registered users, and in some cases, by the amount of business transacted. Growth rates for successful si. nies spend most of their investors\u2019 funds on advertising as they attempt to establish their name in the collective consciousness of consumers. Established companies are concerned about maintaining theirpreexisting brand image. Capacity planning is an optimization process. Service level requirements can be predicted and balanced against their costs. This Sun BluePrints book charts a course through the available techniques and tools. The topics covered in this book can be divided into the following sections:Principles and processes. Scenario planning techniques. The effective use of tools. This chapter presents methods for managing performance and establishing servicelevel agreements. It also examines the IT frameworks designed to providesolutions for business requirements. Additional ITframeworks from ISO FCAPS are presented, and tips are offered for implementation. performance management is the measurement, analysis, and optimization ofcomputer resources to provide an agreed-upon level of service to the end-user. Bydefining performance management and identifying the key components required tosafely and accurately implement performance management in the datacenter, youcan minimize the risks associated with high growth rates. Resourceutilization and resource utilization planning are the cornerstones of capacityplanning. Utilization is a measure of system resource impact, throughput defines thequantity of services, and latency defines the quality of the services being provided. Several layers of resources and resource consumption can be defined, tuned, and measured. The Business layer often provides the most significant opportunities for \u201ctuning\u2019 The Applicationlayer and Hardware layer can also provide a significant and obvious impact on the overall performance of the architecture. The Operating System and Infrastructure layer are often where administrators look for some magic cure. Performance management can be applied in an iterative, cyclic, four-phase approach. The output of each phase is used as input to the following phase. Each phase mustreach a steady state in which the results can be published and transferred to thefollowing phase. Once the four phases are locked in place and all results have beenpublished, those results are fed into the next generation of phases. Historical revisions should be retained for future examination and change analysis. Baselining creates a snapshot of a system as it currently exists and generates reports that describe the system performance. An accurate representation including hardware, software, and operating systemversions is critical to creating an accurate inventory. This configuration inventory isconsidered \u201clocked down\u201d for the life span of the baselining process. Load planning accounts for changes or new demands on the system. Any available data describing the changes and new demands is collected and analyzed. The old snapshot isretained as a historical record of configuration. Load Planning is a cyclical process that comprises three activities: load assessment, load data gathering, and load profiling. Each of these activities may need to berepeated multiple times before valid assumptions can be made about the datacollected. System capacity and resource consumption are modelled, calibrated, and validated against the current baseline data. The model compares the relative resourceconsumptions (measured during baselining activities) measured during the load-gathering phase. Capacity planning produces a new system and service configuration architecture. The relationship betweenresource consumption and system architecture capabilities creates the workloadcharacterization used to model and predict the effect of changes. The operating environment can be adjusted as necessary to meet the established performance requirements defined in the SLAand KPI. Resource management defines the controls, shares, quotas, and priorities necessary to manage the workload. System resource planning also provides additional input for creating the new baseline of systemperformance. Service Level Agreements (SLA) outline the level of availability of mission-critical services between the IT department and end-users. f load planning and capacity planning to improve model accuracy. The SLA is really just a list of expectations for a given service. It defines the transactional capabilities, networklatency projections, availability requirements, maintenance windows, escalationprocedures. ld take in theanalysis of the end-users\u2019 computing resource requirements, capabilities of the ITarchitecture, and formulation of the agreement. SLAs should be used as a basis of understanding between all parties involved. They should define key roles,responsibilities, and expectations that have been discussed and agreed upon by all parties. The SLA should be reviewed and modified regularly. Service Level Agreements (SLAs) can be difficult to work out. They can result in many unpleasantries, not the least of which are the perception of poor service. SLAs can improve the business process, affect the bottom line, andimprove availability. Long-term benefits of SLAs include increased end-user satisfaction, more accurate resource accounting, and more efficientprioritization. SLAs are not more widely used for a number of reasons. The following issues are the most common downfalls. A well-conveyed SLA can help develop higher serviceavailability and greater end-user satisfaction. With the SLA, anIT manager can assign personnel to requirements more effectively. preferable control over IT budgets and increased accuracy inlong-term budgeting can also be direct benefits of SLAs. Production Environment Engineering is the product of careful planning and design. This section addresses the needs of the datacenter production environment by using the ISO FCAPS model, the ITIL (information technology infrastructure library)framework, and the SunReady roadmap to production. This discussion provides an encompassing view of the infrastructure requirements and timelines necessary to successfully launch, manage, and maintain a new datacenter service or refresh an existing one. The dot-com startup Internet business has a related challenge. While their businessmodel relies on exponential growth for survival, they often lack the datacenter policies, procedures, andexperience. By learning from the cumulative experience of their more traditional predecessors, the Dot-com companies can implement best practices for the production environment. Some of the categories defined in the framework are company-wide, some are system-specific, and others are application-specific. Forexample, the security group could manage security policy and administration for theentire company, while a facilities management resource could be designated toseveral disparate datacenters. The FCAPS High-Level IT Framework. re defined five categories of.managed objects, covering the basic categories of enterprise support operations. These categories can be applied to network management systems, which is theintended purpose, or they can be expanded to include the skills, tools, and.capabilities in datacenter operations and management. An ITEF can be further divided to establish functional responsibilities and toprovide more detailed information about the services the ITEF offers. Several levels of criticality can be assigned to thecategories of IT services. FCAPS IT Extended Framework defines an ITEF based on FCAPS. This ITEF is bestrepresented in a tree structure that is viewable as a series of Web pages. The IT service coverage document should include a general statement describing the category of coverage. It should also include statements describing the individual servicesand areas of authority in the IT environment. The servi The change management system is the vehicle for recording ,authorizing ,monitoring , andreporting all changes to the open systems environment. By establishingcategories of coverage, the services provided by the IT department are well-defined, whichallows the establishment of enforcement and auditing policies. The IT service model (ITSM) introduces levels of criticality, levels of service, and \u201cbig rules\u201d to the ITEF. This multilevel approach extends the natural drill-downstructure for IT services. The ITSM can be implemented across platforms and used to create service leveldescriptions. The following is a sample of a service level description for user data backup. It should be modified to adhere to local datacenter policies. The Central Computing and Telecommunication Agency (CCTA) of the UK began publishing volumes of their established best practices for IT operations in 1978. The ITIL has evolved over the years, being studied, adopted, and adapted by many IT organizations. The ITIL establishes policies, not implementations. The ITEF can then beimplemented according to service levels, operating system, hardware, andapplication capabilities. The FCAPS framework was originally generated from the ITIL. The Sun Professional Services team has developed a project methodology for applying the FCAPSframework. This methodology creates a unified extended framework for IT operations. The implementation of supplemental service capabilities shadows the applicationdevelopment life cycle from the architecture phase to the pilot, development,integration, deployment, and sustaining phases. The IT service capabilities can thenbe tested and assessed simultaneously with the application\u2019s functional, stress,crash, and disaster recovery test cycles. Service level management is the key to maintaining the production environment. By establishing the rules of engagement, the capabilities necessary to support the service level objectives, the entire production infrastructure can be monitored and measured. The service desk controls information, brokers requests, audits request completion, andprovides service level reporting services. The service desk also brokers action items for the support organizations, bothinternal and external. With the dispatch of support activities controlled, theaffected end-user service levels can be monitored. Measurement Principles: Anything that is not measured is out of control. Measure the business as well as the computer systems and people. Measure and record trends in performance, cost, and availability. A system should be built to help the IT staff log arbitrary data into performancedatabases. These databases should have summariesand graph plotting that anyone can use. Parameterize each component separately and combine them into anavailability hierarchy. Production environment engineering and performance management may appear tobe obscure forms of art. By studying the collective experiences of the traditionaldatacenter community and applying proven best practices, the art can be converted into a well-understood science. The next chapter provides a methodology for service level management and looks at some problems of capacity estimation across multiple platforms. The next chapter looks at what actions are necessary to keep the modern open-system datacenter well-organized and well-managed as the best of the traditional mainframepredecessors. Measuring and reporting key performance criteria in an applicationdevelopment environment can help set release time expectations for performanceand capacity. These practices can also help you recognize features and identifychanges of performance implications of the application being developed. Use a strict change control process, and schedule batches of changes to manage risk. Arrange for the appropriate staff to be on site during change periods, to concentrate fire-fighting efforts. Service level management is a synthesis of traditional datacentermonitoring and the end-to-end availability of the service. The goal is to eventually be able to measure service performance from an end-user's perspective. Service Level Agreements (SLAs) are negotiated between consumers and providers of ITservices. There could be multiple SLAs between a single business unit and multipleservice providers. Some providers can also have SLAs in place with other providers. The SLA should not be mired in legalese; rather, it should be easily understood by anyone familiar with the service and the specified area of business. The SLAshould be implemented as part of the system runbook and maintained in an areaaccessible to both the business unit using the service. Service definition \u2013 Define the service being covered, location of the service (both servers and end-users), number of users, transaction volume expected,and service capacity. Determine coverage and schedules for the service. The SLA should be unique, based on the service being provided and the n. thod by which the KBIs are measured, and track compliance with the service definition. The process of defining these agreements requires tact and IT skills. An SLA is beneficial to both parties, providing resource consumption and service sizing information. Service Level Management is the process of defining a service. The goal is to include the lowest possible latency and maximum supported transaction rate. The service definition should be:Meaningful, Understandable, Cost-effective, Attainable. The SLA should be cost-effective for both the end-users and the IT department, while still meeting the demands of the business. The goals should be attainable, based on testing and historic performance. Service specifications should be based on the needs of the users and the performance data acquired from previous years (ifavailable) Business hours are defined as 0600 EST to 2000 EST, Monday through Sunday. Maintenance windows are defined from 2030 EST until 0530 EST. No transactions are permitted during these maintenance windows. In the event of a \u201cdisaster\u201d requiring the recovery of data or the restoration ofsignificant portions of the system, the help desk will notify the users of the event. Regular updates will be provided every 30 minutes until theservice is restored. The system shall maintain functional availability of 99.8% of business hours. Outages are periods during defined business hours that fall outside emergency maintenance windows. Planned or scheduled emergency maintenance is not included in outage calculations. The order processing system will sustain a transaction of 30,000 transactions per hour during business hours. 95% of all order transactions will be completed in less than 5 seconds (on average) with up to 300 users online. The system architecture, both hardware and software, corresponds directly to the impact on the service. Benchmarking of the application should be performed to identify the limits of the server\u2019s capabilities. Most IT departments choose not to include KPIs in the actual SLA. KPIs are intended for the IT department personneldirectly involved in the management of the server system. Commercial monitoring tools like SunMC, BMC Patrol, BMC Best/1, andTeamQuest can be configured to measure, monitor, and report on performance status. Most can then be integrated intoenterprise management tools like Solstice Enterprise Manager\u2122 and HP\u2019sOpenView. Table 3-2 lists example tool definitions that correspond to the sample thresholds in Table 3-1. Some metrics are vendor- or release-dependent, so multiple definitions need to be maintained. A KPI document can also include actions for immediate relief or resolution ofperformance events. These actions might include stopping, rescheduling, orprioritizing running batch jobs. The procedures should be tested and updated as necessary. Severity of problems can range anywhere from a single user having slow response to a global outage. To allocate resources appropriately, severity levels should be bedefined. A simple call sheet may suffice for the contact information. The contact information for different components of theproduction environment should be documented in the system runbook. This includes how to contact the help desk itself, owners of different categories of problems, and Vendors responsible for different parts of the environment. Problems should be reported to the IT help desk by phone to (606) 555-5300, by email to help@helpdesk, or by fax to (619) 515-5301. Delays in the resolution of a problem can also result in an escalation in severity. The escalation path and events causing theescalation should be well-documented in the system runbook. The help desk will assign a severity level based on the problem description and will contact the appropriate IT personnel. The IT manager for the service will be notified of the problem every 60minutes, until the problem is resolved. Regular reports should be made available to the end-users, business unitmanagement, and IT management. Reports should indicate how well the IT department is doing with meeting the service description. Meetings should be held monthly or weekly to determine any changes that need to be made. Identify problems that arose during the previous week and possiblerecommendations for ways to improve those problems. A sample weekly report is described in \u201cManagement Viewpoint Implementation\u2019 on page 135. It is much better to resolve questionsearly on in the SLA, rather than later through arbitration. For the end-user department to hire the talent needed, the department would need to spend approximately $480,000 a year. The datacenter environment, rather than the standalone environment, provides higher quality, sharable, standardized resources. By identifying the projects on which engineers are spending the most time, the IT manager can identify recurring problems, find ways to rectify those problems. This increases the ability of the IT staff to addressproblem resolution. It also increases the leverage that the support organizations canwield over vendors. In some IT departments, the IT department staff\u2019s bonuses are directly tied to meetingor exceeding stated service level goals. Failure to meet stated goals results in discounted or free services to the end-users. Arbitration need not be an outside organization, though it very well can be. It can be a person or a panel that can review the problems and reach a final, binding solution. The managers and representatives of the service for both the IT department and theend-users should meet regularly to review the existing SLA and to propose changesto the agreement. Either party may propose changes, but both parties must approve the changes. A complete TCO analysis is a long and involved process, but ca. n the systemrunbook. The system runbook should also contain the point of contact in one of theinvolved organizations. The computing needs for the business function must also be quantified in some way. This quantification can resolve to individual business functions such aspayroll and marketing data or can be further subdivided into more specific metrics. Baselining the performance of business services is the second step in understandingexactly what is happening in the datacenter. Performance baselines should measureeach business function in terms of transaction type, transactional volume, and wherepossible, average response times for the driving server systems. An analysis of an online sales and distribution system might state that:Shift average transactional volumes for the sample period of 10/1/99through 12/31/99 were measured as 12,000 catalog system lookups perhour. For shift hours measured at 11,500 to 12,500 transactions per hour,catalog system lookup response time averaged 3.2 seconds, with a worst-case query response time of 5.1 seconds. By creating a scalar representation of the performance characteristics of a particular business service, we accomplish two goals. We help applicationdevelopers and database administrators identify bottlenecks and inefficiencies inserver and application software. If there is an industry-standard benchmark that closely imitates your workloadcharacteristics, then use that benchmark as a guideline for platform comparisons. Very few business implementations come at all close to the popularindustry-standard benchmarks. Two examples that we can use as samples of market offerings areBMC (formerly BGS) Best/1 and TeamQuest. Best/1 Performance Console for UNIX has an internal database of hardware andoperating capabilities. Performance of a given system is quantified and can be used as input into the BEST/1 modeller for system sizing. Without strict resource management implementations, the rule ofthumb for target utilization is 70%. With resource management software and policies, target utilizations of 90% or more can be safely projected. The Sun Enterprise 10000 platform supports dynamic system domains (DSDs), allowing a single-server platform to act as several distinct servers. System resources can be migrated manually between system domains, allowing administrators to reallocate idle resources. Solaris OE processor sets allow the administrator to bind applications to a set ofdefined processors. processor sets create hard walls of CPU resource allotment foreach designated workload. Idle CPU resources within a processor set cannot be used by other workloads outside that processor set. Resources are defined as either static orrenewable , depending on their particularcharacteristics. A CPU is one example of a renewable resource because CPUresources are dynamic for any given point in time. There is no finite limit of CPUpower; as a machine runs, new processing power is always available. System platforms, operating systems, applicationsoftware, and the business functions that use them are in a continuous evolutionaryprogression. To properly consolidate server functions and satisfy business requirements, a complete understanding of the enterprise is necessary. Even the hardware and software procurement process is circular. Business groupscontract service levels with capacity planners. Capacity planners define a workloadmetric for the service levels contracted. Systems engineers specify a hardwareplatform that can satisfy the workload metric. The next chapter provides a recipe for successful scenario planning. The next chapter presents amethodology for modelling capacity and load. The last chapter suggests processes for quantifying capacity and consolidating workloads. Trending techniques use a mixture of step functions that track changes in yourconfiguration. This process extracts cyclic variations from your data so that you can see the underlying trends. With this technique, you use analytical models topredict the behavior of complex workloads. A Recipe for Successful Scenario Planning. Successful planning must be based on firm foundations. This recipe provides a step-by-step guide to the process. The model should concentrate o. he same job, show them as asimple replicated set. Be ruthless about paringdown the complexity to a minimum to start with. The bottlenecks will change over time, so they must be listed explicitly each time ascenario is modelled. It may be sufficient to look at just the CPU load on a central database server, or you may also need towatch network capacity or a wider range of systems. The primary bottlenecks appear to be the CPU and disk on the back-end database and the CPU on the searchengines. Disk utilizations were found to be very low on the other systems. Measure service levels and workload intensities. Performance modelling assumes that the workload is essentially constant. One of the main modes occurs when backups are taking place. Fast-growing environments will find that the amount of data to back up rapidly. e back-end server\u2019snetwork interface would be a bottleneck during the update. The utilization levels aredifferent in each mode, so they need to be recorded separately. The initial model should be a crude and oversimplified base for planning futurescenarios. The hardest thing to learn for successful modelling is to let go of thedetails. You can build a very useful model with incomplete and inaccurate data. As long as that mixture remains fairly constant, you can average all transactions into ageneric transaction rate. If the back-end server load level fluctuates a lot because of competing applications, you need to perform a workload breakdown. If this is the primary load on the system, then you can probably getaway with using the overall CPU utilization. You need to collect historical data that spans a time period similar to the period you are trying to predict. Lack of data is no excuse; however, you should start to plan immediately. The rest of the scenario modelling and planning process is illustrated with a simpleparameterized spreadsheet in the next section. This section explains how a spreadsheet can be used to quickly generate estimates of capacity and load. In the next section, a simple spreadsheet-based scenario planning model is explained. create a spreadsheet that estimates the \u201cgrowth factor\u201d from a baseline. If additionalinfluences have an impact on the load, they can be computed in a manner similar to the influences described below and can be combined with the other influences. The growth factor is simply thefactor that the original (baseline) load must be multiplied by to get the estimatedload supported at the time in question. If the baseline load was measured in Januaryand the estimated growth factor for June is 2.0, this means that the load can beexpected to increase two-fold from January to June. The units of measurement cancel each other out. The spreadsheet method outlined here is designed for systems with onlyone significant bottleneck. Systems with multiple bottlenecks need to use morecomplex methods.  spreadsheet model is flexible enough to allow fortailoring the models. More complex formulas can be used, or different formulas canbe used for different time spans. Modifying the spreadsheet so that it better estimates the load and capacity of the systems in question is discussed in\u201cTweaking the Model\u201d on page 74. The number of data points (spreadsheet cells) should also be the same for eachinfluence. It is important that the seasonal variations begin with thebaseline in the correct season. The important data is the prime time daily peak level. You can see that the system never sleeps. There is a predictable pattern that peaks in the evening during weekdays and falls off a little during the weekend. The chart inFIGURE 4-6 shows the peak level day by day. In this case, Monday isshown as the busiest day, with Friday a little lower, and a quieter weekend. Sundaysare often busier than Saturdays, partly because systems located in California arenear the end of the international dateline. Data is taken from 1996 Web site hit rate datacollected in 1996 on www.www .sun.com by Adrian Cockcroft. Analysis of variance, or ANOVA, is used to fit a model to the data. If you have at least a years\u2019 worth of collected data, you can use it to generate your \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0estimates for the monthly variations. Keep in mind that if you have limited historicaldata, it is likely that the perceived seasonal effects will be a combination of theactual seasonal effect and other (potentially large) influences. \u201cTweaking the Model\u201d on page 74 offers guidelines on how tomake the model fit the data. The growth of the Internet combined with a growth in awareness of your site maycause a geometric exponential growth in load levels. This can be easily expressed intwo numbers as a growth factor over a time period. For example, growth might beexpressed as 50% per quarter, or doubling in a year. A spreadsheet computes the growth factor and monthly factors for the coming months with a growth rate and duration provided by a user. ibed here is mathematically referred to as geometric exponential growth. It is usually (somewhat incorrectly) referred to asexponential growth. The load at a given time ( Lt) is computed based only on the time ( t-t0). The value t0is zero if tismeasured in relation to it. In the following table, the values used are G= .22 and L0= 1, which makes the following formula: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Lt= 1.14t. This formula is provided in a capacity planning spreadsheet. There is an underlying growth factor involved in the size of the total Internet marketplace. At one point this was doubling about every 100 days, but recentgrowth rates suggest a doubling period of a year or more. Several large Internet sites are seeing business grow at arate of about 50% every three months. Growth rate over several different time ranges (for instance, January to March,March to May, and May to July) and averaging the results may yield a much betterestimate. tune the growth rate as you obtain more data. The spreadsheet available athttp://www .sun.com/blueprints/tools uses a formula that estimates the marketing reach of a campaign. This formula is appropriate insome circumstances; however, other formulas may better represent a marketingimpact depending on your environment. Most formulas can be easily implementedin a spreadsheet to work with the rest of this procedure. A marketing campaign starts in the fourth month to counteract theexpected seasonal drop in load level. It continues for four months, then thememories fade away and a residual increased level of activity is seen. The short-termand residual gain should be something that the marketing department uses to justify the campaign. In practice, many smaller marketing boosts may be modelled rather than one large boost. modelled in the same way as itsimpact on the business. The campaign is not repeated in the second year; the chart inFIGURE 4-9 shows why it was not needed. The factors that affect capacity can be separately computed in aspreadsheet and then combined to get a combined capacity estimation. Manycapacity measurements can be modelled. The two influences on capacity shownbelow are efficiency variations and capacity increases. Capacity increases include anything that allows increased capacity because ofupgrades to the bottleneck. Large changes in the application or transaction mix need to beaccounted for in the scenario plan. Table 4-6 shows the effects of heavy application tuning on the CPU usage pertransaction in the above example. Although these results may be unrepresentative ofwhat can be achieved in many situations, it is sometimes possible to achieve a four-or five-fold performance improvement. The factors in TABLE 4-6 are plotted in FIGURE 4-11. To counteract the increased load levels, the application is tuned and the hardware is Upgrade. ompared to the untuned one. Database tuning or application software upgrades give the first two gains. Then there is a reversal as a new, more complex user interface is introduced. After a while, several more tuning improvements are made, until after two years, the computer resources used per transaction is reduced to 15% of that at the start. Appendix A, \u201dSun Constant Performance Metrics,\u201d shows some performancenumbers for various machines that may be appropriate for projecting the capacity ofnew hardware. A notional level of 70% capacity utilization was also factored in. The third column shows the CPU utilization in each month. Thisassumes a start point, which for this case is entered into the spreadsheet as 70% busyat month zero. The utilization is calculated by taking into account all the differentgrowth factors explained in the next section. The combination of efficiency and hardware capacity upgrades can be plotted on the same axis to see if the system is capable of sustaining the load. The plot in FIGURE 4-13 shows that the combination of a marketing campaign and theseasonal boost overloads is more effective. If this model is correct, something else needs to be done to sustain this futurebusiness scenario. Either a faster system could be installed earlier, or more efficiency could be squeezed out of the system. Another option is that the work itselfcould be split in some way, possibly by functional or geographical aspects. When manipulating the data, bear in mind that the analysis is based on peak loads, not average loads. If you take measurements atintervals of a few minutes, they can be averaged. As soon as you zoom out toconsidering intervals of an hour or more, you want to know the peak level duringthat hour and the peak hour during the day.  monthly data is based on the peak for the wholemonth, and it could occur at any time during the month. For example, if the weekly peak is a Monday, then the monthly peak could occur on any Monday in themonth. Set the workload growth parameters shown in TABLE 4-3 to get a first approximation of the overall long-term growth rate. Tweak the seasonality. Set up marketing factors. Spreadsheet-based models are a good starting point, but are extremely limited andcan be clumsy to use and extend. Large disk subsystems are expensive and may be the primary bottleneck of yourapplication. alues should be moved to be seasonal. This chapter examines the problems in more detail. By identifying the primary bottleneck, theexamination shows where the system can be expected to run out of headroom first. Solaris 7 OE includes the kstat command, which obtains and processes throughput counters. The priority_paging option is available in the appropriate kernel update patch. The latest SE toolkit is available at http://www .setoolkit.com. Solaris 8 OE changes the memorymanagement subsystem in several ways to make paging much simpler tounderstand. There is no need for priority paging because a moresophisticated scheme is used by default. You should see no scanning, a lot morepage reclaims, and a much higher free memory value reported byvmstat. A system can be modelled as a whole or broken down into individual services to build separate models for different functions. Separate SLAs might exist for system performance during system maintenance tasks, systemperformance during business hours. Resource management schemes, such as processor sets and SRM software, can be used to control the backup and reporting systems to reduce impact on OLTP. At this point in the capacity and performance management cycle, intimateknowledge of the business system being modelled is required to prioritize the workloads. The sample period should reflect the maximum period oftime for which degraded system performance is to be tolerated, with a critical threshold of maximum degradation to be allowed. In an online processing systemthat has direct interaction with the paying customer of the business, the sampleperiod could be as low as one minute. isolated from periods of customer interaction. H processing windows and transactional processing should be avoided where possible. Batch reports, database record purging, system backups,and other batch system maintenance should be scheduled into time windows. One immediate change to workload scheduling can be derived from this graph. There appear to be batch reports running during the system maintenance window. By moving these batch reports outside the defined maintenance window and creating a defined shift for reporting tasks, we can enforce resource management.  utilization of system resources when workloads could otherwise coexist without contention or minimized through the proper use of resource management tools. To illustrate this case, midday reports are allowed between 12:00 p.m. and 1:00 a.m., but all other report generation is assigned to the 6:00p.m.-0:00am window. Isolating the different system processing tasks greatly simplifies the implementationof monitoring tools and system resource data collection. Brian\u2019s method is commonly referred to as the \u201cM-value process,\u201d after thetraditional mainframe performance metric. By creating a series of generic terms todefine work on a computer, it is possible to quantify several statistics. These statistics describe the capacity of the machineplatform, as well as the performance characteristics of a given workload. SCPMs are a real-world estimation of Amdahl\u2019s Law applied to system architectures and operating system versions for cases of regular,predictable, and normal workloads. Occasionally, significant patches may influencethe scalability of an operating system. Amdahl\u2019s Law provides the calculations for predicting the benefit of running aworkload on a scalable multiprocessor environment. Measuring the potential parallelization of your workload provides a more accurate model of the performancecharacteristics. Application of Amdahl\u2019s Law measures CPUarchitectures and presumes that there are no bottlenecks in the disk or networksubsystems. Speed-up is calculated using the time taken to accomplish a defined set ofrepresentative transactions. The speed-up factor that is calculated with a single CPU and two CPUs can be used to calculate the scalability factor ( S) of a given workload. S is the percentage of the workload that is parallelized, and not the finite resource constrained.  scalability of a given machine for a given workload can be predicted with theScalability factor. Scalability factor may be reduced if there is resource contention. Adding CPUswill increase the contention, thereby reducing the percentage gains. Lower scalability factors can influence system architecture decisions toward wider server architectures. With deeper architectures, a direct relationship is establishedbetween the scalability of an application architecture and the potential total cost ofownership (TCO) With lower scalability factors, the progressively lower increase inperformance benefit of adding additional CPUs can outweigh thepotential savings in TCO of managing fewer systems. Larger system architecturesgenerally provide more reliability and serviceability features. The scalability of any particular software on a particular server might displaynongeometric scalability characteristics. For most normal workloads that we havemeasured, the variances from the geometric scalability curves are minimal. Factorsthat might cause a significant variance include:CPU cache/code efficiency. Limits to application software threading. The SCPM capacity planning process applies the basis of the Utilization Law to measure how much of a server\u2019s resources are being consumed during a sample period. Examples of resource contention areas include locking mechanisms forexclusive access to resources.  Processing consumes measurable resources in the serverplatform. An observer of the server during the observed time frame canidentify the amount of time that the server resource component is busy processing tasks. The SCPM process measures the amount of work being done on a machine. By measuring realworkloads and their relative disk I/O, CPU, memory, the process can determine how well a system is performing. You can predict the effective resource consumption of that workload. You can alsoproject resource consumption into other SCPM-rated platforms or modify thevolumes of work being performed within the model. The output of sar can easily be imported into a spreadsheet for further Calculations. The %wio column is not considered for this exercise, because it indicates the time that the CPUs werewaiting, and not processing. We can also project the measured system utilization into the system SCPM value to produce the quanta consumed (QC) over time (see FIGURE 5-9 ). This representationcan sometimes be easier to understand than percentage utilization graphs. If we divide the QC on a given platform by the number of users accomplishing that work, we now have the quanta consumed per user (QC \u00f7User). You can thencompute the total users that a given machine will support: SCPM\u00d7Target Utilization \u00f7(QC\u00f7User) Capacity planning with SCPM values provides a simple method for measuring,reporting, and predicting consumption of CPU resources. The same method can bemodified slightly and applied to disk subsystems. The activity being measured is only related to disk activity, so data generated by tape drive, Network File System (NFS), or CD-ROM activity mustbe \u201ccleaned\u201d before being used for capacity planning. The number of disk activities is measured and reported for total disk activity. The number of disk activities during the measured period can be referenced inrelation to the service levels provided in that sample time. By dividing the diskactivities per second by the transactional rate in transactions per second, wecan compute a metric indicating the disk activity per transaction. The relative disk I/O content for a given workload should remain almost constant. Changes to a workload (new features, new data structures) can change the R value. As the system grows, thisvalue should approach reality and become more accurate as we calibrate themeasured average I/O sizes against the data access I-O size.  err on the side of safety andoverestimate the maximum required disk throughput. The combination of these two trendswould cause the R value to reduce over time. Average CPU Utilization 75%Average Disk Reads per Second 3000Average Disk Writes per Second 1500Total Disk Avtivities per Second 4500Filesystem I/O Size 8KB \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0QC (40,000 (Q))  *  (.75) 30,000 QQC / User  /  (300 (Users)  / (50 (Transactions)   600 Q*secS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W /  User (1500 (writes) / (300) 10 IOPS \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0S    User  (4500 (I/Os) /   ( 300 (Use) (Use))   10 IOPSRSR   Users (3000 (reads) (3000) During our monthly load planning meeting, the marketing department presented a growth projection of 50% for the next fiscal quarter. The server running the load has an SCPM value of 40,000 Q, and the load projection shows aprocessing requirement of 45,000Q. From these values, it is obvious that the currentserver will not be able to process the projected workload. If we upgrade the serverthrough module count increases, CPU module upgrades, or a \u201cforklift upgrade\u201d System Utilization =QC\u00f7SCPM (Q) 45,000\u00f772,000 =62.5% = 62.5%. The projected disk utilization can be predicted based on the capabilities of the disk subsystem. OLTP servers generally consume disk, CPU,and memory resources at a similar rate. The SCPM process has proved very effective in modellingand predicting workloads and resource consumption for a large percentage of themachines sampled. The SCPM process is very useful in a first pass of a TCO analysis as part of a serverconsolidation effort. The details of how disk measurements are derived are covered in-depth by Adrian Cockcroft. More complex systems do not follow the same rules as simple systems whenit comes to response time, throughput, and utilization. Even the simple systems are not so simple, so we start by looking at measurements available for a single disk. Then observe combinations, and finally show you how to combine the availablemeasurements into capacity estimates. The most important measurement to take on each disk is the average transfer size. For example, a high-end server 18.2 GB FC-AL drive is the Seagate ST318203FC. A standard disk is SCSI-based and has an embedded controller. The disk drive contains a small microprocessor andabout 1 MB of RAM. It can typically handle up to 64 outstanding requests via SCSItagged-command queuing. Large systems have another level of intelligence and buffering in a RAID controller. The service time, as measured by the device driver, varies according to the load leveland queue length. Disks that spin fasterand seek faster have lower (better) service times. If there is always a request waiting, the device is 100% busy. A busy disk can operate more efficiently than a lightly loaded disk. The difference is that the service time is lower for the busy disk, but theresponse time is longer. This difference is because all the requests are present in thequeue at the start. Solaris OE instrumentation measures a two-stage queue. A reador write command is issued to the device driver and sits in the wait queue until theSCSI bus and disk are both ready. When the command is sent to the disk device, itSector 0. iostat -x was introduced in Solaris 2.6 OE, and at the same time, disk slices, tapes, and NFS mountpoints were added to the underlying kernel data source. The problem with iostat is that it tries to report the new measurements in some of the subsystems. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. Theword wait is being used to mean two different things (seeFIGURE 5-15 ). The meaning of service time ( S) is as close as you can get to the old-style disk service time. To calculate it from iostat output, divide the utilization ( U) by the total number of reads and writes. %iostat -xn \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0  r/s  w/s, kr/s and kw/s wait actv wsvc_t asvc_t  %w  %b device \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021.9 63.5 1159.1 2662.9 0.7 0.0 31.8 0 93 c3t15d0UwaitWait Queue Utilization iostat %w100Bwait\u00d7 worrisomeThires. iostat reports 31.6 vs. 31.8. The difference is due to rounding errors in the reported values, 2.7 and 85.4. With fullprecision, the result is identical, since this is how iostat calculates the response time. This method uses a theoretical model of response time. It assumes that as youapproach 100% utilization with a constant service time, the response time increasesto infinity. This occurs because the disk is much more complex than the model. When a simple system reaches 100% busy, it has alsoreached its maximum throughput. When the device being monitored is an NFS server, hardwareRAID disk subsystem, or striped volume, the situation is clearly much morecomplex. All of these can process many requests in parallel. As long as a single I/O is being serviced at all times, the utilization is reported by iostat as 100%. However, there is enough capacity for additionalI/Os to be serviced in parallel. Compared to a simple device, the service time is the same, but the queue is being drained more quickly. If several requests can be serviced at once, then when a queuing model is used, the model works as if the averageservice time was reduced by the same factor. So, there is some confusion here over how to handle this case. Cached Disk Subsystem Optimizations can be used to improve performance of complex disk systems. The cache allows many optimizations that combine small accesses into fewer largeraccesses. Some of the common optimizations are: read prefetch clustering and modified cacheblocks written back asynchronously. Multiple writes to the same block often occur when file systemmetadata is updating. Multiple writes to adjacent data are coalesced into a singlelarger write. If the larger write exceeds the underlying disk block size, then thereis no need to read a large block. The disk cache can be inserted into the disk subsystem in two places: in the hostcomputer or in the disk array. The Sun Prestoserve and Sun StorEdge\u2122 Fast WriteCache products add nonvolatile memory to the host computer and use it primarily as a write cache. A very small amount of  the large block back. The capacity modelfor reads is the same as that of a simple striped disk setup. The worst-case performance for writes is limited by the write-backrate. There can be a choice ofpolicies for the cache: it might cache only synchronous data. UltraSCSI runs at close to 40 MB/s for large transfers. This interconnect is used for simple disk packs and the Sun D1000and A1000. The transfer latency for small transfers over UltraSCSI is around 0.6 ms. This section provides an overview of performance factors and capacity planningmodels for complex disk subsystems. It includes RAID5 and cached stripe examples. The capacity model for a single disk is simple and has already been described. When disks are concatenated or the request size is less than the interlace, individual requests will only go to one disk. In practice, writes take a small hitand reads a small benefit, but these are second-order effects that can be ignored for the sake of simplicity. Small requests of 2 to 8kilobytes are used and it is impractical to attempt to make a stripe interlace thissmall. The reduced size of the request on each underlying stripe reduces service time for large transfers, but you need to wait for all disks to complete. The operation of RAID5 is described in detail in Configuration and Capacity Planningfor Sun Servers by Brian Wong. In essence, a parity disk is maintained alongside astripe to provide storage that is protected against failure, without the fullduplication overhead of mirroring. Reading the RAID5 combination performs like a normal stripe. It has similar read and write performance and thethroughput of N-1 disks. The service time for the transfer is reduced as the size ofeach write is reduced. The cache provides fast service time for all writes. Interconnect transfer time is the only component. The cache optimizes RAID5 operations because it allows all writes,whether large or small, to be converted to the optimal full-stripe operations. Write-caching for stripes provides greatly reduced service time. It is worthwhile for small transfers, but large transfers should not be cached. For consistently largertransfers, better performance is obtained without a write cache. The configuration parameters for this system are: number of data disks M= 4 (ignore mirrors) RAID5 parity disks I= 64K (use whole disk size if unstriped) Max SCSI rate Brw = 40000K/s or FC-AL Br=Bw= 100000K/ s. Max disk data rate D= 24500K/S. We use the notation of Mdisks, with a workload concurrency of N. mance varies from 1 to stripe width M, depending upon configuration and workload. The performance factor Pcan vary for read and write, but it does not include cacheeffects. There is an effective increase in concurrency at the disk level because each thread ofaccess in the workload is being subdivided by the disk interlace to cause morethreads of disk accesses. This division allows more performance to be obtained froma disk subsystem, up to the limit of the number of disks in the stripe. This example is based on the same iostat data shown above, but for M= 6 and I= 16K interlace. The change in the interlace would change the data reported by \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0iostat in practice, so the results are not comparable with Example 1. The primary benefit of cache is fast response time. There is also a small overhead from copying data to and from thecache on its way to the disk. You should make the pessimistic assumption thatthroughput is unchanged by the cache. Solid-state disks have H= 1.0 for both reads and writes as data is engulfed by the cache. Other disk types have varying values for H, but characteristic values for E. The measured service time calculated from iostat data is. Smeasured =Sdisk(1 \u2013 H)+H\u00d7Sdisk\u00f7E. Main memory provides the best cache for reads, and NVRAM for writes. The only disadvantage of the NVRam cache is that in clustered,highly available disk subsystems, it cannot be used because it is internal to eachsystem. The next chapter concentrates on the importance of observability requirements for information collection and presentation to three different audiences. This chapter looks in detail at gathering, organizing, andpresenting system performance data to these three audiences. Alert-based monitoring should be combined with high-level problem diagnosis. A single-page daily summary of status and problems is more appropriate. In the following sections, techniques and examples for implementing theseviewpoints are presented. The operations viewpoint is demonstrated with the SunManagement Center (SunMC, a.k.a. Sun Enterprise SyMON\u2122 2.1). The engineeringviewpoint is implemented with SunMC and the SE toolkit to collect additionaldetailed data. The management viewpoint is implemented as a spreadsheet. The steps covered in this section illustrate how to use a system monitor to automate the process of tracking multiple systems for unusual or problematic conditions. With SunMC, basic rules are enabled but you can load additional health monitoring tools. If you have never seen a problem with a particular metric, then set its threshold alittle higher than the highest value you have ever seen. This way, you \u201csurround\u201dthe systems being monitored with a multidimensional behavior envelope. If thebehavior goes beyond that envelope in any dimension, an alarm tells you thatsomething abnormal is happening. The number of alarms that occur at each of the problematic levels (e.g.,yellow and red) should be collected daily so it can be reported to management. The alarm log should bearchived. The number of problems analyzed each day matches the number ofproblems reported, so you are keeping on top of the situation. SunMC is becoming increasingly important as a platform for the management and monitoring of Sun systems. Many third-party tools could be used in this role. They are not Sun specific and can managemany kinds of systems from several vendors. SunMC has third-party vendor support for managing Windows NT-basedsystems, and relational databases for performing generic system administrationfunctions. Contact Halcyon Inc. for more details. SunMC can be used to monitor systems using complex rule-based alerts to diagnoseproblems. SunMC\u2019s SNMP version 2 with user security implementation ismore efficient at bulk transfer of data. The SunMC health monitor is based on a set of complex rule objects. The health monitor rules are based on thoseimplemented by Adrian Cockcroft in the SE Toolkit script virtual_adrian.se. A simple rule can be placed on any system. To load the health monitor module, start SunMC with the default administrativedomain. Select the system, pop up a menu, then select the Load Module option from the menu. The Browser tab of the host Details window shows the modules that are loaded. When it is selected, a secondwindow opens that is specific to the system being monitored. Under Local Applications, you will find the Health Monitor module. Inside that you find the eight rules that are implemented to monitor several system components. Handling Alarms in SunMC can monitor hundreds of systems from a single console. SunMC gives each group a \u201ccloud\u201d icon; or, you can load a backgroundimage to form a map or building plan and then position the icons on the map. When a simple rule or one of the health monitoring rules generates an alarm, it islogged by SunMC. At the domain-level console, the worst alarm state for eachsystem being monitored is counted. This means that with only one system beingmonitored, only one alarm will be indicated. If you double-click the alarm or select the alarm and click the Details...button, the Details window for that system opens with its Alarms tab selected. The next step is to select one or all of the alarms and acknowledge them by clicking the Acknowledge button. It takes some time to perform the acknowledgment, since it involves communicating all the way back to the agent on the server being monitored. The Sun BluePrints book Resource Management shows how to separate and monitormultiple workloads with SunMC. Use this capability to track application processes and define alarms that will tell you when something goes wrong. Almost all commercial performancetools supply data in real time for display in ways that are suitable for an operationsmonitoring display. An example of a useful display is a spreadsheet that reads in data logged to a file using the SE toolkit. Main memory is managed by reclaiming itself once it reaches a low threshold, so it will tend to hover around that threshold. The page residence time is another useful measure, as explained here. System is quiet overnight, gets busy and plateaus during the day, then peaks in theearly evening. CPU and disk plots also show that there is extra system CPU timeduring the night and a higher disk data rate associated with an online backup. There are a few times when the page residence time drops to a low level during thebackup period. This value is calculated by dividing the page scan rate into a systemparameter called the handspread pages. An optional setting for Solaris 7 OE turns on priority paging. Solaris 8 OE uses a completely different algorithm, which allows the file cache to be included in the free memoryvalue. The absolute value of free memory is used directly to see how much capacity is available for use. In Solaris8 OE, scanning only occurs when thesystem is extremely short of memory. The example report shown in FIGURE 6-15 is customized for use by a large Int. atus. The report must becustomized to your own situation, so we describe the construction and reasoningbehind the report in detail. Thereport is designed to be delivered each Tuesday. It covers from Monday of theprevious week to the Monday before the report and predicts whether there isenough site capacity to survive the load expected for the following Monday. The nightly online backup took too long and spilling over into the high-loaddaytime period, where it adversely affected user response times. The schedule for hardware upgrade on 12-May is at risk due to component delivery. The entire capacity planning process is described in more detail in Chapter 4,\u201cScenario Planning.\u201d The output from the scenario planning process includes aschedule of upgrade events that increase site capacity and a schedule of expectedincreases in load. The site ran fairly smoothly the past week. A new record bandwidth of 253.8 Mbit/s was seen on Mondaynight. User-visible problems caused by nightly backup overrunning into daytime operation on 14, 17, and18th. The schedule for hardware upgrade on 12-May is at risk due to component deliveryleadtimes. The upgrade is estimated to give a 22% performance increase, which (ifimplemented the day of the report] would reduce the 71% utilizationto 58%. This section gives a view of events over the previous week or so. It includes both problem counts and a graph of activity from Monday of the previousweek through to about midday the next Tuesday. The first row gives the total number of problems that were unresolved at the end ofeach day. The second row shows the overall status of the day according to the severity ofproblems that occurred at any time during the day. The graph is aligned to the daily problem summary. how many problems were resolved during a day and how many new problems were registered. Problems can occur at a relatively constant ratethroughout the week on average, but analysis and resolution tend to take placeduring the week when all the staff are available. Management Viewpoint Implementation 141Thursday night/Friday morning. might be better to schedule downtime at a different time. Two kinds of Web pages on a site can betested: a standardized page that allows comparisons between Web sites, and a site-specific page. A full Keynote report covers the performance of many components of the Web site. Other vendors in this market are Mercury Interactive and Envive Corp. Asingle measurement is sufficient for the management report. The SunMC product provides data on some of the Sun specific capabilities. It does not include a performance database for long-term historical data storage. For a full and detailed picture of what is happening on a Solaris OE system, additional information generally needs to be collected. The standard system utility sar is already set up through cron to collect data. It just needs to be enabled. The sar data files are collected in the directory /var/adm/sa. The default collection scripts overwrite data after one month. The Sun 100 Mb Ethernethme andqfe and the Gigabit Ethernet gecollect many detailed metrics that are not reported by the netstat utility. It is important to obtain byte count metrics for capacity planning. Web server access logs provide a useful source of performance information. The overhead of collecting data from the system utilities is low as long as the collection interval is set to areasonable level, such as 30 seconds. The best organization on disk is to build a collection script that maintains a dailydirectory that contains data files for each tool for each hour. The best starting point is the orcollator.se script, which is in use at many Sun based Internet sites. The disadvantage of using the SE toolkit is that it is unsupported free code. You must devote some engineering resources to maintaining the SE scripts yourself. SAS/ITSV has very good graphical and data analysis capabilities. Some Sun/Solaris OE data can be imported fromorcollator data files to MXG for analysis. Sun\u2019s own tools are currently focused on system managem. This chapter describes several useful Sun and third-party tools for capacityplanning. It also offers several ways to balance peak load both within and across systems. Over time, more of these functions are likely to be integratedinto Sun\u2019s product line. By analyzing the resource requirements of multiple applications on distinct servers,you may be able to consolidate those services on fewer machines. eavor for IT departments to decreasethe datacenter space, simplify management, increase reliability, and decrease costs. Solaris Resource Manager (DR) in the Sun Enterprise 10000 server provides discrete \u2018hard\u2019 partitioning ofdomains. DR also enables you to move a systemboard containing memory and CPUs from one domain to another. This tool can be extremely useful when you combine mixed workload applications on a single server. CPU and virtualmemory to specific applications or users within a single image of Solaris OE. Theseresource allocations act as a form of \u201csoft\u201d partitioning. The SRM software enables the administrator to establish resource usage policies for the CPU and virtual memory resources. Interactive users are alloted an 80% share and the batchprocesses a 10% share of the resources during working hours. Solaris Bandwidth Manager (SBM) software can be used to give each site itsown equal share. Poorly written cgi-binscripts and searches from a single Web site can saturate the server and affect the other sites. The SBM software provides the framework for quality of service (QoS) guarantees and SLAsfor critical networked applications. It allows the administrator to establish controls for the amount ofbandwidth that applications, users, and departments are allowed to use. This can be useful in corporate intranets and can also be used to constrain denial-of-service attacks on the network without shutting off access completely. load sharing facility (LSF) from Platform Computing Corporation is a tool that takes advantage of idle CPU cycles on a network to assist with batch processes. LSF enables users to submit batch jobs and gives the administrator the capability to set up rule sets to prioritize the requests. There are three roles to which hosts can be assigned within a cluster, and hosts can have more than one role. SunMC can be used to manage hundreds of systems running Solaris 2.5.1, 2.6, 7, and 8 OEs from a single console. This module is free of charge to download fromhttp://www .sun.com/softwar e/sunmanagementcenter/hwds/ and is an add-on solution for SunMC. SunMC Hardware Diagnostic Suite enables you to run comprehensive, data-safe, and non-resource-intensive hardware diagnostics testing in enterprise environments. The testing does not corrupt dataon the system and uses minimal system resources. SunMC modules tie into the SunMC alarm framework so users can executecorrective action scripts or programs that eliminate faults. Scheduled routine hardware validation can replace other maintenancethat requires system downtime. CST fills an important hole in the overall strategy of proactive system management. It provides a macroscopic view of the system configurationchanges caused by failures or upgrades and service patterns over extended periods. The CST agent creates a snapshot of the system configuration and determines the changes since the previous snapshot. It then catalogs the event andchanges in a change log file. The agent also provides a facility for maintaining an electronic log of service events. BMC Best/1 is a real-time performance monitor and prediction tool. Users can view the configuration report and change log for any host on the network. Best/1 is a set of tools for UNIX, PC and Mac computers. It can be used to create, edit and analyse data. The tool is not easy to use. Visualizer is one of the few tools that can analyze trend data in terms of cyclicfluctuations. It implements MASF, which is a sophisticated and automated approach to the problems discussed in Chapter 4, \u201cScenario Planning\u201d Patrol has been criticized for imposing a larger load on the system being managed than other tools. The product has a good underlying architecture that scales well in large, distributedinstallations with multiple data storage points. Foglight Software was initially known as Resolute Software and more recently hasbecome a division of Quest Software. RAPS uses very efficient data collectors and has anadvanced distributed storage architecture and a good GUI. SAS has a wide range of statistical analysis products. MXG Toolset is a notable alternative to the SAS/CPE functions that runs on the basic SAS platform. SES Workbench runs onUNIX systems and NT. SES Strategizer runs only on WindowsNT. rvices capability to assist in capacity planning simulation development. SarCheck is a relatively inexpensive tool that can be very useful to help withdiagnosis in an operations environment. On SolarisOE it reads data from sar andps, then writes a detailed explanation of what it sees, almost like an essay. Performance data is monitoredand recorded in real time, with both reporting and alert features. The data can beviewed live, showing system and workload performance measures, while the data isbeing recorded in the TeamQuest database. The performance data for a previouslyrecorded period can also be viewed in context. Capacity Planning with TeamQuest Model 155. The system being modelled is a database server, with the database instance and twomiddleware applications running on the same system. The Stretch Factor statistic represents the time spent waiting for a resource. A higherstretch factor indicates a higher relative quantity of resource wait time. The standard reports produced include severalinteresting statistics for further analysis. The Active Resource report includes resource consumption statistics such asthroughput, service time, wait time, average queue length, and number of server instances. Stretch factors greater than 2.0 shouldbe analyzed\u2014they indicate a significant resource shortage. The calibrated model, including the new workload growth calculations, can now be exported to create graphs and tables of performance data in Microsoft Excel. The first graph that we will examine is the Stretch Factor by Workload graph. The \u201cDatabase_App2\u201d workload appears to have a response time increase, which indicates a resource contention problem. This is to be expected, with the increase inrelative queue wait time to resource service time represented by the stretch factor. We can further analyze this workload\u2019s behavior by using the Components ofResponse Time graph for the \u201cDatabase_App2\u201d workload. The graph in FIGURE 7-7illustrates the time consumed by a transaction, or unit of work within the workload,with time spent in the workload. This is broken down by active resource service timeand active resource queue delay.  CPU Queue Delay is increasing at an alarming rate and that itaccounts for more than half of the per-transaction time by Step 6. This indicates that the system is on the verge of a serious performance degradation. The CPU resource utilization is projected over 95% average utilization by the time the model reaches the sixth step. Step 1 shows that we are running at approximately 70% utilization, with the active resourceutilization for the measured workloads showing very little resource queue delaytime. At Step 2, the workloads arebeginning to show some contention and CPU queue delay times, with CPUutilization just passing 75%. This trend would indicate that we should target 70% for the KPI maximum CPU utilization threshold. System was a Sun Enterprise 10000 serverwith six 250 MHz UltraSPARC CPUs with 4 MB of e-cache. The chosen solution inthis case is to perform a one-to-one upgrade of the CPUs in the system to400 MHz UltraSpARC CPUs. With this solution, we can return to the TeamQuest Model window, create alternatives based on the current model, and modify the system resources to predict the projectedperformance of the growing workload. The Stretch Factor graph now shows stretch factors under 2.0 in all steps, for all workloads modelled. In addition, the \u201cDatabase_App2\u201d workload displays anincremental increase in delay time per unit of work being done. We can also examine the total system CPU utilization by looking at the resultingspreadsheet table data or by viewing the Active Resource Utilization graph. The CPU and disk active resources are graphed over the current, measured workload, as well as over the five steps of compound growth. The Active Resource Utilization graph shows that disks are becoming rather busy throughout the model. Wecould now return to the model and upgrade the disk subsystems to provide moreresource capabilities or move workload resour. This chapter presented a detailed survey of useful tools for capacity planning anddiscussed the benefits of using both Sun and third-party tools. In addition, this chapter offered several ways to balance peak load both within and across systems. Alternative pathing (AP) works in conjunction with dynamicreconfiguration (DR) to provide redundant disk and network controllers and their respective physical links. The main purpose of AP is to sustain continuous network and disk I/O when system boards are detached from amachine or dynamic system domain. Short jobs are said to backfill processors reserved for largejobs. Short jobs fit into the time slot during \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0which the processors are reserved. The blacklist file is read andprocessed at startup. class-based queuing (CBQ) The underlying queuing technology used in the Solaris BandwidthManager (SBM) software. Cluster A collection of computers interconnected via a high-speed interface. Multiple servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if high availability is important, multiple servers are configured as siblings. A Workload Manager (WLM) component that communicates policies, metrics, and control data between Sysplex nodes. A function, used by dynamic reconfiguration, or DR (called from dr_driver ), provides the ability to attach a particular instance of a driver withoutaffecting other instances. E is a function, used by dynamic reconfiguration, or DR (called from dr_driver ), that provides the ability to detach a board that contains the kernel cage (OBPOpenBoot PROM), kernel, and non-pageable memory. The kernel cage canonly be relocated after all of the drivers throughout the entire dynamic system, or DSD (not just on the board being detached) are quiesced. The goal of this group is to tie together usersand applications with network elements, protocols, and services throughspecific relationships. By complying to this model, different network equipment and application vendors should be able to build interoperable network elements. direct control A means of control that operates on the resource you want to control. Dynamic internal service class created by the Workload Manager. Distributed queuing system. Sun Microsystems technology allows system boards to be added (attached) orremoved (detached) from a single server or domain. A Starfire independent hardware entity formed by the logicalassociation of its system boards. Exclusive scheduling is a type of scheduling used by the load sharing facility (LSF) A job only runs exclusively if it issubmitted to an exclusive queue. An exclusive job runs by itself on a host. LSFdoes not send any other jobs to the host until the exclusive job completes. Fairshare scheduling is an alternative to the default first come, first servedscheduling. Fairshare scheduling divides the processing power of the LSFcluster among users and groups. LSF allows fairshare policies to be defined at the queue level. Gigaplane-XB provides main memory access through a point-to-point data router. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Hostview is a graphical user interface (GUI) program that runs on the system serviceprocessor (SSP) machine. Hostview enables you to monitor and control an Enterprise 10000. SunNetManager or HP OpenView products collecting and displaying the data. IBM WorkloadManager (WLM) provides an automated resourcemanagement environment driven by high-level business goals. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. Intimate shared memory is often the largest component of a database\u2019s memoryrequirements. A JavaBeans based framework for developing and deploying dynamic management based applications. Autonomous agents can be deployed in realtime to perform management tasks for devices on the network. When dynamic reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. A lightly damped system is veryresponsive to sudden changes, but will probably oscillate back and forth. A node in a special resource tree used by the Solaris ResourceManager (SRM) software. The SRM is built around lnodes, which are afundamental addition to Solaris kernel. ln nodes correspond to UNIX UIDs, and may represent individual users, groups of users, applications,and special requirements. High-Performance Computing (HPC) package includes the LSF as a vehicle forlaunching parallel applications on an HPC cluster. In addition to starting batchjobs, the L SF also provides load balancing. The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager (SBM) software. The maximum bandwidth is dependent on thepercentage of bandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. NetFlow allows for detailed network measurements that can be sent to other software pack. Network File System (NFS) An application that uses TCP/IP to provide distributed file services. Sterling Software offers a distributed version of NQS calledNQS/Exec. PC NetLink is a product from Sun Microsystems that is based on the AT&T Advanced Server for UNIX. PC NetLink adds functionality that was not previously available onSolaris servers with products such as Samba. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. A network element that does not explicitly support policy control using mechanisms defined in the applicable standard policy. The policy protocol can be any combination ofCOPS, Simple Network Management Protocol (SNMP), and Telnet/CLI. priority A relative importance level that can be given to the work done by a system. priority decay Seeprocess priority decay. Application memory is allocated at a higherpriority than file system memory. processor reservation is a method that allows job slots to be reserved for a parallel job until enough slots are available to start the job. Processor reservation helps to ensure that large parallel jobs are able to run without underutilizing resources. A \u201cprovider domain\u201d is the domain where a system board gets logically detachedfrom to then have it attached to a \u201creceptor domain.\u201d A proxy caching Web server sits between a large number of users and the Internet, funneling all activity through thecache. The SolarisBandwidth Manager (SBM) software provides the means to manage yournetwork resources to provide QoS to network users. Dynamic reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider dynamic system domain. repository access protocol used to communicate between a policy repository and the repository client. RSVP provides a way for an application to communicateits desired level of service to the network components. Scheduler is a component of the Solaris Resource Manager (SRM) software that schedules users and applications. Security policy aims at preventing access to certain resources or allowing designated users to manage subsystems. Service class is a class that defines a set of goals, together with periods, duration, and importance. A number of individual processes and CICS/IMS transactions can be assigned membership to a service class. They will then become subject tospecified goals and constraints. service provider In a network policy, the service provider controls the network infrastructure and may be responsible for the charging and accounting of services. Service time The time it takes for an I/O device to service a request. The SE Toolkit is a freely available butunsupported product for Solaris systems. It can be downloaded fromhttp://www .sun.com/sun-on-net/performance/se3. The SHRScheduler manages the scheduling of individual threads. It alsoportions CPU resources between users. Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris ManagementConsole (SMC) An application that provides a generic framework for gathering together operating system administration tools. Service levelagreements (SLAs) can be defined and translated into SBM software controls and policies. Resource management is done on a per-network basis, often by controlling the priority of data flow through intelligent routers and switches. For High-Performance Computing (HPC), Sun HPC servers use the platform computingload sharing facility (LSF) to perform load balancing. System service processor; Starfire\u2019s system administrator and systemmonitoring interface. The SSP configures the Starfire hardware through aprivate Ethernet link to create domains. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 GB ofmemory and over 20 TB of disk space. StoreX enables management of any storageresource in a heterogeneous distributed environment. SunMC is a Java-based monitor with multiple userconsoles that can monitor multiple systems using the secure extensions to Simple Network Management Protocol version 2 (SNMPv2) to communicate over the network. System level measurements show the basic activity and utilization of the memory system and CPUs. Per-process activity can be aggregated at a per-system level, then combined withnetwork measurements to measure distributed applications. The Solaris BandwidthManager (SBM) software can use this information when classifying a packet. It can also change the information, to influence how the packet is routed. Virtual memory is not directly related to physical memory usage. Virtual Web hosting is a configuration where a single server is configured to respond to hundreds or thousands of Internet addresses. g system will create 16 MB of memory within that application\u2019saddress space, but will not allocate physical memory to it until that memory isread from or written to."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_28",
      "kind": "book",
      "subkind": "1,31-65,80-107,117-148,339-356",
      "title": "Resource Management",
      "source": "Sun Press Blueprints  - McDougall, adrianco etc.",
      "published_date": "1999",
      "url": "./mcp_resources/virtual_adrianco/pdfs/rm_extracted_1_31-65_80-107_117-148_339-356.pdf",
      "content": {
        "metadata": {
          "word_count": 35852,
          "processing_status": "success",
          "processing_errors": [],
          "character_count": 234635,
          "excerpt": "Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource Management\n\nSend comments about this document to:  blueprints@sun.com\n\nThis chapter introduces the business problems that resource management addresses"
        },
        "summary": "Resource Management is published by Sun Microsystems, Inc. 901 San Antonio Road \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Palo Alto, CA 94303-4900 USA650 960-1300 Fax 650 969-9131Resource ManagementPart No. 805-7268-10July 1999, Revision ASun MicroSystems, Jr. The scope of this book can be summarized as: The best way to use combinations of Sun Microsystems products to manageresources. How can we combine lots of small servers into a few big ones? Sun\u2019s range of products provide a lot of built-in flexibility that can be deployed in many ways to meet the requirements of several markets. To manage the service level you provide, you must be able to measure and control the resources consumed by it. The Solaris operating environment is one of the best instrumented UNIXimplementations. Not all commercial tools report Solaris-specific metrics. Resource management for UNIX systems is in its infancy. This chapter describes the overall methodology of service level management. It alsodescribes and compares several approaches to resource management. Service level management is the process by which information technology (IT) infrastructure is planned, designed and implemented. Service level management interactions are shown in FIGURE 2-1. Each interactionconsists of a service definition combined with a workload definition. The processes involved in Service Level Management include creating service and workload definitions and translating from one definition to another. System managers first establish a workload definition and the service level requirements. Vendors then propose a system that meets these requirements. The basis of the sizing estimate can be a published benchmarkperformance. Service Level Management 5benchmark is used as the basis of a sizing estimate. Vendors provide reliability data for system components. They can also provide availability and performance guarantees for production systems. The agreed-upon service levels could be too demanding or too lax. The actual service levels experienced by users with a real workload are subjective measures that are hard to capture. The real service levels cannot always be captured directly, but the measurementstaken are believed to be representative of the real user experience. Atransaction may take place over a wide variety of systems. An order for goods willaffect systems inside and outside the company. System managers create policies that direct the resources of the computer system. A policy workload definition is closely related to the service levelagreement workload definition. The policy can controlresources directly or indirectly. The measured workload and service levels should be analyzed to extract trends. Acapacity planning process can then be used to predict future scenarios. The accrued usage of resources by each user or workload may be accumulated into an accounting system. A complete system implements many control loops. Designing a control loop is more complex and requires explicit understanding of the situation. A brief digression into basiccontrol theory is provided to help explain the behavior of such systems. You apply a control input by turning the steering wheel to the point thatwill get you around the corner. After a delay the car responds, You measure theresponse, compare it with what you wanted, and obtain the error difference. If the difference is zero you don\u2019t need to change the control input. You may decide to over- or under-correct, and apply the correction gradually or quickly. In control terms, you are applying negative feedback to the system. The rate at which you measureand apply corrections is called the control interval. The amount of the error that you feed back changes the behavior of the control loop. When you apply these principles to computer system resource management you can see that it is important to average measurements over an appropriate time scale. l, the system islightly damped and will be very responsive to sudden changes but will probablyoscillate back and forth. If you feed back a small proportion of the error over alonger control interval, thesystem is heavily damped. Diverse Methods occurs both because of the products that are available andbecause of the expectations of the end users who are purchasing solutions. Anetwork-centric methodology can be extended for use in other areas, so can astorage-centric or server-centric viewpoint. SunMicrosystems, Inc. egratedmethodology and solve all resource management problems. The closer you get tothis ideal, the more expensive and complex the tool becomes. So it is harder to justify purchasing it. The system-centric viewpoint focuses on what can be done on a single server using a mixture of hardware and operating system features. Dynamic Reconfiguration (DR),processor sets, Solaris Resource Manager\u2122, and Sun Enterprise\u2122 10000 (also knownas Starfire\u2122) Dynamic System Domains (DSDs) are all system-focused resourcemanagement technologies. The Starfire system uses its own HostView interface running on a separate system serviceprocessor (SSP) to manage domains. The next release of this software will be extended to viewan SSP and all the domains in a Starfiresystem as a special kind of cluster. There is no way to control the usage of memory by a workload. The only way to constrain a workload is to slow down or stop its CPU usage. Sun has two kinds of clusters. The highly integrated SPARCcluster\u2122 product range is focused on improvedavailability in commercial environments. Its management tools will eventuallybecome an integrated extension to the SyMON software. It is notcurrently possible to migrate a running job from one node in a cluster to another, orto checkpoint a job to disk and restart it again later. erconnect utilization andproportion of remote data access are important additional measures. The Solstice\u2122 Enterprise Manager product is a Telco-oriented CMIP and SNMPmanagement system. In theory it could beused to manage computer systems and local area networks, but it was not developed to do this. Network controls are based on delaying and prioritizing packets. There is no user orprocess identifier in a packet, so it is hard to directly map network activity to system activity. Storage has recently moved from being a simple attached computer systemperipheral to a complex managed entity in its own right. Networked storage usingfibre channel puts an interconnection layer in between multiple servers or clusters. Project StoreX is based on a distributed pure Java technologyplatform that can run anywhere a JVM is available. Project StoreXenables management of any storage resource in a heterogeneous distributedenvironment. Project StoreX enables resource management of capacity, performance, andavailability of data storage. Measurements of capacity and performancecharacteristics can be combined with availability policies. Backup and archival policies can be used to automatemigration of data to a tape library. Solaris software can trace storage accesses on a per-process, per device basis. There may be a need for a Project StoreX Solaris Storage BandwidthManager to bridge the two management viewpoints. Large and complex applications such as SAP R/3, Baan, and Oracle Financialscontain their own resource management concepts and controls. For example, OracleFinancials implements its own batch queue system to decouple the generation oflarge reports from the interactive response time of the users. SAP R/3 measures the response time of important transactions and breaks these down into application server time and backend database server time. As many usersconnect to an application server and many database servers connect to a single database, there is no concept of which user is doing work on the backend. The application itself must have the instrumentation to keep track of what is going on. The consolidation process starts when you identify candidate systems and Applications. The next chapter introduces several example workloads. The product overview section of this book discusses their capabilities inmuch greater detail. Local password filesmay need to be merged, and any conflicting port numbers specified in the/etc/services file may need to being cleared. If you use a name service such as NISfor all your password and services information, then the systems should already beseeing the same name space and definitions. The number of system footprints may betoo high with midrange servers. It is hard to reduce the total number of serverseffectively. Consolidated upgrades benefit from systems that can perform dynamic reconfiguration. DSDs are supported on the Solaris 2.5.1, 2.6, and 7 releases. The total number of DSDs can be reduced as applications are consolidated. There is a common set of measurements to collect per workload. o choose relevantprocesses and aggregate to measure them. The remainder is overhead or unplannedactivity. If it is significant, it should be investigated. When you are accumulating measurements don\u2019t accumulate the pscommandCPU% . It\u2019s a decayed average of recent CPU usage, not an accurate measure of actual CPU usage over an interval. You need to measure the actual process CPU time used in each interval by taking the difference of two measurements. One of the simplest policies is to define limits on a measurement and associate itwith an action. Products such as the Sun Enterprise SyMON 2.0 predefine many simple limit rules. An error event is different because it is treated as a discrete on/off event. A complex rule is used to establish the state of a component or asubsystem. When rules are combined, they are ranked so that critical problems takeprecedence over warnings. A hierarchy of rules can be built for a network of systems. Solaris Resource Manager and others like it assign shares to each user according to a policy decided by an administrator. The Solaris BandwidthManager software uses this mechanism to provide a way to specify policies on a per-network packet basis. A control loop is a complex thing to manage because its stabilitycharacteristics, time constant, and damping factor must be set correctly. Policy manipulatescontrols when a measurement deviates from its desired range. Automated goal-based workload management is a feature found only on mainframes running OS/390 software. Some policies are implemented manually as part of operations management. Security and network-based policies can be stored in an LDAP based name service. The Solaris BandwidthManager configuration is updated to take into account that user\u2019s network address. A direct control operates on the resource you want to control. An indirect controlworks via dependent resources. some operate on a per-user basis (such as a file system quota) Solaris Bandwidth Manager product implements a direct control on networkpacket rates. This can be used to implement an indirect control on the CPU resourcestaken up by the NFS server code in the kernel. The terminology definitions are network oriented but apply equally well to systemlevel policies. Some of the terms defined in this standard are listed here. ultiple devicetypes. Policy elements are expected to be independent of which Qualityof Service signaling protocol is used. Examples of policy elements includeidentity of the requesting user or application, user/app credentials, and so on. The general architecture shown FIGURE 3-1 illustrates one common implementationof a policy that combines the use of a policy repository, a PDP , and a PEP. This diagram is not meant to imply that these entities must be located in physicallyseparate devices. The policy protocol can be any combination ofCOPS, SNMP , and Telnet/CLI. Given this rich diversity, a common language isneeded to represent policy rules. The rest of the standard document describes theManagement tool. The Solaris software implements a blanket login ban for non-root users. It also limits the total number of processes that can be started via the nproc kernel tunable. This feature scales with the memoryconfiguration. A further limit is placed on the total amount of processes per user. The main class of application-level controls are those provided by relationaldatabases and transaction processing monitors. The Oracle8 idatabase also implements controls on resourceconsumption and policies for relative importance. On the Starfire system, the CPUs can be partitioned into dynamicsystem domains, and a separate copy of the Solaris operating environment booted in each domain. CPU boards can then be moved from one dynamic system domain toanother. This is described in detail in Chapter 8. SRM works by biasing CPU usage on a per-user basis, using shares to determine the relativeimportance of each user. LSF software implements a distributed batch queuing system where jobs are submitted and, when resources are available, sent to be run on a system. Block input and output counters are not incremented correctly in current releases of the Solaris operating environment. The block counter problem is filed as bugid 1141605 and is fixed in the next release of Solaris software. An application consumes virtual memory when it requests memory from the operating system. Virtual memory usage is not directly related to physical memory usage. Not all virtual memory has physical memory associated with it. When you restrict or control the amount of virtual memory that an application canhave, you are not controlling the amounts of RAM it can have. Virtual memory can be limited at the system level and at the process level. At asystem level, the total amount of virtual memory available is equal to the totalamount of swap space available. Each time virtual memory is used, the amount ofswap space available drops by the same amount. Base Solaris software can do simple resource management of a process\u2019s virtualmemory usage. Limits are enforced perprocess, thus preventing any one process from using an unreasonably large amount of virtual memory. But a user can run many processes, so this does not preventdenial of service attacks. It is important to note that the amount of swap space used by a user does notrelate directly to the sum of all that user\u2019s processes. A user may have threeprocesses, where each shares a single shared memory segment between them. SRM software isconfigured to limit the total virtual memory to 1.012 Gbytes. If one of the three processes has a memory leak, the limit would be hit for that user. Theper-user limits mus mus be set to 1 Gbytes for each user. Physical memory is controlled by applying importancepolicies to different types of memory. In the future, it may be possible to apply limitor allocation style policies to physical memory, but that capability is not available in Solaris today. The Default Memory Allocation Policy is, by default, allocated on a demandbasis with equal importance to each subsystem. When a subsystem requestsmemory, it is allocated from a central pool of free memory. If sufficient memory isavailable in the free pool, then an application\u2019s request is granted. The Solaris memory allocation policy takes into account recent usage in an attempt to choose the correct application from which to steal memory. The most aggressivememory requests gets the majority of the memory assigned to it. This situation can be avoided by configuring physical memory in the system so that there is always enough memory for each application\u2019s requirements. In examples, if we configured the system with 128 Mbytes of memory, then bothnetscape andgimp could execute at the same time. The memory system will takememory from other applications that haven\u2019t used portions of their memoryrecently. For example, if you start a file-based mail tool such as dtmail , the memoryused to cache the file when dt mail reads a 23-Mbyte mail file will be taken from other portions of the system. Priority paging prevents the file system from consuming toomuch memory. Priority paging implements a memory policy with differentimportance factors for different memory types. Application memory is allocated at a higher priority than file system memory. The new memory allocation policy can be extremely important for larger systems. A large database system with a 50 Gbyte+ database on the file system willcontinuously put memory pressure on the database application. But priority paging will ensure that thefile system onlyuses free memory for file system caching. The Solaris Bandwidth Manager product provides controls on network traffic. The product can be used as part of a resource control framework. It can distribute incoming traffic over multiplesystems according to the current load on each system. This chapter describes tools that help perform process-based analysis. Analysis of per-process measurements separates the raw data into applicationworkloads. When several applications are consolidated onto a single system,resource contention can occur. In a distributed environment with discrete applications on separate systems,workloads are analyzed by monitoring the total resource usage of each wholesystem. The busiest systems can be identified and tuned or upgraded. This approach does not work when multiple workloads are combined on a single system. The Solaris software provides a great deal of per-process information. The data can be viewed and processed by a custom-written process monitor. The SE Toolkit is freely available for Solaris systems and is widelyused. The psinfo data is what the pscommand reads and summarizes. The usage data is extra information that includesthe microstate accounting timers. Sun\u2019s developer-oriented Workshop Analyzer usesthis data to help tune code during application development. Thepea.se script is an extended process monitor that acts as a test program for the system. It is based on the microstate accounting information described in chapter 5 of Workload Management 51. There are two display modes: an 80-column format(which is the default and is shown inFIGURE 4-15 ) and the wide mode, which displays much more information. The initial data display includes all processes and shows their average data since the process wascreated. Idle processes are ignored. Thepea.se script is 90 lines of code containing a few simple printf s in a loop. The real work is done in process_class.se (over 500 lines ofcode) It can be used by any other script. When the command is run in wide mode, the following data is added:Metadata input and output blocks per second.Characters transferred by read and write calls. System call per second rate over this interval. CPU time consumed between each context switch. The SE Toolkit alsoincludes a workload class, which sits on top of the process class. If you group them by user name and command, then you can formworkloads, which is a powerful way to view the system. Thepw.se Test Program for Workload Class. 53on user name, command and arguments, and processor set membership. It can work on a first-fit basis, where each process is included only in the first workload that matches. The pw.sh script sets up a workload suitable for monitoring a desktop that is also running a Netscape web server. The script runs with a one minute update rate and uses the wide mode by default. A high number of page faults fora workload indicates that it is either starting a lot of new processes, doing a lot. of I/O, or tha %more pw.sh. t it is short of memory. The script is compiled with the following command: #!/bin/csh -DWIDE pW.se 60. The command is followed by a list of command lines. Once you have collected the data, you can write a rule that examines each process orworkload and determines, using thresholds, whether that workload is CPU-bound,memory-bound or I/O bound. A prototype of this rule is implemented in theSE Toolkit, and it can produce the kind of information shown in the example. The Internet provides a challenge for managing computer systems. External synchronizing events can cause a tidal wave of users to arrive at the server at the same time. Sports-related web sites in the USA get a peak load during key games in the\u201cMarch madness\u201d college basketball season.  proxy caching web servers sit between a large number of users and the Internet. When all the users are active at once, regardless of where they areconnecting to, these proxy cache servers get very busy. Many web sites get little or no activity most of the time. A caching web server acts as an invisible intermediary between a client browser and servers that provide content. It cuts down on overall network traffic andprovides administrative control over web traffic routing. Performance of accesses to all the other virtual sites can be affected, and resource management tools are needed. Caches commonly have a hit rate of about 20 to 30 percent, with only 60 percent ofthe data being cacheable. If cached items are read more than once by different users in areasonably short time interval, the cache will work well. Each cache transaction takes some time tocomplete and adds significant latency. Direct web service allows the browser to contact web servers directly whenever possible. This contact may involve a firewall proxy thatdoes not provide caching to get out to the public Internet. This funnel effect can increase network load at a local level. The real reason to set up a proxy cache intranet infrastructure is the administrativecontrol. Security is a big problem. Setting up the firewall gateways to the Internet so that they route web traffic only to and from the proxy caches. Restricted routing also forces every end user who wants to get out to the Internet to use a proxy cache. The cache makes routing and filtering decisions and logsevery access with the URL and the client IP address. If the corporate policy is \u201cInternet access is provided forbusiness use only during business hours,\u201d then employees who clog up thenetworks with non-business traffic can be identified. Solaris Bandwidth Manager software can be used effectively in this environment to implement policies based on the URL, domain or protocol mix. Apache and Netscape\u2122 proxy caches are extensions of a conventional webserver. The Harvest Cache was the original development and is now a commercial product. TheSquid cache is a freely available spin-off and is the basis for some commercial products. Squid is used at the biggest cache installations, caching traffic at thecountry level for very large Internet service providers. ICP-based connections are much more efficient than individual HTTPtransfers. ICP connections are kept open, and transfers have lower latency. In the Solaris operatingenvironment, it is possible to configure more than one IP address. The Solaris 2.6 operating environment was tuned tospeed up this process. P addresses to an interface and specify virtual interfaces with atrailing colon and number. Use the ndd command to query or set the maximum number of addresses per interface. %ifconfig -a \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0lo0: flags=849<UP,LOOPBACK,RUNNING,MULTICAST> mtu 8232. or each connection, moreprocesses are spawned for the addresses that are most active at any point. The administrator of the web site can log inas that user to configure their own site. Cgi-bin scripts that it starts all run as that user so that accounting, performancemonitoring and resource management tools can manage each virtual web site\u2019sactivity separately. Solaris Bandwidth Manager software can also be used effectively, asthere is an explicit administrative mapping from each network address to the useridthat is running that web server. The SRM software manages resources on web servers by controlling the amount ofCPU and virtual memory. Three basic topologies are used on systems hosting webservers. Resource management is used to control the behavior of a single web server. For example, a web server may be shared among many users. An error in one CGI-BIN program could cause the entire web server to run slowly. You can run each web server as a different UNIX userid by setting a parameter in the web server configuration file. For example, the Solaris Web Server has the followingparameter, as shown inFIGURE 4-25. The memory limit can limit the amount of virtualmemory that the web server can use. Thisprevents any one web server from causinganother to fail from memory allocation. The following are the server wide aliases. Proc mem limit [ memory.plimit ] The per-process memory limit can limit theamount of virtual memory a single CGI-bin process can use. It effectively limits thenumber of concurrent cgi-bin processes. This section examines some aspects of the resource management that apply to thesetypes of problems. It focuses on workload consolidation and resource managementof databases. The success of workload consolidation is bound closely to the ability to partition resources between the applications. The unique characteristics of an NFS workload place multiple resource demands on the system. No single product can provide complete resource management of anNFS workload. The resources used by NFS and the different products or techniquesthat can be used to manage them. NFS servers are implemented in the Solaris kernel as kernel threads and run in thesystem class. You can control the amount of resource allocated to the NFS server inthree ways:Limit the number of NFS threads with nfsd. The actual number of threads required will vary according to the number of requestscoming in and the time each thread spends waiting for disk I/O to service therequest. There is no hard tie between the maximum number of. threads and thenumber of NFS threads that will be on the CPU concurrently. The best approach is to approximate the number. of threads needed (somewhere between 16 to 64 perCPU), and then find out if the NFS. server is doing its job or using too much CPUtime. Solaris 2.6 changed to instrument NFS client mount points thesame way it does disks. NFS mounts are always shown by iostat andsar. The full instrumentation includes the wait queue for commands in the client that have not yet been sent to the server. Utilization ( %busy ) indicates the server mount-point activity level. An NFS server is much more complex than adisk drive and can handle many more simultaneous requests than a single disk drive. NFS uses physical memory in two ways. In the kernel each NFS thread consumessome space for a stack and local data. Outside the kernel the data being servedby NFS consumes memory as it is cached in the file system. The amount of memory used by the file systems for NFS servers is verylarge and much harder to manage. NFS uses a very small amount of swap space, and there should be no inter-workloadswap space issues from NFS. You can manage NFS disk storage using UFS disk quotas. Prioritypaging should be mandatory for any system that has NFS as one of the consolidation applications. Solaris Bandwidth Manager provides the means to do this. You can control amount of resources consumed by NFS indirectly by curtailing theamount of network bandwidth on port 2049. The filter in the above example is for managing outgoing NFS traffic to the 129.146.121.0 network. You could decide to leave out the destination and manageNFS Traffic to all clients, from wherever they come. The key variableismax_bandwidth specifies an upper bound to the consumed bandwidth that never will be exceeded. Thepriority variable is less important. It will be a factor if other types of traffic are being managed. Higher priorities will have lower average latencies. The typical data center strategy is to consolidate more workloads onto each system. Resource management of the whole environment requires careful planning and a solid understanding of theinteraction between these packages and the database. A single database instance provides database services for two different applications that require access to the same data. No single control or policy can assign resources to ensure that adequate CPU is provided tomeet response times. The user issues an http request from the desktop, which must travel across the intranet/Internet to the web server. This component can bemanaged by controlling the bandwidth and network priorities into the web servers. Managing the bandwidth on the networkport on the machine that hosts the web server is a useful strategy to ensure thatkeyhttp requests get the required bandwidth. The Solaris BandwidthManager software can control the bandwidth allocation into the web servers. The database listener process accepts the incomingconnection from database client applications. The database listener forks a database process and connects the clientapplication to the database. The same control as in the web server component (2) is implemented here. The SRM software and resource limits can limit the amount of virtual memory used by processes, users, and workloads. This capability does not manage physicalmemory, but it effectively restricts theamount of global swap space consumed by each user. It is a high risk to ever let adatabase server reach its virtual memory limit. If the limit is reached, the databaseengine may hang or crash, and the database may have to be restarted. The objective for successful consolidation is to provide strong insulation between each database instance. This requires careful resource management of CPU, memory,I/O, and network traffic. System resources are not wasted because spare CPU cycles can be used by any otherworkload requiring them. The more workloads consolidated, the flatter the totalresource usage becomes. The shared memory area is often the largest component of the database\u2019s memory usage. It is the easiest to insulate between instances because the memory isallocated as shared memory. Resource Managements wires down the memory so that it can\u2019t be paged. One instance cannot steal memory from another. D-down memory must be unallocated before it can be removed from the system, requiring quiescing of the database. Private memory is the regular process memory used by each process. The Solaris page cache causes a huge memory demand, which places undue pressure on the database private memory. Priority paging puts a hard fence between the file systems and applications. If you plan to run a database on file systems, consider this a mandatoryrequirement. Ensure that I/O activity from one application does not affect another application inan undesirable way. The best way to insulate I/o within a single database instance is to make sure that application tables are placed on separate I-O devices. This completely insulates one database from another. If you must use a single storage controller, use capacity planning so that sufficient bandwidth is available to combine both. For OLTP applications this is rarely an issue because the bandwidth requirements are so low. Adecision support application is completely different. A single decision support workload can generate several hundred megabytes a second of I/O. When you add CPU resources to an instance ofOracle 7, the Oracle engine automatically picks up those resources. Someother databases must be restarted to use the newly available resources. Tounderstand how different databases work within a resource managementenvironment. The SE Toolkit is based on a C language interpreter that is extended to make all theSolaris measurement interfaces available in an easy form. It is used to prototype ideas that can then be promoted for incorporation inproducts, in particular for Sun Enterprise SyMON 2.0 software. The SEToolkit has been jointly developed by Richard Pettit and Adrian Cockcroft as a\u201cspare time\u201d activity since 1993. e code that takesmetrics and processes them is provided as C source code and runs on the interpreter. The business operation can be broken down into several applications such as sales and distribution, e-commerce web service, email, file, and print. Use aform that makes sense to managers and non-technical staff to represent the part ofthe business that is automated by the computer system. The Application Resource Measurement (ARM) standard aims to instrumentapplication response times. Per process activity can be aggregated at aper system level then combined with network measurements to measuredistributed applications. ARM can be used to measure user response time. All vendors support the one standard, and severalplementations exist. Application vendors have shown interest, so more measurements will become available. CCMS is used by several tools such as BMC Best/1 to providemore detailed application-level management than can be done with just system and device-level information. This information is provided by afacility called CCMS. Data is collected on a Netscape 2.5 proxy cache that is serving most of the thousandor so employees in Sun\u2019s United Kingdom facility. SE Toolkitpercollator.se script can parse Netscape and Squid proxy cache log formats andsummarize the data. The cache finish status was analyzed, and operations are divided into four categories. The NO-CHECK and UP-TO-DATE states are cache hits. The WRITTEN,REFRESHED, and CL-MISMATCH states are misses that cause cache writes. TheDO-NOT-CACHE and NON- CACHEABLE states are uncacheable, and anything elseis an error or incomplete transfer. On a quiet weekday, 280,000 accesses went via this cache, and 56 percent of theaccesses went out to the Internet. 34 percent ofthe accesses hit in the cache, 16 percent missed and caused a cache write, 49 percentof the accesses were not cacheable, and 1 percent ended in some kind of error. The plot in FIGURE 5-1 shows bands of transfer times that depend upon the user\u2019s location. Many users are locally connected, but others are operating over slowernetworks. The transfer time includes the time spent waiting for the remote server torespond. Log files are a rich source of performance information. You can apply thistechnique to many other applications, such as ftp and mail servers, as well as toany other application that can write a line to a file. The underlying data structures provided by the Solaris operating environment are described in full in the proc(4) manual page. The data that psuses is called PIOCPSINFO , and this is what you get back from ioctl . The data is slightly different if you read it from the pseudo-file.  p is a pointer to a prpsinfo structure containing at least the following fields. The structure is a typedef structure with a number of fields for each process. For a multithreaded process, you can get the data for each lightweight process separately. There\u2019s a lot more useful-looking information there, but no sign of thehigh-resolution microstate accounting that /usr/proc/bin/ptime and SEToolkit scripts display. proc(4) returns the process usage information; when applied to an   process file descriptor, PIOCUSAGE returns theprocess usage information. The file format is: proc(4), proc(5),proc(6) and proc(7) P points to a pr usage structure which is filled by the operation. The pr Usage structure contains at least the following fields. The psCommand returns usage information for the specific lWP file descriptor. Pr_dftime;   /* data page fault sleep time */    Timestruc_t   pr_slptime;  /* all other sleep time /* kernel page faults sleep time. pr_wtime; /* wait-cpu (latency) time   u_long,   Pr_majf,  /* major page faults, */ pr_nswap, /* swaps */ u_mrcv, u_msnd, u-msnd. PIOCUSAGE can be applied to a zombie process. PIOCRESET can be used to disable microstate accounting. To access process data you must have access permissions for entries in /proc. In the Solaris 2.6 release, you can access the /proc/pid entry for every process. This means that any user can use the full functionality of ps. Microstate accounting is not turned on by default. It slows the system down slightly. Microstate accounting works as follows: A high-resolution timestamp is taken on every state change, every system call, every pagefault, and every scheduler change. Thenormal measures of CPU user and system time made by sampling can be wrong by20 percent or more because the sample is biased, not random. For example, consider a performance monitor that wakes up every ten seconds,reads some data from the kernel, then prints the results and sleeps. Processes that sleep then consume less than their CPU time quanta always run at the highest timeshare priority. The error is an artifact of the dual functions of the clock interrupt. If twoindependent, unsynchronized interrupts are used (one for scheduling and one forperformance measurement), then the errors will be averaged away over time. You can increase the CPU clock rate to get more accuracy. The best solution is to use a microstate accounting-based tool, or to disablesome of the CPUs so that the measurements are made on a fairly busy system. The data provided by the SE processmonitoring class is shown in Figure 5. double elapsed; /* elapsed time for all lwps in process */double total_user;  \u00a0    /* current totals in seconds */ double total_system;doubletotal_child; /* child processes that have exited */double user_time; \u00a0 /* user time in this interval */double system_time, double trap_time and double child_time. ulong outblocks;   /* output blocks/interval - metadata - metadata only - not interesting */ulong vmem_size; \u00a0 /* size in KB */ ulong maj_faults; /* minf/intervals - always zero - bug? */ulONG total_swaps; /* swapout count */long  priority;\u00a0 /* current sched priority */ long  niceness; /* current nice value */char  sched_class[PRCLSZ]; /* name of class */ulongs messages;  /* msgin+msgout/interVAL */ulongo signals;  \ufffd    \u201c  \u201d; /* signals/Interval */ulangs; /* voluntary Most of the data in FIGURE 5-6 is self explanatory. All times are in seconds in double precision with microsecond accuracy. The minor fault counter seems to be brokenbecause it always reports zero. The inblock andoutblock counters only refer to file system metadata for the old-style buffercache. Many processes have very short life spans. You cannot see such processes with ps, but they may be so frequent that they dominate the load on your system. The overhead of collecting accounting data is always present but isinsignificant. Accounting can be started immediately by using the acctcom command. Add some crontab entries to summarize and check the accounting logs. Collecting and checkpointing the accounting data puts a negligible additional load onto the system. The commands reported are sorted by KCOREMIN, which is the product of theamount of CPU time used and the amount of RAM used while the command wasactive. A high factor means that thiscommand hogs the CPU whenever it is running. CHARS TRNSFD counts thenumber of characters read and written.  /usr/lib/acct/monacct. og30    8   10   9   12   13   14   15   16  17  18  19  20  21  22  24  25  257617.65 688.46 16456.60 0.02 88735308080 2649maker4X 10 426182.31 43.77 5004.30 9736.27 4.38 0.01 803267592 3434wabiprog 53 355574.99 44.32 972.44 8022.87 0.05 355871360 570imagetoo 21 25 Resource Management devices (basically, local disk file system reads and writes) The underlying data that is collected can be seen in the acct (4) manual page. See Chapter 7 for more information onSRM Accounting.  /* 3-bit base-8 exponent in the high */ /* order bits, and a 13-bit fraction */ /* in the low order bits. */ /* memory usage in clicks (pages) */ /* ticks */ The Solaris Bandwidth Manager software has built-in support for Cisco NetFlow\u2122software. This feature allows for detailed network measurements that can be sent to other software packages. NetFlow-enabled devices send out NetFlow datagrams, which contain records for one or more flows. Combining multiple flow records in one datagram reducesnetwork overhead caused by NetFlow. The NetFlow FlowAnalyzer application uses the output from NetFlowFlowCollector. It provides elaborate processing, graphing, and reporting options for network analysis, planning, troubleshooting and more. There are six basic disk access patterns. Read, write, and update operations caneither be sequentially or randomly distributed. This section explains the basic Solaris softwaremeasures and discusses more complex disk subsystems. You cannot automatically tell which processes are causing disk activity. You may be able to work out where the workloadcomes from by looking at how an application was installed. When a large number ofdisks are being reported, the iostat -x variant provides extended statistics. The Starfire system supports a maximum configuration of several thousanddisk drives. When more than one type of data is stored on a disk, it\u2019s  to read because each disk is summarized on a separate line. The size of eachdisk is also growing. Solaris 2.6 has a number of new features to help solve problems. It is now possible to separate root, swap, and home directory activity. Full data is saved from the first SCSI probe to a disk. iostat -M shows throughput in Mbytes/s rather than K bytes/s for high-performance systems. Dead or missing disks can still be identified because there is no need to send themanother SCSI probe. Another option ( -n) translates disk names into a more useful form. Tapes are instrumented in the same way as disks; they appear in sar andiostatautomatically. Tape read/write operations are instrumenting with all the samemeasures that are used for disks. Rewind and scan/seek are omitted from these. The output format and options of sar(1) are fixed by the generic UNIX standard. In the Solaris 2.6release, existing iostat options are unchanged. New options that extendiostat are as follows. Solaris 2.5 includes a self-describing trace output format. A set of libraries allows user-level programs to generate trace data. The trace data helps analyze and debug multithreaded applications. Device statistics: Device Not Ready: 0 No Device: 0 Recoverable: 0Illegal Request: 0 Predictive Failure Analysis: 0. 0. 0000000%iostat -E deities. The tnfxtract routin controls probe execution for both user and kernel traces. While user-level probes canwrite to trace files, the kernel probes write to a ring buffer. This buffer scheme avoids any need tolock the data structures, so there is no performance loss. The command sequence to initiate an I/O trace is quite simple. You run thecommands as root, and you need a directory to hold the output. Thetnfdump program does quite a lot of work to sort the probe events into time order. In the other window we extracted and dumped the data to take a look at it. To really understand the data presented by iostat ,sar, and other tools, you must look at the raw data being collected. A standard disk is SCSI based and has an embedded controller. The diskdrive contains a small microprocessor and about 1 Mbyte of RAM. It can typicallyhandle up to 64 outstanding requests via SCSI tagged-command queuing. In large systems,there is another level of intelligence and buffering. The same reporting mechanism is used for client side NFSmount points and complex disk volumes setup using Solstice\u2122 DiskSuite\u2122software. In the old days, if the device driver sent a request to the disk, the disk would donothing else until it completed the request. Disks that spin fasterand seek faster have lower service times. The problem with iostat is that it tries to report the new measurements in some of the original terminology. The \u201cwait service time\u201d is actually the time spent in the\u201cwait\u201d queue. This is not the right definition of service time in any case. The SE Toolkit uses the kstat (3K)-based data structure. The underlying metrics are cumulativecounters or instantaneous values. We need to take two copies of the above data structure together with ahigh resolution timestamp for each and do some subtraction. Ong    number$;   /* linear disk number */   string   name$;\u00a0 /* name of the device */ /* number of bytes read */\u00a0 ulonglong nread; \u00a0 /* number  of bytes written */\u00a0  longlong  wtime; /* wait queue - time spent waiting */\u00a0 longlong wlentime; /* active/run queue - sum of queue length multiplied by time at that length */}; We assume that all disk commands complete fairly quickly. We obtain the utilization or the busy time as a percentage of the total time. A similar calculation gets us the data rate in kilobytes per second. The meaning of Srunis as close as you can get to the old-style disk service time. The disk can run more than one command at a time. The data structure contains an instantaneous measure of queue length, but we want the average over the time interval. We get this from that strange \u201clength time\u201dproduct by dividing it by the busy time. The Solaris 2.6 disk instrumentation is complete and accurate. Now that it has been extended to tapes, partitions, and client NFS mount points, there much more can bedone with it. As long as a single I/O is being serviced at all times, a single queue is being used. When the device being monitored is an NFSserver, hardware RAID disk subsystem, or a striped volume, it is clearly a muchmore complex situation. All of these can process many requests in parallel. In practice, some other effects come into play. The drives optimize head movement, so that as the queue gets longer, the average service time decreases. In effect, the load oneach disk is divided by the number of disks. Using SNMP counters is a good way to get an overall view of network throughput. The Solaris software provides an SNMP daemon which provides the dat. Most networking devices support SNMP , and the SyMON software can incorporateany third-party MIBs so all links from a switch can be monitored at the same time. Many other commercial and free applications and utilities manage SNMP devices. The collision rate is (Collis / Opkts) * 100% . In our case, that is less than a tenth of a percent. Collisions are absolutely normal and should cause no concernunless collision rates become very high (in the order of 10 to 20 percent or higher) Most of these statistics are related to the Ethernet MAC and the network interfacecard hardware itself. Every network interface card typically has different counters, which canchange with the operating system releases. Sun does not officially support thenetstat option. The SE Toolkit script nx.se lists TCP as if it were an interface, with input and output segment and data rates. For interfaces that provide this information (at present, only leandhme)nx. se reports kilobytes in and out. % /opt/RICHPse/bin/se nx.se.se current. sing in the TCP/IP stack and lack of buffering on input. Defr shows thenumber of defers that took place. A defer happens when an Ethernet tries to sendout a packet, but it finds the medium to be busy. Solaris Bandwidth Manager is a tool for configuring Solaris. The main purpose of Alternate Pathing is to sustain continuous network and disk I/O when system boards are detached. Short jobs are said to backfill processors reserved for large jobs. Blacklist is a file that enables you to specify components, such as system boards, that should not be configured into the system. Class Based Queuing (CBQ) is the underlying queuing technology used in Solaris BandwidthManager. The classifier analyzes the packet protocol,ToS value, URL information, source information, destination information and allocates the packet to a class queue. E servers use the intercacheprotocol (ICP) to talk among themselves and form an explicit hierarchy ofsiblings and parents. If the load would overwhelm a single server or if highavailability is important, multiple servers are configured as siblings. The CommonInformation Model provides a conceptual framework within which it ispossible to organize information about a managed environment. The Oracle8 iDatabase Resource Manager enables the administrator to limit the degree of parallelism of any operation. DDI/DKI is specified in the \u201cWriting Device Drivers\u2019 section of the DriverDeveloper Site 1.0 AnswerBook. These are function call entrypoints that device drivers should implement in order to support DR. DDI_SUSPENDsuspends the drivers to begin the quiesce period. DDI_RESUMEresumes the drivers after the quyingce period, and so on. The goal of this group is tooffer a standard information model and directory schemas. DIMM Dual In-Line memory Module. Diff-Serv addresses network management issues related to end-to-end Quality of Service (QoS) within diverse and complex networks. The set of system resources it understands is host (by name),system architecture, operating system type, amount of memory, and CPU usage. Thus, if the external orstandard service class goal is not being met, the associated DISCs can bemanaged. Dynamic SystemDomains (DSD) Starfire independent hardware entities formed by the logical association of its system boards. rvers which allows system boards to be added (attached) orremoved (detached) from a single server or domain. LSF uses the Load Information Manager (LIM) as its resourcemonitoring tool. To modify or add load indices, an Extended Load InformationManager can be written. Fairsharescheduling divides the processing power of the LSF cluster among users. The FlowCollector aggregates this data, doespreprocessing and filtering, and provides several options to save this data todisk. Other applications such as network analyzing,planning, and billing can use these files as input. Hierarchical fairshare enables resources to be allocated to users in a hierarchical manner. A heavily damped system tends to be sluggishand unresponsive when a large time constant is used. Computer-oriented local and wide area networks are normally managed usingSNMP protocols. Both products provide some visibility intowhat is happening in the computer systems on the network, but they arefocused on network topology. Intimate sharedmemory (ISM) A way of allocating memory so that it can\u2019t be paged. The sharedmemory area is often the largest component of a database\u2019s memoryrequirements. ISP Internet Service Provider, a company that provides Point-of-Presence access to the Internet. JTAG is an alternate communicationsinterface between the SSP machine and the Enterprise 10000 server. A special data structure that controls the dynamic growth of all non-relocatable memory. When Dynamic Reconfiguration(DR) is used to detach a system board containing the kernel cage, it isnecessary to quiesce the operating system. The Load InformationManager process running on each execution host is responsible for collectingload information. The load indices that are collected include: host status,length of run queue, CPU utilization, paging activity, available swap space,available memory, and I/O activity. The Load Share Facility is a vehicle for launching parallel applications on an HPC cluster. When the LSF softwareinitializes, one of the nodes in the cluster is elected to be the master host. maximum bandwidth The amount of spare bandwidth allocated to a class by the Solaris BandwidthManager. The maximum bandwidth is dependent upon the percentage ofbandwidth the class can borrow. Microstate accounting provides much greater accuracy thansampled measurements. mestamp is taken on every state change, every system call, every page fault, and everyscheduler change. A database topology where a single process serves many users. NQS/Exec is geared toward a supercomputer environment. Limitedload balancing is provided as there is no concept of demand queues. There is also no control over interactivebatch jobs. PC NetLink adds file and print services, andenables Solaris servers to act as Microsoft\u00ae Windows NT\u2122 Primary DomainControllers (PDC) or Backup Domain Controllers (BDC) PC NetLink 1.0 offers many new options for utilizing hardware resources and minimizing systemadministration overhead. A policy element contains single units of information necessary for the evaluation of policy rules. Examples of policyelements include the identity of the requesting user or application, user orapplication credentials, and so forth. LSF provides several resource controls toprioritize the order in which batch jobs are run. Batch jobs can be scheduled torun on a first come first served basis, fair sharing between all batch jobs, andpreemptive scheduling. Process Monitor is an optional module within Sun Enterprise SyMON that can be used to view all processes on a system. The Process Monitor can also be configured topattern match and accumulate all the processes that make up a workload. Project StoreX is based on adistributed pure Java framework. It can run on servers from any vendor, interface to other storage management software, and manage any kind of attachment.  Proxy caches are used in corporate intranets and at ISPs. Solaris Bandwidth Manager provides the means to manage your network resources to provide Quality of Service to network users. receptor DSD Dynamic Reconfiguration (DR) on the Starfire allows the logical detachment of a system board from a provider DSD. The same system board is then attached to a receptorDSD. The Solaris Resource Manager (SRM) ensures that a particular user or application receives its fair share of resources. It requires each hopfrom end-to-end be RSVP-enabled, including the application itself. Solaris software can provide a great deal of per-process information that is notcollected and displayed by the ps command or Sun Enterprise SyMON 2.0software. The data can be viewed and processed by a custom written processmonitor. Service Level Agreement captures expectations and interactions between end users, system managers,vendors, and computer systems. In essence, this is analogous to the SRMlnode, which effectively defines a resource management policy that can be besubscribed to. The time it takes for an I/O device to service a request can be complex tomeasure. The SolarisResource Manager (SRM) is based on ShareII. SHR Scheduler is a component of the Solaris Resource Manager (SRM) Users are dynamically allocated CPU time in proportion to thenumber of shares they possess. Solaris Resource Manager software is the key enabler for serverconsolidation and increased system resource utilization. Service LevelAgreements can be defined and translated into Solaris Bandwidth Managercontrols and policies. Computer-oriented local and wide area networks are normally managed using SNMP protocols. Resource management is done on a per-networkbasis, often by controlling the priority of data flows. The SSP configures the Starfire hardware, through a private ethernetlink, to create domains. The SSP collects hardware logs, provides bootfunctions, and produces consoles for each domain. Sun Enterprise 10000 is a highly scalable 64-processor (UltraSparc II) SMP server with up to 64 Gbytes of memory and over 20 Tbytes of disk space. Sun Enterprise SyMON 2.0 is a Java-basedmonitor with multiple user consoles that can monitor  storage hardware. System level measurements show the basic activity and utilization of the memory system and CPUs. Perprocess activity can be aggregated at a per system level then combined withnetwork measurements to measure distributed applications. Trace normal form (TNF) is a format used to implement tracing. It makes it possible to trace the execution steps of user and kernel processes. ToS is a header field contained in IP packets. Virtual memory is not directly related to physical memory usage. Virtual web hosting is often used in situations where web sites receive little or no activity most of the time.  \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0WLM SeeIBM Workload Manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0XCF SeeCross-System Coupling Facility. a large geographical area. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0W LM SeeIBm Workload manager. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 XCF See cross-system coupling facility."
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_book_29",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: Java and the Internet",
      "source": "Prentice Hall - adrianco and Rich Pettit",
      "published_date": "1998",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_book_30",
      "kind": "book",
      "subkind": "",
      "title": "Sun Performance and Tuning: SPARC and Solaris",
      "source": "Prentice Hall - adrianco",
      "published_date": "1995",
      "url": "",
      "content": {},
      "tags": [
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_story_31",
      "kind": "story",
      "subkind": "",
      "title": "Microservices and DevOps",
      "source": "Infoq Charles Humble",
      "published_date": "7/11/2014",
      "url": "https://www.infoq.com/interviews/adrian-cockcroft-microservices-devops/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_story_32",
      "kind": "story",
      "subkind": "",
      "title": "High availability",
      "source": "Richard Seroter on Infoq",
      "published_date": "",
      "url": "https://www.infoq.com/articles/cockcroft-high-availability/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_33",
      "kind": "story",
      "subkind": "",
      "title": "Failing Over Without Falling Over",
      "source": "Stackoverflow",
      "published_date": "2020",
      "url": "https://stackoverflow.blog/2020/10/23/adrian-cockcroft-aws-failover-chaos-engineering-fault-tolerance-distaster-recovery/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_36",
      "kind": "youtube",
      "subkind": "",
      "title": "Adrian Cockcroft Appearance Playlist on YouTube",
      "source": "Many different accounts - technical content",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8Ce-3VPplFg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_37",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Enterprise Indigestion at NVIDIA GTC",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_38",
      "kind": "story",
      "subkind": "",
      "title": "SC23 update",
      "source": "InsideHPC",
      "published_date": "2023",
      "url": "https://insidehpc.com/2023/11/sc23-top500-trends-the-ai-hpc-crossover-chiplet-standardization-the-emergence-of-ucie-and-cxl-advancements/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_39",
      "kind": "story",
      "subkind": "",
      "title": "SC22 predictions",
      "source": "InsideHPC",
      "published_date": "2022",
      "url": "https://insidehpc.com/2022/12/sc22-cxl3-0-the-future-of-hpc-interconnects-and-frontier-vs-fugaku/",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_40",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Why did Docker Catch on Quickly and Why is it so Interesting",
      "source": "The New Stack",
      "published_date": "2014",
      "url": "https://thenewstack.io/why-did-docker-catch-on-quickly-and-why-is-it-so-interesting/",
      "content": {
        "text": "\n\nDocker has rapidly become \u201cthe new thing\u201d for computing in 2014. Why did that happen so quickly, and how has a Linux-application container become so popular? Broadly, I think, Docker is a great example of how to build a viral, developer-oriented product.\n\nA developer can figure out what Docker does, install it and do something useful with it in 15 minutes. I first heard this \u201crule\u201d from Marten Mickos when talking about why MySQL was so successful: low friction to try it out, a simple concept and useful functionality.\n\nDocker is a great name and it has a cute logo. It resonates with what the product does and is easy to remember. Engineering-oriented founders sometimes seem to think that names and logos don\u2019t matter if the product is good enough, but a great name can turbocharge adoption and build a valuable brand.\n\nThe Docker product came from a non-threatening source, a small startup (DotCloud) that was able to broadly partner across the whole industry. If the same product had come from an established enterprise technology player, there would have been much more push-back from that player\u2019s competitors, and the market would probably have split into several competing technologies.\n\nThe rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. So everyone is having to cooperate around a single container solution. This is great for the developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny, somewhat, as Docker did an end-run around their strategy.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nEven if you don\u2019t know or care what Docker is, I think it offers a very relevant playbook for how to build a developer-led business model.\n\nGet ubiquity first, become the category leader, then convert that into business value and revenue opportunities later.\n\nThat leaves the remaining question of, what does Docker do that is interesting, and who might it compete with? I have four separate answers:\n\nPortability : Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. This is a similar benefit to the Java \u201cwrite once, run anyware\u201d idea, but is more robust and is generalized to \u201cbuild anything once, run anywhere.\u201d\n\nSpeed : Start-up time for a container is around a second. Public-cloud virtual machines (VMs) take from tens of seconds to several minutes, because they boot a full operating system every time, and booting a VM on a laptop can take minutes. To counter this advantage, VMware has just announced (but not shipped) a technology called Fargo, that clones an existing VM in a second or so.\n\nConfiguration : The Docker container captures the exact configuration of a version of an application. To upgrade the application in production, the container is usually replaced with a new version, which takes a few seconds. The layers of components that go into the configuration are kept separate and can be inspected and rebuilt easily. This changes configuration management to be largely a build-time activity, so, for example, a Chef recipe might be used to build a Docker container, but at runtime there is no need to use the Chef services to create many identical copies of a Docker container. Used in this way, Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack.\n\nDocker Hub App-store : Docker containers are shared in a public registry at hub.docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Because containers are very portable, this provides a very useful cross platform \u201capp store\u201d for applications and component microservices that can be assembled into applications. Other attempts to build \u201capp stores\u201d are tied to a specific platform (e.g., the AWS Marketplace or Ubuntu\u2019s Juju Charms) or tool (e.g., the Chef Supermarket), and it seems likely that Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities.\n\nOne reason Docker is interesting is that all four answers are each individually useful, but can be used in combination. This causes cross-pollination of ideas and patterns. For example, someone might start using Docker because they like the speed and portability, but find that they end up adopting the configuration and Docker hub patterns as well.\n\nThe Docker technology is still fairly new; work is underway to add missing features, and a large ecosystem of related projects and companies is forming around it. There\u2019s a lot of interest in the technology from the VC community, as we try to figure out whom to fund to do what, and how the story will play out in the longer term.\n\nAdrian Cockcroft is a technology fellow\u00a0at Battery Ventures.",
        "summary": "Docker is a great example of how to build a viral, developer-oriented product. A developer can figure out what Docker does, install it and do something useful with it in 15 minutes. The rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. This is great for developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny. Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack. Docker containers are shared in a public registry at hub. docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities. There\u2019s a lot of interest in the technology from the VC community.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "containers",
        "docker",
        "microservices",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_41",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "A Conference to Discuss Developer-Driven Infrastructure",
      "source": "The New Stack",
      "published_date": "2015",
      "url": "https://thenewstack.io/a-conference-to-discuss-developer-driven-infrastructure/",
      "content": {
        "text": "\n\nCompetitive pressures have pushed speed of development to be one of the highest\u00a0priorities for business today. Improved tools and techniques have moved the state\u00a0of the art in agile development from monolithic updates every few weeks, to\u00a0continuous delivery of microservices several times a day. The move to \u201crun what\u00a0you wrote\u201d and developer-driven infrastructure means that developers are not only\u00a0delivering products faster, but are also responsible for the efficiency and safety of those\u00a0products in production. Lean development techniques that take waste out of the\u00a0process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. The security blanket approach of wrapping a firewall around an\u00a0insecure monolithic product has been replaced by penetration testing of every\u00a0microservice, sophisticated use of identity and access management mechanisms,\u00a0encryption as the default and fine-grain security key management.\n\nThese three core areas form a story arc through GOTO London: Agile, Lean and Rugged. In each of them we will discuss the state of the art and emerging directions\u00a0that will set you up with a broad view of these key concerns for developers in 2016.\n\nThis is the first ever GOTO London conference. The core story is being structured as\u00a0a single track. Many conferences have an initial keynote session for everyone,\u00a0followed by a wide selection of talks running at the same time. This forces you to\u00a0decide what to miss and move from room to room. It also means that an individual\u00a0track has trouble building and maintaining context, and encourages repetitive, self-contained presentations. The approach is great for variety, but can be frustrating.\n\nFor GOTO London, we will start the week with two optional training days on Monday and Tuesday, Sept. 14-15, then kick off the event with two full days of single-track talks Wednesday and Thursday, Sept. 16 -17. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences,\u00a0presentations about open source tools and talks about the latest hot products.\n\nFor the single-track days, everyone will be in the same room, building up a shared\u00a0context as the curated story arc unfolds. You won\u2019t miss anything, you won\u2019t waste\u00a0time moving from room to room. You will get to know each other and the speakers\u00a0much better. The talks will be a bit shorter, more focused, and will build on each other, reducing repetition and elementary introductions.\n\nThere is a repeated pattern for the single-track days. In each half day, there will be\u00a0four half-hour talks, followed by a half-hour panel session with all four speakers\u00a0discussing each other\u2019s talks with each other and the audience.\n\nAfter leading a workshop on microservices, I\u00a0will also introduce the\u00a0conference as a whole on Wednesday morning.\n\nIntroducing Agile , Dan North will\u00a0focus on the impact of new ideas and tooling supporting faster development.\n\nNicole Forsgren, Ph.D., will introduce\u00a0Lean, showing how DevOps culture and practices\u00a0reduce waste and improve outcomes.\n\nIntroducing Rugged , Joshua Corman will discuss how developers can build the core concepts of security into their\u00a0applications to protect information from attack. These four speakers will end the\u00a0morning with the first panel session.\n\nThe afternoon continues with four speakers digging deeper into the relatively\u00a0unfamiliar territory of Rugged systems.\n\nWhat kind of attacks are we seeing? What\u00a0tools are available for developers to automate security testing?\n\nHow can we\u00a0manipulate keys, identity and access, safely and easily?\n\nThe afternoon ends with\u00a0Joshua Corman joining the four speakers for a panel discussion.\n\nThe first evening is the conference party, with an entertaining and informative\u00a0keynote presentation by Ines Sombra and Adrian Colyer. Ines runs the San Francisco Papers-We-Love meetup, and Adrian publishes a daily blog post called The Morning\u00a0Paper. Over the last year, there has been a rush of interest in academic research\u00a0papers resulting in Papers-We-Love meetups all over the world. People are finding\u00a0hidden gems, radical ideas, or fundamental turning points in computer science, and\u00a0having fun presenting their own interpretations. We hope you will be inspired to\u00a0attend your local meetup, read The Morning Paper and share your own ideas.\n\nThe second day starts with four presentations on Lean. The Lean Enterprise book\u00a0has become required reading as these ideas spread into the mainstream. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer as their top priority. We\u00a0will also look at developer-oriented techniques for measurement, analysis and\u00a0optimization. Nicole will return to lead the panel session.\n\nDigging into Agile, we will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave , and see what it takes for a startup to \u201ccatch and surf the\u00a0wave\u201d when a product goes viral and everyone else dives in. At the other extreme, we will examine the latest ideas to speed up development at a financial institution\u00a0and for embedded hardware, and finish with Rachel Davies on Extreme\u00a0Programming in the 21st Century. Dan North then leads the closing panel.\n\nWe wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups.\n\nThe final day of the conference puts the concepts into practice, with deep dives into\u00a0open source tools and the latest products. The multi-track talks are longer, with\u00a0more time between them to switch rooms and for hallway conversations.\n\nWe expect that GOTO London attendees will have an enjoyable and memorable\u00a0experience learning the concepts and tools needed to be agile, lean and rugged for\u00a02016 and beyond.\n\nWeaveworks is a sponsor of The New Stack.\n\nFeatured image via Flickr Creative Commons.",
        "summary": "Developers are responsible for the efficiency and safety of products in production. Lean development techniques that take waste out of the process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. This is the first ever GOTO London conference. The core story is being structured as a single track. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences, presentations about open source. There is a repeated pattern for the single-track days. In each half day, there will be four half-hour talks. The talks will be a bit shorter, more focused, and will build on each other. The first evening is the conference party, with an entertaining and informative presentation by Ines Sombra and Adrian Colyer. Over the last year, there has been a rush of interest in academic research papers resulting in Papers-We-Love meetups all over the world. The second day starts with four presentations on Lean. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer. We will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave. We wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups. The final day of the conference puts the concepts into practice, with deep dives into open source tools and the latest products.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "agile",
        "architecture",
        "devops",
        "docker",
        "infrastructure",
        "lean",
        "microservices",
        "security"
      ]
    },
    {
      "id": "virtual_adrianco_story_42",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "The Rise of Continuous Resilience",
      "source": "The New Stack",
      "published_date": "2020",
      "url": "https://thenewstack.io/the-rise-of-continuous-resilience/",
      "content": {
        "text": "\n\nAmazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21.\n\nI sometimes ask a CIO whether they have a backup datacenter. Most will say yes, as it\u2019s a normal part of a business continuity plan for disaster recovery. In some industries, like financial services, it\u2019s a regulated requirement and there\u2019s an annual visit from an auditor to make sure it\u2019s in place.\n\nWhen I ask how often they test their failover process, people start to look uncomfortable. Some admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. When I ask what the failover test looks like, it\u2019s normally one application at a time, in a planned migration from the primary data center to the backup. It\u2019s rare for people to test an entire data center by cutting its power or network connections at an arbitrary time. I did hear once from a bank that had two data centers and switched between them every weekend so that one data center was primary on even-numbered weeks and the other was primary on odd-numbered weeks. If they ever had a problem mid-week, they knew what to do and that they could rely on it working smoothly. I\u2019ve asked this question a lot over several years, and have had only a handful of good answers.\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nThe combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d\n\nDisasters don\u2019t happen very often, but unfortunately, when data centers lose power, drop network connectivity, lose their cooling system, catch fire or are flooded, the whole data center goes offline. Usually at an inconvenient time, with little or no warning. During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater, and they still don\u2019t work once the water recedes \u2014 as they are then full of mud and other debris. Even if the data center isn\u2019t in the basement, sometimes the backup generators are, or the fuel tanks for the generators, or some critical network equipment. There are lots of examples of disasters like this in the press, so why are so many companies that have a business continuity plan in the news when disaster strikes?\n\nThe short answer is that it\u2019s hard to get a disaster recovery data center implemented, and too much work and too risky to test it frequently. Each installation is a very complex, fully customized \u201csnowflake.\u201d The configuration of the two data centers drifts apart, so that when the failover is needed the failover process itself fails and the applications don\u2019t work. Even such basic things as backups for data need to be tested regularly, by attempting the restore process. I\u2019ve heard of some embarrassing data loss situations, where the backups failed and this wasn\u2019t discovered until months later when a database failed and a restore was needed.\n\nI call this \u201cavailability theater\u201d \u2014 everyone is going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nSo how can we make this better? There are two technology trends coming together to create a more productized solution, that is tested frequently enough to be sure it\u2019s going to work when it\u2019s needed. The combination of cloud computing and chaos engineering is leading to \u201c continuous resilience. \u201d\n\nThe last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. They ended up doing region evacuation testing about once every two weeks.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nAvailability Theater: Everyone going through the motions as if they had a real disaster recovery plan, but it\u2019s all play-acting.\n\nIn these tests, they drain all the traffic from a region and show that they can still run Netflix on the two remaining regions; and no-one notices! (Netflix operates from AWS regions in Virginia, Oregon and Dublin). The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope \u2014 a \u201c Chaos first \u201d policy.\n\nThe reason Netflix was able to implement this is that cloud regions are different to data centers in two critical ways. Firstly, they are completely API driven, and the entire state of an AWS account can be extracted and compared across regions. Secondly, the versions and behaviors of the AWS services in each region don\u2019t drift apart over time the way data centers do.\n\nMost customers will have a mixture of multiregion workloads. Some customer-facing services, like a mobile back-end that needs to be online all the time, can be built active-active, with traffic spread across multiple regions. Workloads like a marketplace, which needs a consistent view of trades, are more likely to be operating in a primary region \u2014 with failover to a secondary, after a hopefully short outage.\n\nNetflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong, and to make sure that the system has enough resilience to absorb failures without causing customer-visible problems. Nowadays more companies are setting up chaos engineering teams, hardening their systems, running game day exercises, and using some of the chaos engineering tools and services that are developing as the market matures.\n\nAWS has been investing in our services to provide support for multiregion applications, for both active-active operation and primary-secondary failover use cases. In the last few years, we\u2019ve added cross-region data replication and global table support to Amazon S3, DynamoDB, Aurora MySQL and Aurora Postgres . AWS also acquired a disaster recovery service called CloudEndure , which continuously replicates application instances and databases across regions. We\u2019ve also extended AWS Cloudwatch to support cross-account and multiregion dashboards .\n\nAs usual, we are listening to what our customers ask for, and our partners and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.\n\nReaders might also like the Architecting resilient systems on AWS session from AWS re:Invent 2019.\n\nFeature image via Pixabay.",
        "summary": "Amazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21. Some CIOs admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. The combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater. It's hard to get a disaster recovery data center implemented. Each installation is a very complex, fully customized \u201csnowflake.\u201d Even such basic things as backups for data need to be tested regularly. The last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. Netflix operates from AWS regions in Virginia, Oregon and Dublin. The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope. Netflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong. Nowadays more companies are setting up chaos engineering teams. AWS Cloudwatch extended to support cross-account and multiregion dashboards. Partner and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "netflix",
        "resilience"
      ]
    },
    {
      "id": "virtual_adrianco_story_43",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Nvidia GTC: Hyperscaler Happiness and Enterprise Indigestion",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/nvidia-gtc-hyperscaler-happiness-and-enterprise-indigestion/",
      "content": {
        "text": "\n\nSAN JOSE, Calif. \u2014 On Monday, March 18, Jensen Huang , the CEO of Nvidia gave his annual keynote at the company\u2019s GTC conference here, setting out their technology roadmap for the next year. Every large technology company does an event like this, but right now Nvidia is leading the industry , and speeding up their release cadence to a point that is hard for everyone to keep up with. It\u2019s hard for competitors, hard for standards bodies, and especially hard for enterprises that are trying to find their feet in the new world of AI deployments. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware , and everyone else just has to try and keep up. I think this is the most significant computing technology announcement we are likely to see this year.\n\nThe GPU servers that are currently being used in volume production such as the AWS p5 have two Intel CPUs and eight Nvidia H100 GPUs as the nodes that can be clustered. The eight GPUs share memory via NVlink at 900GBytes/s, but the Intel CPU doesn\u2019t have an NVlink interface. Larger configurations are linked by eight 400Gbit/s Ethernet or Infiniband networks per node, and with hundreds to thousands of GPUs there is a lot of network traffic.\n\nTo speed up AI training workloads on the network both NVswitch and Infiniband switches have built-in processing power to efficiently perform operations like centrally averaging the output of all the GPUs. This was announced a few years ago as the Scalable Hierarchical Aggregation and Reduction Protocol ( SHARP ) architecture, but it\u2019s not clear to me how heavily it\u2019s being used.\n\nLast year in his keynote Huang announced the Grace Hopper GH200 combined CPU/GPU architecture. The Grace CPU is the first by Nvidia to use Arm architecture rather than Intel and has an NVlink interface to couple it directly to the Hopper H100-based GPU.\n\nNVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of CPU-hosted memory, and 24 Terabytes of GPU-hosted high bandwidth memory, as a single shared memory system image.\n\nThis very large memory is needed to train the largest models, as fitting the model into GPU memory is a big bottleneck for training. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect, with the CPUs around the edge. This is reversed from more conventional architectures where the CPUs are driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when a GH200 GPU wants to send data to another GH200 GPU, it goes directly, by writing to shared high-bandwidth memory. This is far faster than an H100 sending via a PCI bus to an Intel CPU then over a network to another Intel CPU then via PCI to the other H100 GPU.\n\nGH200 Now Shipping\n\nGH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published, although we\u2019ve heard that the hardware and software stack is currently not quite ready for mainstream production use.\u00a0Nvidia told me:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nThis year Huang announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer.\n\nTo get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together with a 10 Terabyte/s chiplet interface, along with 192 Gbytes of high bandwidth HBM3e memory chiplets inside the package for a total of 208 billion transistors.\n\nBlackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU). There are 480 GBytes of memory on Grace, and 384 Gbytes of high bandwidth memory on the two Blackwells, for a total of 864 GBytes in each GB200.\n\nThe main performance changes from GH100 to GB200 are four times higher performance for AI training workloads, 30 times higher for inference workloads, and 25 times better energy efficiency overall for the Blackwell GPU. The raw speed is 10 Petaflops of FP8 and 20 Petaflops of FP4. NVlink doubles its bandwidth to 1.8 Terabytes/s, and the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576.\n\nThe internal architecture for training workloads is similar between Hopper and Blackwell, but by getting two Hopper-equivalent chiplets into each package, and having two packages per GB200 module, there\u2019s four times the performance for FP8 compared to GH200. Blackwell adds FP4 for inference that doubles the performance (which gets us to 8x) and there are additional optimizations for inference that give rise to the 30x claim. The 25x energy efficiency claim appears to be a blended mix of the 30x inference and 4x training workloads.\n\nNetting this out, on a GPU-to-GPU basis, Blackwell is twice the performance of Hopper for training.\n\nConnecting It All\n\nTo connect this all together there\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8 for computing shared averages between GPUs.\n\nThere\u2019s an entry-level DGX B200 system that\u2019s air-cooled and is rated at half the performance of the GB200 node, it\u2019s designed as a plug-in replacement for the existing H100-based systems, with eight single Blackwell GPUs and two Intel CPUs.\n\nThe GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards each with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The NVL72 is designed to support trillion parameter model training and inference and will be available first from AWS , Azure and Google Cloud later in 2024, then via the usual partners.\n\nEight of the NVL72 racks can be interconnected using NVlink to make the full-size 576 GPU DGX SuperPod with 240 Terabytes of memory in a single shared domain. Using Infiniband to cluster these together in 72 or 576 GPU-sized memory domains, tens of thousands of GPUs can be used to train the largest models.\n\nThere was also a doubling in the bandwidth of the Infiniband and Ethernet product line to 800Gbit/s. Quoting Nvidia:\n\n\u201cThe Quantum-X800 platform sets a new standard in delivering the highest performance for AI-dedicated Infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC, which together achieve an industry-leading end-to-end throughput of 800Gb/s. This is 5x higher bandwidth capacity and a 9x increase of 14.4Tflops of In-Network Computing with Nvidia\u2019s Scalable Hierarchical Aggregation and Reduction Protocol (SHARPv4) compared to the previous generation.\n\nThe Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. Utilizing the Spectrum SN5600 800Gb/s switch and the Nvidia BlueField-3 SuperNIC, the Spectrum-X800 platform provides advanced feature sets crucial for multitenant generative AI clouds and large enterprises.\u201d\n\nI\u2019m impressed at both the pace of development and the extremely high performance of the systems and interconnects. The main challenge appears to be building software that can operate these systems reliably and keep them running. That\u2019s what I\u2019d expect for an architectural transition like this, which highlights the biggest issue with very large coherent memory clusters, failure rates increase proportionally to the cluster size, and are likely to crash the entire node.\n\nThat\u2019s annoying when a node is eight H100 GPUs and you lose one of 32 nodes in a 256GPU system, but it\u2019s a much bigger issue if an entire 256 GH200 SuperPOD fails. Each Blackwell chip includes extensive automated self-test and predictive failure modeling to help offset the large size of the system.\n\nThe 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit, and an industry-leading capacity to train extremely large AI models efficiently. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain size.\n\nDeal with AWS\n\nThe previously announced deal with AWS to deploy a cluster of GH200s for Nvidia called Project Ceiba has been upgraded (and delayed somewhat) to be a GB200-based system with over 20,000 GPUs instead. This raises an interesting point, since the GB200 is so much faster than the GH200, and follows on perhaps a year behind it, how many customers will want to wait and switch their orders to the GB200?\n\nThere\u2019s currently a supply shortage so everyone is waiting for deliveries for many months anyway. For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. As more workloads go to production and inference for very big models becomes more important, moving to Blackwell is going to make a lot more sense.\n\nThe bigger challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years.\n\nThat makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation. Old GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU.\n\nNew Capabilities for Building AI Apps and More\n\nHuang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services but has also packaged up individual LLM components from many partners into deployable microservice containers called Nvidia Inference Microservices \u2014 NIMs, which will be available from ai.Nvidia.com and in online marketplaces.\n\nThese include Helm charts for deployment via Kubernetes clusters. This is a very helpful capability, as managing AI applications that keep breaking as the software stack they depend on gets old, month by month, is a big headache. Having Nvidia or its partners package up and maintain a well-tested stack as a container seems valuable.\n\nThere were a lot more announcements for markets like automotive, healthcare, quantum computing, digital twins, and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new hardware specifications and the interconnect. They go far beyond the industry standard designs from the CXL consortium, based on extensions of the PCI bus interface, that are an order of magnitude less bandwidth than NVlink and several years later in its development process.\n\nI\u2019ve been excited to see the development of CXL over the last few years, it has its place, for memory pooling and more dynamic fabric management, but it\u2019s not going to be competitive for GPU-based AI workloads. Enterprises that want to buy standard interfaces and have a long life for their installed GPU hardware are currently out of luck.\n\nI\u2019ve been predicting the onset of very large memory systems for several years, and I named the architecture pattern Petalith, for petascale monolith . I think it will take a while for the software to catch up and become optimized for these systems, but it will unlock the ability to build new kinds of applications, not just for AI models, but including models as a common building block.\n\nIt\u2019s impressive to see Nvidia executing well, the people I know who work there are happy, their market capitalization has grown a lot recently, and the rest of the industry is trying to figure out how to compete.\n\nArm, AWS and Google are sponsors of The New Stack.",
        "summary": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "containers",
        "infrastructure",
        "kubernetes",
        "microservices",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_story_44",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Sustainability: Comparing AWS, Azure and Google",
      "source": "The New Stack",
      "published_date": "2024",
      "url": "https://thenewstack.io/sustainability-how-did-amazon-azure-google-perform-in-2023/",
      "content": {
        "text": "\n\nThe leading cloud providers and the mammoth global companies that run them are adopting environmentally sustainable practices, but it is a complicated work in progress.\n\nAs Amazon , Google and Microsoft continue to grow, they\u2019re all grappling with the challenges of reducing their carbon emissions in the f ace of increased resource usage of AI, while making the right investments in renewable energy infrastructure around th\u200b\u200be world.\n\nCustomers of cloud providers need detailed information to understand and optimize the sustainability of their own workloads . While Amazon is making the biggest sustainability investments and getting some good results, it has historically been the least transparent cloud provider.\n\nAmazon released its sustainability report covering the whole of 2023 on July 9, Google released its report on July 2 , and Microsoft released its in May .\n\nI\u2019ve been leading the Real Time Cloud project at the Green Software Foundation for the last year or so. We have made a deep dive into the information available, identifying gaps and subtle differences in the data from each cloud provider.\u00a0 I\u2019m going to compare these three reports to see what\u2019s new since last year; my analysis here is my own, not the project\u2019s.\n\nAmazon Leads in Carbon Output Reductions\n\nAmazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 MmTCO2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ (grams of carbon dioxide per gross merchandise dollar) to 80.8 gCO2/$.\n\nThis continues a trend that started last year, of a small reduction in total carbon despite growing the business, as more of the internal projects started over the last few years mature and begin to have material impact. There\u2019s a long way still to go but it\u2019s a commendable result.\n\nGoogle however, increased its total carbon by 13% to 14.3 MmTCO2e for 2023, on top of an increase the previous year, slipping on carbon progress due to investment in data centers for AI, which was also the story at Microsoft, up from 16.5 to 17.2 MmTCO2e.\n\nThe physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors, and Amazon\u2019s revenue growth rate is a bit lower. But over the years, Amazon has also been more aggressive in buying renewable energy than Google and Microsoft.\n\nAmazon\u2019s carbon footprint was reduced for all categories apart from fossil fuels, as its delivery fleet continues to grow as a business, faster than those vehicles are being electrified. This trend is driven in part by Amazon bringing more deliveries in-house; in the report, the category that includes third-party delivery carbon declined 4%.\n\nIncreased focus on rail-based delivery in Europe and India is reducing delivery carbon. The electric delivery fleet grew more than sevenfold, to 19,000 vans, but still has a long way to go before it makes a material difference.\n\nOver time, the electricity use of Amazon for retail and delivery is adding a lot more demand in addition to the growth of data centers, compared to\u00a0 Microsoft and Google. This is why Amazon is investing in so many more energy projects.\n\nRenewable Energy Projects: Making Good Progress\n\nThe most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon , including retail and deliveries.\n\nThis is much more than just the AWS data center usage. Energy matching means that renewable energy is being generated and put into a grid somewhere in the world in an amount that matches any non-renewable energy that\u2019s being used.\n\nThe energy projects that make up the portfolio are geographically positioned as close as possible to where the power is needed. But it\u2019s not always possible for them to be located nearby, and the amount of energy generated doesn\u2019t exactly match how much is currently being used locally.\n\nThe Amazon generation portfolio increased from 20GW to 28GW, making it the world\u2019s largest purchaser of renewable energy for the fourth year in a row. Google added half as much, 4GW in 2023, which is still a record for the company.\n\nForty-two new utility-scale projects came online for Amazon in 2023. The Amazon sustainability report also states that the company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has also purchased a nuclear-powered data center in Pennsylvania.\n\nAmazon\u2019s energy storage increased from 445MW in 2022 to 1.3GW in 2023 to help smooth out the daily supply and demand mismatch as solar and wind come and go. It\u2019s now become common for new solar power developments to include battery storage, as the peak daily output could be curtailed and the excess can be stored then supplied to the grid in the evening.\n\n100% Carbon-Free Energy Match Still Emits Carbon\n\nThe 100% carbon-free energy match metric uses the annual \u201cmarket-based\u201d method, which is based on energy purchases, averaged over the whole year. As we noted previously, the in-house generating capacity won\u2019t ever match the right location at the right time, so there\u2019s a market for trading excess renewable energy.\n\nRenewable energy certificates (RECs) can be traded for up to a year, so when companies have final data for 2023, additional \u201cunbundled RECs\u201d can be purchased as needed to cover for the non-renewable energy that was used, and obtain any desired percentage, at a cost.\n\nAmazon committed to reach 100% by 2030, with a goal of 2025, and is proud to get to 100% in 2023. It looks less impressive to realize that Amazon has finally caught up with Google, which has been at 100% carbon-free energy match using the same annual market-based method for the last seven years. The Amazon sustainability report states that there was 2.79 MmTCO2e of energy-related carbon emissions (known as Scope 2). In other words, when Amazon reached 100% carbon-free energy, there were still 2.79 million tons of carbon equivalent emitted.\n\nA few years ago Google started measuring and reporting a much more stringent \u201c24\u00d77\u201d hourly matching location-based method. Microsoft has dabbled with 24\u00d77, but only for the Azure Sweden region, and AWS hasn\u2019t done anything with it.\n\nAmazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report : \u201cAmazon\u2019s Scope 2 (Indirect) GHG emissions: Location-based method (LBM) are 15.67 MmTCO2e.\u201d Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by the location-based method. When the methods were defined, the report\u2019s authors didn\u2019t consider that large end users of energy could also be large generators of energy across multiple locations.\n\nGoogle discloses location-based data to customers for all its cloud regions individually, and AWS needs to follow suit and start to report Scope 2 location-based data on a per-region basis.\n\nGiven the locations and sizes of the Amazon renewable power projects, I think AWS will often have lower location-based method carbon than Google and Azure. But AWS needs to publish its results \u2014 regardless of whether those numbers are better or not.\n\nAmazon Is Leaning Into Future Energy Capacity\n\nThe new Amazon report includes a statement that the company is \u201cpurchasing additional environmental attributes (such as renewable energy credits) to signal our support for renewable energy in the grids where we operate, in line with the expected generation of the projects we have contracted.\u201d\n\nIt takes two to three years to bring a wind or solar farm online, so Amazon buys RECs on the open market that match the future generation capacity that they have committed to building. This is a good policy, as unbundled REC purchases have a bad reputation when they are used across regions and countries as a cheap substitute for actual investments. But here they are being purchased to match investments in new generating capacity.\n\nHowever, there is also a footnote on page 26 of the sustainability report:\n\n\u201cAWS aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases (e.g., renewable energy in the same grid is not available), AWS may procure renewable energy attributes in other locations.\u201d\n\nA similar footnote in the audit report on energy also reads:\n\n\u201cAmazon takes a global approach to calculating the percentage of electricity consumed by Amazon\u2019s global operations matched by renewable energy sources. Amazon aims to procure renewable electricity in the same grids where it consumes electricity. In certain cases, Amazon may procure renewable energy in other locations.\u201d\n\nThis is an unfortunate fact of life: there just isn\u2019t enough renewable energy in regions in Asia in particular, so cross-border RECs are used. This is the first time I\u2019ve seen Amazon make this statement, and Amazon joins Google and Microsoft, who\u2019ve been buying cross-border RECs and carbon offsets for many years.\n\nAmazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. This covers all of the Canada, China, Europe, India, Japan and U.S. regions. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Indonesia, Israel, Korea, Singapore, South Africa, and the United Arab Emirates. The difference since 2022 is that the \u201cin-market\u201d wording was added and Japan was added to the previously published 100% list.\n\nHowever Amazon has far more generating capacity in Asia than either Google or Microsoft, and Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances.\n\nThe problem with this is that by the market method, cloud energy use in Asia will be reported as zero carbon, because cloud providers will pay extra by buying RECs elsewhere. However, increased use of regions in Asia will cause extra carbon to be emitted compared to the same use in Europe or the U.S., and to measure the difference the location-based method needs to be used and reported.\n\nMore Investments in De-Carbonizing Supply Chains\n\nAmazon announced a new program to engage major suppliers to decarbonize its supply chain with Amazon Sustainability Exchange , sharing internally developed playbooks and sustainability science models for things like renewable concrete and hydrogen. There\u2019s some interesting work and good advice here; it was put together by sustainability scientists in the Amazon central sustainability team.\n\nLow-carbon concrete and steel for construction is a good investment and is growing fast, up from 16 AWS data centers to 36 in 2023, but it\u2019s still a small impact on the carbon total. Efficiency and electrification upgrades and the use of local solar power on existing buildings are having a bigger effect.\n\nAWS, Azure and Google have all started to transition to bio-diesel (mostly from recycled cooking oil) for backup generators, and AWS has reduced the use of air freight for deliveries and started to use electric vehicles for trucking equipment to data centers. This is worthwhile, as it helps develop a market for the supply of biodiesel globally, although this is currently a very small part of the total carbon footprint.\n\nAmazon is correctly positioning investment in nature-based solutions and carbon capture as a second priority to direct reductions in carbon emissions. But it\u2019s a worthwhile additional investment, as it\u2019s expected to be necessary to scale these solutions over time to meet the 2040 Net Zero Climate Pledge goal, as the remaining direct emissions become harder to eliminate.\n\nSustainable Water Resources: Good Progress\n\nSustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers, which has increased a lot in recent years. A few years ago the cloud providers started to measure and report their water usage, and they all have targets to use less, and to return more clean water to the locations where they operate.\n\nThere are two ways to measure water, one is how much water is used as a proportion of the energy being used in a data center. Water usage effectiveness (WUE) is measured in liters per kilowatt hour. The other is how much water is replenished as a proportion of what is used.\n\nAWS\u2019s WUE improved by 5% from 0.19 to 0.18 Liters/kWh in 2023, averaged across all the facilities AWS runs globally. It\u2019s unfortunate that AWS only provided a global average, which removes any opportunity to optimize via workload placement.\n\nMicrosoft Azure provided WUE on a region-by-region basis, varying from zero to over 2 Liters/kWh. Google did not report WUE figures.\n\nAWS reports 41% \u201cwater positive\u201d as a new metric in 2023, with a 100% goal. This appears to be another name for \u201cwater replenishment.\u201d\n\nGoogle reports improving from 6% to 18% water replenishment in 2023, but the company has a goal of eventually returning more water than it uses. Google was dinged for poor water use a few years ago, and is clearly playing catch up, while AWS has already made good progress.\n\nAzure doesn\u2019t provide a replenishment figure. This is the kind of thing the Green Software Foundation\u2019s Real Time Cloud project is working on, trying to get all the cloud providers to standardize on the same metrics with the same names by pointing out the gaps.\n\nAmazon\u2019s Reporting Needs More Transparency.\n\nAWS had previously published data center versus cloud carbon reduction comparison reports for Asia, Europe and the U.S. that were used as the basis for a lot of marketing claims over the years. They were getting a bit old and were updated by a new carbon reduction comparison report in June.\n\nAccenture was commissioned to write the report, and while it\u2019s a helpful document and the claims it makes do seem reasonable, there\u2019s no transparency in the calculations, and a lot of private AWS data was used by Accenture that isn\u2019t available for customers to use to model their own specific situations.\n\nCustomers need more than marketing claims, and the report could have included a lot more of the underlying data and calculations, not just the headline results.\n\nAWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago and releases quarterly updates. Microsoft Azure publishes PUE data for all its regions.\n\nWhen you measure energy use for a workload in a cloud region (e.g. by collecting NVIDIA GPU energy use metrics), you need to multiply by PUE to account for how much energy was supplied to the data center to cover cooling and transmission inefficiencies then multiply by the carbon content of that energy.\n\nPUE is much higher in hot and humid climates. Of course, some workloads have to be in a specific region. But a significant amount of compute and storage capacity (like backup/archives) could be located anywhere. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data.\n\nThe other data provided by both Google and Azure is the companies\u2019 carbon-free energy (CFE) proportion on a per-region basis. This takes the local method grid carbon mix into account, then subtracts out the locally generated capacity from renewable projects that are up and running. It\u2019s a good way of indicating which regions are benefiting from low-carbon energy that can be used for optimizing global workload placement.\n\nThere is still no mention of Scope 3 supply chain reporting for AWS customers (Scope 3 refers to emissions from assets not owned or controlled by an organization, but that the organization\u2019s supply chain directly affects.)\n\nIt has been many years since Azure and GCP started to report Scope 3 and it is now years since AWS promised that Scope 3 would be provided. They\u2019ve told me they have a team working on it, but until something is released, this is a huge gap. Microsoft released a good white paper on their Scope 3 methodology back in 2021.\n\nThe AWS Customer Carbon Footprint Tool (CCFT) was embarrassing when it was initially released in 2022, and it has made no progress in the years since then. It is going to report zero for everyone for their Scope 1 and 2 carbon footprint, according to the market methodology, has no Scope 3 data, and aggregates too much data together.\n\nI recently tried to use the CCFT data to track progress for a company, and the three usage categories EC2, S3 and Other combined with three geographies EU, Americas and Asia made it impossible to figure out what was going on. The Other category in the Americas is dominating, but that\u2019s all it tells you.\n\nCustomer carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. Completely useless.\n\nIn summary, Amazon is making decent progress towards a reduced carbon footprint, while Google and Microsoft are slipping. However, someone at AWS needs to read and follow the advice given in the Amazon Exchange Carbon Measurement Guide . The guidance says that metrics and transparency are needed, and AWS is still at ground zero, with no transparency, and no progress on metrics that their customers have been asking for for years.\n\nAdditional Resources\n\nAmazon Reporting website\n\nAmazon Carbon Methodology\n\nAmazon Renewable Energy Methodology\n\nCarbon Free Energy\n\nAWS Sustainability Reporting Framework Summary\n\nAmazon Exchange Carbon Measurement and Reporting guidelines\n\nWater Positive Methodology\n\nAccenture report on AWS carbon reduction\n\nAmazon Energy Audit\n\nAmazon Scope 1 and 2 Audit\n\nAmazon Scope 3 Audit",
        "summary": "Amazon released its sustainability report covering the whole of 2023 on July 9. Google released its report on July 2 and Microsoft released its in May. Customers of cloud providers need detailed information to understand and optimize their own workloads. Amazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 Mm TCO 2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ to 80.8 g CO2/$. This continues a trend that started last year, of a small reduction in total carbon despite growing the business. The physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors. Amazon has also been more aggressive in buying renewable energy than Google and Microsoft. The most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon. Amazon generation portfolio increased from 20GW to 28GW. Forty-two new utility-scale projects came online for Amazon in 2023. The company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has finally caught up with Google, which has been at 100% carbon-free energy match for the last seven years. Amazon committed to reach 100% by 2030, with a goal of 2025. Amazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report. Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by theLocation-based method. It takes two to three years to bring a wind or solar farm online. Amazon buys RECs on the open market that match the future generation capacity. This is a good policy, as unbundled REC purchases have a bad reputation. Amazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Japan and U.S. Amazon has far more generating capacity in Asia than either Google or Microsoft. Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances. Sustainability scientists in the Amazon central sustainability team. Low-carbon concrete and steel for construction is a good investment and is growing fast. Amazon is correctly positioning investment in nature-based solutions. Sustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers. Cloud providers have targets to use less, and to return more clean water to the locations where they operate. In 2023, water use in the U.S. is expected to rise from 0.3% to 0.4%. The U.N. has set a goal of 0.5% water use by the end of the year. AWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago. PUE is much higher in hot and humid climates. Microsoft Azure publishes PUE data for all its regions. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data. There is still no mention of Scope 3 supply chain reporting for AWS customers. Scope 3 refers to emissions from assets not owned or controlled by an organization. It has been many years since Azure and GCP started to report Scope 3. AWS carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. AWS Sustainability Reporting Framework Summary. Water Positive Methodology. Accenture report on AWS carbon reduction. Amazon Exchange Carbon Measurement and Reporting guidelines. Amazon Energy Audit.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "metrics",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_45",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "Cloud PUE: Comparing AWS, Azure and GCP Global Regions",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/cloud-pue-comparing-aws-azure-and-gcp-global-regions/",
      "content": {
        "text": "\n\nIn December 2024 Amazon released Power Usage Effectiveness (PUE) data for many of their AWS cloud regions, and along with existing data from Microsoft Azure and Google Cloud , there is finally enough data to make some comparisons for regions spread around the world and see what changed from 2022 to 2023.\n\nPUE is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center, and losses in the transmission and conversion of electricity on its way to the computers. Energy is measured (and paid for) at the meter as it enters the building, and a PUE of 1.15 means that an extra 15% of the total energy used by the computers is used for cooling and overheads. PUE varies between about 1.04 and 2.0 in practice.\n\nAll the cloud providers run very efficient data center hardware configurations, and they have, in general, become more efficient over time. However, it is harder to cool data centers in warm and humid environments, so PUE for data centers in the tropics will tend to be at the high end of the range, and PUE for data centers in cold and dry parts of the world are at the low end of the range.\n\nOne way to use less energy for cooling is to use more water, so there is also a natural tension between PUE and Water Usage Effectiveness (WUE in liters/kWh). The cloud providers have all recently invested heavily in optimizing for WUE as well, so the latest data centers tend to have good PUE and good WUE, but this requires the best and latest technology. Lower-cost and older data centers typical of the kind that enterprises own tend to be closer to 2.0 PUE and have a high WUE as well. WUE was explained and compared across cloud providers in the story I wrote for The New Stack in July 2024.\n\nSome of the regions deployed by cloud providers around the world are hosted by local service providers rather than in dedicated data centers built by the cloud providers. In this case the PUE is not included in the public numbers for two reasons, one is that it\u2019s hard to attribute and allocate part of a shared resource when there is no information about what is in the rest of the datacenter, and the other is that the over-all PUE is often proprietary information that is owned by the service provider and they don\u2019t allow it to be shared openly. Some additional PUE estimates may be available privately under a non-disclosure agreement so it\u2019s worth asking your provider if you are operating in a region that doesn\u2019t have a public PUE number.\n\nAmazon/AWS PUE Data\n\nThe new PUE information from Amazon is described on their sustainability page . They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. There is a short PUE Methodology pdf that basically says that they are following the relevant international (ISO) and European (CEN) standards. The global average 2023 PUE for AWS is 1.15, with AMER at 1.14, EMEA at 1.12 and APAC at 1.28. They say their best individual data center facility in Europe has a PUE of 1.04. Still, their best region is Melbourne, Australia, with a PUE of 1.08, and their worst is Hyderabad, India, with a PUE of 1.50. From year to year, AWS shows improvements in many regions.\n\nAWS PUE data as stated in January 2025.\n\nMicrosoft Azure PUE Data\n\nMicrosoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions, but when they were updated in December 2024 the numeric data was omitted. Instead, a webpage provides a summary that provides information for only 11 out of 27 regions, that is offset from the calendar year and doesn\u2019t cover all of 2023. We contacted the currently responsible teams within Microsoft and found that whoever disclosed the original 2022 data had moved on, and the 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table .\n\nThe Microsoft web page PUE data as stated in January 2025.\n\nMicrosoft discloses their best PUE of 1.11 in Wyoming, USA, and their worst at 1.35 in Illinois. Their sustainability targets don\u2019t mention PUE.\n\nGoogle Cloud Platform GCP PUE Data\n\nGoogle has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. For the same reasons as AWS and Azure discussed above, they don\u2019t report public PUE data on every region they operate in.\n\nGCP PUE data as stated in January 2025.\n\nGoogle\u2019s best PUE result is 1.07 in Oregon, and its worst is 1.19 in both Singapore and Nevada. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while until they start to fill up with equipment. This can be seen in the GCP regional data. Some locations support Google products that are not part of GCP regions, and there is a mapping of GCP cloud regions to the Google PUE data that we performed with help from Google engineers as part of producing the GSF cloud region metadata table .\n\nComparisons Between AWS, GCP and Azure\n\nWhile all the major cloud providers have industry-leading PUE numbers that are likely to be much better than local data center alternatives, the numbers above show that GCP has the best transparency, with more data over a longer time period, and the best overall PUE for the regions being disclosed. Microsoft went from providing the most data for 2022 to the least data for 2023 and also has the highest PUE values overall. AWS sits in-between, with two years of data, and PUE values that are worse than GCP but mostly better than Azure overall. AWS includes data for regions like Hyderabad with higher PUE that aren\u2019t disclosed at all by other cloud providers, and they have an excellent stated goal of 1.08 for their new data center builds.\n\nRegional Comparisons\n\nIf you can choose which cloud provider to use in a particular region, and you\u2019d like to make the most efficient use of the energy needed, then the above PUE data provides data for some scenarios.\n\nVirginia is the biggest cloud region in the world, AWS PUE is 1.15, Azure is 1.14 and GCP is 1.08. For the nearby regions in Ohio AWS PUE is 1.12, GCP is 1.10.\n\nSingapore is a major region in Asia for all the cloud providers, with a challenging tropical climate. AWS PUE is 1.30, Azure is 1.34, and GCP is 1.13 or 1.19 for each of their two facilities.\n\nIreland is one of the largest regions in Europe. AWS PUE is 1.10, Azure is 1.19, and GCP is 1.08.\n\nWhat Is a Good PUE Assumption for the GPUs Powering the AI Boom?\n\nThere is a lot of concern about how much power the massive build-out of GPU capacity is going to need. Whatever power the GPUs use directly needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. Looking at the above data, I think there are two situations to think about. GPUs that are put into older enterprise data centers tend to overwhelm the existing infrastructure that wasn\u2019t designed for very high power density, so the PUE is likely to be bad, I\u2019d guess 1.5 or higher. However, the massive new data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. The latest, most efficient designs will allow more GPUs to be powered and cooled in a given location, and I\u2019d assume a PUE of 1.08, regardless of who is building it.\n\nGreen Software Foundation Real-Time Cloud Project\n\nI\u2019ve been leading the GSF real-time-cloud project for the last year or so. We\u2019ve spent a lot of time discovering many sources, interfaces and products that collect and report energy and carbon data and documented their relationships in a large Miro flow chart. We\u2019ve also published regional metadata collected from GCP, AWS and Azure and summarized it into a single table covering the 2022 data sets. We\u2019re in the process of updating it to include the latest data releases to cover 2023 and are planning to produce estimates for 2024 and 2025 before the cloud providers disclose their data so that workloads that are running today can have some data to use that we think is the best guess available.",
        "summary": "Power Usage Effectiveness (PUE) is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center. PUE varies between about 1.04 and 2.0 in practice. PUE for data centers in the tropics will tend to be at the high end of the range. PUE for centers in cold and dry parts of the world are at the low end. Lower-cost and older data centers typical of the kind enterprises own tend to have a high WUE as well. The new PUE information from Amazon is described on their sustainability page. They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. Some additional PUE estimates may be available privately under a non-disclosure agreement. Microsoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions. The 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table. Google has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while. GCP has the best transparency, with more data over a longer time period. Microsoft went from providing the most data for 2022 to the least data for 2023. AWS sits in-between, with two years of data, and PUE values that are worse than GCP.  AWS PUE is 1.15 in Virginia, Azure 1.14 in Ireland and GCP 1.08 in Ohio. PUE needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. New data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. I\u2019d assume a PUE of 1.08, regardless of who is building it. . at are running today can have some data to use that we think is the best guess available. at the time of this article. We are happy to provide the best data we can.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "infrastructure",
        "platform",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_46",
      "kind": "story",
      "subkind": "tns-post-body-content",
      "title": "NVIDIA Unveils Next Gen Rubin and Feynman Architectures",
      "source": "The New Stack",
      "published_date": "2025",
      "url": "https://thenewstack.io/nvidia-unveils-next-gen-rubin-and-feynman-architectures-pushing-ai-power-limits/",
      "content": {
        "text": "\n\nSAN JOSE, Calif \u2014 NVIDIA has cemented its central position in the technology industry over the last few years. It has the most in-demand products and amongst the biggest revenue and biggest market capitalization. It\u2019s a once-in-a-lifetime moment for everyone involved, as the Talking Heads once sang, but the torrent of product releases is extremely hard to keep up with and make sense of.\n\nThere is an impressive amount of business agility and relentless progress, but sometimes headline-making announcements are quietly forgotten or superseded by even better products before people have had a chance to get their hands on them. NVIDIA is announcing new hardware that effectively obsoletes its previous products well before they are available, but customers are buying whatever they can get, as it appears.\n\nMeanwhile \u201cthe new stack\u201d of AI software is maturing rapidly, easy-to-use, pre-tuned, high-performance open source models are available, and a plethora of AI-based services and tools are competing for everyone\u2019s attention. Development is also being accelerated by a variety of AI-backed tools that are improving rapidly.\n\nLast year, I predicted that enterprise customers would find it extremely difficult to manage the pace of change, and get \u201centerprise indigestion,\u201d but that hyperscalers would be happy. This year, I think the hyperscaler customers and cloud providers have indigestion as well. It takes a few years to plan and build a data center, but the specification of the racks and the power density keeps going up, so they must be replanning before they are finished building. NVIDIA is responding by releasing a very wide range of packaging options to target various points in the market and providing more information about its future roadmap, which helps but is making it even harder to keep track of what\u2019s going on.\n\nHow Did We Get Here?\n\nBefore we can make sense of what was announced by Jensen Huang , the CEO of NVIDIA, in his annual GTC keynote Tuesday , we need to establish some context by looking back at previous keynotes and additional announcements over the last year. There is a big disconnect between the GPUs that most people are using now (the ones that just started shipping, but that most people haven\u2019t used yet) and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail.\n\nThe GPU servers that most people are familiar with right now have two Intel CPUs and eight NVIDIA Hopper H100 GPUs as the nodes that can be clustered. They were announced three years ago and are the mainstream workhorse underpinning most of the AI-based products and services we use today.\n\nTwo years ago in his keynote, Jensen announced the Grace Hopper GH200 combined CPU/GPU architecture. Grace was the first CPU designed by NVIDIA and uses ARM architecture rather than Intel, with an NVlink interface to couple it directly to a slightly upgraded Hopper H200 GPU. While it shipped in volume, the GH200 one-to-one mix of CPU to GPU is the wrong ratio for many AI training customers, who wanted more of their dollars and power budget to be spent purely on additional GPU capacity. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as the H100s.\n\nGH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. This is reversed from conventional architectures where the CPUs are in the center (that\u2019s what the C in CPU stands for!) driving interconnection traffic, and GPUs are attached to them.\n\nThe difference is that when an H200 GPU wants to send data to another H200 GPU in the same system in a cluster, it goes directly across NVlink. This is far faster than a traditional GPU sending via a PCI bus to an Intel CPU then over a network to another Intel CPU, then via PCI to the other GPU. However, when one Grace CPUs wants to communicate with another, the data passes through at least two Hopper GPUs to get there.\n\nGH200 systems seem to be more suited to the HPC Supercomputer market than the much bigger AI training and inference market. NVIDIA told me in 2024:\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\n\u201cThe largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019, Texas Advanced Computing Center \u2018Vista\u2019 and the J\u00fclich Supercomputing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Combined, these GH200-powered centers represent some 200 exaflops of AI performance to drive scientific innovation.\u201d\n\nBlackwell GPU\n\nIn 2024, Jensen announced the Blackwell architecture GPU and the GB200 system. To get around limitations in the maximum size of chips that can be made, the Blackwell GPU is made from two of the largest possible chips, connected directly together. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (addressing the issue with the 1:1 CPU/GPU ratio in GH200).\n\nBlackwell GB200 modules have around four times the silicon area dedicated to GPU, and the same area for CPU, compared to GH200 modules. Two modules fit on a 1 rack-unit-high, water-cooled motherboard, and 18 compute boards fit in a 72 GPU NVL72 rack, along with switchboards to interconnect them.\n\nNVIDIA has introduced several variations of the Blackwell and packaging options since launch, including a reduced-performance single-chip B200A variation that can be configured to be air-cooled as part of its MGX line of products. In the MGX GB200A NVL36 rack, each board takes two rack units and has a single Grace CPU resulting in a 9-CPU/36-GPU rack that needs 40kW of cooling, which is still a challenge but is easier to deploy in existing data centers.\n\nAt the end of 2024, it was disclosed that a new process from chip supplier TSMC allows for 50% higher performance Blackwell Ultra \u2013 B300 GPUs. These also use even more power, pushing the NVL72 rack from 132kW to 163kW, but it appears that many of the orders and plans that were in place for B200 have now switched to Blackwell Ultra.\n\nIt also has more flexibility in its design options, allowing customers like AWS to customize its deployments to integrate with its custom packaging and network architecture. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72 so that existing installations can extend their lifespan, and work done on productizing data centers to support the original NVL72 rack isn\u2019t wasted.\n\nNVIDIA revealing Blackwell Ultra NVL72 onstage at GTC.\n\nThe highly structured AWS cloud architecture doesn\u2019t naturally support dropping in a product like NVL72, and while Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave , the latest AWS GPU that is generally available as of March 18, 2025 is the p5 that has eight H200 with two Intel CPUs . Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. In the header shot for this story, you can see the variants of Blackwell-based racks that are deployed by each provider.\n\nLooking closely, it appears that most of the racks including Azure and Oracle cloud are NVL72, but AWS and GCP are using the air-cooled MGX36, and HP has its own custom Cray architecture rack for HPC that hosts 244 Blackwell GPUs. I haven\u2019t seen any new announcements from AWS at GTC and confirmed that it has no announced Blackwell-based products right now after visiting its expo booth. And you may ask yourself, how did they get here?\n\nThere was already some disclosure in mid-2024 but Jensen provided more details of the next-generation 2026 Vera CPU and Rubin GPU , which has even higher per-rack power requirements. To reduce \u201chyperscaler indigestion,\u201d the first version of Rubin also delivers into the same NVL72 rack architecture, with another bump in power consumption and a change in naming convention to Vera Rubin NVL144. Jensen said that they are reverting to calling a two-die module two GPUs for Rubin rather than one, as Blackwell was too confusing.\n\nNVIDIA revealing Vera Rubin NVL144 onstage at GTC.\n\nThe Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance, up from 1.1 exaflops for Blackwell Ultra GB300, and 1.2 exaflops of FP8 for training, up from 0.36, which is 3.3x the performance. It\u2019s targeted to ship in the second half of 2026.\n\nEscalating per-rack power requirements are just one of the problems, but the next-generation Rubin Ultra racks are up to 600kW according to Jensen, containing 576 GPUs in 144 modules that each have four Rubin GPUs sharing 1 terabyte of high-bandwidth memory. This is useful information for people designing for data center builds completing 2-3 years out who want to take these next-generation racks. However, it emphasizes that running an AI factory data center is a new set of challenges, even for the hyperscalers and cloud providers, let alone the more traditional enterprise customers.\n\nNVIDIA revealing Robin Ultra NVL576 onstage at GTC.\n\nLooking beyond Rubin Ultra in 2027, the next-generation architecture will be named Feynman in 2028, so we can expect to hear more about the performance of that GPU architecture next year.\n\nNVIDIA showing its next-gen architecture onstage at GTC.\n\nAt the lower end of the scale, NVIDIA is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. These are particularly useful for accelerated computer-aided engineering (CAE) workloads and Jensen mentioned that many of the standard CAE tools for chip simulation and design, mechanical simulation, etc., are now accelerated.\n\nNVIDIA revealing AI infrastructure for enterprise computing onstage at GTC.\n\nBenchmarking\n\nLast year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. Blackwell adds a new FP4 capability for inference that doubles the performance and there are additional optimizations for inference that give rise to a 30x improvement claim. I analyzed the 2024 benchmark claims in detail and decided that a more reasonable expectation for configurations of up to eight Blackwell GPUs that most people actually use would be 2.5x the H100 (not 4x) for training, and 8-10x the H100 (not 30x) for inference. These are good improvements in any case. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.\n\nThe way Jensen positioned performance was in the context of an AI Factory, which needs to operate in a sweet spot between large efficient batches of work vs. quick responses to interactive queries. That\u2019s where the extra memory and memory bandwidth makes the GPU operate closer to its maximum potential capacity. To further optimize this workload, NVIDIA will release a new open source distributed inference serving library called Dynamo (which isn\u2019t on GitHub as I write this). This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.\n\nNVIDIA announcing Dynamo onstage at GTC.\n\nThe latest performance claim is that with Dynamo and scale-up benefits from the larger NVL72, Blackwell is \u201c40x better\u201d than Hopper at AI Factory hyperscale inference serving. I think the benchmark was explained better this year, than last year, and it\u2019s a relevant application-level benchmark, but I also hope that people don\u2019t assume that a single Blackwell is 30x or 40x faster than a single H100.\n\nRepeated attempts to architect the largest-scale SuperPods at GTC.\n\nWater-Cooled Optical Switches\n\nA big announcement from two years ago was that NVlink could replace the network for up to 256 GH200s combined into a single system connected by 900GB/s interfaces to NVswitch chips. That would have contained 120TB of CPU-hosted memory and 24TB of GPU-hosted high-bandwidth memory, but this configuration seems to have never shipped in volume.\n\nNVlink is a copper-cabling system that operates inside a rack and can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. So in addition to the over-provisioning of CPU capacity with GH200, an additional issue was the high cost of NVlink optical transceivers for building such a big system. For AI training workloads, it\u2019s common to arrange GPUs in a \u201crail\u201d architecture where they are connected in parallel lines that don\u2019t need to cross-connect. This seems plausible to configure using copper NVlink from one rack to the next on each side, but a system configured this way is only useful for training workloads.\n\nThere was an update in 2024 to the shared memory cluster plans, abandoning the GH200 approach and focusing on a GB200-based configuration where the maximum number of GPUs in the shared memory cluster more than doubles from 256 to 576. Again, this seems like a theoretical architecture at this point.\n\nI haven\u2019t been able to find anyone talking about more than 36 or 72 GPUs, and Coreweave \u2019s documentation implies that they configure the NVL72 racks as individual 4-GPU boards networked together. The previously announced deal with AWS to deploy a cluster of GH200s for NVIDIA called Project Ceiba was upgraded (and delayed somewhat) to be a GB200 NVL72-based system with more than 20,000 GPUs instead, as of October 2024. It was not mentioned this year at GTC. I expect that to switch again to be based on GB300 if it\u2019s ever completed.\n\nA key new announcement for 2025 is a very efficient, highly integrated water-cooled optical switch, NVIDIA Photonics. This is going to take out a lot of the complexity, cost and power consumption of larger clusters. The optics terminate directly on the switch chip without needing expensive transceivers. This was probably the biggest surprise announcement in the GTC keynote.\n\nThe latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d and it can all be wired together with low-latency copper cables. This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. A prototype of the packaging using Blackwell GPUs and current switch chips was shown in the expo and I took a few pictures.\n\nCan You Afford To Own GPUs?\n\nSame as it ever was. The challenge for customers buying GPUs is that they need to decide what the real useful life of a GPU is before it\u2019s obsolete. It\u2019s far shorter than CPUs, so instead of depreciating like most hardware over 5 or 6 years as seems to be common nowadays, it likely makes sense to depreciate GPUs over 2 or 3 years. That makes them even more expensive to own, and perhaps it\u2019s better to get whatever the latest GPU is available from cloud providers and have them deal with depreciation.\n\nOld GPU capacity tends to end up as a cheap option on a spot market but at some point, it will cost more to power it than it\u2019s worth as a GPU. Jensen said during his keynote that H100 GPUs aren\u2019t generally useful at this point for building AI factories, and sales of Blackwell are much higher than Hopper already.\n\nNew Capabilities for Building AI Apps and More\n\nJust like last year, there were many more announcements for markets like robotics, automotive, healthcare, quantum computing, digital twins and cool virtual reality demos that you can watch in the keynote or read about elsewhere, but I\u2019m most impressed by the new GPU hardware specifications and the optical interconnect.\n\nThe new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU, the new Dynamo software to operate inference efficiently, optimized models, a large range of third-party packages to get started with and a choice of development tools to help write the remaining code for you.\n\nJensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors at building the software that makes it easy to build applications and has open sourced most of it. Letting the days go by, once in a lifetime.\n\nNVIDIA onstage at GTC showing CUDA-X for every industry.",
        "summary": "NVIDIA is announcing new hardware that effectively obsoletes its previous products. Customers are buying whatever they can get, as it appears. \"The new stack\" of AI software is maturing rapidly. Last year I predicted that enterprise customers would find it extremely difficult to manage the pace of change. This year, I think the hyperscaler customers and cloud providers have indigestion as well. Development is also being accelerated by a variety of AI-backed tools. There is a big disconnect between the GPUs that most people are using now and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail. GH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as H100s. The largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019 and the J\u00fclich Supercom computing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Blackwell is made from two of the largest possible chips, connected directly together. Two modules fit on a 1 rack-unit-high, water-cooled motherboard. 18 compute boards fit in a 72 GPU NVL72 rack. Blackwell Ultra \u2013 B300 has 50% higher performance than B200. New process from chip supplier TSMC allows for even more power. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72. Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave. The Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance. It\u2019s targeted to ship in the second half of 2026. The next-generation Rubin Ultra racks are up to 600kW. The next-generation architecture will be named Feynman in 2028. The company is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. Last year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.  NVIDIA will release a new open source distributed inference serving library called Dynamo. This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.  NVlink is a copper-cabling system that operates inside a rack. It can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. The NVL72-based system with more than 20,000 GPUs will be deployed by October 2024. GPUs in the shared memory cluster more than doubles from 256 to 576. The latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. The new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU. Jensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors.",
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "engineering",
        "gcp",
        "hpc",
        "infrastructure",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_48",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable: Chips, AI, Search, Quantum, Bitcoin, Fusion \u2013 OXD26",
      "source": "OrionX Download",
      "published_date": "2025",
      "url": "https://orionx.net/2025/03/analyst-roundtable-chips-ai-search-quantum-bitcoin-fusion-oxd26/",
      "content": {},
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_49",
      "kind": "podcast",
      "subkind": "",
      "title": "SC24, Supercomputing, CryptoSuper500, Quantum, RISC-V Summit \u2013 OXD25",
      "source": "OrionX Download",
      "published_date": "2024",
      "url": "https://orionx.net/2024/12/sc24-supercomputing-cryptosuper500-quantum-risc-v-summit-oxd25/",
      "content": {},
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_50",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Scaling AWS Costs to Match the Business",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/scaling-aws-costs-to-match-the-business-f9k",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "scaling"
      ]
    },
    {
      "id": "virtual_adrianco_story_51",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Why are services slow sometimes?",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/why-are-services-slow-sometimes-mn3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_52",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "If at first you don't get an answer...",
      "source": "Dev.to",
      "published_date": "2020",
      "url": "https://dev.to/aws/if-at-first-you-don-t-get-an-answer-3e85",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_story_53",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Sustainability transformation and DevSusOps",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/aws/what-is-sustainability-transformation-32hi",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_story_54",
      "kind": "story",
      "subkind": "crayons-article__main",
      "title": "Measuring energy usage",
      "source": "Dev.to",
      "published_date": "2021",
      "url": "https://dev.to/adrianco/measuring-energy-usage-5ip",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_cadcf9fb",
      "kind": "file",
      "subkind": "presentation",
      "title": "Systems for Innovation Villanova",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Systems for Innovation Villanova.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_5bcbe0dc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Speeding Innovation Singapore 2019",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Speeding Innovation Singapore 2019.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_b665be9d",
      "kind": "file",
      "subkind": "presentation",
      "title": "YOW22-Greatest-Hits-Brisbane",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/YOW22-Greatest-Hits-Brisbane.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_76a699cd",
      "kind": "file",
      "subkind": "presentation",
      "title": "ChoosingUsingLosing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/ChoosingUsingLosing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_27b829e5",
      "kind": "file",
      "subkind": "presentation",
      "title": "Cloud Trends 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Cloud Trends 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_file_60526469",
      "kind": "file",
      "subkind": "presentation",
      "title": "OSSV Lock-In 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/OSSV Lock-In 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_ce8da570",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices TempleUniv Cool AWS Stuff",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices TempleUniv Cool AWS Stuff.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_7b574db1",
      "kind": "file",
      "subkind": "presentation",
      "title": "QConLondon Netflix Retrospective",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/QConLondon Netflix Retrospective.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_file_7d8d13fc",
      "kind": "file",
      "subkind": "presentation",
      "title": "Gophercon 2016 CSG",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Gophercon 2016 CSG.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_f0b33bd4",
      "kind": "file",
      "subkind": "presentation",
      "title": "NetflixWorkshop",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/NetflixWorkshop.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d90ba45d",
      "kind": "file",
      "subkind": "presentation",
      "title": "CMG-cloud-computing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CMG-cloud-computing.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_db153f9c",
      "kind": "file",
      "subkind": "presentation",
      "title": "bil2010-millicomputing",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bil2010-millicomputing.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_d02dc96d",
      "kind": "file",
      "subkind": "presentation",
      "title": "netflixoncloudfordevandops",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/netflixoncloudfordevandops.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_c6fb1e62",
      "kind": "file",
      "subkind": "presentation",
      "title": "bottleneckanalysis",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/bottleneckanalysis.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_821640e2",
      "kind": "file",
      "subkind": "presentation",
      "title": "Microservices Workshop 2016",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Microservices Workshop 2016.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_file_f150f910",
      "kind": "file",
      "subkind": "presentation",
      "title": "Resiliency - Failure Modes and STPA",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/Resiliency - Failure Modes and STPA.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_100f72ca",
      "kind": "file",
      "subkind": "presentation",
      "title": "migratingnetflixtocassandra",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/migratingnetflixtocassandra.pdf",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_file_fe8671ea",
      "kind": "file",
      "subkind": "presentation",
      "title": "CloudNative",
      "source": "file",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/file/CloudNative.pptx",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_infoqvideo_bafc0c54",
      "kind": "infoqvideo",
      "subkind": "",
      "title": "What we learned and didn t learn from Netflix",
      "source": "infoqvideo",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/infoqvideo/What_we_learned_and_didn_t_learn_from_Netflix.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_951fa883",
      "kind": "youtube",
      "subkind": "",
      "title": "061 Open source  open data  open standards with Adrian Cockcroft and Zaheda Bhorat  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htUGrDBKAMg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_eeaa0d1d",
      "kind": "youtube",
      "subkind": "",
      "title": "086 Keynote  Cloud Trends  DevOps and Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=c0wSmr-u5vQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_80cdf542",
      "kind": "youtube",
      "subkind": "",
      "title": "040 Innovating at speed   AWS and Formula 1",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=D7usPAR9a1k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3733c7ad",
      "kind": "youtube",
      "subkind": "",
      "title": "001 SC24  Supercomputing  CryptoSuper500  Quantum  RISC V Summit   OXD25",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=d5mr6Ib5ygQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_787fb720",
      "kind": "youtube",
      "subkind": "",
      "title": "015 Ep 113 Platform Engineering Teams Done RIGHT with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=muKaB4f2qJU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_16014850",
      "kind": "youtube",
      "subkind": "",
      "title": "060 AWS Summit Series 2017   Chicago  Keynote",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=tMUKC6eBjL8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c3f015ec",
      "kind": "youtube",
      "subkind": "",
      "title": "035 AWS Executive Forum 2019   Positioning Large Transformation Efforts to the Board",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=L2N9y9ovKuk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_e8c8e73a",
      "kind": "youtube",
      "subkind": "",
      "title": "009 Adrian Cockcroft   A Journey from Chaos Monkey to Sustainability",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jwRVtUEND8c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_d5f9baf9",
      "kind": "youtube",
      "subkind": "",
      "title": "023 Coffee with Mr  IoT  Adrian Cockcroft  Amazon s pathway to sustainability through technology",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hFVxkdHh9To&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_67c72125",
      "kind": "youtube",
      "subkind": "",
      "title": "088 Systems for Innovation   Adrian Cockcroft  at USI",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=-vlOG3UIp9c&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_71a9366a",
      "kind": "youtube",
      "subkind": "",
      "title": "051 Migrating to Cloud   Lessons from Netflix  Brought Up to Date",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=XrWII4ewrXA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_54b715c7",
      "kind": "youtube",
      "subkind": "",
      "title": "062 AWS Summit Stockholm 2017  Opening Keynote with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=GgkAhTtZugc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_cb35d110",
      "kind": "youtube",
      "subkind": "",
      "title": "063 AWS Summit Seoul 2017   Day2 \uae30\uc870\uc5f0\uc124 Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=061bsq0jVYU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_115dc1fa",
      "kind": "youtube",
      "subkind": "",
      "title": "029 Keynote   Modern App Development   Adrian Cockcroft  VP Cloud Architecture Strategy  AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=wYCLbLrEoqs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7eb1f382",
      "kind": "youtube",
      "subkind": "",
      "title": "045 Open Source in the enterprise with Zaheda Bhorat and Adrian Cockcroft  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=YMKsEmlw6V4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_1b3f4a2e",
      "kind": "youtube",
      "subkind": "",
      "title": "From Netflix to the Cloud DevOps Microservices and Sustainability",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/From_Netflix_to_the_Cloud_DevOps_Microservices_and_Sustainability.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_92754b58",
      "kind": "youtube",
      "subkind": "",
      "title": "080 DOES15   Adrian Cockcroft   Systems for Innovation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SaMIiLF1w20&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_42a586a4",
      "kind": "youtube",
      "subkind": "",
      "title": "008 Speeding Up Innovation   Adrian Cockcroft   YOW  2019",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=QlqQmHLXNiY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_804c4369",
      "kind": "youtube",
      "subkind": "",
      "title": "042 Developing a Chaos Architecture Mindset   Adrian Cockcroft   GOTO 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vHl7EZ5o0uY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_42d534fd",
      "kind": "youtube",
      "subkind": "",
      "title": "017 Mik   One  Adrian Cockcroft  Episode 24 ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=fDgqiA2yZ7g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_d8950de9",
      "kind": "youtube",
      "subkind": "",
      "title": "043 Rapid Development   Why Serverless First Is Like Building with Lego",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5siD210Grr4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_9192c555",
      "kind": "youtube",
      "subkind": "",
      "title": "049 Adrian Cockcroft on Chaos Architecture",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ja6n5etN8hk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_98dafaa1",
      "kind": "youtube",
      "subkind": "",
      "title": "010 Don t Miss   How Cloud Architect Adrian Cockcroft Transformed Tech   A Candid Conversation with Jani",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=vNIDssi4FK4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_ae43484b",
      "kind": "youtube",
      "subkind": "",
      "title": "082 Panel   Nicole Forsgren  Joshua Corman  Dan North   Adrian Cockcroft   GOTO 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=1qyljMck9Ng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_190f19af",
      "kind": "youtube",
      "subkind": "",
      "title": "012 2023 Monitorama Live Stream Day 1",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CRJcc1TqBhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1354b4cb",
      "kind": "youtube",
      "subkind": "",
      "title": "048 Adrian Cockcroft on Mapping Your Stack",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mzIdKGCOf1g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_64253f0e",
      "kind": "youtube",
      "subkind": "",
      "title": "028 Failing Over without Falling Over   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R3_ccsuPoD8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_48a8bc62",
      "kind": "youtube",
      "subkind": "",
      "title": "Platform Engineering Done Right",
      "source": "youtube",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/youtube/Platform_Engineering_Done_Right.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c272be4e",
      "kind": "youtube",
      "subkind": "",
      "title": "005 From Code to Climate with Adrian Cockcroft  Clip ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5YF68KFGSzk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a5299866",
      "kind": "youtube",
      "subkind": "",
      "title": "018 Mik   One  Adrian Cockcroft Episode 15",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=nFiauu-qMV4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_65620fe2",
      "kind": "youtube",
      "subkind": "",
      "title": "036 Speeding Up Innovation   Adrian Cockcroft   Craft 2019",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=rnygCAvVBj8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_f9cf2dda",
      "kind": "youtube",
      "subkind": "",
      "title": "084 Welcome Keynote   Adrian Cockcroft   GOTO 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=SBp7AWelOhM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_373e2d35",
      "kind": "youtube",
      "subkind": "",
      "title": "078 microXchg 2016   Adrian Cockcroft   Analyzing Response Time Distributions for Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5DPr4x76nvQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_24288b27",
      "kind": "youtube",
      "subkind": "",
      "title": "033 Adrian Cockcroft   Speeding Up Innovation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4jYYrkmAnS0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1679c42f",
      "kind": "youtube",
      "subkind": "",
      "title": "032 Episode 216  Adrian Cockcroft on the Modern Cloud based Platform",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qQzPmzVfmYA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_1e6bfc50",
      "kind": "youtube",
      "subkind": "",
      "title": "054 Adrian Cockcroft   Migrating to Cloud  Netflix Cloud Journey ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=U88QkaDGx6k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c4c389c9",
      "kind": "youtube",
      "subkind": "",
      "title": "068 Paul Borrill on Time clocks and the reordering of events",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CWF3QnfihL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_57c52234",
      "kind": "youtube",
      "subkind": "",
      "title": "020 ACD22 1 01 Adrian Cockcroft AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Qu1MLtHg2so&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c03c94b2",
      "kind": "youtube",
      "subkind": "",
      "title": "094 Fast Delivery   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=hvFo3Q2PIQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_07d6530b",
      "kind": "youtube",
      "subkind": "",
      "title": "073 Purpose Driven Development   Adrian Cockcroft   SpringOne Platform",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=sll9RNHz3t0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_500e5ef2",
      "kind": "youtube",
      "subkind": "",
      "title": "007  HPCpodcast 77  Adrian Cockcroft on Future Architectures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=2jrCvon9oTE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0cac4e48",
      "kind": "youtube",
      "subkind": "",
      "title": "096 Fast Delivery   Adrian Cockcroft   adrianco ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=icvtIK83I_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8d6db02e",
      "kind": "youtube",
      "subkind": "",
      "title": "090 Reflections on Monitorama 2015 with Adrian Cockcroft and James Turnbull",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LuA1AjorCQs&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_95ce6b03",
      "kind": "youtube",
      "subkind": "",
      "title": "011 Microservices Retrospective   What We Learned  and Didn t Learn  from Netflix",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=TOM6UhCetQ0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_4377a633",
      "kind": "youtube",
      "subkind": "",
      "title": "069 Simplifying the Future   Adrian Cockcroft  Battery Ventures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DGK6jjamzfY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_2482bda5",
      "kind": "youtube",
      "subkind": "",
      "title": "044 Top Technical Talent Programs   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=CSL-vNQW3SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_777deef1",
      "kind": "youtube",
      "subkind": "",
      "title": "025 Day 1 Keynote by  Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=O2UzvvtHkL4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_1420d740",
      "kind": "youtube",
      "subkind": "",
      "title": "019 Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=6aC_nes9LIk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_35576314",
      "kind": "youtube",
      "subkind": "",
      "title": "052 Developing a Chaos Architecture Mindset   Adrian Cockcroft  AWS ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4VleTKY0QAM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3d0218df",
      "kind": "youtube",
      "subkind": "",
      "title": "098 Deep Dive into the Cloud Native Open Source with NetflixOSS   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=R2kKmMyqTfc&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_89895a4e",
      "kind": "youtube",
      "subkind": "",
      "title": "050 Adrian Cockcroft  AWS   KubeCon   CloudNativeCon EU 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=iqVtGmNgSJk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_302761eb",
      "kind": "youtube",
      "subkind": "",
      "title": "014 Chaos Carnival 2023 Day 2 Keynote 1   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=noKPM4UlJDk&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_ea2d012f",
      "kind": "youtube",
      "subkind": "",
      "title": "041 Adrian Cockroft   Chaos Engineering   What is it  and where it s going    Chaos Conf 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=cefJd2v037U&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_377868ec",
      "kind": "youtube",
      "subkind": "",
      "title": "030 Managing Failure Modes in Microservice Architectures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=v5Gwi9AYvm4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9d0e9948",
      "kind": "youtube",
      "subkind": "",
      "title": "038 Safety Margins and Availability with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NRCQngvRNng&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_4449e385",
      "kind": "youtube",
      "subkind": "",
      "title": "079 Adrian Cockroft   DevOps Enterprise Summit 2015   theCUBE    DOES15",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pfEKVDWUknU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_cd125e22",
      "kind": "youtube",
      "subkind": "",
      "title": "056 Keynote  Cloud Native at AWS   Adrian Cockcroft  Amazon Web Services",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5U-6sxR5DaQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_b23f3d66",
      "kind": "youtube",
      "subkind": "",
      "title": "046 Dynamic Non Events   Adrian Cockcroft   GOTO 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=mFQRn_m2mP4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_259d42a3",
      "kind": "youtube",
      "subkind": "",
      "title": "067 GopherCon 2016  Communicating Sequential Goroutines   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=gO1qF19y6KQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_20107970",
      "kind": "youtube",
      "subkind": "",
      "title": "057 Cloud Trends   Adrian Cockcroft   GOTO 2017",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=EDZBYbEwhm8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6466ddfb",
      "kind": "youtube",
      "subkind": "",
      "title": "097 Fast Delivery  Adrian Cockcroft  nginxconf 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=5qJ_BibbMLw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_3b583ca2",
      "kind": "youtube",
      "subkind": "",
      "title": "099 AWS re Invent 2014    ARC201  Cloud Native Cost Optimization",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LZwlkqERv2g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "cloud",
        "cloud native"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_f2d154d4",
      "kind": "youtube",
      "subkind": "",
      "title": "075 OSCON Roundtable  The Future of IT Infrastructure",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=9q08veg5WkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "infrastructure"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_befeedd7",
      "kind": "youtube",
      "subkind": "",
      "title": "053 Adrian Cockcroft on The New De Normal   Untangling  Kitchen Sink  Database Schemas",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Y6nKD-sK6tg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_30fc2a90",
      "kind": "youtube",
      "subkind": "",
      "title": "059 Live from the NY Summit   Interview with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7CJrAhpZlXE&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_a926f53d",
      "kind": "youtube",
      "subkind": "",
      "title": "002 It s Complicated    Adrian Cockcroft   YOW  2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=NJ-3eNx8iBo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_68114147",
      "kind": "youtube",
      "subkind": "",
      "title": "066 The Evolution of Microservices",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=b8TDodu5E0k&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6ef2b329",
      "kind": "youtube",
      "subkind": "",
      "title": "085 OpenStack Silicon Valley 2015   Web Services and Microservices  The Effect on Vendor Lock In",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ewuw1s4cnJA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3aafdc07",
      "kind": "youtube",
      "subkind": "",
      "title": "022 DevOpsDays Silicon Valley 2018   Dynamic Non Events by Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=C9VchTAd7AM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_2c496658",
      "kind": "youtube",
      "subkind": "",
      "title": "058 AWS Summit Tel Aviv 2017  Keynote with Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Cg79AaEAPwA&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c057b69d",
      "kind": "youtube",
      "subkind": "",
      "title": "016 Fireside Chat  Adrian Cockcroft Talks with the Authors of The Value Flywheel Effect",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=713_6MBW7q0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_3d7bc203",
      "kind": "youtube",
      "subkind": "",
      "title": "087 OpenStack Now Podcast  Episode 5  Adrian Cockcroft  Battery Ventures",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=jOxFFmoRkzw&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_29ad458b",
      "kind": "youtube",
      "subkind": "",
      "title": "071  The Evolution of Microservices   Adrian Cockroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Mg4Cs2K7f98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7d59869a",
      "kind": "youtube",
      "subkind": "",
      "title": "031 AWS re Invent 2019  Innovation at speed  ARC203 ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=8ona5ZTu4_E&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_0aa723a6",
      "kind": "youtube",
      "subkind": "",
      "title": "095 Migrating to Cloud Native with Microservices   Adrian Cockcroft   GOTO 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=DvLvHnHNT2w&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "cloud native",
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_56b5f9de",
      "kind": "youtube",
      "subkind": "",
      "title": "024 Adrian Cockcroft Presents  The Startup Lifecycle",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=uRcSFXafIWM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_d6c56b5f",
      "kind": "youtube",
      "subkind": "",
      "title": "091 Adrian Cockcroft on Microservices and the Importance of Next Gen Apps for Businesses",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4ClmJxVz1SM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_5f4fa47d",
      "kind": "youtube",
      "subkind": "",
      "title": "064 microXchg 2017   Adrian Cockcroft  Shrinking Microservices to Functions",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=ZgxZCXouBkY&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_70e09c9a",
      "kind": "youtube",
      "subkind": "",
      "title": "004  022   Kubernetes for Humans with Adrian Cockcroft  Nubank ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=xSy-usyvWC4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_195319bf",
      "kind": "youtube",
      "subkind": "",
      "title": "070 Monitoring Microservices   A Challenge   Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Q3-XKQbMkXg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_3dd8223b",
      "kind": "youtube",
      "subkind": "",
      "title": "006 Adrian Cockcroft  OrionX   Supercloud 5",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=phZabEbs2-4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_f34c92b2",
      "kind": "youtube",
      "subkind": "",
      "title": "089 Adrian Cockcroft   DockerCon 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=K4rcnaiyc_M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6dc90cfb",
      "kind": "youtube",
      "subkind": "",
      "title": "027 Map Camp 2020   Maps  Games and Morality",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=qtMHOuGw0lI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_712cfd77",
      "kind": "youtube",
      "subkind": "",
      "title": "092  Monitoring Microservices  A Challenge    Adrian Cockcroft Keynote",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=smEuX-Hq6RI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_2214dc0a",
      "kind": "youtube",
      "subkind": "",
      "title": "083 AWS re Invent 2015    SPOT304  How Adrian Cockcroft Helped Move Customers to AWS",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=LMYYJuh9t70&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_2dfd0089",
      "kind": "youtube",
      "subkind": "",
      "title": "081 Adrian Cockcroft Interview at DevOps Summit 2015",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=htIO8ydywa4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "devops"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_7a2d0f63",
      "kind": "youtube",
      "subkind": "",
      "title": "013 Adrian s Greatest Hits  B Sides   Re issues   Adrian Cockcroft   YOW  2022",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=kc9dyTF2PjI&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_0252811b",
      "kind": "youtube",
      "subkind": "",
      "title": "047 Adrian Cockcroft on Digital Transformation",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=7lDWXtNjVyQ&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_65efc3e6",
      "kind": "youtube",
      "subkind": "",
      "title": "021 The Lotus Elise was unlike any other car ever made   Revelations with Jason Cammisa   Ep  21",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=h0jXhOmL7Xg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_5c96d39f",
      "kind": "youtube",
      "subkind": "",
      "title": "055 Adrian Cockcroft on the Evolution of Business Logic from Monoliths  to Microservices  to Functions",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=aBcG57Gw9k0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_c82e0a5c",
      "kind": "youtube",
      "subkind": "",
      "title": "026 AWS re Invent 2020  Adrian Cockcroft s architecture trends and topics for 2021",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=4rLVJFHfK98&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_b4111520",
      "kind": "youtube",
      "subkind": "",
      "title": "076 Microservices  What s Missing ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=p848Dr3EtQg&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_f99866b7",
      "kind": "youtube",
      "subkind": "",
      "title": "037 Adrian Cockcroft on the importance of the culture at Amazon",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=eMXMJ2lV1_g&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6f4889c5",
      "kind": "youtube",
      "subkind": "",
      "title": "039 Adrian Cockcroft  AWS   AWS re Invent 2018",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=lhHmXNDvd3M&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_91c8b7bd",
      "kind": "youtube",
      "subkind": "",
      "title": "065 It s Simple      Adrian Cockcroft   GOTO 2016",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=JSnOk-4m7C4&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9ef244b9",
      "kind": "youtube",
      "subkind": "",
      "title": "074 Mendix World 2016 Adrian Cockcroft Keynote  It s Simple",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Bn2WLIpPxX8&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_6d2fc92d",
      "kind": "youtube",
      "subkind": "",
      "title": "072 It s Simple    Adrian Cockcroft   GOTO 2016",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=a8Re9Cvv6nU&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_48d9d7e0",
      "kind": "youtube",
      "subkind": "",
      "title": "093 The State of the Art in Microservices by Adrian Cockcroft",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=pwpxq9-uw_0&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_youtube_6b905594",
      "kind": "youtube",
      "subkind": "",
      "title": "034 Open Source Force Multipliers   Adrian Cockcroft  Amazon Web Services ",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=E1aAG2iftMo&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_9294eecb",
      "kind": "youtube",
      "subkind": "",
      "title": "100 Keynote Speakers Adrian Cockcroft and Nicholas Heller Interview at TM Forum Live 2014",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=dWPdsd8m8KM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_youtube_8aeb3627",
      "kind": "youtube",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "youtube",
      "published_date": "",
      "url": "https://www.youtube.com/watch?v=Ev3rIMLujTM&list=PL_KXMLr8jNTkLhrFZBPjuVp8KXcCXbIo5",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_summaries_231201a4",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/ad47cc33639a3f9bef442de1de1351cb_summary.txt",
      "content": {
        "text": "Amazon Web Services (AWS) sponsored this post, in anticipation of Failover Conf on April 21. Some CIOs admit that they have never tested failover, or that it\u2019s too much work and too disruptive to plan and implement the test. The combination of cloud computing and chaos engineering is leading to \u201ccontinuous resilience.\u201d During Hurricane Sandy, the storm surge flooded basements in Jersey City and Manhattan. It turns out that computers don\u2019t work underwater. It's hard to get a disaster recovery data center implemented. Each installation is a very complex, fully customized \u201csnowflake.\u201d Even such basic things as backups for data need to be tested regularly. The last big project I worked on when I was Cloud Architect at Netflix in 2013, was their active-active multiregion architecture. At that time we tested region failover about once a quarter. In subsequent years, Netflix found that they needed to test more often to catch problems earlier. Netflix operates from AWS regions in Virginia, Oregon and Dublin. The only way to get to this way of operating is to set up the failover testing first, then test that every application deployed into the environment can cope. Netflix is also a leader in Chaos Engineering. It has a team that runs experiments to see what happens when things go wrong. Nowadays more companies are setting up chaos engineering teams. AWS Cloudwatch extended to support cross-account and multiregion dashboards. Partner and Professional Services teams are working with customers as they migrate their business continuity plans to AWS.",
        "metadata": {
          "word_count": 251,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "aws",
        "cloud",
        "engineering",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_e6264bc8",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/4c0dc893e5f3b100dac6d23008722f7a_summary.txt",
      "content": {
        "text": " NVIDIA is announcing new hardware that effectively obsoletes its previous products. Customers are buying whatever they can get, as it appears. \"The new stack\" of AI software is maturing rapidly. Last year I predicted that enterprise customers would find it extremely difficult to manage the pace of change. This year, I think the hyperscaler customers and cloud providers have indigestion as well. Development is also being accelerated by a variety of AI-backed tools. There is a big disconnect between the GPUs that most people are using now and those that are being announced this week. I\u2019ve summarized them here, with links to in-depth stories for people interested in more detail. GH200 was the first attempt at a new shared memory system architecture where the GPU is driving the NVlink cluster interconnect, and the CPUs are hanging on around the edge. Most H200 GPUs are actually being delivered in the same kind of 8-way with two Intel CPU packages as H100s. The largest early systems are in Supercomputing including the Swiss National Computer Center \u2018Alps\u2019 system, Los Alamos National Labs \u2018Venado\u2019 and the J\u00fclich Supercom computing Centre\u2019s JUPITER system with close to 24,000 GH200 Superchips. These systems and more will be coming online throughout 2024. Blackwell is made from two of the largest possible chips, connected directly together. Two modules fit on a 1 rack-unit-high, water-cooled motherboard. 18 compute boards fit in a 72 GPU NVL72 rack. Blackwell Ultra \u2013 B300 has 50% higher performance than B200. New process from chip supplier TSMC allows for even more power. The Blackwell Ultra provides a rack-compatible upgrade to the NVL72. Amazon Web Services is currently only shipping Blackwell privately to its biggest customers. Blackwell GPUs are available from cloud providers including Azure in late 2024, Google Cloud in January 2025, and Coreweave. The Vera Rubin NVL144 has 3.6 exaflops of FP4 inference performance. It\u2019s targeted to ship in the second half of 2026. The next-generation Rubin Ultra racks are up to 600kW. The next-generation architecture will be named Feynman in 2028. The company is launching a range of desktop and deskside Blackwell-based workstations for the enterprise market. Last year, NVIDIA claimed Blackwell was 4x the performance per GPU for FP8 compared to H100. For 2025, the B300 has a 50% increase in raw floating point performance and increased memory capacity to 288GB.  NVIDIA will release a new open source distributed inference serving library called Dynamo. This helps push the sweet spot further out to have lower interactive latency for larger batches. They are also releasing FP4 optimized open source models.  NVlink is a copper-cabling system that operates inside a rack. It can reach the next rack over, but won\u2019t reach far enough to build a fully interconnected SuperPod. The NVL72-based system with more than 20,000 GPUs will be deployed by October 2024. GPUs in the shared memory cluster more than doubles from 256 to 576. The latest attempt to build a SuperPod appears to be the Rubin NVL576, where the attitude after two previous attempts seems to be, \u201cscrew it, put everything in the same rack\u201d This is truly an HPC supercomputer approach, similar to the HP Cray Blackwell rack with 288 GPUs. The new stack for AI development has some kind of Blackwell GPU at the base, most likely obtained from one of the GPU cloud vendors like Coreweave rather than AWS or GCP. There\u2019s a CUDA variant for every industry to drive the GPU. Jensen started with his favorite slide and I\u2019ll end with it. This is far more than fast hardware: NVIDIA is doing a much better job than its competitors.",
        "metadata": {
          "word_count": 601,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "aws",
        "azure",
        "cloud",
        "gcp",
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_8a745726",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/3af4c8ed93f7db4c79fdb2b08166055e_summary.txt",
      "content": {
        "text": "Amazon released its sustainability report covering the whole of 2023 on July 9. Google released its report on July 2 and Microsoft released its in May. Customers of cloud providers need detailed information to understand and optimize their own workloads. Amazon reduced total carbon 3% from 70.74 MmTCO2e (millions of metric tons of carbon dioxide equivalent) to 68.82 Mm TCO 2e. Normalized against the growth of the business, they were down 13% from 93.0 gCO2/$ to 80.8 g CO2/$. This continues a trend that started last year, of a small reduction in total carbon despite growing the business. The physical delivery and manufacturing business of Amazon dominates its carbon footprint to a much greater extent than its more software-focused competitors. Amazon has also been more aggressive in buying renewable energy than Google and Microsoft. The most progress so far has been made in renewable energy. Carbon-free energy matching was up from 90% to 100% for all of Amazon. Amazon generation portfolio increased from 20GW to 28GW. Forty-two new utility-scale projects came online for Amazon in 2023. The company is investing in nuclear energy alongside solar, wind and battery projects. Amazon has finally caught up with Google, which has been at 100% carbon-free energy match for the last seven years. Amazon committed to reach 100% by 2030, with a goal of 2025. Amazon hides its location-based energy emissions in a footnote of the Scope 1 and 2 Audit report. Most of the difference comes from Amazon\u2019s generation projects, which aren\u2019t counted by theLocation-based method. It takes two to three years to bring a wind or solar farm online. Amazon buys RECs on the open market that match the future generation capacity. This is a good policy, as unbundled REC purchases have a bad reputation. Amazon provides a list of 22 AWS regions where 100% renewable energy is procured in market. Cross-border RECs are used to reach 100% match for Australia, Bahrain, Brazil, Hong Kong, Japan and U.S. Amazon has far more generating capacity in Asia than either Google or Microsoft. Amazon positions cross-border RECs as a last resort, which is the best we can expect in the circumstances. Sustainability scientists in the Amazon central sustainability team. Low-carbon concrete and steel for construction is a good investment and is growing fast. Amazon is correctly positioning investment in nature-based solutions. Sustainability is about a lot more than carbon and climate change. There are global shortages of clean water in particular, and concern about the amount of water used to cool data centers. Cloud providers have targets to use less, and to return more clean water to the locations where they operate. In 2023, water use in the U.S. is expected to rise from 0.3% to 0.4%. The U.N. has set a goal of 0.5% water use by the end of the year. AWS is still not providing power usage effectiveness (PUE) information at all. Google published updated PUE data per region in its sustainability report a few weeks ago. PUE is much higher in hot and humid climates. Microsoft Azure publishes PUE data for all its regions. It\u2019s just not possible to optimize global workload placement without regional PUE and WUE data. There is still no mention of Scope 3 supply chain reporting for AWS customers. Scope 3 refers to emissions from assets not owned or controlled by an organization. It has been many years since Azure and GCP started to report Scope 3. AWS carbon tracking tools from GCP and Azure give you all the details you need, but with AWS you can\u2019t see which region or what service. Escalating to AWS support has produced nothing. AWS Sustainability Reporting Framework Summary. Water Positive Methodology. Accenture report on AWS carbon reduction. Amazon Exchange Carbon Measurement and Reporting guidelines. Amazon Energy Audit.",
        "metadata": {
          "word_count": 628,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "azure",
        "cloud",
        "gcp",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_aed6a0bb",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/7a6f0946e5c033cac2578c197f73c236_summary.txt",
      "content": {
        "text": "Jensen Huang, CEO of Nvidia, gave his annual keynote at the company\u2019s GTC conference. Nvidia is focused on driving forward the state of the art for the hyperscaler users of AI hardware. I think this is the most significant computing technology announcement we are likely to see this year. NVlink replaces the network for up to 256 GH200s combined into a single system connected by 900 Gbyte/s interfaces to NVswitch chips. That allows up to 120 Terabytes of data to be stored. GH200 systems have now started shipping and will deploy in volume this year. At GTC there is a display of 11 partners displaying them, and benchmarks are beginning to be published. The most significant thing about this system architecture is that the GPU is driving the NVlink interconnect. Nvidia announced the next generation Blackwell architecture GPU and the GB200 system. The Blackwell name is in honor of David Blackwell , an African American mathematician and game theory pioneer. Blackwell also includes high-speed encryption and decompression engines so it can operate directly on encrypted and compressed data without involving the CPU. One Grace CPU is combined with two Blackwell GPU packages to make a GB200 node (GH200 has one CPU and one GPU) There\u2019s a new fifth-generation NVswitch design that supports the 1.8 Terabytes/s interfaces from each Blackwell GPU. The NVswitch chips have a much higher performance SHARP v4 processing capacity of 3.6 Teraflops of FP8. The GB200 is delivered in a water-cooled 120kW rack-sized system package called the GB200 NVL72. This contains 18 boards each with two GH200 nodes, for a total of 72 Blackwell GPUs, and nine switch boards with two NVswitch chips which have 14.4 Terabytes/s of bandwidth per board. The Spectrum-X800 platform delivers optimized networking performance for AI cloud and enterprise infrastructure. It includes the Nvidia Quantum Q3400 switch and the Nvidia ConnectXR-8 SuperNIC. The main challenge appears to be building software that can operate these systems reliably. The 72 GB200 system package is likely a good compromise between the failure rate, the ability to physically package, ship and cool a unit. It will be interesting to see if the largest configurations use the NVL72 as their building block or configure for the 576 GB200 SuperPOD domain. The GB200 is so much faster than the GH200, how many customers will want to wait and switch their orders to the GB200? For training workloads that use FP8 the per-GPU gain is offset by using twice the silicon area (and cost) so it is less of an issue. New Capabilities for Building AI Apps and More. Huang announced some new capabilities that make it easier for customers to deploy AI-based applications. Nvidia has expanded its range of API-based services. The new CXL architecture is based on extensions of the PCI bus. It's not going to be competitive for GPU-based AI workloads. But it's a step in the right direction. It\u2019s impressive to see Nvidia executing well, the people I know who work there are happy. I think it will take a while for the software to become optimized for these systems.",
        "metadata": {
          "word_count": 514,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "architecture",
        "cloud",
        "infrastructure",
        "performance",
        "platform"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_c7179e5f",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/e32aacaccbfe9ec1285bf2c574ea38fd_summary.txt",
      "content": {
        "text": "Developers are responsible for the efficiency and safety of products in production. Lean development techniques that take waste out of the process are augmented by cloud-native applications that autoscale capacity on a just-in-time basis. This is the first ever GOTO London conference. The core story is being structured as a single track. On Friday, Sept. 18, the conference is organized as multiple tracks, combining end-user experiences, presentations about open source. There is a repeated pattern for the single-track days. In each half day, there will be four half-hour talks. The talks will be a bit shorter, more focused, and will build on each other. The first evening is the conference party, with an entertaining and informative presentation by Ines Sombra and Adrian Colyer. Over the last year, there has been a rush of interest in academic research papers resulting in Papers-We-Love meetups all over the world. The second day starts with four presentations on Lean. Value chain\u00a0mapping provides the tools for making better decisions about which parts of your\u00a0architecture need to focus on being faster, cheaper or safer. We will take a close look at the Docker ecosystem with Alexis\u00a0Richardson of Weave. We wrap up the second day with a keynote by Rod Johnson on the world of Silicon Valley startups. The final day of the conference puts the concepts into practice, with deep dives into open source tools and the latest products.",
        "metadata": {
          "word_count": 236,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "docker",
        "lean"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_c1a58dbe",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/120fb5d4b7c18c7e364683d88278cf21_summary.txt",
      "content": {
        "text": "Power Usage Effectiveness (PUE) is a measure of how efficiently a data center uses energy. In addition to the energy that the computers themselves use, there is energy needed to cool the data center. PUE varies between about 1.04 and 2.0 in practice. PUE for data centers in the tropics will tend to be at the high end of the range. PUE for centers in cold and dry parts of the world are at the low end. Lower-cost and older data centers typical of the kind enterprises own tend to have a high WUE as well. The new PUE information from Amazon is described on their sustainability page. They also talk in some detail about new data center technology that will result in a PUE of 1.08 for what they are currently building. Some additional PUE estimates may be available privately under a non-disclosure agreement. Microsoft published a lot of detailed information for 2022 as data center fact sheets including PUE estimates for all of their regions. The 2022 data has since been removed from their website. We\u2019ve archived it as part of the GSF cloud region metadata table. Google has been disclosing PUE data on a quarterly basis for years and includes it in their 2024 annual sustainability report with data for five years, 2019\u20132023. The AWS PUE discussion mentions that as new empty data center buildings are added to a region, it can cause PUE to be worse for a while. GCP has the best transparency, with more data over a longer time period. Microsoft went from providing the most data for 2022 to the least data for 2023. AWS sits in-between, with two years of data, and PUE values that are worse than GCP.  AWS PUE is 1.15 in Virginia, Azure 1.14 in Ireland and GCP 1.08 in Ohio. PUE needs to be multiplied by the PUE of that datacenter or cloud region before accounting for its demand on the grid or its carbon footprint. New data centers being purpose-built for GPU deployments are, in many cases, limited by their available power sources. I\u2019d assume a PUE of 1.08, regardless of who is building it. . at are running today can have some data to use that we think is the best guess available. at the time of this article. We are happy to provide the best data we can.",
        "metadata": {
          "word_count": 393,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "aws",
        "azure",
        "cloud",
        "gcp",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_summaries_08d362a7",
      "kind": "summaries",
      "subkind": "",
      "title": "077 Battery Ventures  Simulating and Visualizing Large Scale Cassandra Deployments",
      "source": "summaries",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/summaries/1e8acda77d69c5f5c173fd37a864980c_summary.txt",
      "content": {
        "text": "Docker is a great example of how to build a viral, developer-oriented product. A developer can figure out what Docker does, install it and do something useful with it in 15 minutes. The rapid adoption rate took everyone by surprise, and now it\u2019s too late to build a competitor. This is great for developers and end users, but means that several platform-as-a-service (PaaS) vendors have lost control of their destiny. Docker is a portable container that packages any Linux application or service. A package that is created and tested on a developer\u2019s laptop using any language or framework can run unmodified on any public cloud, any private cloud or a bare-metal server. Docker removes much of the need to use tools like CFEngine, Puppet, Chef, Ansible or SaltStack. Docker containers are shared in a public registry at hub. docker.com. This is organized similarly to Github, and already contains tens of thousands of containers. Docker Hub will end up as a far bigger source of off-the-shelf software components and monetization opportunities. There\u2019s a lot of interest in the technology from the VC community.",
        "metadata": {
          "word_count": 182,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "containers",
        "docker"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_c3ac9176",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_76e7a5e0",
      "kind": "podcast",
      "subkind": "",
      "title": "Supercomputing 24 CryptoSuper500 Quantum RISC V OXD25.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Supercomputing_24_CryptoSuper500_Quantum_RISC_V_OXD25.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_3a5123e2",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/940fbd39-3936-441f-8087-4ce99b7be568/audio/050961ea-530c-49a0-ae27-046982856d34/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_363e396b",
      "kind": "podcast",
      "subkind": "",
      "title": "022 Kubernetes for Humans with Adrian Cockcroft Nubank Komodor info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/022_Kubernetes_for_Humans_with_Adrian_Cockcroft_Nubank_Komodor_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "kubernetes"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_869d1844",
      "kind": "podcast",
      "subkind": "",
      "title": "From Netflix to the Cloud Adrian Cockroft on DevOps Microservices and Sustainability.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628032&ei=YBDeZ-z8KIKRsfIPj6-B4Qo&ip=71.198.156.212&id=o-ALRdyd3qfKsy1G9HxamFg5Jlp29gbc4zvxFPOtQwDmJy&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606432%2C&mh=8F&mm=31%2C26&mn=sn-n4v7sns7%2Csn-a5msenek&ms=au%2Conr&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcMw4zqjQxITUESI3F4OdfRvFZOxrqDk-TUOZn4x7eCCs205IRITjiaZ0dzbr0GdxJw9s8JX8zL8&spc=_S3wKgfT7Xqfy-T0aMoIB3PSIXueOFKlyiIvrfsx4jJ7TPkKtHTx8VviE8JJhRrTMnY7&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRgIhAMckZWnpsB3fBlBTaiYaBnYTtPLf7dxFgvURPohpW8q6AiEA2NBOIgXi3JB21VU2yn8fZq8omdk17NCABrNukbiJMNo%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRgIhANvmiGQSkP7c4tahT-l3TG0XWq4jeEcTCua8y5FqJnCqAiEAzd4tM3x1zU7kiX_g6k48CXhwlGAIQfqZEgKqK60yRQY%3D",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "cloud",
        "devops",
        "microservices",
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_06bf9f18",
      "kind": "podcast",
      "subkind": "",
      "title": "youtube FY3asCV9qOE.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://rr3---sn-n4v7sns7.googlevideo.com/videoplayback?expire=1742628024&ei=WBDeZ7nKBbOWsfIPhpCtgA4&ip=71.198.156.212&id=o-AJVQQivjY88dhhg9gljXdceHbQmr6Lyc6-02WD9zBIiZ&itag=251&source=youtube&requiressl=yes&xpc=EgVo2aDSNQ%3D%3D&met=1742606424%2C&mh=8F&mm=31%2C29&mn=sn-n4v7sns7%2Csn-o097znsd&ms=au%2Crdu&mv=m&mvi=3&pl=22&rms=au%2Cau&initcwndbps=4841250&bui=AccgBcNS3vFqoapn2rXkMIxcI1MX3Z2bIKmie9TZ-dqQIiYCvHUuJdg73wmlfWCcdJ8yTntmdBmuruuL&spc=_S3wKi4z4DPgT8nu312evHyNCFxrXsReJNelRbqUtemQiLgCxg34NboTSt1hY9o94IFI&vprv=1&svpuc=1&mime=audio%2Fwebm&rqh=1&gir=yes&clen=54880774&dur=3681.281&lmt=1724191440499505&mt=1742605973&fvip=3&keepalive=yes&c=ANDROID&txp=6208224&sparams=expire%2Cei%2Cip%2Cid%2Citag%2Csource%2Crequiressl%2Cxpc%2Cbui%2Cspc%2Cvprv%2Csvpuc%2Cmime%2Crqh%2Cgir%2Cclen%2Cdur%2Clmt&sig=AJfQdSswRQIhANKPHEhIXUjzklRAhlQU1ZhVlRkwrVgx3EH94x7mBDQFAiBJKmW4DbD5xYvMlvB-rKLq6fr3QTxp9wjfHFmTCFWWnA%3D%3D&lsparams=met%2Cmh%2Cmm%2Cmn%2Cms%2Cmv%2Cmvi%2Cpl%2Crms%2Cinitcwndbps&lsig=AFVRHeAwRAIgEDNtYM-MWxZhuooRi_jNP5Q51AN6oKRsEnxFblq3qJYCIBhhL1wzEun4pSgzvsfjcyq_PzgIST3xi5naspt8wASR",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4457c57a",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_AWS_Sun_eBay_Netflix_and_Others_Vet_Adrian_Cockcroft_Talks_Cloud_HPC_AI_and_the_Amazon_Sustainability_Data_Initiative_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f3e33df0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast AWS Sun eBay Netflix and Others Vet Adrian Cockcroft Talks Cloud HPC AI and the Amazon Sustainability Data Initiative High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "http://orionx.net/wp-content/uploads/2022/09/036@HPCpodcas_Adrian-Cockcroft_Cloud_ESG_20220914.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai",
        "aws",
        "cloud",
        "hpc",
        "netflix",
        "performance",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9b33411",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ1MzB9fX1dfQ__&Signature=M7LCJcOysGLmo0khzuJvnby05s8k6RXaFozS0DQzzfhv41VUmroFPw5UmZJRY6CXvE4lwVXQ8kK4hnRzIqmYJijoP7f8Shxb46VOfwtKvJPjLO~Nv9zWOHkfJH0x2Z0SZvD-cMzI1zQJHYEbAMmAp3sSmy04vfQ9TMpzmHNvtHFWb~xa3hwXIlEWJQ538OSY2ahtPOnFZgrsxCQTaXC0hVihgRWlWMLSsQ0MF-4JvXc4kXl75j6GAPC7LAKWxc6mBTPrst8MDV6dgPHcUFURE5aIkQceyUjGSM5lun1T0tjyp8vcwiEe~yjt64gh6gXpIGAPJDHJmCX7aDMPgfS0Kg__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_af2c8e31",
      "kind": "podcast",
      "subkind": "",
      "title": "Podcast Adrian Cockcroft on Serverless Continuous Resilience Wardley Mapping Large Memory Systems and Sustainability info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Podcast_Adrian_Cockcroft_on_Serverless_Continuous_Resilience_Wardley_Mapping_Large_Memory_Systems_and_Sustainability_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "podcast",
        "resilience",
        "serverless",
        "sustainability"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_aec0d230",
      "kind": "podcast",
      "subkind": "",
      "title": "The New Monitoring for Services That Feed from LLMs.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_New_Monitoring_for_Services_That_Feed_from_LLMs.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "monitoring"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_a2854b1c",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_10b527e1",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Nvidia_s_Superchips_for_AI_Radical_but_a_Work_in_Progress.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_4d217b30",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/05/057@HPCpodcast_Adrian-Cockcroft_Carbon-ESG_20230503.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_4ca2fe2b",
      "kind": "podcast",
      "subkind": "",
      "title": "Nvidia s Superchips for AI Radical but a Work in Progress The New Stack.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/5672b58f-7201-4e0e-b0af-da702259d97f/episodes/c27ff629-b50f-4c9d-a4b8-154adb18e4a5/audio/21249ddc-8d38-4138-8e5d-612df9cfb2b5/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f9e2e9e5",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://traffic.megaphone.fm/SED1616041688.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1d8d897c",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Architecture for the Cloud InfoQ.info",
      "source": "podcast",
      "published_date": "",
      "url": "http://ress.infoq.com/downloads/mp3downloads/interviews/infoq-12-nov-adrian-cockcroft.mp3?Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6IioiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDkxNzN9LCJJcEFkZHJlc3MiOnsiQVdTOlNvdXJjZUlwIjoiMC4wLjAuMC8wIn19fV19&Signature=Fj4ki7BoXgztHB73H9~FwnuDJgKS1LnMG2XTsJD6D8Gbk0-6sMON8rsDY1mdGExfsssrSZnvsbdfLTcmYEhd8w07G0RyJzxmFRa25asaM~MSJIOgdZjRZ3A7j0WfQ631a3YOC2blChyTN62yUQl1YFNxSFF8yQeHkMOqiugL8v4_&Key-Pair-Id=APKAIMZVI7QH4C5YKH6Q",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "architecture",
        "cloud"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_f131fbf4",
      "kind": "podcast",
      "subkind": "",
      "title": "Schedulers with Adrian Cockcroft Software Engineering Daily.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Schedulers_with_Adrian_Cockcroft_Software_Engineering_Daily.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "engineering"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7262cb55",
      "kind": "podcast",
      "subkind": "",
      "title": "Sam Newman Magpie Talkshow Episode 22 Adrian Cockcroft.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Sam_Newman_Magpie_Talkshow_Episode_22_Adrian_Cockcroft.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b9c74b59",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 15 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_15_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_764ae438",
      "kind": "podcast",
      "subkind": "",
      "title": "unknown info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/unknown_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_af469a08",
      "kind": "podcast",
      "subkind": "",
      "title": "Decarbonization ESG w Adrian Cockcroft HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Decarbonization_ESG_w_Adrian_Cockcroft_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_b2a0efa0",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/HPCpodcast_Tech_Analyst_Adrian_Cockcroft_on_Trends_Driving_Future_HPC_Architectures_High_Performance_Computing_News_Analysis_insideHPC.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_33a2686c",
      "kind": "podcast",
      "subkind": "",
      "title": "When Netflix Bet On AWS.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cdn.simplecast.com/audio/62cedf3d-2540-4354-85b4-8a5e293d1104/episodes/bb12eeb0-7db8-413c-b7b1-aa3a17a1ae64/audio/91c97a87-12d7-4be9-b628-85e82601b948/default_tc.mp3",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "netflix"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_132b3cb0",
      "kind": "podcast",
      "subkind": "",
      "title": "The Evolution of Microservices with Adrian Cockroft info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/The_Evolution_of_Microservices_with_Adrian_Cockroft_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_1811ccd6",
      "kind": "podcast",
      "subkind": "",
      "title": "HPCpodcast Tech Analyst Adrian Cockcroft on Trends Driving Future HPC Architectures High Performance Computing News Analysis insideHPC 1 .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://orionx.net/wp-content/uploads/2023/11/077@HPCpodcast_Adrian-Cockcrofy_Future-Architectures_20231130.mp3?_=1",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "hpc",
        "performance"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_e90e6bf6",
      "kind": "podcast",
      "subkind": "",
      "title": "Analyst Roundtable Chips AI Search Quantum Bitcoin Fusion OXD26.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Analyst_Roundtable_Chips_AI_Search_Quantum_Bitcoin_Fusion_OXD26.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "ai"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_c264cb7b",
      "kind": "podcast",
      "subkind": "",
      "title": "Episode 24 Adrian Cockcroft Flow Framework info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Episode_24_Adrian_Cockcroft_Flow_Framework_info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_ffd1f118",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Microservices Terraservices and Serverless Computing.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/PDgfxySYwdWO.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvUERnZnh5U1l3ZFdPLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDM4MTJ9fX1dfQ__&Signature=XDo1OiQ7kO0RHU3Y5WAHECtOp57K7B1AXwhxGedTt2jQibFn0Kv8dhm-Oa~YtSbBpiXaF~C2wu5qPqmOPcz~dxoxiprzRxlr-IZ1dsiT0KrCV6Z-b9uQFTsXlwXtGHmABI4oBCocspLkVf-Zi4znHEPFE7HmjnscSrw~rvmiuLY9eLHc4pR3hLQtFkg-ALmovjKh281H8Pa0RZSK3rQcNe-vKREBK6MZ7D9KyP3AhQ8RAvOEvRFhdoFDlOQ~9F1HuY3Ak06u7SNvHnk7qq1Ut~f61NAN0Fzz-4~S0844RCfzMDNQuM3z6zWQjYArarAEYMnRvOEKcFN2fjiPMOxgaw__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": [
        "microservices",
        "serverless"
      ]
    },
    {
      "id": "virtual_adrianco_podcast_7bd25a6f",
      "kind": "podcast",
      "subkind": "",
      "title": "Magpie Talkshow Episode 22 Adrian Cockcroft Spaghetti Monster Simulation Edition .info",
      "source": "podcast",
      "published_date": "",
      "url": "https://cf-hls-opus-media.sndcdn.com/playlist/DwsToh00RUjc.64.opus/playlist.m3u8?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiKjovL2NmLWhscy1vcHVzLW1lZGlhLnNuZGNkbi5jb20vcGxheWxpc3QvRHdzVG9oMDBSVWpjLjY0Lm9wdXMvcGxheWxpc3QubTN1OCoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE3NDI2MDQ3OTR9fX1dfQ__&Signature=Gvi1r7yNumawbEeJuvVaDfsMUiBQDoJHIR1e7r~QADr7yNVD1D03NomYjlAfDXQaKNpRcomVgYN~bAJ03fJuNjTUn39oWi33Vh9-8S6J0pAXhZWXC6FyamS7ea4ZJ5DKgrhsWQn5v~FVeGQ5K-K87~lOqNDsmFUHNA2DeShEiNZz3uWjmgFNj-SVhCfFhZDQreJac117RfAZNmNB4eIifgHa806fPBl2FelYrdo17yhaHWE2Yak6FEjBtJQEjDEn3cyFikDrtbh2~~5idW6P6VyBrmWtNMJpwm5RJNrSNKIJpleK5XTZ3xhGQF6GM93tMe~ahD37nnDVTLxrvWpkWA__&Key-Pair-Id=APKAI6TU7MMXM5DG6EPQ",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    },
    {
      "id": "virtual_adrianco_podcast_99cc1983",
      "kind": "podcast",
      "subkind": "",
      "title": "Adrian Cockcroft on Future Architectures HPCpodcast.info",
      "source": "podcast",
      "published_date": "",
      "url": "https://raw.githubusercontent.com/adrianco/meGPT/main/downloads/virtual_adrianco/podcast/Adrian_Cockcroft_on_Future_Architectures_HPCpodcast.info.json",
      "content": {
        "metadata": {
          "word_count": 0,
          "processing_status": "success",
          "processing_errors": []
        }
      },
      "tags": []
    }
  ]
}